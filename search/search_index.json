{"config": {"indexing": "full", "lang": ["en"], "min_search_length": 3, "prebuild_index": false, "separator": "[\\s\\-]+"}, "docs": [{"location": "", "text": "Home sweet home \u2691 this is my personal place to collect knowledge. Some knowledge might be useful for others. I keep stuff here to remember hopefully. I am the real Captain Solaris and this is, what i know. This book is forked from the Blue Book .", "title": "Introduction"}, {"location": "#home-sweet-home", "text": "this is my personal place to collect knowledge. Some knowledge might be useful for others. I keep stuff here to remember hopefully. I am the real Captain Solaris and this is, what i know. This book is forked from the Blue Book .", "title": "Home sweet home"}, {"location": "abstract_syntax_trees/", "text": "Abstract syntax trees (AST) is a tree representation of the abstract syntactic structure of text (often source code) written in a formal language. Each node of the tree denotes a construct occurring in the text. The syntax is \"abstract\" in the sense that it does not represent every detail appearing in the real syntax, but rather just the structural or content-related details. For instance, grouping parentheses are implicit in the tree structure, so these do not have to be represented as separate nodes. Likewise, a syntactic construct like an if-condition-then statement may be denoted by means of a single node with three branches. This distinguishes abstract syntax trees from concrete syntax trees, traditionally designated parse trees. Parse trees are typically built by a parser during the source code translation and compiling process. Once built, additional information is added to the AST by means of subsequent processing, e.g., contextual analysis. Abstract syntax trees are also used in program analysis and program transformation systems. How to construct an AST \u2691 TBD pyparsing looks to be a good candidate to implement ASTs.", "title": "Abstract Syntax Trees"}, {"location": "abstract_syntax_trees/#how-to-construct-an-ast", "text": "TBD pyparsing looks to be a good candidate to implement ASTs.", "title": "How to construct an AST"}, {"location": "activitywatch/", "text": "ActivityWatch is a bundle of software that tracks your computer activity. You are, by default, the sole owner of your data. ActivityWatch is: A set of watchers that record relevant information about what you do and what happens on your computer (such as if you are AFK or not, or which window is currently active). A way of storing data collected by the watchers. A dataformat accomodating most logging needs due to its flexibility. An ecosystem of tools to help users extend the software to fit their needs. Installation \u2691 Download the latest release Unpack it and move it for example to ~/.local/bin/activitywatch . Add the aw-qt executable to the autostart. It will start the web interface at http://localhost:5600 and will capture the data. Configuration \u2691 First go to the settings page of the Web UI, you can define there the rules for the categories. More advanced settings can be changed on the files, but I had no need to go there yet. The used directories are: Data: ~/.local/share/activitywatch . Config: ~/.config/activitywatch . Logs: ~/.cache/activitywatch/log . Cache: ~/.cache/activitywatch . Watchers \u2691 By default ActivityWatch comes with the next watchers: aw-watcher-afk : Watches for mouse & keyboard activity to detect if the user is active. aw-watcher-window : Watches the active window and its title. But you can add more, such as: aw-watcher-web : The official browser extension, supports Chrome and Firefox. Watches properties of the active tab like title, URL, and incognito state. It doesn't work if you Configure it to Never remember history , or if you use incognito mode It's known not to be very accurate . The overall time spent in the browser shown by the aw-watcher-window is greater than the one shown in aw-watcher-web-firefox . aw-watcher-vim : Watches the actively edited file and associated metadata like path, language, and project name (folder name of git root). It's impressive, plug and play: It still doesn't add the branch information , it could be useful to give hints of what task you're working on inside a project. They even show you how to create your own watcher . Syncing \u2691 There is currently no syncing support . You'll need to export the data (under Raw Data , Export all buckets as JSON ), and either tweak it so it can be imported, or analyze the data through other processes. Issues \u2691 Syncing support : See how to merge the data from the different devices. Firefox not logging data : Once it's solved, try it again. Making it work in incognito mode : Try it once it's solved. Add branch information in vim watcher : try it once it's out. Web tracking is not accurate : Test the solution once it's implemented. Physical Activity Monitor Integration (gadgetbridge) : Try it once there is a solution. References \u2691 Home Docs Git", "title": "ActivityWatch"}, {"location": "activitywatch/#installation", "text": "Download the latest release Unpack it and move it for example to ~/.local/bin/activitywatch . Add the aw-qt executable to the autostart. It will start the web interface at http://localhost:5600 and will capture the data.", "title": "Installation"}, {"location": "activitywatch/#configuration", "text": "First go to the settings page of the Web UI, you can define there the rules for the categories. More advanced settings can be changed on the files, but I had no need to go there yet. The used directories are: Data: ~/.local/share/activitywatch . Config: ~/.config/activitywatch . Logs: ~/.cache/activitywatch/log . Cache: ~/.cache/activitywatch .", "title": "Configuration"}, {"location": "activitywatch/#watchers", "text": "By default ActivityWatch comes with the next watchers: aw-watcher-afk : Watches for mouse & keyboard activity to detect if the user is active. aw-watcher-window : Watches the active window and its title. But you can add more, such as: aw-watcher-web : The official browser extension, supports Chrome and Firefox. Watches properties of the active tab like title, URL, and incognito state. It doesn't work if you Configure it to Never remember history , or if you use incognito mode It's known not to be very accurate . The overall time spent in the browser shown by the aw-watcher-window is greater than the one shown in aw-watcher-web-firefox . aw-watcher-vim : Watches the actively edited file and associated metadata like path, language, and project name (folder name of git root). It's impressive, plug and play: It still doesn't add the branch information , it could be useful to give hints of what task you're working on inside a project. They even show you how to create your own watcher .", "title": "Watchers"}, {"location": "activitywatch/#syncing", "text": "There is currently no syncing support . You'll need to export the data (under Raw Data , Export all buckets as JSON ), and either tweak it so it can be imported, or analyze the data through other processes.", "title": "Syncing"}, {"location": "activitywatch/#issues", "text": "Syncing support : See how to merge the data from the different devices. Firefox not logging data : Once it's solved, try it again. Making it work in incognito mode : Try it once it's solved. Add branch information in vim watcher : try it once it's out. Web tracking is not accurate : Test the solution once it's implemented. Physical Activity Monitor Integration (gadgetbridge) : Try it once there is a solution.", "title": "Issues"}, {"location": "activitywatch/#references", "text": "Home Docs Git", "title": "References"}, {"location": "adr/", "text": "ADR are short text documents that captures an important architectural decision made along with its context and consequences. The whole document should be one or two pages long. Written as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments. Pros: We have a clear log of the different decisions taken, which can help newcomers to understand past decisions. It can help in the discussion of such changes. Architecture decisions recorded in small modular readable documents. Cons: More time is required for each change, as we need to document and discuss it. How to use them \u2691 We will keep a collection of architecturally significant decisions, those that affect the structure, non-functional characteristics, dependencies, interfaces or construction techniques. There are different templates you can start with, being the most popular Michael Nygard's one . The documents are stored in the project repository under the doc/arch directory, with a name convention of NNN-title_with_underscores.md , where NNN is a monotonically increasing number. If a decision is reversed, we'll keep the old one around, but mark it as superseded, as it's still relevant to know that it was a decision, but is no longer. ADR template \u2691 Using Michael Nygard's template as a starting point, I'm going to use these sections: Title : A short noun phrase that describes the change. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\". Date : Creation date of the document. Status : The ADRs go through the following phases: Draft : We are using the ADR to build the idea, so everything can change. Proposed : We have a solid proposal on the Decision to solve the Context. Accepted : We have agreed on the proposal, from now on the document can't be changed! Rejected : We have agreed not to solve the Context. Deprecated : The Context no longer applies, so the solution is no longer needed. Superseded : We have found another ADR that better solves the Context. Context : This section describes the situation we're trying to solve, including technological, political, social, and project local aspects. The language in this section is neutral, simply describing facts. Proposals : Analysis of the different solutions for the situation defined in the Context. Decision : Clear summary of the selected proposal. It is stated in full sentences, with active voice. Consequences : Description of the resulting context, after applying the decision. All consequences should be listed here, not just the positive ones. I'm using the following Ultisnip vim snippet: snippet adr \"ADR\" Date: `date + %Y - % m - % d ` # Status <!-- What is the status? Draft , Proposed , Accepted , Rejected , Deprecated or Superseded? --> $ 1 # Context <!-- What is the issue that we' re seeing that is motivating this decision or change? --> $ 0 # Proposals <!-- What are the possible solutions to the problem described in the context --> # Decision <!-- What is the change that we' re proposing and/or doing? --> # Consequences <!-- What becomes easier or more difficult to do because of this change? --> endsnippet Usage in a project \u2691 When starting a project, I'll do it by the ADRs, that way you evaluate the problem, structure the idea and leave a record of your initial train of thought. I found useful to: Define the general problem at high level in 001-high_level_problem_analysis.md . Describe the problem you want to solve in the Context. Reflect the key points to solve the problem at the start of the Proposals section. Go one by one analyzing possible outcomes trying not to dive deep into details and having at least two proposals for each key point (hard!). Build an initial proposal in the Decision section by reviewing that all the Context points have been addressed and summarizing each of the Proposal key points' outcomes. Review the positive and negative Consequences for each actor involved with the solution, such as: The final user that is going to consume the outcome. The middle user that is going to host and maintain the solution. Ourselves as developers. Use the problem definition of 001 and draft the phases of the solution at 002 . Create another ADR for each of the phases, getting a level closer to the final implementation. Use 00X for the early drafts. Once you give it a number try not to change the file name, or you'll need to manually update the references you make. As the project starts to grow, the relationships between the ADRs will get more complex, it's useful to create an ADR landing page, where the user can follow the logic between them. MermaidJS can be used to create a nice diagram that shows this information. In the mkdocs-newsletter I've used the next structure: graph TD 001[001: High level analysis] 002[002: Initial MkDocs plugin design] 003[003: Selected changes to record] 004[004: Article newsletter structure] 005[005: Article newsletter creation] 001 -- Extended --> 002 002 -- Extended --> 003 002 -- Extended --> 004 002 -- Extended --> 005 003 -- Extended --> 004 004 -- Extended --> 005 click 001 \"https://lyz-code.github.io/mkdocs-newsletter/adr/001-initial_approach\" _blank click 002 \"https://lyz-code.github.io/mkdocs-newsletter/adr/002-initial_plugin_design\" _blank click 003 \"https://lyz-code.github.io/mkdocs-newsletter/adr/003-select_the_changes_to_record\" _blank click 004 \"https://lyz-code.github.io/mkdocs-newsletter/adr/004-article_newsletter_structure\" _blank click 005 \"https://lyz-code.github.io/mkdocs-newsletter/adr/005-create_the_newsletter_articles\" _blank 001:::accepted 002:::accepted 003:::accepted 004:::accepted 005:::accepted classDef draft fill:#CDBFEA; classDef proposed fill:#B1CCE8; classDef accepted fill:#B1E8BA; classDef rejected fill:#E8B1B1; classDef deprecated fill:#E8B1B1; classDef superseeded fill:#E8E5B1; Where we define: The nodes with their title. The relationship between the ADRs. The link to the ADR article so it can be clicked. The state of the ADR. Tools \u2691 Although adr-tools exist, I feel it's an overkill to create new documents and search on an existing codebase. We are now used to using other tools for the similar purpose, like Vim or grep. References \u2691 Joel Parker guide on ADRs Michael Nygard post on ARDs", "title": "Architecture Decision Record"}, {"location": "adr/#how-to-use-them", "text": "We will keep a collection of architecturally significant decisions, those that affect the structure, non-functional characteristics, dependencies, interfaces or construction techniques. There are different templates you can start with, being the most popular Michael Nygard's one . The documents are stored in the project repository under the doc/arch directory, with a name convention of NNN-title_with_underscores.md , where NNN is a monotonically increasing number. If a decision is reversed, we'll keep the old one around, but mark it as superseded, as it's still relevant to know that it was a decision, but is no longer.", "title": "How to use them"}, {"location": "adr/#adr-template", "text": "Using Michael Nygard's template as a starting point, I'm going to use these sections: Title : A short noun phrase that describes the change. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\". Date : Creation date of the document. Status : The ADRs go through the following phases: Draft : We are using the ADR to build the idea, so everything can change. Proposed : We have a solid proposal on the Decision to solve the Context. Accepted : We have agreed on the proposal, from now on the document can't be changed! Rejected : We have agreed not to solve the Context. Deprecated : The Context no longer applies, so the solution is no longer needed. Superseded : We have found another ADR that better solves the Context. Context : This section describes the situation we're trying to solve, including technological, political, social, and project local aspects. The language in this section is neutral, simply describing facts. Proposals : Analysis of the different solutions for the situation defined in the Context. Decision : Clear summary of the selected proposal. It is stated in full sentences, with active voice. Consequences : Description of the resulting context, after applying the decision. All consequences should be listed here, not just the positive ones. I'm using the following Ultisnip vim snippet: snippet adr \"ADR\" Date: `date + %Y - % m - % d ` # Status <!-- What is the status? Draft , Proposed , Accepted , Rejected , Deprecated or Superseded? --> $ 1 # Context <!-- What is the issue that we' re seeing that is motivating this decision or change? --> $ 0 # Proposals <!-- What are the possible solutions to the problem described in the context --> # Decision <!-- What is the change that we' re proposing and/or doing? --> # Consequences <!-- What becomes easier or more difficult to do because of this change? --> endsnippet", "title": "ADR template"}, {"location": "adr/#usage-in-a-project", "text": "When starting a project, I'll do it by the ADRs, that way you evaluate the problem, structure the idea and leave a record of your initial train of thought. I found useful to: Define the general problem at high level in 001-high_level_problem_analysis.md . Describe the problem you want to solve in the Context. Reflect the key points to solve the problem at the start of the Proposals section. Go one by one analyzing possible outcomes trying not to dive deep into details and having at least two proposals for each key point (hard!). Build an initial proposal in the Decision section by reviewing that all the Context points have been addressed and summarizing each of the Proposal key points' outcomes. Review the positive and negative Consequences for each actor involved with the solution, such as: The final user that is going to consume the outcome. The middle user that is going to host and maintain the solution. Ourselves as developers. Use the problem definition of 001 and draft the phases of the solution at 002 . Create another ADR for each of the phases, getting a level closer to the final implementation. Use 00X for the early drafts. Once you give it a number try not to change the file name, or you'll need to manually update the references you make. As the project starts to grow, the relationships between the ADRs will get more complex, it's useful to create an ADR landing page, where the user can follow the logic between them. MermaidJS can be used to create a nice diagram that shows this information. In the mkdocs-newsletter I've used the next structure: graph TD 001[001: High level analysis] 002[002: Initial MkDocs plugin design] 003[003: Selected changes to record] 004[004: Article newsletter structure] 005[005: Article newsletter creation] 001 -- Extended --> 002 002 -- Extended --> 003 002 -- Extended --> 004 002 -- Extended --> 005 003 -- Extended --> 004 004 -- Extended --> 005 click 001 \"https://lyz-code.github.io/mkdocs-newsletter/adr/001-initial_approach\" _blank click 002 \"https://lyz-code.github.io/mkdocs-newsletter/adr/002-initial_plugin_design\" _blank click 003 \"https://lyz-code.github.io/mkdocs-newsletter/adr/003-select_the_changes_to_record\" _blank click 004 \"https://lyz-code.github.io/mkdocs-newsletter/adr/004-article_newsletter_structure\" _blank click 005 \"https://lyz-code.github.io/mkdocs-newsletter/adr/005-create_the_newsletter_articles\" _blank 001:::accepted 002:::accepted 003:::accepted 004:::accepted 005:::accepted classDef draft fill:#CDBFEA; classDef proposed fill:#B1CCE8; classDef accepted fill:#B1E8BA; classDef rejected fill:#E8B1B1; classDef deprecated fill:#E8B1B1; classDef superseeded fill:#E8E5B1; Where we define: The nodes with their title. The relationship between the ADRs. The link to the ADR article so it can be clicked. The state of the ADR.", "title": "Usage in a project"}, {"location": "adr/#tools", "text": "Although adr-tools exist, I feel it's an overkill to create new documents and search on an existing codebase. We are now used to using other tools for the similar purpose, like Vim or grep.", "title": "Tools"}, {"location": "adr/#references", "text": "Joel Parker guide on ADRs Michael Nygard post on ARDs", "title": "References"}, {"location": "aerial_silk/", "text": "Aerial Silk is a type of performance in which one or more artists perform aerial acrobatics while hanging from a fabric. The fabric may be hung as two pieces, or a single piece, folded to make a loop, classified as hammock silks. Performers climb the suspended fabric without the use of safety lines and rely only on their training and skill to ensure safety. They use the fabric to wrap, suspend, drop, swing, and spiral their bodies into and out of various positions. Aerial silks may be used to fly through the air, striking poses and figures while flying. Some performers use dried or spray rosin on their hands and feet to increase the friction and grip on the fabric. Warming up \u2691 Arm twist \u2691 . Leave the silk at your left, take it with the left hand with your arm straight up and you thumb pointing away from you. . Start twisting in z > 0 your arm from your shoulder until the thumb points to your front. . Keep on twisting and follow the movement with the rest of your upper body until you're hanging from that arm and stretching. You shouldn't move your feet in the whole process. . Repeat with the other arm. Ball controlled inversions \u2691 . From standing position with each hand in a tissue, give it two or three loops to each hand and invert passing your legs between each tissue. . Bend your knees so that you become a ball and from that position. . While alive: . Keep on rotating 90 degrees more until your shins are parallel to the ground facing down. . Change the direction of the rotation and rotate 180 degrees until your shins are parallel to the ground but facing up. Inverted arm twist warmup \u2691 . From standing position with each hand in a tissue, give it two or three loops to each hand and invert passing your legs between each tissue. . Move your legs up until your body is parallel to the tissues and your shoulders are rotated back so your chest goes forward. . While alive: . Start lowering your feet using the tissues as a guide, while doing so start twisting your arms at shoulder level so that your chest goes to your back in a cat like position until your body limit. . Go back up doing the twist in the other direction till you're back up. Inverted knee to elbow \u2691 . From standing position with each hand in a tissue, give it two or three loops to each hand and invert passing your legs between each tissue. . Move your legs up until your body is parallel to the tissues and your shoulders are rotated back so your chest goes forward. . While alive: . Start lowering your feet using the tissues as a guide for your knees until they are at elbow level. Don't bend your knees! . Go back up. Horizontal pull-ups \u2691 . From standing position with each hand in a tissue, ask for a partner to take your legs until you're horizontal hanging from your hands and feet. . While alive: . Keeping your body straight, do a pull up with your arms. . Slowly unbend your elbows and stretch back your arms. The next level would be that you use your leg strength to grip your partner's hips instead of her holding your feet. Basic movements \u2691 Splitting the tissues \u2691 . Go to the desired height and get into a comfortable position, such as seated over your feet on a Russian climb. . Open the palm of one hand at eye level holding the whole tissue . Keep your bellybutton close to the tissue . With the other hand pinch the side of the silk and start walking with your fingers till you find the break between tissues. Figures \u2691 Waist lock \u2691 . Climb to the desired height . Leave the tissue to the side you want to do the lock to, for example the left (try the other side as well). . Leave your right hand above your left, with your arm straight. Your left hand should be at breast level with your elbow bent. . With straight legs with pointed toes, bring your left leg a little bit to the front, while the right one catches the silk . Bright the right leg up trying to get the silk as close to your waist as possible. . Rotate your body in z>0 towards your left hand until you're looking down . Release your hands. Ideas \u2691 Locking shoulder blades when gripping the silks \u2691 When you are going to hang yourself from your hands: . Unlock your shoulder blades moving your chest to the back. . Embrace the silk with your arms twisting your hands inwards . Grip the silk and lock back your shoulder blades together as if you were holding an apple between them. That movement will make your hands twist in the other direction until your wrists are between you and the tissue. Safety \u2691 When rolling up silk over your legs, always leave room for the knee, do loops above and below but never over.", "title": "Aerial Silk"}, {"location": "aerial_silk/#warming-up", "text": "", "title": "Warming up"}, {"location": "aerial_silk/#arm-twist", "text": ". Leave the silk at your left, take it with the left hand with your arm straight up and you thumb pointing away from you. . Start twisting in z > 0 your arm from your shoulder until the thumb points to your front. . Keep on twisting and follow the movement with the rest of your upper body until you're hanging from that arm and stretching. You shouldn't move your feet in the whole process. . Repeat with the other arm.", "title": "Arm twist"}, {"location": "aerial_silk/#ball-controlled-inversions", "text": ". From standing position with each hand in a tissue, give it two or three loops to each hand and invert passing your legs between each tissue. . Bend your knees so that you become a ball and from that position. . While alive: . Keep on rotating 90 degrees more until your shins are parallel to the ground facing down. . Change the direction of the rotation and rotate 180 degrees until your shins are parallel to the ground but facing up.", "title": "Ball controlled inversions"}, {"location": "aerial_silk/#inverted-arm-twist-warmup", "text": ". From standing position with each hand in a tissue, give it two or three loops to each hand and invert passing your legs between each tissue. . Move your legs up until your body is parallel to the tissues and your shoulders are rotated back so your chest goes forward. . While alive: . Start lowering your feet using the tissues as a guide, while doing so start twisting your arms at shoulder level so that your chest goes to your back in a cat like position until your body limit. . Go back up doing the twist in the other direction till you're back up.", "title": "Inverted arm twist warmup"}, {"location": "aerial_silk/#inverted-knee-to-elbow", "text": ". From standing position with each hand in a tissue, give it two or three loops to each hand and invert passing your legs between each tissue. . Move your legs up until your body is parallel to the tissues and your shoulders are rotated back so your chest goes forward. . While alive: . Start lowering your feet using the tissues as a guide for your knees until they are at elbow level. Don't bend your knees! . Go back up.", "title": "Inverted knee to elbow"}, {"location": "aerial_silk/#horizontal-pull-ups", "text": ". From standing position with each hand in a tissue, ask for a partner to take your legs until you're horizontal hanging from your hands and feet. . While alive: . Keeping your body straight, do a pull up with your arms. . Slowly unbend your elbows and stretch back your arms. The next level would be that you use your leg strength to grip your partner's hips instead of her holding your feet.", "title": "Horizontal pull-ups"}, {"location": "aerial_silk/#basic-movements", "text": "", "title": "Basic movements"}, {"location": "aerial_silk/#splitting-the-tissues", "text": ". Go to the desired height and get into a comfortable position, such as seated over your feet on a Russian climb. . Open the palm of one hand at eye level holding the whole tissue . Keep your bellybutton close to the tissue . With the other hand pinch the side of the silk and start walking with your fingers till you find the break between tissues.", "title": "Splitting the tissues"}, {"location": "aerial_silk/#figures", "text": "", "title": "Figures"}, {"location": "aerial_silk/#waist-lock", "text": ". Climb to the desired height . Leave the tissue to the side you want to do the lock to, for example the left (try the other side as well). . Leave your right hand above your left, with your arm straight. Your left hand should be at breast level with your elbow bent. . With straight legs with pointed toes, bring your left leg a little bit to the front, while the right one catches the silk . Bright the right leg up trying to get the silk as close to your waist as possible. . Rotate your body in z>0 towards your left hand until you're looking down . Release your hands.", "title": "Waist lock"}, {"location": "aerial_silk/#ideas", "text": "", "title": "Ideas"}, {"location": "aerial_silk/#locking-shoulder-blades-when-gripping-the-silks", "text": "When you are going to hang yourself from your hands: . Unlock your shoulder blades moving your chest to the back. . Embrace the silk with your arms twisting your hands inwards . Grip the silk and lock back your shoulder blades together as if you were holding an apple between them. That movement will make your hands twist in the other direction until your wrists are between you and the tissue.", "title": "Locking shoulder blades when gripping the silks"}, {"location": "aerial_silk/#safety", "text": "When rolling up silk over your legs, always leave room for the knee, do loops above and below but never over.", "title": "Safety"}, {"location": "afew/", "text": "afew is an initial tagging script for notmuch mail . Its basic task is to provide automatic tagging each time new mail is registered with notmuch . In a classic setup, you might call it after notmuch new in an offlineimap post sync hook. It can do basic thing such as adding tags based on email headers or maildir folders, handling killed threads and spam. In move mode, afew will move mails between maildir folders according to configurable rules that can contain arbitrary notmuch queries to match against any searchable attributes. Installation \u2691 First install the requirements: sudo apt-get install notmuch python-notmuch python-dev python-setuptools Then configure notmuch . Finally install the program: pip3 install afew Usage \u2691 To tag new emails use: afew -v --tag --new References \u2691 Git Docs", "title": "afew"}, {"location": "afew/#installation", "text": "First install the requirements: sudo apt-get install notmuch python-notmuch python-dev python-setuptools Then configure notmuch . Finally install the program: pip3 install afew", "title": "Installation"}, {"location": "afew/#usage", "text": "To tag new emails use: afew -v --tag --new", "title": "Usage"}, {"location": "afew/#references", "text": "Git Docs", "title": "References"}, {"location": "aleph/", "text": "Aleph is a tool for indexing large amounts of both documents (PDF, Word, HTML) and structured (CSV, XLS, SQL) data for easy browsing and search. It is built with investigative reporting as a primary use case. Aleph allows cross-referencing mentions of well-known entities (such as people and companies) against watchlists, e.g. from prior research or public datasets. Install the development environment \u2691 As a first step, check out the source code of Aleph from GitHub: git clone https://github.com/alephdata/aleph.git cd aleph/ Also, please execute the following command to allow ElasticSearch to map its memory: sysctl -w vm.max_map_count = 262144 Then enable the use of pdb by adding the next lines into the docker-compose.dev.yml file, under the api service configuration. stdin_open : true tty : true With the settings in place, you can use make all to set everything up and launch the web service. This is equivalent to the following steps: make build to build the docker images for the application and relevant services. make upgrade to run the latest database migrations and create/update the search index. make web to run the web-based API server and the user interface. In a separate shell, run make worker to start a worker. If you do not start a worker, background jobs (for example ingesting new documents) won\u2019t be processed. Open http://localhost:8080/ in your browser to visit the web frontend. Create a shell to do the operations with make shell . Create the main user within that shell running aleph createuser --name = \"demo\" \\ --admin \\ --password = demo \\ demo@demo.com Load some sample data by running aleph crawldir /aleph/contrib/testdata Debugging the code \u2691 To debug the code, you can create pdb breakpoints in the code you cloned, and run the actions that trigger the breakpoint. To be able to act on it, you need to be attached to the api by running: docker attach aleph_api_1 You don't need to reload the page for it to load the changes, it does it dynamically. Troubleshooting \u2691 Problems accessing redis locally \u2691 If you're with the VPN connected, turn it off. PDB behaves weird \u2691 Sometimes you have two traces at the same time, so each time you run a PDB command it jumps from pdb trace. Quite confusing. Try to c the one you don't want so that you're left with the one you want. Or put the pdb trace in a conditional that only matches one of both threads. References \u2691 Docs Git", "title": "aleph"}, {"location": "aleph/#install-the-development-environment", "text": "As a first step, check out the source code of Aleph from GitHub: git clone https://github.com/alephdata/aleph.git cd aleph/ Also, please execute the following command to allow ElasticSearch to map its memory: sysctl -w vm.max_map_count = 262144 Then enable the use of pdb by adding the next lines into the docker-compose.dev.yml file, under the api service configuration. stdin_open : true tty : true With the settings in place, you can use make all to set everything up and launch the web service. This is equivalent to the following steps: make build to build the docker images for the application and relevant services. make upgrade to run the latest database migrations and create/update the search index. make web to run the web-based API server and the user interface. In a separate shell, run make worker to start a worker. If you do not start a worker, background jobs (for example ingesting new documents) won\u2019t be processed. Open http://localhost:8080/ in your browser to visit the web frontend. Create a shell to do the operations with make shell . Create the main user within that shell running aleph createuser --name = \"demo\" \\ --admin \\ --password = demo \\ demo@demo.com Load some sample data by running aleph crawldir /aleph/contrib/testdata", "title": "Install the development environment"}, {"location": "aleph/#debugging-the-code", "text": "To debug the code, you can create pdb breakpoints in the code you cloned, and run the actions that trigger the breakpoint. To be able to act on it, you need to be attached to the api by running: docker attach aleph_api_1 You don't need to reload the page for it to load the changes, it does it dynamically.", "title": "Debugging the code"}, {"location": "aleph/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "aleph/#problems-accessing-redis-locally", "text": "If you're with the VPN connected, turn it off.", "title": "Problems accessing redis locally"}, {"location": "aleph/#pdb-behaves-weird", "text": "Sometimes you have two traces at the same time, so each time you run a PDB command it jumps from pdb trace. Quite confusing. Try to c the one you don't want so that you're left with the one you want. Or put the pdb trace in a conditional that only matches one of both threads.", "title": "PDB behaves weird"}, {"location": "aleph/#references", "text": "Docs Git", "title": "References"}, {"location": "alot/", "text": "alot is a terminal-based mail user agent based on the notmuch mail indexer . It is written in python using the urwid toolkit and features a modular and command prompt driven interface to provide a full MUA experience. Installation \u2691 sudo apt-get install alot Configuration \u2691 Alot reads the INI config file ~/.config/alot/config . That file is not created by default, if you don't want to start from scratch, you can use pazz's alot configuration , in particular the [accounts] section. UI interaction \u2691 Basic movement is done with: Move up and down: j / k , arrows and page up and page down. Cancel prompts: Escape Select highlighted element: Enter . Update buffer: @ . The interface shows one buffer at a time, basic buffer management is done with: Change buffer: Tab and Shift-Tab . Close the current buffer: d List all buffers: ; . The buffer type or mode (displayed at the bottom left) determines which prompt commands are available. Usage information on any command can be listed by typing help YOURCOMMAND to the prompt. The key bindings for the current mode are listed upon pressing ? . You can always run commands with : . Troubleshooting \u2691 Remove emails \u2691 Say you want to remove emails from the provider's server but keep them in the notmuch database. There is no straight way to do it, you need to tag them with a special tag like deleted and then remove them from the server with a post-hook. Theme not found \u2691 I don't know why but apt-get didn't install the default themes, you need to create the ~/.config/alot/themes and copy the contents of the themes directory . References \u2691 Git Docs Wiki FAQ", "title": "alot"}, {"location": "alot/#installation", "text": "sudo apt-get install alot", "title": "Installation"}, {"location": "alot/#configuration", "text": "Alot reads the INI config file ~/.config/alot/config . That file is not created by default, if you don't want to start from scratch, you can use pazz's alot configuration , in particular the [accounts] section.", "title": "Configuration"}, {"location": "alot/#ui-interaction", "text": "Basic movement is done with: Move up and down: j / k , arrows and page up and page down. Cancel prompts: Escape Select highlighted element: Enter . Update buffer: @ . The interface shows one buffer at a time, basic buffer management is done with: Change buffer: Tab and Shift-Tab . Close the current buffer: d List all buffers: ; . The buffer type or mode (displayed at the bottom left) determines which prompt commands are available. Usage information on any command can be listed by typing help YOURCOMMAND to the prompt. The key bindings for the current mode are listed upon pressing ? . You can always run commands with : .", "title": "UI interaction"}, {"location": "alot/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "alot/#remove-emails", "text": "Say you want to remove emails from the provider's server but keep them in the notmuch database. There is no straight way to do it, you need to tag them with a special tag like deleted and then remove them from the server with a post-hook.", "title": "Remove emails"}, {"location": "alot/#theme-not-found", "text": "I don't know why but apt-get didn't install the default themes, you need to create the ~/.config/alot/themes and copy the contents of the themes directory .", "title": "Theme not found"}, {"location": "alot/#references", "text": "Git Docs Wiki FAQ", "title": "References"}, {"location": "amazfit_band_5/", "text": "Amazfit Band 5 it's the affordable fitness tracker I chose to buy because: It's supported by gadgetbridge . It has a SpO2 sensor which enhances the quality of the sleep metrics It has sleep metrics, not only time but also type of sleep (light, deep, REM). It has support with Alexa, not that I'd use that, but it would be cool if once I've got my personal voice assistant , I can use it through the band. Sleep detection quality \u2691 The sleep tracking using Gadgetbridge is not good at all . After two nights, the band has not been able to detect when I woke in the middle of the night, or when I really woke up, as I usually stay in the bed for a time before standing up. I'll try with the proprietary application soon and compare results. If it doesn't work either, I might think of getting a specific device like withings sleep analyzer which seems to have much more accuracy and useful insights. I've sent them an email to see if it's possible to extract the data before it reach their servers, and they confirmed that there is no way. Maybe you can route the requests to their servers to one of your own, bring up an http server and reverse engineer the communication. Karlicoss, the author of the awesome HPI uses the Emfit QS , so that could be another option. Firmware updates \u2691 Gadgetbridge people have a guide on how to upgrade the firmware , you need to get the firmware from the geek doing forum though, so it is interesting to create an account and watch the post.", "title": "Amazfit Band 5"}, {"location": "amazfit_band_5/#sleep-detection-quality", "text": "The sleep tracking using Gadgetbridge is not good at all . After two nights, the band has not been able to detect when I woke in the middle of the night, or when I really woke up, as I usually stay in the bed for a time before standing up. I'll try with the proprietary application soon and compare results. If it doesn't work either, I might think of getting a specific device like withings sleep analyzer which seems to have much more accuracy and useful insights. I've sent them an email to see if it's possible to extract the data before it reach their servers, and they confirmed that there is no way. Maybe you can route the requests to their servers to one of your own, bring up an http server and reverse engineer the communication. Karlicoss, the author of the awesome HPI uses the Emfit QS , so that could be another option.", "title": "Sleep detection quality"}, {"location": "amazfit_band_5/#firmware-updates", "text": "Gadgetbridge people have a guide on how to upgrade the firmware , you need to get the firmware from the geek doing forum though, so it is interesting to create an account and watch the post.", "title": "Firmware updates"}, {"location": "android_tips/", "text": "Extend the life of your battery \u2691 Research has shown that keeping your battery charged between 0% and 80% can make your battery's lifespan last 2x longer than when you use a full battery cycle from 0-100%. As a non root user you can install Accubattery (not in F-droid :( ) to get an alarm when the battery reaches 80% so that you can manually unplug it. Instead of leaving the mobile charge in the night and stay connected at 100% a lot of hours until you unplug, charge it throughout the day.", "title": "Android Tips"}, {"location": "android_tips/#extend-the-life-of-your-battery", "text": "Research has shown that keeping your battery charged between 0% and 80% can make your battery's lifespan last 2x longer than when you use a full battery cycle from 0-100%. As a non root user you can install Accubattery (not in F-droid :( ) to get an alarm when the battery reaches 80% so that you can manually unplug it. Instead of leaving the mobile charge in the night and stay connected at 100% a lot of hours until you unplug, charge it throughout the day.", "title": "Extend the life of your battery"}, {"location": "anki/", "text": "Anki is a program which makes remembering things easy. Because it's a lot more efficient than traditional study methods, you can either greatly decrease your time spent studying, or greatly increase the amount you learn. Anyone who needs to remember things in their daily life can benefit from Anki. Since it is content-agnostic and supports images, audio, videos and scientific markup (via LaTeX), the possibilities are endless. Interacting with python \u2691 Configuration \u2691 Although there are some python libraries: genanki py-anki I think the best way is to use AnkiConnect The installation process is similar to other Anki plugins and can be accomplished in three steps: Open the Install Add-on dialog by selecting Tools | Add-ons | Get Add-ons... in Anki. Input 2055492159 into the text box labeled Code and press the OK button to proceed. Restart Anki when prompted to do so in order to complete the installation of Anki-Connect. Anki must be kept running in the background in order for other applications to be able to use Anki-Connect. You can verify that Anki-Connect is running at any time by accessing localhost:8765 in your browser. If the server is running, you will see the message Anki-Connect displayed in your browser window. Usage \u2691 Every request consists of a JSON-encoded object containing an action , a version , contextual params , and a key value used for authentication (which is optional and can be omitted by default). Anki-Connect will respond with an object containing two fields: result and error . The result field contains the return value of the executed API, and the error field is a description of any exception thrown during API execution (the value null is used if execution completed successfully). Sample successful response: { \"result\" : [ \"Default\" , \"Filtered Deck 1\" ], \"error\" : null } Samples of failed responses: { \"result\" : null , \"error\" : \"unsupported action\" } { \"result\" : null , \"error\" : \"guiBrowse() got an unexpected keyword argument 'foobar'\" } For compatibility with clients designed to work with older versions of Anki-Connect, failing to provide a version field in the request will make the version default to 4. To make the interaction with the API easier, I'm using the next adapter: class Anki : \"\"\"Define the Anki adapter.\"\"\" def __init__ ( self , url : str = \"http://localhost:8765\" ) -> None : \"\"\"Initialize the adapter.\"\"\" self . url = url def requests ( self , action : str , params : Optional [ Dict [ str , str ]] = None ) -> Response : \"\"\"Do a request to the server.\"\"\" if params is None : params = {} response = requests . post ( self . url , json = { \"action\" : action , \"params\" : params , \"version\" : 6 } ) . json () if len ( response ) != 2 : raise Exception ( \"response has an unexpected number of fields\" ) if \"error\" not in response : raise Exception ( \"response is missing required error field\" ) if \"result\" not in response : raise Exception ( \"response is missing required result field\" ) if response [ \"error\" ] is not None : raise Exception ( response [ \"error\" ]) return response [ \"result\" ] You can find the full adapter in the fala project. Decks \u2691 Get all decks \u2691 With the adapter: self . requests ( \"deckNames\" ) Or with curl : curl localhost:8765 -X POST -d '{\"action\": \"deckNames\", \"version\": 6}' Create a new deck \u2691 self . requests ( \"createDeck\" , { \"deck\" : deck }) References \u2691 Homepage Anki-Connect reference", "title": "Anki"}, {"location": "anki/#interacting-with-python", "text": "", "title": "Interacting with python"}, {"location": "anki/#configuration", "text": "Although there are some python libraries: genanki py-anki I think the best way is to use AnkiConnect The installation process is similar to other Anki plugins and can be accomplished in three steps: Open the Install Add-on dialog by selecting Tools | Add-ons | Get Add-ons... in Anki. Input 2055492159 into the text box labeled Code and press the OK button to proceed. Restart Anki when prompted to do so in order to complete the installation of Anki-Connect. Anki must be kept running in the background in order for other applications to be able to use Anki-Connect. You can verify that Anki-Connect is running at any time by accessing localhost:8765 in your browser. If the server is running, you will see the message Anki-Connect displayed in your browser window.", "title": "Configuration"}, {"location": "anki/#usage", "text": "Every request consists of a JSON-encoded object containing an action , a version , contextual params , and a key value used for authentication (which is optional and can be omitted by default). Anki-Connect will respond with an object containing two fields: result and error . The result field contains the return value of the executed API, and the error field is a description of any exception thrown during API execution (the value null is used if execution completed successfully). Sample successful response: { \"result\" : [ \"Default\" , \"Filtered Deck 1\" ], \"error\" : null } Samples of failed responses: { \"result\" : null , \"error\" : \"unsupported action\" } { \"result\" : null , \"error\" : \"guiBrowse() got an unexpected keyword argument 'foobar'\" } For compatibility with clients designed to work with older versions of Anki-Connect, failing to provide a version field in the request will make the version default to 4. To make the interaction with the API easier, I'm using the next adapter: class Anki : \"\"\"Define the Anki adapter.\"\"\" def __init__ ( self , url : str = \"http://localhost:8765\" ) -> None : \"\"\"Initialize the adapter.\"\"\" self . url = url def requests ( self , action : str , params : Optional [ Dict [ str , str ]] = None ) -> Response : \"\"\"Do a request to the server.\"\"\" if params is None : params = {} response = requests . post ( self . url , json = { \"action\" : action , \"params\" : params , \"version\" : 6 } ) . json () if len ( response ) != 2 : raise Exception ( \"response has an unexpected number of fields\" ) if \"error\" not in response : raise Exception ( \"response is missing required error field\" ) if \"result\" not in response : raise Exception ( \"response is missing required result field\" ) if response [ \"error\" ] is not None : raise Exception ( response [ \"error\" ]) return response [ \"result\" ] You can find the full adapter in the fala project.", "title": "Usage"}, {"location": "anki/#decks", "text": "", "title": "Decks"}, {"location": "anki/#get-all-decks", "text": "With the adapter: self . requests ( \"deckNames\" ) Or with curl : curl localhost:8765 -X POST -d '{\"action\": \"deckNames\", \"version\": 6}'", "title": "Get all decks"}, {"location": "anki/#create-a-new-deck", "text": "self . requests ( \"createDeck\" , { \"deck\" : deck })", "title": "Create a new deck"}, {"location": "anki/#references", "text": "Homepage Anki-Connect reference", "title": "References"}, {"location": "anonymous_feedback/", "text": "Anonymous Feedback is a communication tool where people share feedback to teammates or other organizational members while protecting their identities. Why would you need anonymous feedback? \u2691 Ideally, everyone in your company should be able to give feedback publicly and not anonymously. They should share constructive criticism and not shy away from direct feedback if they believe and trust that their opinions will be heard and addressed. However, to achieve this ideal, people need to feel that they are in a safe space , a place or environment in which they feel confident that they will not be exposed to discrimination, criticism, harassment, or any other emotional or physical harm. The work place is usually not considered a safe space by the employees because they may: Fear of being judged : We want people to like us and not just in our personal lives, but in our professional lives as well. It also seems to bear a bigger importance that our supervisor likes us because he holds the power over our career and financial security. So we live in a constant state of anxiety of what might happen if our manager doesn't like us. Fear of losing their job : It\u2019s a form of self-preservation, abstaining from saying something that may be perceived as wrong to someone in a position of authority. Fear of being singled out : Giving direct feedback puts you in the spotlight. Being highlighted against the rest of the employees might be seen as a threat, especially by people belonging to a different race, gender, national origin, or other identities than most of their coworkers. Feel insecure : People may distrust their colleagues, because they just arrived at the organization or may have negative past experiences either with them or with similar people. They may not have a solid stance on an issue, be shy or have problems of self esteem. Distrust the open-door internal policies : Past experiences in other companies may lead the employee not to trust open-doors policies until they have seen them in practice. Not knowing the internal processes of the organization : As a Slack study shows , 55 percent of business owners described their organization as very transparent, but only 18 percent of their employees would agree. For all these reasons, some employees may remain silent when asked for direct feedback, to speak up against an internal issue or in need to report a colleague or manager. These factors are further amplified if: The person belongs to a minority group inside the organization. The greater the difference in position between the talking parties. It's more difficult to talk to the CEO than to the immediate manager. Until the safe space is built where direct feedback is viable, anonymous feedback gives these employees a mechanism to raise their concerns, practice their feedback-giving skills, test the waters, and understand how people perceive their constructive (and sometimes critical) opinions, thus building the needed trust. Pros and cons \u2691 Pros of Anonymous Feedback: Employees can express themselves freely and provide valuable insights : On topics that are considered sensitive, you\u2019ll often find employees who are afraid to share their opinions. But when employees have the option to use anonymous feedback, you will be offering a safe space for them to share their honest, constructive feedback about sensitive workplace issues, without fear of being judged, victimized, radicalized or labelled in any way. A formal, non-anonymous feedback form will only reveal some of the superficial, non-threatening issues that affect the workplace, without mentioning the most important, underlying problems. The real problems that no one talks about because they know they are so important that they could stir things up. In fact, these controversial, important issues are the ones that need to be brought to the table as soon as possible. They should be addressed by the entire team before they become a source of unhappiness, conflict and lack of productivity. An anonymous feedback instrument gives you real power over those issues because it doesn't matter who brought it up, but that it\u2019s resolved. For a manager, that insight is invaluable. It builds trust : Anonymous feedback allows the employee see how management reacts to feedback, understand how people perceive their constructive (and sometimes critical) opinions, how are the open-door policies being applied and build up self esteem. It offers a sense of security : Anonymity soothes the employee anxiety and creates a greater willingness to share our ideas and opinions. It allows every voice to be heard and respected : In workplaces, where they practice direct or attributed feedback, leaders may give preference to some voices over others. Due to our unconscious biases, people of higher authority, backgrounds, or eloquence tend to command respect and attention. In such situations, the issues they raise are likely to get immediate attention than those raised by the rest of the group. However, when feedback is collected anonymously, it eliminates biases and allows leaders to focus entirely on the feedback. It encourages new employees to share their opinions : Research has shown that new employees, who happen to be less senior or influential, see anonymous feedback as more appropriate for formal and informal evaluations than their older colleagues. Typically, the last thing a new employee wants is to start on the wrong foot, so they maintain a neutral stance. Using anonymous feedback can make new employees feel more comfortable sharing their real opinions on workplace issues. Cons of Anonymous Feedback: It can breed hostility : According to this Harvard Business Review article , anonymity often sets off a \u201cwitch hunt\u201d, where leaders seek to know the source of a negative comment. On the one hand, employees can hide behind anonymity to say personal and hurtful things about their colleagues or leaders. On the other hand, leaders may take constructive feedback as a personal attack and become suspicious and hostile to all their employees. It can be less impactful than attributed feedback : When using attributed feedback where responses carry the employees\u2019 names, information can be analyzed for relevance and impact. However, with anonymous feedback, it can be difficult to analyze information accurately. It is not uncommon for companies who choose to practice anonymous feedback, to find less specific responses since details may reveal respondents\u2019 identities. Vague feedback from employees would have less power to influence behaviors or drive change in the organization. It can be difficult to act on : Since anonymous feedback is often difficult to trace, it can be challenging for the organization to get context or follow up on important issues, especially when a problem is peculiar to an individual. How to request anonymous feedback \u2691 When requesting for anonymous feedback on an organizational level, it is necessary to: Set expectations for employees : Let your colleagues know how important their feedback is to the organization. Also, assure them that their responses will be non-identifiable (no identifiable names, titles, or other demographic details). According to a Harvard Business Review article , \u201crespondents are much more likely to participate if they are confident that personal anonymity is guaranteed.\u201d Set those expectations to increase the chances of response from them. Deploy a feedback platform : Use a trusted feedback platform to send feedback requests to the rest of the employees. How to Act on Anonymous Feedback \u2691 Once you have sent the anonymous feedback, be sure to: Gather and share the findings : A significant issue with employee feedback is that the data often ends up unused. After collecting the results, share the data\u2014the positives and negatives\u2014with everyone. Doing this shows transparency and makes your colleagues develop a positive attitude toward future requests for feedback. Get everyone involved : Engage employees, managers, and leaders in discussing and analyzing the feedback findings. Doing this helps to build trust and develop actionable ideas to move the organization forward. Identify the key issues : From the discussions and analysis, identify the key issues and understand how they would impact the organization, once addressed. Define and act on the next steps : The purpose of collecting feedback would be pointless if the next steps aren't defined. Real improvement comes from knowing and working on the next steps. References \u2691 Osasumwen Arigbe article on Diversity, Inclusion, and Anonymous Feedback Paula Clapon article Why anonymous employee feedback is the better alternative Julian Cook article . I haven't used it's text, but it's written for managers in their language, it may help someone there.", "title": "Anonymous Feedback"}, {"location": "anonymous_feedback/#why-would-you-need-anonymous-feedback", "text": "Ideally, everyone in your company should be able to give feedback publicly and not anonymously. They should share constructive criticism and not shy away from direct feedback if they believe and trust that their opinions will be heard and addressed. However, to achieve this ideal, people need to feel that they are in a safe space , a place or environment in which they feel confident that they will not be exposed to discrimination, criticism, harassment, or any other emotional or physical harm. The work place is usually not considered a safe space by the employees because they may: Fear of being judged : We want people to like us and not just in our personal lives, but in our professional lives as well. It also seems to bear a bigger importance that our supervisor likes us because he holds the power over our career and financial security. So we live in a constant state of anxiety of what might happen if our manager doesn't like us. Fear of losing their job : It\u2019s a form of self-preservation, abstaining from saying something that may be perceived as wrong to someone in a position of authority. Fear of being singled out : Giving direct feedback puts you in the spotlight. Being highlighted against the rest of the employees might be seen as a threat, especially by people belonging to a different race, gender, national origin, or other identities than most of their coworkers. Feel insecure : People may distrust their colleagues, because they just arrived at the organization or may have negative past experiences either with them or with similar people. They may not have a solid stance on an issue, be shy or have problems of self esteem. Distrust the open-door internal policies : Past experiences in other companies may lead the employee not to trust open-doors policies until they have seen them in practice. Not knowing the internal processes of the organization : As a Slack study shows , 55 percent of business owners described their organization as very transparent, but only 18 percent of their employees would agree. For all these reasons, some employees may remain silent when asked for direct feedback, to speak up against an internal issue or in need to report a colleague or manager. These factors are further amplified if: The person belongs to a minority group inside the organization. The greater the difference in position between the talking parties. It's more difficult to talk to the CEO than to the immediate manager. Until the safe space is built where direct feedback is viable, anonymous feedback gives these employees a mechanism to raise their concerns, practice their feedback-giving skills, test the waters, and understand how people perceive their constructive (and sometimes critical) opinions, thus building the needed trust.", "title": "Why would you need anonymous feedback?"}, {"location": "anonymous_feedback/#pros-and-cons", "text": "Pros of Anonymous Feedback: Employees can express themselves freely and provide valuable insights : On topics that are considered sensitive, you\u2019ll often find employees who are afraid to share their opinions. But when employees have the option to use anonymous feedback, you will be offering a safe space for them to share their honest, constructive feedback about sensitive workplace issues, without fear of being judged, victimized, radicalized or labelled in any way. A formal, non-anonymous feedback form will only reveal some of the superficial, non-threatening issues that affect the workplace, without mentioning the most important, underlying problems. The real problems that no one talks about because they know they are so important that they could stir things up. In fact, these controversial, important issues are the ones that need to be brought to the table as soon as possible. They should be addressed by the entire team before they become a source of unhappiness, conflict and lack of productivity. An anonymous feedback instrument gives you real power over those issues because it doesn't matter who brought it up, but that it\u2019s resolved. For a manager, that insight is invaluable. It builds trust : Anonymous feedback allows the employee see how management reacts to feedback, understand how people perceive their constructive (and sometimes critical) opinions, how are the open-door policies being applied and build up self esteem. It offers a sense of security : Anonymity soothes the employee anxiety and creates a greater willingness to share our ideas and opinions. It allows every voice to be heard and respected : In workplaces, where they practice direct or attributed feedback, leaders may give preference to some voices over others. Due to our unconscious biases, people of higher authority, backgrounds, or eloquence tend to command respect and attention. In such situations, the issues they raise are likely to get immediate attention than those raised by the rest of the group. However, when feedback is collected anonymously, it eliminates biases and allows leaders to focus entirely on the feedback. It encourages new employees to share their opinions : Research has shown that new employees, who happen to be less senior or influential, see anonymous feedback as more appropriate for formal and informal evaluations than their older colleagues. Typically, the last thing a new employee wants is to start on the wrong foot, so they maintain a neutral stance. Using anonymous feedback can make new employees feel more comfortable sharing their real opinions on workplace issues. Cons of Anonymous Feedback: It can breed hostility : According to this Harvard Business Review article , anonymity often sets off a \u201cwitch hunt\u201d, where leaders seek to know the source of a negative comment. On the one hand, employees can hide behind anonymity to say personal and hurtful things about their colleagues or leaders. On the other hand, leaders may take constructive feedback as a personal attack and become suspicious and hostile to all their employees. It can be less impactful than attributed feedback : When using attributed feedback where responses carry the employees\u2019 names, information can be analyzed for relevance and impact. However, with anonymous feedback, it can be difficult to analyze information accurately. It is not uncommon for companies who choose to practice anonymous feedback, to find less specific responses since details may reveal respondents\u2019 identities. Vague feedback from employees would have less power to influence behaviors or drive change in the organization. It can be difficult to act on : Since anonymous feedback is often difficult to trace, it can be challenging for the organization to get context or follow up on important issues, especially when a problem is peculiar to an individual.", "title": "Pros and cons"}, {"location": "anonymous_feedback/#how-to-request-anonymous-feedback", "text": "When requesting for anonymous feedback on an organizational level, it is necessary to: Set expectations for employees : Let your colleagues know how important their feedback is to the organization. Also, assure them that their responses will be non-identifiable (no identifiable names, titles, or other demographic details). According to a Harvard Business Review article , \u201crespondents are much more likely to participate if they are confident that personal anonymity is guaranteed.\u201d Set those expectations to increase the chances of response from them. Deploy a feedback platform : Use a trusted feedback platform to send feedback requests to the rest of the employees.", "title": "How to request anonymous feedback"}, {"location": "anonymous_feedback/#how-to-act-on-anonymous-feedback", "text": "Once you have sent the anonymous feedback, be sure to: Gather and share the findings : A significant issue with employee feedback is that the data often ends up unused. After collecting the results, share the data\u2014the positives and negatives\u2014with everyone. Doing this shows transparency and makes your colleagues develop a positive attitude toward future requests for feedback. Get everyone involved : Engage employees, managers, and leaders in discussing and analyzing the feedback findings. Doing this helps to build trust and develop actionable ideas to move the organization forward. Identify the key issues : From the discussions and analysis, identify the key issues and understand how they would impact the organization, once addressed. Define and act on the next steps : The purpose of collecting feedback would be pointless if the next steps aren't defined. Real improvement comes from knowing and working on the next steps.", "title": "How to Act on Anonymous Feedback"}, {"location": "anonymous_feedback/#references", "text": "Osasumwen Arigbe article on Diversity, Inclusion, and Anonymous Feedback Paula Clapon article Why anonymous employee feedback is the better alternative Julian Cook article . I haven't used it's text, but it's written for managers in their language, it may help someone there.", "title": "References"}, {"location": "ansible_snippets/", "text": "Get the hosts of a dynamic ansible inventory \u2691 ansible-inventory -i environments/production --graph You can also use the --list flag to get more info of the hosts. Speed up the stat module \u2691 The stat module calculates the checksum and the md5 of the file in order to get the required data. If you just want to check if the file exists use: - name : Verify swapfile status stat : path : \"{{ common_swapfile_location }}\" get_checksum : no get_md5 : no get_mime : no get_attributes : no register : swap_status changed_when : not swap_status.stat.exists Stop running docker containers \u2691 - name : Get running containers docker_host_info : containers : yes register : docker_info - name : Stop running containers docker_container : name : \"{{ item }}\" state : stopped loop : \"{{ docker_info.containers | map(attribute='Id') | list }}\" Moving a file remotely \u2691 Funnily enough, you can't without a command . You could use the copy module with: - name : Copy files from foo to bar copy : remote_src : True src : /path/to/foo dest : /path/to/bar - name : Remove old files foo file : path=/path/to/foo state=absent But that doesn't move, it copies and removes, which is not the same. To make the command idempotent you can use a stat task before. - name : stat foo stat : path=/path/to/foo register : foo_stat - name : Move foo to bar command : mv /path/to/foo /path/to/bar when : foo_stat.stat.exists", "title": "Ansible Snippets"}, {"location": "ansible_snippets/#get-the-hosts-of-a-dynamic-ansible-inventory", "text": "ansible-inventory -i environments/production --graph You can also use the --list flag to get more info of the hosts.", "title": "Get the hosts of a dynamic ansible inventory"}, {"location": "ansible_snippets/#speed-up-the-stat-module", "text": "The stat module calculates the checksum and the md5 of the file in order to get the required data. If you just want to check if the file exists use: - name : Verify swapfile status stat : path : \"{{ common_swapfile_location }}\" get_checksum : no get_md5 : no get_mime : no get_attributes : no register : swap_status changed_when : not swap_status.stat.exists", "title": "Speed up the stat module"}, {"location": "ansible_snippets/#stop-running-docker-containers", "text": "- name : Get running containers docker_host_info : containers : yes register : docker_info - name : Stop running containers docker_container : name : \"{{ item }}\" state : stopped loop : \"{{ docker_info.containers | map(attribute='Id') | list }}\"", "title": "Stop running docker containers"}, {"location": "ansible_snippets/#moving-a-file-remotely", "text": "Funnily enough, you can't without a command . You could use the copy module with: - name : Copy files from foo to bar copy : remote_src : True src : /path/to/foo dest : /path/to/bar - name : Remove old files foo file : path=/path/to/foo state=absent But that doesn't move, it copies and removes, which is not the same. To make the command idempotent you can use a stat task before. - name : stat foo stat : path=/path/to/foo register : foo_stat - name : Move foo to bar command : mv /path/to/foo /path/to/bar when : foo_stat.stat.exists", "title": "Moving a file remotely"}, {"location": "antifascism/", "text": "Antifascism is a method of politics, a locus of individual and group self-indentification, it's a transnational movement that adapted preexisting socialist, anarchist, and communist currents to a sudden need to react to the far right menace ( Mark p. 11 ). It's based on the idea that any oppression form can't be allowed, and should be actively fought with whatever means are necessary. Usually sharing space and even blending with other politic stances that share the same principle, such as intersectional feminism . Read the references The articles under this section are the brushstrokes I use to learn how to become an efficient antifascist. It assumes that you identify yourself as an antifascist, so I'll go straight to the point, skipping much of the argumentation that is needed to sustain these ideas. I'll add links to Mark's and Pol's awesome books, which I strongly recommend you to buy, as they both are jewels that everyone should read. Despite the criminalization and stigmatization by the mainstream press and part of the society, antifascism is a rock solid organized movement with a lot of history, that has invested blood, tears and lives to prevent us from living in a yet more horrible world. The common stereotype is a small group of leftist young people that confront the nazis on the streets, preventing them from using the public space, and from further organizing through direct action and violence if needed. If you don't identify yourself with this stereotype, don't worry, they are only a small (but essential) part of antifascism, there are so many and diverse ways to be part of the antifascist movement that in fact, everyone can (and should) be an antifascist. What is fascism \u2691 Fascism in Paxton's words is: ... a form of political behavior marked by obsessive preoccupation with community decline, humiliation, or victimhood and by compensatory cults of unity, energy, and purity, in which a mass-based party of commited nationalist militians, working in uneasy but effective collaboration with traditional elites, abandons democratic liberties and pursues with redemptive violence and without ethical or legal restrains goals of internal cleansing and external expansion. They are nourished by the people's weariness with the corruption and inoperability of the traditional political parties, and the growing fear and anguish of an uncertain economic situation. They continuously adapt, redefine and reappropriate concepts under an irreverent, politically incorrect and critical spirit, to spread the old discourse of the privileged against the oppressed. They dress themselves as antisystems, pursuing the liberty behind the authority, and accepting the democratic system introducing totalitarianism nuances ( Pol p.20 ). How to identify fascism \u2691 We need to make sure that we use the term well, otherwise we run into the risk of the word loosing meaning. But equally important is not to fall in a wording discussion that paralyzes us. One way to make it measurable is to use Kimberl\u00e9 Williams Crenshaw intersectionality theory , which states that individuals experience oppression or privilege based on a belonging to a plurality of social categories, to measure how close an action or discourse follows fascism principles ( Pol p.26 ). Source Fascism has always been carried out by people with many privileges (the upper part of the diagram) against collectives under many oppressions (the lower part of the diagram). We can then state that the more the oppressions a discourse defends and perpetuates, the more probable it is to be fascist. If it also translates into physical or verbal aggressions, escalates into the will to transform that discourse into laws that backs up those aggressions, or tries to build a government under those ideas, then we clearly have a political roadmap towards fascism. The fact that they don't propose to abolish the democracy or try to send people to concentration camps doesn't mean they are not fascist. First, we don't need them to commit the exact same crimes that the fascists of last century made to put at risk some social collectives, and secondly, history tells us that classic fascism movements didn't show their true intentions in their early phases. Fascism shifts their form and particular characteristics based on place and time. Waiting to see it clear is risking being late to fight it. Therefore whenever we see a discourse that comes from a privileged person against a oppressed one, we should fight it immediately, once fought, you can analyze if it was fascist or not ( Pol p.28 ) How to fight fascism \u2691 There are many ways to fight it, the book Todo el mundo puede ser Antifa: Manual practico para destruir el fascismo of Pol Andi\u00f1ach gathers some of them. One way we've seen pisses them off quite much is when they are ridiculed and they evocate the image of incompetence. It's a fine line to go, because if it falls into a pity image then it may strengthen their victim role. References \u2691 Antifa: The anti-fascist handbook by Mark Bray Todo el mundo puede ser Antifa: Manual practico para destruir el fascismo de Pol Andi\u00f1ach Magazines \u2691 Hope not Hate Searchlight Podcasts \u2691 Hope not Hate", "title": "Antifascism"}, {"location": "antifascism/#what-is-fascism", "text": "Fascism in Paxton's words is: ... a form of political behavior marked by obsessive preoccupation with community decline, humiliation, or victimhood and by compensatory cults of unity, energy, and purity, in which a mass-based party of commited nationalist militians, working in uneasy but effective collaboration with traditional elites, abandons democratic liberties and pursues with redemptive violence and without ethical or legal restrains goals of internal cleansing and external expansion. They are nourished by the people's weariness with the corruption and inoperability of the traditional political parties, and the growing fear and anguish of an uncertain economic situation. They continuously adapt, redefine and reappropriate concepts under an irreverent, politically incorrect and critical spirit, to spread the old discourse of the privileged against the oppressed. They dress themselves as antisystems, pursuing the liberty behind the authority, and accepting the democratic system introducing totalitarianism nuances ( Pol p.20 ).", "title": "What is fascism"}, {"location": "antifascism/#how-to-identify-fascism", "text": "We need to make sure that we use the term well, otherwise we run into the risk of the word loosing meaning. But equally important is not to fall in a wording discussion that paralyzes us. One way to make it measurable is to use Kimberl\u00e9 Williams Crenshaw intersectionality theory , which states that individuals experience oppression or privilege based on a belonging to a plurality of social categories, to measure how close an action or discourse follows fascism principles ( Pol p.26 ). Source Fascism has always been carried out by people with many privileges (the upper part of the diagram) against collectives under many oppressions (the lower part of the diagram). We can then state that the more the oppressions a discourse defends and perpetuates, the more probable it is to be fascist. If it also translates into physical or verbal aggressions, escalates into the will to transform that discourse into laws that backs up those aggressions, or tries to build a government under those ideas, then we clearly have a political roadmap towards fascism. The fact that they don't propose to abolish the democracy or try to send people to concentration camps doesn't mean they are not fascist. First, we don't need them to commit the exact same crimes that the fascists of last century made to put at risk some social collectives, and secondly, history tells us that classic fascism movements didn't show their true intentions in their early phases. Fascism shifts their form and particular characteristics based on place and time. Waiting to see it clear is risking being late to fight it. Therefore whenever we see a discourse that comes from a privileged person against a oppressed one, we should fight it immediately, once fought, you can analyze if it was fascist or not ( Pol p.28 )", "title": "How to identify fascism"}, {"location": "antifascism/#how-to-fight-fascism", "text": "There are many ways to fight it, the book Todo el mundo puede ser Antifa: Manual practico para destruir el fascismo of Pol Andi\u00f1ach gathers some of them. One way we've seen pisses them off quite much is when they are ridiculed and they evocate the image of incompetence. It's a fine line to go, because if it falls into a pity image then it may strengthen their victim role.", "title": "How to fight fascism"}, {"location": "antifascism/#references", "text": "Antifa: The anti-fascist handbook by Mark Bray Todo el mundo puede ser Antifa: Manual practico para destruir el fascismo de Pol Andi\u00f1ach", "title": "References"}, {"location": "antifascism/#magazines", "text": "Hope not Hate Searchlight", "title": "Magazines"}, {"location": "antifascism/#podcasts", "text": "Hope not Hate", "title": "Podcasts"}, {"location": "antifascist_actions/", "text": "Collection of amazing and inspiring antifa actions. 2022 \u2691 An open data initiative to map spanish hate crimes \u2691 The project Crimenes de Odio have created an open database of the hate crimes registered in the spanish state. An open data initiative to map spanish fascist icons \u2691 The project Deber\u00edaDesaparecer have created an open database of the remains of the spanish fascist regime icons. The visualization they've created is astonishing, and they've provided a form so that anyone can contribute to the dataset. 2021 \u2691 A fake company and five million recycled flyers \u2691 A group of artists belonging to the Center for political beauty created a fake company Flyerservice Hahn and convinced more than 80 regional sections of the far right party AfD to hire them to deliver their electoral propaganda. They gathered five million flyers, with a total weight of 72 tons. They justify that they wouldn't be able to lie to the people, so they did nothing in the broader sense of the word. They declared that they are the \"world wide leader in the non-delivery of nazi propaganda\" . At the start of the electoral campaign, they went to the AfD stands, and they let their members to give them flyers the throw them to the closest bin. \"It's something that any citizen can freely do, we have only industrialized the process\". They've done a crowdfunding to fund the legal process that may result.", "title": "Antifa Actions"}, {"location": "antifascist_actions/#2022", "text": "", "title": "2022"}, {"location": "antifascist_actions/#an-open-data-initiative-to-map-spanish-hate-crimes", "text": "The project Crimenes de Odio have created an open database of the hate crimes registered in the spanish state.", "title": "An open data initiative to map spanish hate crimes"}, {"location": "antifascist_actions/#an-open-data-initiative-to-map-spanish-fascist-icons", "text": "The project Deber\u00edaDesaparecer have created an open database of the remains of the spanish fascist regime icons. The visualization they've created is astonishing, and they've provided a form so that anyone can contribute to the dataset.", "title": "An open data initiative to map spanish fascist icons"}, {"location": "antifascist_actions/#2021", "text": "", "title": "2021"}, {"location": "antifascist_actions/#a-fake-company-and-five-million-recycled-flyers", "text": "A group of artists belonging to the Center for political beauty created a fake company Flyerservice Hahn and convinced more than 80 regional sections of the far right party AfD to hire them to deliver their electoral propaganda. They gathered five million flyers, with a total weight of 72 tons. They justify that they wouldn't be able to lie to the people, so they did nothing in the broader sense of the word. They declared that they are the \"world wide leader in the non-delivery of nazi propaganda\" . At the start of the electoral campaign, they went to the AfD stands, and they let their members to give them flyers the throw them to the closest bin. \"It's something that any citizen can freely do, we have only industrialized the process\". They've done a crowdfunding to fund the legal process that may result.", "title": "A fake company and five million recycled flyers"}, {"location": "antitransphobia/", "text": "Anti-transphobia being reductionist is the opposition to the collection of ideas and phenomena that encompass a range of negative attitudes, feelings or actions towards transgender people or transness in general. Transphobia can include fear, aversion, hatred, violence, anger, or discomfort felt or expressed towards people who do not conform to social gender expectations. It is often expressed alongside homophobic views and hence is often considered an aspect of homophobia. It's yet another clear case of privileged people oppressing even further already oppressed collectives. We can clearly see it if we use the ever useful Kimberl\u00e9 Williams Crenshaw intersectionality theory diagram. Source TERF \u2691 TERF is an acronym for trans-exclusionary radical feminist . The term originally applied to the minority of feminists that expressed transphobic sentiments such as the rejection of the assertion that trans women are women, the exclusion of trans women from women's spaces, and opposition to transgender rights legislation. The meaning has since expanded to refer more broadly to people with trans-exclusionary views who may have no involvement with radical feminism. Arguments against theories that deny the reality of trans people \u2691 This section is a direct translation from Alana Portero's text called Definitions . Sex is a medical category and gender a social category \u2691 Sex is a medical category, it's not a biological one. According to body features like the chromosome structure and genitalia appearance, medicine assigns the sex (hence gender) to the bodies. In the case of intersexual people, they are usually mutilated and hormonated so that their bodies fit into one of the two options contemplated by the medicine. Gender is the term we use to refer to how a person feels about themselves as a boy/man, a girl/woman or non-binary. Since birth, we're told what's appropriate (and what isn't) for each gender. These are the gender roles. It's not the same gender than gender role: the gender determines how you interact with the other roles. For example, a woman can take traditionally understood male roles gender roles, that doesn't mean that she is or isn't a woman. The problem arises when these two oppressions are mixed up: cissexism (the believe that bodies have an immutable gender defined by the sex assigned at birth) and misogyny (the base of feminine oppression). When you mixing them up you get the idea that the trans movement erases the feminine structural oppression, when in reality, it broadens the scope and makes it more precise, as they suffer the same misogyny than the cis women. Women are killed for being women. They are socially assigned the responsibility for care, they are prevented from having individual will and they are deterred from accessing resources. This structural violence is suffered by all women regardless of the sex assigned at birth. Questioning the adjudication of gender to the bodies and questioning the roles assigned to the genders are complementary paths for the feminism liberation. Avoid the interested manipulation of the sexual or gender identity \u2691 The sexual or gender identity determines whether there is correspondence with the gender assigned at birth. When there isn't, it concerns a trans person. The sex and gender terms represent the same reality, being sex the medical term, and gender the academic one. Equally transexual and transgender represent the same reality, although these last have a pathologizing undertone, the term trans is preferred. Avoid the fears of letting trans people be \u2691 Some are afraid that the trans women negatively affect the statistics of unemployment, laboral inequality, feminization of the poverty and machist violence, and they contradict the problems of the cis women. Trans people usually have a greater unemployment rate (85% in Spain), so the glass ceiling is not yet even a concern, and they are also greatly affected by machist violence. The queer theory doesn't erase or blur women as a political subject. Thinking that it risks the rights and achievements earned through the feminist movement shows a complete misunderstanding of the theory. Women are not an entity \u2691 Women are not an entity, they are a group of people that are placed below men in the social scale, each with her own unique experience. The woman identity belongs to any person that identifies herself with it. The fight against discrimination and towards inclusion politics should be mandatory for all society, and shouldn't be used against the trans people. References \u2691 Wikipedia page on Transphobia", "title": "Anti-transphobia"}, {"location": "antitransphobia/#terf", "text": "TERF is an acronym for trans-exclusionary radical feminist . The term originally applied to the minority of feminists that expressed transphobic sentiments such as the rejection of the assertion that trans women are women, the exclusion of trans women from women's spaces, and opposition to transgender rights legislation. The meaning has since expanded to refer more broadly to people with trans-exclusionary views who may have no involvement with radical feminism.", "title": "TERF"}, {"location": "antitransphobia/#arguments-against-theories-that-deny-the-reality-of-trans-people", "text": "This section is a direct translation from Alana Portero's text called Definitions .", "title": "Arguments against theories that deny the reality of trans people"}, {"location": "antitransphobia/#sex-is-a-medical-category-and-gender-a-social-category", "text": "Sex is a medical category, it's not a biological one. According to body features like the chromosome structure and genitalia appearance, medicine assigns the sex (hence gender) to the bodies. In the case of intersexual people, they are usually mutilated and hormonated so that their bodies fit into one of the two options contemplated by the medicine. Gender is the term we use to refer to how a person feels about themselves as a boy/man, a girl/woman or non-binary. Since birth, we're told what's appropriate (and what isn't) for each gender. These are the gender roles. It's not the same gender than gender role: the gender determines how you interact with the other roles. For example, a woman can take traditionally understood male roles gender roles, that doesn't mean that she is or isn't a woman. The problem arises when these two oppressions are mixed up: cissexism (the believe that bodies have an immutable gender defined by the sex assigned at birth) and misogyny (the base of feminine oppression). When you mixing them up you get the idea that the trans movement erases the feminine structural oppression, when in reality, it broadens the scope and makes it more precise, as they suffer the same misogyny than the cis women. Women are killed for being women. They are socially assigned the responsibility for care, they are prevented from having individual will and they are deterred from accessing resources. This structural violence is suffered by all women regardless of the sex assigned at birth. Questioning the adjudication of gender to the bodies and questioning the roles assigned to the genders are complementary paths for the feminism liberation.", "title": "Sex is a medical category and gender a social category"}, {"location": "antitransphobia/#avoid-the-interested-manipulation-of-the-sexual-or-gender-identity", "text": "The sexual or gender identity determines whether there is correspondence with the gender assigned at birth. When there isn't, it concerns a trans person. The sex and gender terms represent the same reality, being sex the medical term, and gender the academic one. Equally transexual and transgender represent the same reality, although these last have a pathologizing undertone, the term trans is preferred.", "title": "Avoid the interested manipulation of the sexual or gender identity"}, {"location": "antitransphobia/#avoid-the-fears-of-letting-trans-people-be", "text": "Some are afraid that the trans women negatively affect the statistics of unemployment, laboral inequality, feminization of the poverty and machist violence, and they contradict the problems of the cis women. Trans people usually have a greater unemployment rate (85% in Spain), so the glass ceiling is not yet even a concern, and they are also greatly affected by machist violence. The queer theory doesn't erase or blur women as a political subject. Thinking that it risks the rights and achievements earned through the feminist movement shows a complete misunderstanding of the theory.", "title": "Avoid the fears of letting trans people be"}, {"location": "antitransphobia/#women-are-not-an-entity", "text": "Women are not an entity, they are a group of people that are placed below men in the social scale, each with her own unique experience. The woman identity belongs to any person that identifies herself with it. The fight against discrimination and towards inclusion politics should be mandatory for all society, and shouldn't be used against the trans people.", "title": "Women are not an entity"}, {"location": "antitransphobia/#references", "text": "Wikipedia page on Transphobia", "title": "References"}, {"location": "asyncio/", "text": "asyncio is a library to write concurrent code using the async/await syntax. asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc. asyncio is often a perfect fit for IO-bound and high-level structured network code. Note \" Asyncer looks very useful\" Tips \u2691 Limit concurrency \u2691 Use asyncio.Semaphore . sem = asyncio . Semaphore ( 10 ) async with sem : # work with shared resource Note that this method is not thread-safe. References \u2691 Docs Awesome Asyncio Roguelynn tutorial Libraries to explore \u2691 Asyncer", "title": "asyncio"}, {"location": "asyncio/#tips", "text": "", "title": "Tips"}, {"location": "asyncio/#limit-concurrency", "text": "Use asyncio.Semaphore . sem = asyncio . Semaphore ( 10 ) async with sem : # work with shared resource Note that this method is not thread-safe.", "title": "Limit concurrency"}, {"location": "asyncio/#references", "text": "Docs Awesome Asyncio Roguelynn tutorial", "title": "References"}, {"location": "asyncio/#libraries-to-explore", "text": "Asyncer", "title": "Libraries to explore"}, {"location": "aws_savings_plan/", "text": "Saving plans offer a flexible pricing model that provides savings on AWS usage. You can save up to 72 percent on your AWS compute workloads. !!! note \"Please don't make Jeff Bezos even richer, try to pay as less money to AWS as you can.\" Savings Plans provide savings beyond On-Demand rates in exchange for a commitment of using a specified amount of compute power (measured per hour) for a one or three year period. When you sign up for Savings Plans, the prices you'll pay for usage stays the same through the plan term. You can pay for your commitment using All Upfront, Partial upfront, or No upfront payment options. Plan types: Compute Savings Plans provide the most flexibility and prices that are up to 66 percent off of On-Demand rates. These plans automatically apply to your EC2 instance usage, regardless of instance family (for example, m5, c5, etc.), instance sizes (for example, c5.large, c5.xlarge, etc.), Region (for example, us-east-1, us-east-2, etc.), operating system (for example, Windows, Linux, etc.), or tenancy (for example, Dedicated, default, Dedicated Host). With Compute Savings Plans, you can move a workload from C5 to M5, shift your usage from EU (Ireland) to EU (London). You can continue to benefit from the low prices provided by Compute Savings Plans as you make these changes. EC2 Instance Savings Plans provide savings up to 72 percent off On-Demand, in exchange for a commitment to a specific instance family in a chosen AWS Region (for example, M5 in Virginia). These plans automatically apply to usage regardless of size (for example, m5.xlarge, m5.2xlarge, etc.), OS (for example, Windows, Linux, etc.), and tenancy (Host, Dedicated, Default) within the specified family in a Region. With an EC2 Instance Savings Plan, you can change your instance size within the instance family (for example, from c5.xlarge to c5.2xlarge) or the operating system (for example, from Windows to Linux), or move from Dedicated tenancy to Default and continue to receive the discounted rate provided by your EC2 Instance Savings Plan. Standard Reserved Instances : The old reservation system, you reserve an instance type and you can get up to 72 percent of discount. The lack of flexibility makes them inferior to the new EC2 instance plans. Convertible Reserved Instances : Same as the Standard Reserved Instances but with more flexibility. Discounts range up to 66%, similar to the new Compute Savings Plan, which again gives more less the same discounts with more flexibility, so I wouldn't use this plan either. Understanding how Savings Plans apply to your AWS usage \u2691 If you have active Savings Plans, they apply automatically to your eligible AWS usage to reduce your bill. Savings Plans apply to your usage after the Amazon EC2 Reserved Instances (RI) are applied. Then EC2 Instance Savings Plans are applied before Compute Savings Plans because Compute Savings Plans have broader applicability. They calculate your potential savings percentages of each combination of eligible usage. This percentage compares the Savings Plans rates with your current On-Demand rates. Your Savings Plans are applied to your highest savings percentage first. If there are multiple usages with equal savings percentages, Savings Plans are applied to the first usage with the lowest Savings Plans rate. Savings Plans continue to apply until there are no more remaining usages, or your commitment is exhausted. Any remaining usage is charged at the On-Demand rates. Savings plan example \u2691 In this example, you have the following usage in a single hour: 4x r5.4xlarge Linux, shared tenancy instances in us-east-1, running for the duration of a full hour. 1x m5.24xlarge Linux, dedicated tenancy instance in us-east-1, running for the duration of a full hour. Pricing example: Type On-Demand rate Compute Savings Plans rate CSP Savings percentage EC2 Instance Savings Plans rate EC2IS percentage r5.4xlarge $1.00 $0.70 30% $0.60 40% m5.24xlarge $10.00 $8.20 18% $7.80 22% They've included other products in the example but I've removed them for the sake of simplicity Scenario 1: Savings Plan apply to all usage \u2691 You purchase a one-year, partial upfront Compute Savings Plan with a $50.00/hour commitment. Your Savings Plan covers all of your usage because multiplying each of your usages by the equivalent Compute Savings Plans is $47.13. This is still less than the $50.00/hour commitment. Without Savings Plans, you would be charged at On-Demand rates in the amount of $59.10. Scenario 2: Savings Plans apply to some usage \u2691 You purchase a one-year, partial upfront Compute Savings Plan with a $2.00/hour commitment. In any hour, your Savings Plans apply to your usage starting with the highest discount percentage (30 percent). Your $2.00/hour commitment is used to cover approximately 2.9 units of this usage. The remaining 1.1 units are charged at On-Demand rates, resulting in $1.14 of On-Demand charges for r5. The rest of your usage are also charged at On-Demand rates, resulting in $55.10 of On-Demand charges. The total On-Demand charges for this usage are $56.24. Scenario 3: Savings Plans and EC2 reserved instances apply to the usage \u2691 You purchase a one-year, partial upfront Compute Savings Plan with an $18.20/hour commitment. You have two EC2 Reserved Instances (RI) for r5.4xlarge Linux shared tenancy in us-east-1. First, the Reserve Instances covers two of the r5.4xlarge instances. Then, the Savings Plans rate is applied to the remaining r5.4xlarge and the rest of the usage, which exhausts the hourly commitment of $18.20. Scenario 4: Multiple Savings Plans apply to the usage \u2691 You purchase a one-year, partial upfront EC2 Instance Family Savings Plan for the r5 family in us-east-1 with a $3.00/hour commitment. You also have a one-year, partial upfront Compute Savings Plan with a $16.80/hour commitment. Your EC2 Instance Family Savings Plan (r5, us-east-1) covers all of the r5.4xlarge usage because multiplying the usage by the EC2 Instance Family Savings Plan rate is $2.40. This is less than the $3.00/hour commitment. Next, the Compute Savings Plan is applied to rest of the resource usage, if it doesn't cover the whole expense, then On demand rates will apply. Monitoring the savings plan \u2691 Monitoring is an important part of your Savings Plans usage. Understanding the Savings Plan that you own, how they are applying to your usage, and what usage is being covered are important parts of optimizing your costs with Savings Plans. You can monitor your usage in multiple forms. Using the inventory : The Savings Plans Inventory page shows a detailed overview of the Savings Plans that you own, or have queued for future purchase. To view your Inventory page: Open the AWS Cost Management console . In the navigation pane, under Savings Plans, choose Inventory. Using the utilization report : Savings Plans utilization shows you the percentage of your Savings Plans commitment that you're using across your On-Demand usage. You can use your Savings Plans utilization report to visually understand how your Savings Plans apply to your usage over the configured time period. Along with a visualized graph, the report shows high-level metrics based on your selected Savings Plan, filters, and lookback periods. Utilization is calculated based on how your Savings Plans applied to your usage over the lookback period. For example, if you have a $10/ hour commitment, and your usage billed with Savings Plans rates totals to $9.80 for the hour, your utilization for that hour is 98 percent. You can find high-level metrics in the Utilization report section: On-Demand Spend Equivalent : The amount you would have spent on the same usage if you didn\u2019t commit to Savings Plans. This amount is the equivalent On-Demand cost based on current On-Demand rates. Savings Plans spend : Your Savings Plans commitment spend over the lookback period. Total Net Savings : The amount you saved using Savings Plans commitments over the selected time period, compared to the On-Demand cost estimate. To access your utilization report: Open the AWS Cost Management console . On the navigation pane, choose Savings Plans. In the left pane, choose Utilization report. Using the coverage report : The Savings Plans coverage report shows how much of your eligible spend was covered by your Savings Plans, based on the selected time period. You can find the following high-level metrics in the Coverage report section: Average Coverage : The aggregated Savings Plans coverage percentage based on the selected filters and look-back period. Additional potential savings : Your potential savings amount based on your Savings Plans recommendations. This is shown as a monthly amount. On-Demand spend not covered : The amount of eligible savings spend that was not covered by Savings Plans or Reserved Instances over the lookback period. To access your utilization report: Open the AWS Cost Management console . On the navigation pane, choose Savings Plans. In the left pane, choose Coverage report. Doing your savings plan \u2691 Go to the AWS savings plan simulator and check the different instances you were evaluating.", "title": "AWS Savings plan"}, {"location": "aws_savings_plan/#understanding-how-savings-plans-apply-to-your-aws-usage", "text": "If you have active Savings Plans, they apply automatically to your eligible AWS usage to reduce your bill. Savings Plans apply to your usage after the Amazon EC2 Reserved Instances (RI) are applied. Then EC2 Instance Savings Plans are applied before Compute Savings Plans because Compute Savings Plans have broader applicability. They calculate your potential savings percentages of each combination of eligible usage. This percentage compares the Savings Plans rates with your current On-Demand rates. Your Savings Plans are applied to your highest savings percentage first. If there are multiple usages with equal savings percentages, Savings Plans are applied to the first usage with the lowest Savings Plans rate. Savings Plans continue to apply until there are no more remaining usages, or your commitment is exhausted. Any remaining usage is charged at the On-Demand rates.", "title": "Understanding how Savings Plans apply to your AWS usage"}, {"location": "aws_savings_plan/#savings-plan-example", "text": "In this example, you have the following usage in a single hour: 4x r5.4xlarge Linux, shared tenancy instances in us-east-1, running for the duration of a full hour. 1x m5.24xlarge Linux, dedicated tenancy instance in us-east-1, running for the duration of a full hour. Pricing example: Type On-Demand rate Compute Savings Plans rate CSP Savings percentage EC2 Instance Savings Plans rate EC2IS percentage r5.4xlarge $1.00 $0.70 30% $0.60 40% m5.24xlarge $10.00 $8.20 18% $7.80 22% They've included other products in the example but I've removed them for the sake of simplicity", "title": "Savings plan example"}, {"location": "aws_savings_plan/#scenario-1-savings-plan-apply-to-all-usage", "text": "You purchase a one-year, partial upfront Compute Savings Plan with a $50.00/hour commitment. Your Savings Plan covers all of your usage because multiplying each of your usages by the equivalent Compute Savings Plans is $47.13. This is still less than the $50.00/hour commitment. Without Savings Plans, you would be charged at On-Demand rates in the amount of $59.10.", "title": "Scenario 1: Savings Plan apply to all usage"}, {"location": "aws_savings_plan/#scenario-2-savings-plans-apply-to-some-usage", "text": "You purchase a one-year, partial upfront Compute Savings Plan with a $2.00/hour commitment. In any hour, your Savings Plans apply to your usage starting with the highest discount percentage (30 percent). Your $2.00/hour commitment is used to cover approximately 2.9 units of this usage. The remaining 1.1 units are charged at On-Demand rates, resulting in $1.14 of On-Demand charges for r5. The rest of your usage are also charged at On-Demand rates, resulting in $55.10 of On-Demand charges. The total On-Demand charges for this usage are $56.24.", "title": "Scenario 2: Savings Plans apply to some usage"}, {"location": "aws_savings_plan/#scenario-3-savings-plans-and-ec2-reserved-instances-apply-to-the-usage", "text": "You purchase a one-year, partial upfront Compute Savings Plan with an $18.20/hour commitment. You have two EC2 Reserved Instances (RI) for r5.4xlarge Linux shared tenancy in us-east-1. First, the Reserve Instances covers two of the r5.4xlarge instances. Then, the Savings Plans rate is applied to the remaining r5.4xlarge and the rest of the usage, which exhausts the hourly commitment of $18.20.", "title": "Scenario 3: Savings Plans and EC2 reserved instances apply to the usage"}, {"location": "aws_savings_plan/#scenario-4-multiple-savings-plans-apply-to-the-usage", "text": "You purchase a one-year, partial upfront EC2 Instance Family Savings Plan for the r5 family in us-east-1 with a $3.00/hour commitment. You also have a one-year, partial upfront Compute Savings Plan with a $16.80/hour commitment. Your EC2 Instance Family Savings Plan (r5, us-east-1) covers all of the r5.4xlarge usage because multiplying the usage by the EC2 Instance Family Savings Plan rate is $2.40. This is less than the $3.00/hour commitment. Next, the Compute Savings Plan is applied to rest of the resource usage, if it doesn't cover the whole expense, then On demand rates will apply.", "title": "Scenario 4: Multiple Savings Plans apply to the usage"}, {"location": "aws_savings_plan/#monitoring-the-savings-plan", "text": "Monitoring is an important part of your Savings Plans usage. Understanding the Savings Plan that you own, how they are applying to your usage, and what usage is being covered are important parts of optimizing your costs with Savings Plans. You can monitor your usage in multiple forms. Using the inventory : The Savings Plans Inventory page shows a detailed overview of the Savings Plans that you own, or have queued for future purchase. To view your Inventory page: Open the AWS Cost Management console . In the navigation pane, under Savings Plans, choose Inventory. Using the utilization report : Savings Plans utilization shows you the percentage of your Savings Plans commitment that you're using across your On-Demand usage. You can use your Savings Plans utilization report to visually understand how your Savings Plans apply to your usage over the configured time period. Along with a visualized graph, the report shows high-level metrics based on your selected Savings Plan, filters, and lookback periods. Utilization is calculated based on how your Savings Plans applied to your usage over the lookback period. For example, if you have a $10/ hour commitment, and your usage billed with Savings Plans rates totals to $9.80 for the hour, your utilization for that hour is 98 percent. You can find high-level metrics in the Utilization report section: On-Demand Spend Equivalent : The amount you would have spent on the same usage if you didn\u2019t commit to Savings Plans. This amount is the equivalent On-Demand cost based on current On-Demand rates. Savings Plans spend : Your Savings Plans commitment spend over the lookback period. Total Net Savings : The amount you saved using Savings Plans commitments over the selected time period, compared to the On-Demand cost estimate. To access your utilization report: Open the AWS Cost Management console . On the navigation pane, choose Savings Plans. In the left pane, choose Utilization report. Using the coverage report : The Savings Plans coverage report shows how much of your eligible spend was covered by your Savings Plans, based on the selected time period. You can find the following high-level metrics in the Coverage report section: Average Coverage : The aggregated Savings Plans coverage percentage based on the selected filters and look-back period. Additional potential savings : Your potential savings amount based on your Savings Plans recommendations. This is shown as a monthly amount. On-Demand spend not covered : The amount of eligible savings spend that was not covered by Savings Plans or Reserved Instances over the lookback period. To access your utilization report: Open the AWS Cost Management console . On the navigation pane, choose Savings Plans. In the left pane, choose Coverage report.", "title": "Monitoring the savings plan"}, {"location": "aws_savings_plan/#doing-your-savings-plan", "text": "Go to the AWS savings plan simulator and check the different instances you were evaluating.", "title": "Doing your savings plan"}, {"location": "aws_snippets/", "text": "Find if external IP belongs to you \u2691 You can list the network interfaces that match the IP you're searching for aws ec2 describe-network-interfaces --filters Name = association.public-ip,Values = \"{{ your_ip_address}}\"", "title": "AWS Snippets"}, {"location": "aws_snippets/#find-if-external-ip-belongs-to-you", "text": "You can list the network interfaces that match the IP you're searching for aws ec2 describe-network-interfaces --filters Name = association.public-ip,Values = \"{{ your_ip_address}}\"", "title": "Find if external IP belongs to you"}, {"location": "aws_waf/", "text": "AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting. You can also customize rules that filter out specific traffic patterns. Extracting information \u2691 You can configure the WAF to write it's logs into S3, Kinesis or a Cloudwatch log group. S3 saves the data in small compressed files which are difficult to analyze, Kinesis makes sense if you post-process the data on a log system such as graylog , the last one allows you to use the WAF's builtin cloudwatch log insights which has the next interesting reports: . * Top 100 Ip addresses * Top 100 countries * Top 100 hosts * Top 100 terminating rules . Nevertheless, it still lacks some needed reports to analyze the traffic. But it's quite easy to build them yourself in Cloudwatch Log Insights. If you have time I'd always suggest to avoid using proprietary AWS tools, but sadly it's the quickest way to get results. Creating Log Insights queries \u2691 Inside the Cloudwatch site, on the left menu you'll see the Logs tab, and under it Log Insights . There you can write the query you want to run. Once it returns the expected result, you can save it. Saved queries can be seen on the right menu, under Queries . If you later change the query, you'll see a blue dot beside the query you last run. The query will remain changed until you click on Actions and then Reset . Useful Queries \u2691 Top IPs \u2691 Is a directory to save the queries to analyze a count of requests aggregated by ips. Top IPs query \u2691 fields httpRequest.clientIp | stats count(*) as requestCount by httpRequest.clientIp | sort requestCount desc | limit 100 Top IPs by uri \u2691 fields httpRequest.clientIp | filter httpRequest.uri like \"/\" | stats count(*) as requestCount by httpRequest.clientIp | sort requestCount desc | limit 100 Top URIs \u2691 Is a directory to save the queries to analyze a count of requests aggregated by uris. Top URIs query \u2691 This report shows all the uris that are allowed to pass the WAF. fields httpRequest.uri | filter action like \"ALLOW\" | stats count(*) as requestCount by httpRequest.uri | sort requestCount desc | limit 100 Top URIs of a termination rule \u2691 fields httpRequest.uri | filter terminatingRuleId like \"AWS-AWSManagedRulesUnixRuleSet\" | stats count(*) as requestCount by httpRequest.uri | sort requestCount desc | limit 100 Top URIs of an IP \u2691 fields httpRequest.uri | filter @message like \"6.132.241.132\" | stats count(*) as requestCount by httpRequest.uri | sort requestCount desc | limit 100 Top URIs of a Cloudfront ID \u2691 fields httpRequest.uri | filter httpSourceId like \"CLOUDFRONT_ID\" | stats count(*) as requestCount by httpRequest.uri | sort requestCount desc | limit 100 WAF Top terminating rules \u2691 Report that shows the top rules that are blocking the content. fields terminatingRuleId | filter terminatingRuleId not like \"Default_Action\" | stats count(*) as requestCount by terminatingRuleId | sort requestCount desc | limit 100 Top blocks by Cloudfront ID \u2691 fields httpSourceId | filter terminatingRuleId not like \"Default_Action\" | stats count(*) as requestCount by httpSourceId | sort requestCount desc | limit 100 Top allows by Cloudfront ID \u2691 fields httpSourceId | filter terminatingRuleId like \"Default_Action\" | stats count(*) as requestCount by httpSourceId | sort requestCount desc | limit 100 WAF Top countries \u2691 fields httpRequest.country | stats count(*) as requestCount by httpRequest.country | sort requestCount desc | limit 100 Requests by \u2691 Is a directory to save the queries to show the requests filtered by a criteria. Requests by IP \u2691 fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId | sort @timestamp desc | filter @message like \"6.132.241.132\" | limit 100 Requests by termination rule \u2691 fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId | sort @timestamp desc | filter terminatingRuleId like \"AWS-AWSManagedRulesUnixRuleSet\" | limit 100 Requests by URI \u2691 fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId | sort @timestamp desc | filter httpRequest.uri like \"wp-json\" | limit 100 WAF Top Args of an URI \u2691 fields httpRequest.args | filter httpRequest.uri like \"/\" | stats count(*) as requestCount by httpRequest.args | sort requestCount desc | limit 100 Analysis workflow \u2691 To analyze the WAF insights you can: Analyze the traffic of the top IPs Analyze the top URIs Analyze the terminating rules Analyze the traffic of the top IPS \u2691 For IP in the WAF Top IPs report, do: Analyze the top uris of that IP to see if they are legit requests or if it contains malicious requests. If you want to get the details of a particular request, you can use the requests by uri report. For request in malicious requests: If it's not being filtered by the WAF update your WAF rules. If the IP is malicious mark it as problematic . Analyze the top uris \u2691 For uri in the WAF Top URIs report, do: For argument in the top arguments of that uri report, see if they are legit requests or if it's malicious. If you want to get the details of a particular request, you can use the requests by uri report. For request in malicious requests: If it's not being filtered by the WAF update your WAF rules. For IP in top uris by IP report: Mark IP as problematic . Analyze the terminating rules \u2691 For terminating rule in the WAF Top terminating rules report, do: For IP in the top ips by termination rule mark it as problematic . After some time you can see which rules are not being triggered and remove them. With the requests by termination rule you can see which requests are being blocked and try to block it in another rule set and merge both. Mark IP as problematic \u2691 To process an problematic IP: Add it to the captcha list. If it is already in the captcha list and is still triggering problematic requests, add it to the block list. Add a task to remove the IP from the block or captcha list X minutes or hours in the future. References \u2691 Homepage", "title": "WAF"}, {"location": "aws_waf/#extracting-information", "text": "You can configure the WAF to write it's logs into S3, Kinesis or a Cloudwatch log group. S3 saves the data in small compressed files which are difficult to analyze, Kinesis makes sense if you post-process the data on a log system such as graylog , the last one allows you to use the WAF's builtin cloudwatch log insights which has the next interesting reports: . * Top 100 Ip addresses * Top 100 countries * Top 100 hosts * Top 100 terminating rules . Nevertheless, it still lacks some needed reports to analyze the traffic. But it's quite easy to build them yourself in Cloudwatch Log Insights. If you have time I'd always suggest to avoid using proprietary AWS tools, but sadly it's the quickest way to get results.", "title": "Extracting information"}, {"location": "aws_waf/#creating-log-insights-queries", "text": "Inside the Cloudwatch site, on the left menu you'll see the Logs tab, and under it Log Insights . There you can write the query you want to run. Once it returns the expected result, you can save it. Saved queries can be seen on the right menu, under Queries . If you later change the query, you'll see a blue dot beside the query you last run. The query will remain changed until you click on Actions and then Reset .", "title": "Creating Log Insights queries"}, {"location": "aws_waf/#useful-queries", "text": "", "title": "Useful Queries"}, {"location": "aws_waf/#top-ips", "text": "Is a directory to save the queries to analyze a count of requests aggregated by ips.", "title": "Top IPs"}, {"location": "aws_waf/#top-ips-query", "text": "fields httpRequest.clientIp | stats count(*) as requestCount by httpRequest.clientIp | sort requestCount desc | limit 100", "title": "Top IPs query"}, {"location": "aws_waf/#top-ips-by-uri", "text": "fields httpRequest.clientIp | filter httpRequest.uri like \"/\" | stats count(*) as requestCount by httpRequest.clientIp | sort requestCount desc | limit 100", "title": "Top IPs by uri"}, {"location": "aws_waf/#top-uris", "text": "Is a directory to save the queries to analyze a count of requests aggregated by uris.", "title": "Top URIs"}, {"location": "aws_waf/#top-uris-query", "text": "This report shows all the uris that are allowed to pass the WAF. fields httpRequest.uri | filter action like \"ALLOW\" | stats count(*) as requestCount by httpRequest.uri | sort requestCount desc | limit 100", "title": "Top URIs query"}, {"location": "aws_waf/#top-uris-of-a-termination-rule", "text": "fields httpRequest.uri | filter terminatingRuleId like \"AWS-AWSManagedRulesUnixRuleSet\" | stats count(*) as requestCount by httpRequest.uri | sort requestCount desc | limit 100", "title": "Top URIs of a termination rule"}, {"location": "aws_waf/#top-uris-of-an-ip", "text": "fields httpRequest.uri | filter @message like \"6.132.241.132\" | stats count(*) as requestCount by httpRequest.uri | sort requestCount desc | limit 100", "title": "Top URIs of an IP"}, {"location": "aws_waf/#top-uris-of-a-cloudfront-id", "text": "fields httpRequest.uri | filter httpSourceId like \"CLOUDFRONT_ID\" | stats count(*) as requestCount by httpRequest.uri | sort requestCount desc | limit 100", "title": "Top URIs of a Cloudfront ID"}, {"location": "aws_waf/#waf-top-terminating-rules", "text": "Report that shows the top rules that are blocking the content. fields terminatingRuleId | filter terminatingRuleId not like \"Default_Action\" | stats count(*) as requestCount by terminatingRuleId | sort requestCount desc | limit 100", "title": "WAF Top terminating rules"}, {"location": "aws_waf/#top-blocks-by-cloudfront-id", "text": "fields httpSourceId | filter terminatingRuleId not like \"Default_Action\" | stats count(*) as requestCount by httpSourceId | sort requestCount desc | limit 100", "title": "Top blocks by Cloudfront ID"}, {"location": "aws_waf/#top-allows-by-cloudfront-id", "text": "fields httpSourceId | filter terminatingRuleId like \"Default_Action\" | stats count(*) as requestCount by httpSourceId | sort requestCount desc | limit 100", "title": "Top allows by Cloudfront ID"}, {"location": "aws_waf/#waf-top-countries", "text": "fields httpRequest.country | stats count(*) as requestCount by httpRequest.country | sort requestCount desc | limit 100", "title": "WAF Top countries"}, {"location": "aws_waf/#requests-by", "text": "Is a directory to save the queries to show the requests filtered by a criteria.", "title": "Requests by"}, {"location": "aws_waf/#requests-by-ip", "text": "fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId | sort @timestamp desc | filter @message like \"6.132.241.132\" | limit 100", "title": "Requests by IP"}, {"location": "aws_waf/#requests-by-termination-rule", "text": "fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId | sort @timestamp desc | filter terminatingRuleId like \"AWS-AWSManagedRulesUnixRuleSet\" | limit 100", "title": "Requests by termination rule"}, {"location": "aws_waf/#requests-by-uri", "text": "fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId | sort @timestamp desc | filter httpRequest.uri like \"wp-json\" | limit 100", "title": "Requests by URI"}, {"location": "aws_waf/#waf-top-args-of-an-uri", "text": "fields httpRequest.args | filter httpRequest.uri like \"/\" | stats count(*) as requestCount by httpRequest.args | sort requestCount desc | limit 100", "title": "WAF Top Args of an URI"}, {"location": "aws_waf/#analysis-workflow", "text": "To analyze the WAF insights you can: Analyze the traffic of the top IPs Analyze the top URIs Analyze the terminating rules", "title": "Analysis workflow"}, {"location": "aws_waf/#analyze-the-traffic-of-the-top-ips", "text": "For IP in the WAF Top IPs report, do: Analyze the top uris of that IP to see if they are legit requests or if it contains malicious requests. If you want to get the details of a particular request, you can use the requests by uri report. For request in malicious requests: If it's not being filtered by the WAF update your WAF rules. If the IP is malicious mark it as problematic .", "title": "Analyze the traffic of the top IPS"}, {"location": "aws_waf/#analyze-the-top-uris", "text": "For uri in the WAF Top URIs report, do: For argument in the top arguments of that uri report, see if they are legit requests or if it's malicious. If you want to get the details of a particular request, you can use the requests by uri report. For request in malicious requests: If it's not being filtered by the WAF update your WAF rules. For IP in top uris by IP report: Mark IP as problematic .", "title": "Analyze the top uris"}, {"location": "aws_waf/#analyze-the-terminating-rules", "text": "For terminating rule in the WAF Top terminating rules report, do: For IP in the top ips by termination rule mark it as problematic . After some time you can see which rules are not being triggered and remove them. With the requests by termination rule you can see which requests are being blocked and try to block it in another rule set and merge both.", "title": "Analyze the terminating rules"}, {"location": "aws_waf/#mark-ip-as-problematic", "text": "To process an problematic IP: Add it to the captcha list. If it is already in the captcha list and is still triggering problematic requests, add it to the block list. Add a task to remove the IP from the block or captcha list X minutes or hours in the future.", "title": "Mark IP as problematic"}, {"location": "aws_waf/#references", "text": "Homepage", "title": "References"}, {"location": "bash_snippets/", "text": "Do the remainder or modulus of a number \u2691 expr 5 % 3 Update a json file with jq \u2691 Save the next snippet to a file, for example jqr and add it to your $PATH . #!/bin/zsh query = \" $1 \" file = $2 temp_file = \" $( mktemp ) \" # Update the content jq \" $query \" $file > \" $temp_file \" # Check if the file has changed cmp -s \" $file \" \" $temp_file \" if [[ $? -eq 0 ]] ; then /bin/rm \" $temp_file \" else /bin/mv \" $temp_file \" \" $file \" fi Imagine you have the next json file: { \"property\" : true , \"other_property\" : \"value\" } Then you can run: jqr '.property = false' status.json And then you'll have: { \"property\" : false , \"other_property\" : \"value\" }", "title": "Bash snippets"}, {"location": "bash_snippets/#do-the-remainder-or-modulus-of-a-number", "text": "expr 5 % 3", "title": "Do the remainder or modulus of a number"}, {"location": "bash_snippets/#update-a-json-file-with-jq", "text": "Save the next snippet to a file, for example jqr and add it to your $PATH . #!/bin/zsh query = \" $1 \" file = $2 temp_file = \" $( mktemp ) \" # Update the content jq \" $query \" $file > \" $temp_file \" # Check if the file has changed cmp -s \" $file \" \" $temp_file \" if [[ $? -eq 0 ]] ; then /bin/rm \" $temp_file \" else /bin/mv \" $temp_file \" \" $file \" fi Imagine you have the next json file: { \"property\" : true , \"other_property\" : \"value\" } Then you can run: jqr '.property = false' status.json And then you'll have: { \"property\" : false , \"other_property\" : \"value\" }", "title": "Update a json file with jq"}, {"location": "beancount/", "text": "Beancount is a Python double entry accounting command line tool similar to ledger . Installation \u2691 pip3 install beancount Tools \u2691 beancount is the core component, it's a declarative language. It parses a text file, and produces reports from the resulting data structures. bean-check \u2691 bean-check is the program you use to verify that your input syntax and transactions work correctly. All it does is load your input file and run the various plugins you configured in it, plus some extra validation checks. bean-check /path/to/file.beancount If there are no errors, there should be no output, it should exit quietly. bean-report \u2691 This is the main tool used to extract specialized reports to the console in text or one of the various other formats. For a graphic exploration of your data, use the fava web application instead. bean-report /path/to/file.beancount {{ report_name }} There are many reports available, to get a full list run bean-report --help-reports Report names sometimes may accept arguments, if they do so use : bean-report /path/to/file.beancount balances:Vanguard To get the balances \u2691 bean-report {{ path/to/file.beancount }} balances | treeify To get the journal \u2691 bean-report {{ path/to/file.beancount }} journal To get the holdings \u2691 To get the aggregations for the total list of holdings bean-report {{ path/to/file.beancount }} holdings To get the accounts \u2691 bean-report {{ path/to/file.beancount }} accounts bean-query \u2691 bean-query is a command-line tool that acts like a client to that in-memory database in which you can type queries in a variant of SQL. It has it's own document bean-query /path/to/file.beancount bean-web \u2691 Deprecated use fava instead bean-web serves all the reports on a web server that runs on your computer bean-web /path/to/file.beancount It will serve on localhost:8080 bean-doctor \u2691 This is a debugging tool used to perform various diagnostics and run debugging commands, and to help provide information for reporting bugs. bean-format \u2691 Pure text processing tool will reformat Beancount input to right-align all the numbers at the same, minimal column. bean-example \u2691 Generates an example Beancount input file. bean-identify \u2691 Given a messy list of downloaded files automatically identify which of your configured importers is able to handle them and print them out. This is to be used for debugging and figuring out if your configuration is properly associating a suitable importer for each of the files you downloaded. bean-extract \u2691 Extracts transactions and statement date from each file, if at all possible. This produces some Beancount input text to be moved to your input file. bean-extract {{ path/to/config.config }} {{ path/to/source/files }} The tool calls methods on importer objects. You must provide a list of such importers; this list is the configuration for the importing process. For each file found, each of the importers is called to assert whether it can or cannot handle that file. If it deems that it can, methods can be called to produce a list of transactions extract a date, or produce a cleaned up filename for the downloaded file. The configuration should be a python3 module in which you instantiate the importers and assign the list to the module-level \"CONFIG\" variable #!/usr/bin/env python3 from myimporters.bank import acmebank from myimporters.bank import chase \u2026 CONFIG = [ acmebank . Importer (), chase . Importer (), \u2026 ] Writing an importer \u2691 Each of the importers must comply with a particular protocol and implement at least some of its methods. The full detail of the protocol is in the source of importer.py \"\"\"Importer protocol. All importers must comply with this interface and implement at least some of its methods. A configuration consists in a simple list of such importer instances. The importer processes run through the importers, calling some of its methods in order to identify, extract and file the downloaded files. Each of the methods accept a cache.FileMemo object which has a 'name' attribute with the filename to process, but which also provides a place to cache conversions. Use its convert() method whenever possible to avoid carrying out the same conversion multiple times. See beancount.ingest.cache for more details. Synopsis: name(): Return a unique identifier for the importer instance. identify(): Return true if the identifier is able to process the file. extract(): Extract directives from a file's contents and return of list of entries. file_account(): Return an account name associated with the given file for this importer. file_date(): Return a date associated with the downloaded file (e.g., the statement date). file_name(): Return a cleaned up filename for storage (optional). Just to be clear: Although this importer will not raise NotImplementedError exceptions (it returns default values for each method), you NEED to derive from it in order to do anything meaningful. Simply instantiating this importer will not match not provide any useful information. It just defines the protocol for all importers. \"\"\" __copyright__ = \"Copyright (C) 2016 Martin Blais\" __license__ = \"GNU GPLv2\" from beancount.core import flags class ImporterProtocol : \"Interface that all source importers need to comply with.\" # A flag to use on new transaction. Override this flag in derived classes if # you prefer to create your imported transactions with a different flag. FLAG = flags . FLAG_OKAY def name ( self ): \"\"\"Return a unique id/name for this importer. Returns: A string which uniquely identifies this importer. \"\"\" cls = self . __class__ return ' {} . {} ' . format ( cls . __module__ , cls . __name__ ) __str__ = name def identify ( self , file ): \"\"\"Return true if this importer matches the given file. Args: file: A cache.FileMemo instance. Returns: A boolean, true if this importer can handle this file. \"\"\" def extract ( self , file ): \"\"\"Extract transactions from a file. Args: file: A cache.FileMemo instance. Returns: A list of new, imported directives (usually mostly Transactions) extracted from the file. \"\"\" def file_account ( self , file ): \"\"\"Return an account associated with the given file. Note: If you don't implement this method you won't be able to move the files into its preservation hierarchy; the bean-file command won't work. Also, normally the returned account is not a function of the input file--just of the importer--but it is provided anyhow. Args: file: A cache.FileMemo instance. Returns: The name of the account that corresponds to this importer. \"\"\" def file_name ( self , file ): \"\"\"A filter that optionally renames a file before filing. This is used to make tidy filenames for filed/stored document files. The default implementation just returns the same filename. Note that a simple RELATIVE filename must be returned, not an absolute filename. Args: file: A cache.FileMemo instance. Returns: The tidied up, new filename to store it as. \"\"\" def file_date ( self , file ): \"\"\"Attempt to obtain a date that corresponds to the given file. Args: file: A cache.FileMemo instance. Returns: A date object, if successful, or None if a date could not be extracted. (If no date is returned, the file creation time is used. This is the default.) A summary of the methods you need to, or may want to implement: name() : Provides a unique id for each importer instance. It's convenient to be able to refer to your importers with a unique name; it gets printed out by the identification process. identify() : This method just returns true if this importer can handle the given file. You must implement this method , and all the tools invoke it ot figure out the list of (file, importer) pairs. extract() : This is called to attempt to extract some Beancount directives from the file contents. It must create the directives by instatiating the objects define in beancout.core.data and return them. from beancount.ingest import importer class Importer ( importer . ImporterProtocol ): def identify ( self , file ): \u2026 # Override other methods\u2026 Some importer examples: mterwill gist wzyboy importers bean-file \u2691 bean-file filing documents. It si able to identify which document belongs to which account, it can move the downloaded file to the documents archive automatically. Basic concepts \u2691 Beancount transaction \u2691 2014-05-23 * \"CAFE MOGADOR NEW YO\" \"Dinner with Caroline\" Liabilities:US:BofA:CreditCard -98.32 USD Expenses:Restaurant Currencies must be entirely in capital letters. Account names do not admit spaces. Description strings must be quoted. Dates are only parsed in YYYY-MM-DD format. Tags must begin with # and links with ^ . Beancount Operators \u2691 Open \u2691 All accounts need to be declared open in order to accept amounts posted to them. YYYY-MM-DD open {{ account_name }} [{{ ConstrainCurrency }}] Close \u2691 YYYY-MM-DD close {{ account_name }} It's useful to insert a balance of 0 units just before closing an account, just to make sure its contents are empty as you close it. Commodity \u2691 It can be used to declare currencies, financial instruments, commodities... It's optional YYYY-MM-DD commodity {{ currency_name }} Transactions \u2691 YYYY-MM-DD txn \"[{{ payee }}]\" \"{{ Comment }}\" {{ Account1 }} {{ value}} [{{ Accountn-1 }} {{ value }}] {{ Accountn }} Payee is a string that represents an external entity that is involved in the transaction. Payees are sometimes useful on transactions that post amounts to Expense accounts, whereby the account accumulates a category of expenses from multiple business As transactions is the most common, you can substitute txn for a flag, by default : * * : Completed transaction, known amounts, \"this looks correct\" * ! : Incomplete transaction, needs confirmation or revision, \"this looks incorrect\" You can also attach flags to the postings themselves, if you want to flag one of the transaction's legs in particular: 2014-05-05 * \"Transfer from Savings account\" Assets:MyBank:Checking -400.00 USD ! Assets:MyBank:Savings This is useful in the intermediate stage of de-duping transactions Tags vs Payee \u2691 You can tag your transactions with #{{tag_name}} , so you can later filter or generate reports based on that tag. Therefore the Payee could be used as whom or who pays and the tag for the context. For example, for a trip I could use the tag #34C3 To mark a series of transactions with tags use the following syntax pushtag #berlin-trip-2014 2014-04-23 * \"Flight to Berlin\" Expenses:Flights -1230.27 USD Liabilities:CreditCard ... poptag #berlin-trip-2014 Links \u2691 Transactions can also be linked together. You may think of the link as a special kind of tag that can be used to group together a set of financially related transactions over time. 2014-02-05 * \"Invoice for January\" ^invoice-acme-studios-jan14 Income:Clients:ACMEStudios -8450.00 USD Assets:AccountsReceivable ... 2014-02-20 * \"Check deposit - payment from ACME\" ^invoice-acme-studios-jan14 Assets:BofA:Checking 8450.00 USD Assets:AccountsReceivable Balance \u2691 A balance assertion is a way for you to input your statement balance into the flow of transactions. It tells Beancount to verify that the number of units of a particular commodity in some account should equal some expected value at some point in time. If no error is reported, you should have some confidence that the list of transactions that precedes it in this account is highly likely to be correct. This is useful in practice because in many cases some transactions can get imported separately from the accounts of each of their postings. As all other non-transaction directives, it applies at the beginning of it's date . Just imagine that the balance checks occurs right after midnight on that day. YYYY-MM-DD balance {{ account_name }} {{ amount }} Pad \u2691 A padding directive automatically inserts a transaction that will make the subsequent balance assertion succeed, if it is needed. It inserts the difference needed to fulfill that balance assertion. Being subsequent in date order, not in the order of the declarations in the file. YYYY-MM-DD pad {{ account_name }} {{ account_name_to_pad }} The first account is the account to credit the automatically calculated amount to. This is the account that should have a balance assertion following it. The second leg is the source where the funds will come from, and this is almost always some Equity account. 1990-05-17 open Assets:Cash EUR 1990-05-17 pad Assets:Cash Equity:Opening-Balances 2017-12-26 balance Assets:Cash 250 EUR You could also insert pad entries between balance assertions so as to fix un registered transactions Notes \u2691 A note directive is simply used to attach a dated comment to the journal of a particular account. this can be useful to record facts and claims associated with a financial event. YYYY-MM-DD note {{ account_name }} {{ comment }} Document \u2691 A Document directive can be used to attach an external file to the journal of an account. The filename gets rendered as a browser link in the journals of the web interface for the corresponding account and you should be able to click on it to view the contents of the file itself. YYYY-MM-DD {{ account_name }} {{ path/to/document }} Includes \u2691 This allows you to split up large input files into multiple files. include {{ path/to/file.beancount }} The path could be relative or absolute. Library usage \u2691 Beancount can also be used as a Python library. There are some articles in the documentation where you can start seeing how to use it: scripting plugins , external contributions and the api reference . Although I found it more pleasant to read the source code itself as it's really well documented (both by docstrings and type hints). References \u2691 Homepage Git Docs Awesome beancount Docs in google Vim plugin", "title": "beancount"}, {"location": "beancount/#installation", "text": "pip3 install beancount", "title": "Installation"}, {"location": "beancount/#tools", "text": "beancount is the core component, it's a declarative language. It parses a text file, and produces reports from the resulting data structures.", "title": "Tools"}, {"location": "beancount/#bean-check", "text": "bean-check is the program you use to verify that your input syntax and transactions work correctly. All it does is load your input file and run the various plugins you configured in it, plus some extra validation checks. bean-check /path/to/file.beancount If there are no errors, there should be no output, it should exit quietly.", "title": "bean-check"}, {"location": "beancount/#bean-report", "text": "This is the main tool used to extract specialized reports to the console in text or one of the various other formats. For a graphic exploration of your data, use the fava web application instead. bean-report /path/to/file.beancount {{ report_name }} There are many reports available, to get a full list run bean-report --help-reports Report names sometimes may accept arguments, if they do so use : bean-report /path/to/file.beancount balances:Vanguard", "title": "bean-report"}, {"location": "beancount/#to-get-the-balances", "text": "bean-report {{ path/to/file.beancount }} balances | treeify", "title": "To get the balances"}, {"location": "beancount/#to-get-the-journal", "text": "bean-report {{ path/to/file.beancount }} journal", "title": "To get the journal"}, {"location": "beancount/#to-get-the-holdings", "text": "To get the aggregations for the total list of holdings bean-report {{ path/to/file.beancount }} holdings", "title": "To get the holdings"}, {"location": "beancount/#to-get-the-accounts", "text": "bean-report {{ path/to/file.beancount }} accounts", "title": "To get the accounts"}, {"location": "beancount/#bean-query", "text": "bean-query is a command-line tool that acts like a client to that in-memory database in which you can type queries in a variant of SQL. It has it's own document bean-query /path/to/file.beancount", "title": "bean-query"}, {"location": "beancount/#bean-web", "text": "Deprecated use fava instead bean-web serves all the reports on a web server that runs on your computer bean-web /path/to/file.beancount It will serve on localhost:8080", "title": "bean-web"}, {"location": "beancount/#bean-doctor", "text": "This is a debugging tool used to perform various diagnostics and run debugging commands, and to help provide information for reporting bugs.", "title": "bean-doctor"}, {"location": "beancount/#bean-format", "text": "Pure text processing tool will reformat Beancount input to right-align all the numbers at the same, minimal column.", "title": "bean-format"}, {"location": "beancount/#bean-example", "text": "Generates an example Beancount input file.", "title": "bean-example"}, {"location": "beancount/#bean-identify", "text": "Given a messy list of downloaded files automatically identify which of your configured importers is able to handle them and print them out. This is to be used for debugging and figuring out if your configuration is properly associating a suitable importer for each of the files you downloaded.", "title": "bean-identify"}, {"location": "beancount/#bean-extract", "text": "Extracts transactions and statement date from each file, if at all possible. This produces some Beancount input text to be moved to your input file. bean-extract {{ path/to/config.config }} {{ path/to/source/files }} The tool calls methods on importer objects. You must provide a list of such importers; this list is the configuration for the importing process. For each file found, each of the importers is called to assert whether it can or cannot handle that file. If it deems that it can, methods can be called to produce a list of transactions extract a date, or produce a cleaned up filename for the downloaded file. The configuration should be a python3 module in which you instantiate the importers and assign the list to the module-level \"CONFIG\" variable #!/usr/bin/env python3 from myimporters.bank import acmebank from myimporters.bank import chase \u2026 CONFIG = [ acmebank . Importer (), chase . Importer (), \u2026 ]", "title": "bean-extract"}, {"location": "beancount/#writing-an-importer", "text": "Each of the importers must comply with a particular protocol and implement at least some of its methods. The full detail of the protocol is in the source of importer.py \"\"\"Importer protocol. All importers must comply with this interface and implement at least some of its methods. A configuration consists in a simple list of such importer instances. The importer processes run through the importers, calling some of its methods in order to identify, extract and file the downloaded files. Each of the methods accept a cache.FileMemo object which has a 'name' attribute with the filename to process, but which also provides a place to cache conversions. Use its convert() method whenever possible to avoid carrying out the same conversion multiple times. See beancount.ingest.cache for more details. Synopsis: name(): Return a unique identifier for the importer instance. identify(): Return true if the identifier is able to process the file. extract(): Extract directives from a file's contents and return of list of entries. file_account(): Return an account name associated with the given file for this importer. file_date(): Return a date associated with the downloaded file (e.g., the statement date). file_name(): Return a cleaned up filename for storage (optional). Just to be clear: Although this importer will not raise NotImplementedError exceptions (it returns default values for each method), you NEED to derive from it in order to do anything meaningful. Simply instantiating this importer will not match not provide any useful information. It just defines the protocol for all importers. \"\"\" __copyright__ = \"Copyright (C) 2016 Martin Blais\" __license__ = \"GNU GPLv2\" from beancount.core import flags class ImporterProtocol : \"Interface that all source importers need to comply with.\" # A flag to use on new transaction. Override this flag in derived classes if # you prefer to create your imported transactions with a different flag. FLAG = flags . FLAG_OKAY def name ( self ): \"\"\"Return a unique id/name for this importer. Returns: A string which uniquely identifies this importer. \"\"\" cls = self . __class__ return ' {} . {} ' . format ( cls . __module__ , cls . __name__ ) __str__ = name def identify ( self , file ): \"\"\"Return true if this importer matches the given file. Args: file: A cache.FileMemo instance. Returns: A boolean, true if this importer can handle this file. \"\"\" def extract ( self , file ): \"\"\"Extract transactions from a file. Args: file: A cache.FileMemo instance. Returns: A list of new, imported directives (usually mostly Transactions) extracted from the file. \"\"\" def file_account ( self , file ): \"\"\"Return an account associated with the given file. Note: If you don't implement this method you won't be able to move the files into its preservation hierarchy; the bean-file command won't work. Also, normally the returned account is not a function of the input file--just of the importer--but it is provided anyhow. Args: file: A cache.FileMemo instance. Returns: The name of the account that corresponds to this importer. \"\"\" def file_name ( self , file ): \"\"\"A filter that optionally renames a file before filing. This is used to make tidy filenames for filed/stored document files. The default implementation just returns the same filename. Note that a simple RELATIVE filename must be returned, not an absolute filename. Args: file: A cache.FileMemo instance. Returns: The tidied up, new filename to store it as. \"\"\" def file_date ( self , file ): \"\"\"Attempt to obtain a date that corresponds to the given file. Args: file: A cache.FileMemo instance. Returns: A date object, if successful, or None if a date could not be extracted. (If no date is returned, the file creation time is used. This is the default.) A summary of the methods you need to, or may want to implement: name() : Provides a unique id for each importer instance. It's convenient to be able to refer to your importers with a unique name; it gets printed out by the identification process. identify() : This method just returns true if this importer can handle the given file. You must implement this method , and all the tools invoke it ot figure out the list of (file, importer) pairs. extract() : This is called to attempt to extract some Beancount directives from the file contents. It must create the directives by instatiating the objects define in beancout.core.data and return them. from beancount.ingest import importer class Importer ( importer . ImporterProtocol ): def identify ( self , file ): \u2026 # Override other methods\u2026 Some importer examples: mterwill gist wzyboy importers", "title": "Writing an importer"}, {"location": "beancount/#bean-file", "text": "bean-file filing documents. It si able to identify which document belongs to which account, it can move the downloaded file to the documents archive automatically.", "title": "bean-file"}, {"location": "beancount/#basic-concepts", "text": "", "title": "Basic concepts"}, {"location": "beancount/#beancount-transaction", "text": "2014-05-23 * \"CAFE MOGADOR NEW YO\" \"Dinner with Caroline\" Liabilities:US:BofA:CreditCard -98.32 USD Expenses:Restaurant Currencies must be entirely in capital letters. Account names do not admit spaces. Description strings must be quoted. Dates are only parsed in YYYY-MM-DD format. Tags must begin with # and links with ^ .", "title": "Beancount transaction"}, {"location": "beancount/#beancount-operators", "text": "", "title": "Beancount Operators"}, {"location": "beancount/#open", "text": "All accounts need to be declared open in order to accept amounts posted to them. YYYY-MM-DD open {{ account_name }} [{{ ConstrainCurrency }}]", "title": "Open"}, {"location": "beancount/#close", "text": "YYYY-MM-DD close {{ account_name }} It's useful to insert a balance of 0 units just before closing an account, just to make sure its contents are empty as you close it.", "title": "Close"}, {"location": "beancount/#commodity", "text": "It can be used to declare currencies, financial instruments, commodities... It's optional YYYY-MM-DD commodity {{ currency_name }}", "title": "Commodity"}, {"location": "beancount/#transactions", "text": "YYYY-MM-DD txn \"[{{ payee }}]\" \"{{ Comment }}\" {{ Account1 }} {{ value}} [{{ Accountn-1 }} {{ value }}] {{ Accountn }} Payee is a string that represents an external entity that is involved in the transaction. Payees are sometimes useful on transactions that post amounts to Expense accounts, whereby the account accumulates a category of expenses from multiple business As transactions is the most common, you can substitute txn for a flag, by default : * * : Completed transaction, known amounts, \"this looks correct\" * ! : Incomplete transaction, needs confirmation or revision, \"this looks incorrect\" You can also attach flags to the postings themselves, if you want to flag one of the transaction's legs in particular: 2014-05-05 * \"Transfer from Savings account\" Assets:MyBank:Checking -400.00 USD ! Assets:MyBank:Savings This is useful in the intermediate stage of de-duping transactions", "title": "Transactions"}, {"location": "beancount/#tags-vs-payee", "text": "You can tag your transactions with #{{tag_name}} , so you can later filter or generate reports based on that tag. Therefore the Payee could be used as whom or who pays and the tag for the context. For example, for a trip I could use the tag #34C3 To mark a series of transactions with tags use the following syntax pushtag #berlin-trip-2014 2014-04-23 * \"Flight to Berlin\" Expenses:Flights -1230.27 USD Liabilities:CreditCard ... poptag #berlin-trip-2014", "title": "Tags vs Payee"}, {"location": "beancount/#links", "text": "Transactions can also be linked together. You may think of the link as a special kind of tag that can be used to group together a set of financially related transactions over time. 2014-02-05 * \"Invoice for January\" ^invoice-acme-studios-jan14 Income:Clients:ACMEStudios -8450.00 USD Assets:AccountsReceivable ... 2014-02-20 * \"Check deposit - payment from ACME\" ^invoice-acme-studios-jan14 Assets:BofA:Checking 8450.00 USD Assets:AccountsReceivable", "title": "Links"}, {"location": "beancount/#balance", "text": "A balance assertion is a way for you to input your statement balance into the flow of transactions. It tells Beancount to verify that the number of units of a particular commodity in some account should equal some expected value at some point in time. If no error is reported, you should have some confidence that the list of transactions that precedes it in this account is highly likely to be correct. This is useful in practice because in many cases some transactions can get imported separately from the accounts of each of their postings. As all other non-transaction directives, it applies at the beginning of it's date . Just imagine that the balance checks occurs right after midnight on that day. YYYY-MM-DD balance {{ account_name }} {{ amount }}", "title": "Balance"}, {"location": "beancount/#pad", "text": "A padding directive automatically inserts a transaction that will make the subsequent balance assertion succeed, if it is needed. It inserts the difference needed to fulfill that balance assertion. Being subsequent in date order, not in the order of the declarations in the file. YYYY-MM-DD pad {{ account_name }} {{ account_name_to_pad }} The first account is the account to credit the automatically calculated amount to. This is the account that should have a balance assertion following it. The second leg is the source where the funds will come from, and this is almost always some Equity account. 1990-05-17 open Assets:Cash EUR 1990-05-17 pad Assets:Cash Equity:Opening-Balances 2017-12-26 balance Assets:Cash 250 EUR You could also insert pad entries between balance assertions so as to fix un registered transactions", "title": "Pad"}, {"location": "beancount/#notes", "text": "A note directive is simply used to attach a dated comment to the journal of a particular account. this can be useful to record facts and claims associated with a financial event. YYYY-MM-DD note {{ account_name }} {{ comment }}", "title": "Notes"}, {"location": "beancount/#document", "text": "A Document directive can be used to attach an external file to the journal of an account. The filename gets rendered as a browser link in the journals of the web interface for the corresponding account and you should be able to click on it to view the contents of the file itself. YYYY-MM-DD {{ account_name }} {{ path/to/document }}", "title": "Document"}, {"location": "beancount/#includes", "text": "This allows you to split up large input files into multiple files. include {{ path/to/file.beancount }} The path could be relative or absolute.", "title": "Includes"}, {"location": "beancount/#library-usage", "text": "Beancount can also be used as a Python library. There are some articles in the documentation where you can start seeing how to use it: scripting plugins , external contributions and the api reference . Although I found it more pleasant to read the source code itself as it's really well documented (both by docstrings and type hints).", "title": "Library usage"}, {"location": "beancount/#references", "text": "Homepage Git Docs Awesome beancount Docs in google Vim plugin", "title": "References"}, {"location": "beautifulsoup/", "text": "BeautifulSoup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. import requests from bs4 import BeautifulSoup request = requests . get ( '{{ url }}' ) soup = BeautifulSoup ( request . text , \"html.parser\" ) Here are some simple ways to navigate that data structure: soup . title # <title>The Dormouse's story</title> soup . title . name # u'title' soup . title . string # u'The Dormouse's story' soup . title . parent . name # u'head' soup . p # <p class=\"title\"><b>The Dormouse's story</b></p> soup . p [ 'class' ] # u'title' soup . a # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> soup . find_all ( 'a' ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] soup . find ( id = \"link3\" ) # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a> Installation \u2691 pip install beautifulsoup4 The default parser html.parser doesn't work with HTML5, so you'll probably need to use the html5lib parser, it's not included by default, so you might need to install it as well pip install html5lib Usage \u2691 Kinds of objects \u2691 Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. But you\u2019ll only ever have to deal with about four kinds of objects: Tag , NavigableString , BeautifulSoup , and Comment . Tag \u2691 A Tag object corresponds to an XML or HTML tag in the original document: soup = BeautifulSoup ( '<b class=\"boldest\">Extremely bold</b>' ) tag = soup . b type ( tag ) # <class 'bs4.element.Tag'> The most important features of a tag are its name and attributes . Name \u2691 Every tag has a name , accessible as .name : tag . name # u'b' If you change a tag\u2019s name, the change will be reflected in any HTML markup generated by Beautiful Soup:. tag . name = \"blockquote\" tag # <blockquote class=\"boldest\">Extremely bold</blockquote> Attributes \u2691 A tag may have any number of attributes. The tag <b id=\"boldest\"> has an attribute id whose value is boldest . You can access a tag\u2019s attributes by treating the tag like a dictionary: tag [ 'id' ] # u'boldest' You can access that dictionary directly as .attrs : tag . attrs # {u'id': 'boldest'} You can add, remove, and modify a tag\u2019s attributes. Again, this is done by treating the tag as a dictionary: tag [ 'id' ] = 'verybold' tag [ 'another-attribute' ] = 1 tag # <b another-attribute=\"1\" id=\"verybold\"></b> del tag [ 'id' ] del tag [ 'another-attribute' ] tag # <b></b> tag [ 'id' ] # KeyError: 'id' print ( tag . get ( 'id' )) # None Multi-valued attributes \u2691 HTML 4 defines a few attributes that can have multiple values. HTML 5 removes a couple of them, but defines a few more. The most common multi-valued attribute is class (that is, a tag can have more than one CSS class). Others include rel , rev , accept-charset , headers , and accesskey . Beautiful Soup presents the value(s) of a multi-valued attribute as a list: css_soup = BeautifulSoup ( '<p class=\"body\"></p>' ) css_soup . p [ 'class' ] # [\"body\"] css_soup = BeautifulSoup ( '<p class=\"body strikeout\"></p>' ) css_soup . p [ 'class' ] # [\"body\", \"strikeout\"] If an attribute looks like it has more than one value, but it\u2019s not a multi-valued attribute as defined by any version of the HTML standard, Beautiful Soup will leave the attribute alone: id_soup = BeautifulSoup ( '<p id=\"my id\"></p>' ) id_soup . p [ 'id' ] # 'my id' When you turn a tag back into a string, multiple attribute values are consolidated: rel_soup = BeautifulSoup ( '<p>Back to the <a rel=\"index\">homepage</a></p>' ) rel_soup . a [ 'rel' ] # ['index'] rel_soup . a [ 'rel' ] = [ 'index' , 'contents' ] print ( rel_soup . p ) # <p>Back to the <a rel=\"index contents\">homepage</a></p> If you parse a document as XML, there are no multi-valued attributes: NavigableString \u2691 A string corresponds to a bit of text within a tag. Beautiful Soup uses the NavigableString class to contain these bits of text: tag . string # u'Extremely bold' type ( tag . string ) # <class 'bs4.element.NavigableString'> A NavigableString is just like a Python Unicode string, except that it also supports some of the features described in Navigating the tree and Searching the tree. You can convert a NavigableString to a Unicode string with unicode() : unicode_string = unicode ( tag . string ) unicode_string # u'Extremely bold' type ( unicode_string ) # <type 'unicode'> You can\u2019t edit a string in place, but you can replace one string with another, using replace_with() : tag . string . replace_with ( \"No longer bold\" ) tag # <blockquote>No longer bold</blockquote> BeautifulSoup \u2691 The BeautifulSoup object represents the parsed document as a whole. For most purposes, you can treat it as a Tag object. This means it supports most of the methods described in Navigating the tree and Searching the tree. Navigating the tree \u2691 Going down \u2691 Tags may contain strings and other tags. These elements are the tag\u2019s children. Beautiful Soup provides a lot of different attributes for navigating and iterating over a tag\u2019s children. Note that Beautiful Soup strings don\u2019t support any of these attributes, because a string can\u2019t have children. Navigating using tag names \u2691 The simplest way to navigate the parse tree is to say the name of the tag you want. If you want the <head> tag, just say soup.head : soup . head # <head><title>The Dormouse's story</title></head> soup . title # <title>The Dormouse's story</title> You can do use this trick again and again to zoom in on a certain part of the parse tree. This code gets the first <b> tag beneath the <body> tag: soup . body . b # <b>The Dormouse's story</b> Using a tag name as an attribute will give you only the first tag by that name: soup . a # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> If you need to get all the <a> tags, or anything more complicated than the first tag with a certain name, you\u2019ll need to use one of the methods described in Searching the tree, such as find_all() : soup . find_all ( 'a' ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] .contents and .children \u2691 A tag\u2019s children are available in a list called .contents : head_tag = soup . head head_tag # <head><title>The Dormouse's story</title></head> head_tag . contents [ < title > The Dormouse 's story</title>] title_tag = head_tag . contents [ 0 ] title_tag # <title>The Dormouse's story</title> title_tag . contents # [u'The Dormouse's story'] Instead of getting them as a list, you can iterate over a tag\u2019s children using the .children generator: for child in title_tag . children : print ( child ) # The Dormouse's story .descendants \u2691 The .contents and .children attributes only consider a tag\u2019s direct children. For instance, the <head> tag has a single direct child\u2013the <title> tag: head_tag . contents # [<title>The Dormouse's story</title>] But the <title> tag itself has a child: the string The Dormouse\u2019s story . There\u2019s a sense in which that string is also a child of the <head> tag. The .descendants attribute lets you iterate over all of a tag\u2019s children, recursively: its direct children, the children of its direct children, and so on:. for child in head_tag . descendants : print ( child ) # <title>The Dormouse's story</title> # The Dormouse's story .string \u2691 If a tag has only one child, and that child is a NavigableString , the child is made available as .string : title_tag . string # u'The Dormouse's story' If a tag\u2019s only child is another tag, and that tag has a .string , then the parent tag is considered to have the same .string as its child: head_tag . contents # [<title>The Dormouse's story</title>] head_tag . string # u'The Dormouse's story' If a tag contains more than one thing, then it\u2019s not clear what .string should refer to, so .string is defined to be None : print ( soup . html . string ) # None .strings and .stripped_strings \u2691 If there\u2019s more than one thing inside a tag, you can still look at just the strings. Use the .strings generator: for string in soup . strings : print ( repr ( string )) # u\"The Dormouse's story\" # u'\\n\\n' # u\"The Dormouse's story\" # u'\\n\\n' These strings tend to have a lot of extra whitespace, which you can remove by using the .stripped_strings generator instead: for string in soup . stripped_strings : print ( repr ( string )) # u\"The Dormouse's story\" # u\"The Dormouse's story\" # u'Once upon a time there were three little sisters; and their names were' # u'Elsie' Going up \u2691 Continuing the \u201cfamily tree\u201d analogy, every tag and every string has a parent: the tag that contains it. .parent \u2691 You can access an element\u2019s parent with the .parent attribute. title_tag = soup . title title_tag # <title>The Dormouse's story</title> title_tag . parent # <head><title>The Dormouse's story</title></head> .parents \u2691 You can iterate over all of an element\u2019s parents with .parents . Going sideways \u2691 When a document is pretty-printed, siblings show up at the same indentation level. You can also use this relationship in the code you write. .next_sibling and .previous_sibling \u2691 You can use .next_sibling and .previous_sibling to navigate between page elements that are on the same level of the parse tree:. sibling_soup . b . next_sibling # <c>text2</c> sibling_soup . c . previous_sibling # <b>text1</b> The <b> tag has a .next_sibling , but no .previous_sibling , because there\u2019s nothing before the <b> tag on the same level of the tree. For the same reason, the <c> tag has a .previous_sibling but no .next_sibling : print ( sibling_soup . b . previous_sibling ) # None print ( sibling_soup . c . next_sibling ) # None In real documents, the .next_sibling or .previous_sibling of a tag will usually be a string containing whitespace. < a href = \"http://example.com/elsie\" class = \"sister\" id = \"link1\" > Elsie </ a > < a href = \"http://example.com/lacie\" class = \"sister\" id = \"link2\" > Lacie </ a > < a href = \"http://example.com/tillie\" class = \"sister\" id = \"link3\" > Tillie </ a > You might think that the .next_sibling of the first <a> tag would be the second <a> tag. But actually, it\u2019s a string: the comma and newline that separate the first <a> tag from the second: link = soup . a link # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> link . next_sibling # u',\\n' The second <a> tag is actually the .next_sibling of the comma: link . next_sibling . next_sibling # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> .next_siblings and .previous_siblings \u2691 You can iterate over a tag\u2019s siblings with .next_siblings or .previous_siblings : for sibling in soup . a . next_siblings : print ( repr ( sibling )) # u',\\n' # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> # u' and\\n' # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a> # u'; and they lived at the bottom of a well.' # None for sibling in soup . find ( id = \"link3\" ) . previous_siblings : print ( repr ( sibling )) # ' and\\n' # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> # u',\\n' # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> # u'Once upon a time there were three little sisters; and their names were\\n' # None Searching the tree \u2691 By passing in a filter to an argument like find_all() , you can zoom in on the parts of the document you\u2019re interested in. Kinds of filters \u2691 A string \u2691 The simplest filter is a string. Pass a string to a search method and Beautiful Soup will perform a match against that exact string. This code finds all the <b> tags in the document: soup . find_all ( 'b' ) # [<b>The Dormouse's story</b>] A regular expression \u2691 If you pass in a regular expression object, Beautiful Soup will filter against that regular expression using its search() method. This code finds all the tags whose names start with the letter b ; in this case, the <body> tag and the <b> tag: import re for tag in soup . find_all ( re . compile ( \"^b\" )): print ( tag . name ) # body # b A list \u2691 If you pass in a list, Beautiful Soup will allow a string match against any item in that list. This code finds all the <a> tags and all the <b> tags: soup . find_all ([ \"a\" , \"b\" ]) # [<b>The Dormouse's story</b>, # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] A function \u2691 If none of the other matches work for you, define a function that takes an element as its only argument. The function should return True if the argument matches, and False otherwise. Here\u2019s a function that returns True if a tag defines the class attribute but doesn\u2019t define the id attribute: def has_class_but_no_id ( tag ): return tag . has_attr ( 'class' ) and not tag . has_attr ( 'id' ) Pass this function into find_all() and you\u2019ll pick up all the <p> tags: soup . find_all ( has_class_but_no_id ) # [<p class=\"title\"><b>The Dormouse's story</b></p>, # <p class=\"story\">Once upon a time there were...</p>, # <p class=\"story\">...</p>] find_all() \u2691 The find_all() method looks through a tag\u2019s descendants and retrieves all descendants that match your filters. soup . find_all ( \"title\" ) # [<title>The Dormouse's story</title>] soup . find_all ( \"p\" , \"title\" ) # [<p class=\"title\"><b>The Dormouse's story</b></p>] soup . find_all ( \"a\" ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] soup . find_all ( id = \"link2\" ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>] import re soup . find ( string = re . compile ( \"sisters\" )) # u'Once upon a time there were three little sisters; and their names were\\n' The name argument \u2691 Pass in a value for name and you\u2019ll tell Beautiful Soup to only consider tags with certain names. Text strings will be ignored, as will tags whose names that don\u2019t match. This is the simplest usage: soup . find_all ( \"title\" ) # [<title>The Dormouse's story</title>] The keyword arguments \u2691 Any argument that\u2019s not recognized will be turned into a filter on one of a tag\u2019s attributes. If you pass in a value for an argument called id , Beautiful Soup will filter against each tag\u2019s id attribute: soup . find_all ( id = 'link2' ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>] You can filter an attribute based on a string, a regular expression, a list, a function, or the value True. You can filter multiple attributes at once by passing in more than one keyword argument: soup . find_all ( href = re . compile ( \"elsie\" ), id = 'link1' ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">three</a>] Searching by CSS class \u2691 It\u2019s very useful to search for a tag that has a certain CSS class, but the name of the CSS attribute, class , is a reserved word in Python. Using class as a keyword argument will give you a syntax error. As of Beautiful Soup 4.1.2, you can search by CSS class using the keyword argument class_ : soup . find_all ( \"a\" , class_ = \"sister\" ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] The string argument \u2691 With string you can search for strings instead of tags. soup . find_all ( string = \"Elsie\" ) # [u'Elsie'] soup . find_all ( string = [ \"Tillie\" , \"Elsie\" , \"Lacie\" ]) # [u'Elsie', u'Lacie', u'Tillie'] soup . find_all ( string = re . compile ( \"Dormouse\" )) [ u \"The Dormouse's story\" , u \"The Dormouse's story\" ] def is_the_only_string_within_a_tag ( s ): \"\"\"Return True if this string is the only child of its parent tag.\"\"\" return ( s == s . parent . string ) soup . find_all ( string = is_the_only_string_within_a_tag ) # [u\"The Dormouse's story\", u\"The Dormouse's story\", u'Elsie', u'Lacie', u'Tillie', u'...'] Although string is for finding strings, you can combine it with arguments that find tags: Beautiful Soup will find all tags whose .string matches your value for string. soup . find_all ( \"a\" , string = \"Elsie\" ) # [<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>] The limit argument \u2691 find_all() returns all the tags and strings that match your filters. This can take a while if the document is large. If you don\u2019t need all the results, you can pass in a number for limit . The recursive argument \u2691 If you call mytag.find_all() , Beautiful Soup will examine all the descendants of mytag . If you only want Beautiful Soup to consider direct children, you can pass in recursive=False . Calling a tag is like calling find_all() \u2691 Because find_all() is the most popular method in the Beautiful Soup search API, you can use a shortcut for it. If you treat the BeautifulSoup object or a Tag object as though it were a function, then it\u2019s the same as calling find_all() on that object. These two lines of code are equivalent: soup . find_all ( \"a\" ) soup ( \"a\" ) find() \u2691 find() is like find_all() but returning just one result. find_parent() and find_parents() \u2691 These methods work their way up the tree, looking at a tag\u2019s (or a string\u2019s) parents. find_next_siblings() and find_next_sibling() \u2691 These methods use .next_siblings to iterate over the rest of an element\u2019s siblings in the tree. The find_next_siblings() method returns all the siblings that match, and find_next_sibling() only returns the first one: first_link = soup . a first_link # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> first_link . find_next_siblings ( \"a\" ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] To go in the other direction you can use find_previous_siblings() and find_previous_sibling() Modifying the tree \u2691 replace_with \u2691 PageElement.replace_with() removes a tag or string from the tree, and replaces it with the tag or string of your choice: markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>' soup = BeautifulSoup ( markup ) a_tag = soup . a new_tag = soup . new_tag ( \"b\" ) new_tag . string = \"example.net\" a_tag . i . replace_with ( new_tag ) a_tag # <a href=\"http://example.com/\">I linked to <b>example.net</b></a> Sometimes it doesn't work. If it doesn't use: a_tag . clear () a_tag . append ( new_tag ) Tips \u2691 Show content beautified / prettified \u2691 Use print(soup.prettify()) . Cleaning escaped HTML code \u2691 soup = BeautifulSoup ( s . replace ( r \" \\\" \" , '\"' ) . replace ( r \"\\/\" , \"/\" ), \"html.parser\" ) References \u2691 Docs", "title": "BeautifulSoup"}, {"location": "beautifulsoup/#installation", "text": "pip install beautifulsoup4 The default parser html.parser doesn't work with HTML5, so you'll probably need to use the html5lib parser, it's not included by default, so you might need to install it as well pip install html5lib", "title": "Installation"}, {"location": "beautifulsoup/#usage", "text": "", "title": "Usage"}, {"location": "beautifulsoup/#kinds-of-objects", "text": "Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. But you\u2019ll only ever have to deal with about four kinds of objects: Tag , NavigableString , BeautifulSoup , and Comment .", "title": "Kinds of objects"}, {"location": "beautifulsoup/#tag", "text": "A Tag object corresponds to an XML or HTML tag in the original document: soup = BeautifulSoup ( '<b class=\"boldest\">Extremely bold</b>' ) tag = soup . b type ( tag ) # <class 'bs4.element.Tag'> The most important features of a tag are its name and attributes .", "title": "Tag"}, {"location": "beautifulsoup/#name", "text": "Every tag has a name , accessible as .name : tag . name # u'b' If you change a tag\u2019s name, the change will be reflected in any HTML markup generated by Beautiful Soup:. tag . name = \"blockquote\" tag # <blockquote class=\"boldest\">Extremely bold</blockquote>", "title": "Name"}, {"location": "beautifulsoup/#attributes", "text": "A tag may have any number of attributes. The tag <b id=\"boldest\"> has an attribute id whose value is boldest . You can access a tag\u2019s attributes by treating the tag like a dictionary: tag [ 'id' ] # u'boldest' You can access that dictionary directly as .attrs : tag . attrs # {u'id': 'boldest'} You can add, remove, and modify a tag\u2019s attributes. Again, this is done by treating the tag as a dictionary: tag [ 'id' ] = 'verybold' tag [ 'another-attribute' ] = 1 tag # <b another-attribute=\"1\" id=\"verybold\"></b> del tag [ 'id' ] del tag [ 'another-attribute' ] tag # <b></b> tag [ 'id' ] # KeyError: 'id' print ( tag . get ( 'id' )) # None", "title": "Attributes"}, {"location": "beautifulsoup/#multi-valued-attributes", "text": "HTML 4 defines a few attributes that can have multiple values. HTML 5 removes a couple of them, but defines a few more. The most common multi-valued attribute is class (that is, a tag can have more than one CSS class). Others include rel , rev , accept-charset , headers , and accesskey . Beautiful Soup presents the value(s) of a multi-valued attribute as a list: css_soup = BeautifulSoup ( '<p class=\"body\"></p>' ) css_soup . p [ 'class' ] # [\"body\"] css_soup = BeautifulSoup ( '<p class=\"body strikeout\"></p>' ) css_soup . p [ 'class' ] # [\"body\", \"strikeout\"] If an attribute looks like it has more than one value, but it\u2019s not a multi-valued attribute as defined by any version of the HTML standard, Beautiful Soup will leave the attribute alone: id_soup = BeautifulSoup ( '<p id=\"my id\"></p>' ) id_soup . p [ 'id' ] # 'my id' When you turn a tag back into a string, multiple attribute values are consolidated: rel_soup = BeautifulSoup ( '<p>Back to the <a rel=\"index\">homepage</a></p>' ) rel_soup . a [ 'rel' ] # ['index'] rel_soup . a [ 'rel' ] = [ 'index' , 'contents' ] print ( rel_soup . p ) # <p>Back to the <a rel=\"index contents\">homepage</a></p> If you parse a document as XML, there are no multi-valued attributes:", "title": "Multi-valued attributes"}, {"location": "beautifulsoup/#navigablestring", "text": "A string corresponds to a bit of text within a tag. Beautiful Soup uses the NavigableString class to contain these bits of text: tag . string # u'Extremely bold' type ( tag . string ) # <class 'bs4.element.NavigableString'> A NavigableString is just like a Python Unicode string, except that it also supports some of the features described in Navigating the tree and Searching the tree. You can convert a NavigableString to a Unicode string with unicode() : unicode_string = unicode ( tag . string ) unicode_string # u'Extremely bold' type ( unicode_string ) # <type 'unicode'> You can\u2019t edit a string in place, but you can replace one string with another, using replace_with() : tag . string . replace_with ( \"No longer bold\" ) tag # <blockquote>No longer bold</blockquote>", "title": "NavigableString"}, {"location": "beautifulsoup/#beautifulsoup", "text": "The BeautifulSoup object represents the parsed document as a whole. For most purposes, you can treat it as a Tag object. This means it supports most of the methods described in Navigating the tree and Searching the tree.", "title": "BeautifulSoup"}, {"location": "beautifulsoup/#navigating-the-tree", "text": "", "title": "Navigating the tree"}, {"location": "beautifulsoup/#going-down", "text": "Tags may contain strings and other tags. These elements are the tag\u2019s children. Beautiful Soup provides a lot of different attributes for navigating and iterating over a tag\u2019s children. Note that Beautiful Soup strings don\u2019t support any of these attributes, because a string can\u2019t have children.", "title": "Going down"}, {"location": "beautifulsoup/#navigating-using-tag-names", "text": "The simplest way to navigate the parse tree is to say the name of the tag you want. If you want the <head> tag, just say soup.head : soup . head # <head><title>The Dormouse's story</title></head> soup . title # <title>The Dormouse's story</title> You can do use this trick again and again to zoom in on a certain part of the parse tree. This code gets the first <b> tag beneath the <body> tag: soup . body . b # <b>The Dormouse's story</b> Using a tag name as an attribute will give you only the first tag by that name: soup . a # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> If you need to get all the <a> tags, or anything more complicated than the first tag with a certain name, you\u2019ll need to use one of the methods described in Searching the tree, such as find_all() : soup . find_all ( 'a' ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]", "title": "Navigating using tag names"}, {"location": "beautifulsoup/#contents-and-children", "text": "A tag\u2019s children are available in a list called .contents : head_tag = soup . head head_tag # <head><title>The Dormouse's story</title></head> head_tag . contents [ < title > The Dormouse 's story</title>] title_tag = head_tag . contents [ 0 ] title_tag # <title>The Dormouse's story</title> title_tag . contents # [u'The Dormouse's story'] Instead of getting them as a list, you can iterate over a tag\u2019s children using the .children generator: for child in title_tag . children : print ( child ) # The Dormouse's story", "title": ".contents and .children"}, {"location": "beautifulsoup/#descendants", "text": "The .contents and .children attributes only consider a tag\u2019s direct children. For instance, the <head> tag has a single direct child\u2013the <title> tag: head_tag . contents # [<title>The Dormouse's story</title>] But the <title> tag itself has a child: the string The Dormouse\u2019s story . There\u2019s a sense in which that string is also a child of the <head> tag. The .descendants attribute lets you iterate over all of a tag\u2019s children, recursively: its direct children, the children of its direct children, and so on:. for child in head_tag . descendants : print ( child ) # <title>The Dormouse's story</title> # The Dormouse's story", "title": ".descendants"}, {"location": "beautifulsoup/#string", "text": "If a tag has only one child, and that child is a NavigableString , the child is made available as .string : title_tag . string # u'The Dormouse's story' If a tag\u2019s only child is another tag, and that tag has a .string , then the parent tag is considered to have the same .string as its child: head_tag . contents # [<title>The Dormouse's story</title>] head_tag . string # u'The Dormouse's story' If a tag contains more than one thing, then it\u2019s not clear what .string should refer to, so .string is defined to be None : print ( soup . html . string ) # None", "title": ".string"}, {"location": "beautifulsoup/#strings-and-stripped_strings", "text": "If there\u2019s more than one thing inside a tag, you can still look at just the strings. Use the .strings generator: for string in soup . strings : print ( repr ( string )) # u\"The Dormouse's story\" # u'\\n\\n' # u\"The Dormouse's story\" # u'\\n\\n' These strings tend to have a lot of extra whitespace, which you can remove by using the .stripped_strings generator instead: for string in soup . stripped_strings : print ( repr ( string )) # u\"The Dormouse's story\" # u\"The Dormouse's story\" # u'Once upon a time there were three little sisters; and their names were' # u'Elsie'", "title": ".strings and .stripped_strings"}, {"location": "beautifulsoup/#going-up", "text": "Continuing the \u201cfamily tree\u201d analogy, every tag and every string has a parent: the tag that contains it.", "title": "Going up"}, {"location": "beautifulsoup/#parent", "text": "You can access an element\u2019s parent with the .parent attribute. title_tag = soup . title title_tag # <title>The Dormouse's story</title> title_tag . parent # <head><title>The Dormouse's story</title></head>", "title": ".parent"}, {"location": "beautifulsoup/#parents", "text": "You can iterate over all of an element\u2019s parents with .parents .", "title": ".parents"}, {"location": "beautifulsoup/#going-sideways", "text": "When a document is pretty-printed, siblings show up at the same indentation level. You can also use this relationship in the code you write.", "title": "Going sideways"}, {"location": "beautifulsoup/#next_sibling-and-previous_sibling", "text": "You can use .next_sibling and .previous_sibling to navigate between page elements that are on the same level of the parse tree:. sibling_soup . b . next_sibling # <c>text2</c> sibling_soup . c . previous_sibling # <b>text1</b> The <b> tag has a .next_sibling , but no .previous_sibling , because there\u2019s nothing before the <b> tag on the same level of the tree. For the same reason, the <c> tag has a .previous_sibling but no .next_sibling : print ( sibling_soup . b . previous_sibling ) # None print ( sibling_soup . c . next_sibling ) # None In real documents, the .next_sibling or .previous_sibling of a tag will usually be a string containing whitespace. < a href = \"http://example.com/elsie\" class = \"sister\" id = \"link1\" > Elsie </ a > < a href = \"http://example.com/lacie\" class = \"sister\" id = \"link2\" > Lacie </ a > < a href = \"http://example.com/tillie\" class = \"sister\" id = \"link3\" > Tillie </ a > You might think that the .next_sibling of the first <a> tag would be the second <a> tag. But actually, it\u2019s a string: the comma and newline that separate the first <a> tag from the second: link = soup . a link # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> link . next_sibling # u',\\n' The second <a> tag is actually the .next_sibling of the comma: link . next_sibling . next_sibling # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>", "title": ".next_sibling and .previous_sibling"}, {"location": "beautifulsoup/#next_siblings-and-previous_siblings", "text": "You can iterate over a tag\u2019s siblings with .next_siblings or .previous_siblings : for sibling in soup . a . next_siblings : print ( repr ( sibling )) # u',\\n' # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> # u' and\\n' # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a> # u'; and they lived at the bottom of a well.' # None for sibling in soup . find ( id = \"link3\" ) . previous_siblings : print ( repr ( sibling )) # ' and\\n' # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> # u',\\n' # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> # u'Once upon a time there were three little sisters; and their names were\\n' # None", "title": ".next_siblings and .previous_siblings"}, {"location": "beautifulsoup/#searching-the-tree", "text": "By passing in a filter to an argument like find_all() , you can zoom in on the parts of the document you\u2019re interested in.", "title": "Searching the tree"}, {"location": "beautifulsoup/#kinds-of-filters", "text": "", "title": "Kinds of filters"}, {"location": "beautifulsoup/#a-string", "text": "The simplest filter is a string. Pass a string to a search method and Beautiful Soup will perform a match against that exact string. This code finds all the <b> tags in the document: soup . find_all ( 'b' ) # [<b>The Dormouse's story</b>]", "title": "A string"}, {"location": "beautifulsoup/#a-regular-expression", "text": "If you pass in a regular expression object, Beautiful Soup will filter against that regular expression using its search() method. This code finds all the tags whose names start with the letter b ; in this case, the <body> tag and the <b> tag: import re for tag in soup . find_all ( re . compile ( \"^b\" )): print ( tag . name ) # body # b", "title": "A regular expression"}, {"location": "beautifulsoup/#a-list", "text": "If you pass in a list, Beautiful Soup will allow a string match against any item in that list. This code finds all the <a> tags and all the <b> tags: soup . find_all ([ \"a\" , \"b\" ]) # [<b>The Dormouse's story</b>, # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]", "title": "A list"}, {"location": "beautifulsoup/#a-function", "text": "If none of the other matches work for you, define a function that takes an element as its only argument. The function should return True if the argument matches, and False otherwise. Here\u2019s a function that returns True if a tag defines the class attribute but doesn\u2019t define the id attribute: def has_class_but_no_id ( tag ): return tag . has_attr ( 'class' ) and not tag . has_attr ( 'id' ) Pass this function into find_all() and you\u2019ll pick up all the <p> tags: soup . find_all ( has_class_but_no_id ) # [<p class=\"title\"><b>The Dormouse's story</b></p>, # <p class=\"story\">Once upon a time there were...</p>, # <p class=\"story\">...</p>]", "title": "A function"}, {"location": "beautifulsoup/#find_all", "text": "The find_all() method looks through a tag\u2019s descendants and retrieves all descendants that match your filters. soup . find_all ( \"title\" ) # [<title>The Dormouse's story</title>] soup . find_all ( \"p\" , \"title\" ) # [<p class=\"title\"><b>The Dormouse's story</b></p>] soup . find_all ( \"a\" ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] soup . find_all ( id = \"link2\" ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>] import re soup . find ( string = re . compile ( \"sisters\" )) # u'Once upon a time there were three little sisters; and their names were\\n'", "title": "find_all()"}, {"location": "beautifulsoup/#the-name-argument", "text": "Pass in a value for name and you\u2019ll tell Beautiful Soup to only consider tags with certain names. Text strings will be ignored, as will tags whose names that don\u2019t match. This is the simplest usage: soup . find_all ( \"title\" ) # [<title>The Dormouse's story</title>]", "title": "The name argument"}, {"location": "beautifulsoup/#the-keyword-arguments", "text": "Any argument that\u2019s not recognized will be turned into a filter on one of a tag\u2019s attributes. If you pass in a value for an argument called id , Beautiful Soup will filter against each tag\u2019s id attribute: soup . find_all ( id = 'link2' ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>] You can filter an attribute based on a string, a regular expression, a list, a function, or the value True. You can filter multiple attributes at once by passing in more than one keyword argument: soup . find_all ( href = re . compile ( \"elsie\" ), id = 'link1' ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">three</a>]", "title": "The keyword arguments"}, {"location": "beautifulsoup/#searching-by-css-class", "text": "It\u2019s very useful to search for a tag that has a certain CSS class, but the name of the CSS attribute, class , is a reserved word in Python. Using class as a keyword argument will give you a syntax error. As of Beautiful Soup 4.1.2, you can search by CSS class using the keyword argument class_ : soup . find_all ( \"a\" , class_ = \"sister\" ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]", "title": "Searching by CSS class"}, {"location": "beautifulsoup/#the-string-argument", "text": "With string you can search for strings instead of tags. soup . find_all ( string = \"Elsie\" ) # [u'Elsie'] soup . find_all ( string = [ \"Tillie\" , \"Elsie\" , \"Lacie\" ]) # [u'Elsie', u'Lacie', u'Tillie'] soup . find_all ( string = re . compile ( \"Dormouse\" )) [ u \"The Dormouse's story\" , u \"The Dormouse's story\" ] def is_the_only_string_within_a_tag ( s ): \"\"\"Return True if this string is the only child of its parent tag.\"\"\" return ( s == s . parent . string ) soup . find_all ( string = is_the_only_string_within_a_tag ) # [u\"The Dormouse's story\", u\"The Dormouse's story\", u'Elsie', u'Lacie', u'Tillie', u'...'] Although string is for finding strings, you can combine it with arguments that find tags: Beautiful Soup will find all tags whose .string matches your value for string. soup . find_all ( \"a\" , string = \"Elsie\" ) # [<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>]", "title": "The string argument"}, {"location": "beautifulsoup/#the-limit-argument", "text": "find_all() returns all the tags and strings that match your filters. This can take a while if the document is large. If you don\u2019t need all the results, you can pass in a number for limit .", "title": "The limit argument"}, {"location": "beautifulsoup/#the-recursive-argument", "text": "If you call mytag.find_all() , Beautiful Soup will examine all the descendants of mytag . If you only want Beautiful Soup to consider direct children, you can pass in recursive=False .", "title": "The recursive argument"}, {"location": "beautifulsoup/#calling-a-tag-is-like-calling-find_all", "text": "Because find_all() is the most popular method in the Beautiful Soup search API, you can use a shortcut for it. If you treat the BeautifulSoup object or a Tag object as though it were a function, then it\u2019s the same as calling find_all() on that object. These two lines of code are equivalent: soup . find_all ( \"a\" ) soup ( \"a\" )", "title": "Calling a tag is like calling find_all()"}, {"location": "beautifulsoup/#find", "text": "find() is like find_all() but returning just one result.", "title": "find()"}, {"location": "beautifulsoup/#find_parent-and-find_parents", "text": "These methods work their way up the tree, looking at a tag\u2019s (or a string\u2019s) parents.", "title": "find_parent() and find_parents()"}, {"location": "beautifulsoup/#find_next_siblings-and-find_next_sibling", "text": "These methods use .next_siblings to iterate over the rest of an element\u2019s siblings in the tree. The find_next_siblings() method returns all the siblings that match, and find_next_sibling() only returns the first one: first_link = soup . a first_link # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> first_link . find_next_siblings ( \"a\" ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] To go in the other direction you can use find_previous_siblings() and find_previous_sibling()", "title": "find_next_siblings() and find_next_sibling()"}, {"location": "beautifulsoup/#modifying-the-tree", "text": "", "title": "Modifying the tree"}, {"location": "beautifulsoup/#replace_with", "text": "PageElement.replace_with() removes a tag or string from the tree, and replaces it with the tag or string of your choice: markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>' soup = BeautifulSoup ( markup ) a_tag = soup . a new_tag = soup . new_tag ( \"b\" ) new_tag . string = \"example.net\" a_tag . i . replace_with ( new_tag ) a_tag # <a href=\"http://example.com/\">I linked to <b>example.net</b></a> Sometimes it doesn't work. If it doesn't use: a_tag . clear () a_tag . append ( new_tag )", "title": "replace_with"}, {"location": "beautifulsoup/#tips", "text": "", "title": "Tips"}, {"location": "beautifulsoup/#show-content-beautified-prettified", "text": "Use print(soup.prettify()) .", "title": "Show content beautified / prettified"}, {"location": "beautifulsoup/#cleaning-escaped-html-code", "text": "soup = BeautifulSoup ( s . replace ( r \" \\\" \" , '\"' ) . replace ( r \"\\/\" , \"/\" ), \"html.parser\" )", "title": "Cleaning escaped HTML code"}, {"location": "beautifulsoup/#references", "text": "Docs", "title": "References"}, {"location": "beets/", "text": "Beets is a music management library used to get your music collection right once and for all. It catalogs your collection, automatically improving its metadata as it goes using the MusicBrainz database. Then it provides a set of tools for manipulating and accessing your music. Through plugins it supports: Fetch or calculate all the metadata you could possibly need: album art, lyrics, genres, tempos, ReplayGain levels, or acoustic fingerprints. Get metadata from MusicBrainz, Discogs, or Beatport. Or guess metadata using songs\u2019 filenames or their acoustic fingerprints. Transcode audio to any format you like. Check your library for duplicate tracks and albums or for albums that are missing tracks. Browse your music library graphically through a Web browser and play it in any browser that supports HTML5 Audio. Still, if beets doesn't do what you want yet, writing your own plugin is easy if you know a little Python. Or you can use it as a library . Installation \u2691 pip install beets References \u2691 Git Docs Homepage", "title": "Beets"}, {"location": "beets/#installation", "text": "pip install beets", "title": "Installation"}, {"location": "beets/#references", "text": "Git Docs Homepage", "title": "References"}, {"location": "book_binding/", "text": "Book binding is the process of physically assembling a book of codex format from an ordered stack of paper sheets that are folded together into sections called signatures or sometimes left as a stack of individual sheets. Several signatures are then bound together along one edge with a thick needle and sturdy thread. References \u2691 http://tuxgraphics.org/npa/book-binding/ https://www.instructables.com/id/How-to-bind-your-own-Hardback-Book/ https://www.instructables.com/id/Binding-a-Book-With-Common-Materials/ https://www.instructables.com/id/Perfect-Bound-Paperback-Notebook/ Videos \u2691 https://www.youtube.com/watch?v=S2FRKbQI2kY https://www.youtube.com/watch?v=Av_rU-yOPd4 https://www.youtube.com/watch?v=9O4kFTOEh6k https://www.youtube.com/watch?v=XGQ5P8QVHSg", "title": "Book binding"}, {"location": "book_binding/#references", "text": "http://tuxgraphics.org/npa/book-binding/ https://www.instructables.com/id/How-to-bind-your-own-Hardback-Book/ https://www.instructables.com/id/Binding-a-Book-With-Common-Materials/ https://www.instructables.com/id/Perfect-Bound-Paperback-Notebook/", "title": "References"}, {"location": "book_binding/#videos", "text": "https://www.youtube.com/watch?v=S2FRKbQI2kY https://www.youtube.com/watch?v=Av_rU-yOPd4 https://www.youtube.com/watch?v=9O4kFTOEh6k https://www.youtube.com/watch?v=XGQ5P8QVHSg", "title": "Videos"}, {"location": "book_management/", "text": "Book management is the set of systems and processes to get and categorize books so it's easy to browse and discover new content. It involves the next actions: Automatically index and download metadata of new books. Notify the user when a new book is added. Monitor the books of an author, and get them once they are released. Send books to the e-reader. A nice interface to browse the existent library, with the possibility of filtering by author, genre, years, tags or series. An interface to preview or read the items. An interface to rate and review library items. An interface to discover new content based on the ratings and item metadata. I haven't yet found a single piece of software that fulfills all these needs, so we need to split it into subsystems. Downloader and indexer. Gallery browser. Review system. Content discovery. Downloader and indexer \u2691 System that monitors the availability of books in a list of indexers, when they are available, they download it to a directory of the server. The best one that I've found is Readarr , it makes it easy to search for authors and books, supports a huge variety of indexers (such as Archive.org), and download clients (such as torrent clients). It can be used as a limited gallery browser, you can easily see the books of an author or series, but it doesn't yet support the automatic fetch of genres or tags. I haven't found an easy way of marking elements as read, prioritize the list of books to read, or add a user rating. Until these features are added (if they ever are), we need to use it in parallel with a better gallery browser. Gallery browser \u2691 System that shows the books in the library in a nice format, allowing the user to filter out the contents, prioritize them, mark them as read, rate them and optionally sync books with the ereader. Calibre-web is a beautiful solution, without trying it, it looks like it supports all of the required features, but it doesn't work well with Readarr. Readarr has support to interact with Calibre content server by defining a root folder to be managed by Calibre , but the books you want to have Readarr recognize on initial library import must already be in Calibre. Books within the folder and not in Calibre will be ignored. So you'll need to do the first import in Calibre, instead of Readarr (which is quite pleasant). Note also that you cannot add Calibre integration to a root folder after it's created. Calibre-web interacts directly with the Sqlite database of Calibre, so it doesn't expose the Calibre Content Server, therefore is not compatible with Readarr. To make it work, you'd need to have both the calibre server and the calibre-web running at the same time, which has led to database locks ( 1 , and 2 ) that the calibre-web developer has tried to avoid by controlling the database writes , and said that : If you start Calibre first and afterwards Calibre-Web, Calibre indeed locks the database and doesn't allow Calibre-Web to access the database (metadata.db) file. Starting Calibre-Web and afterwards Calibre should work. The problem comes when Readarr writes in the database through calibre to add books, and calibre-web tries to write too to add user ratings or other metadata. Another option would be to only run calibre-web and automatically import the books once they are downloaded by Readarr. calibre-web is not going to support a watch directory feature, the author recommends to use a cron script to do it. I haven't tried this path yet. Another option would be to assume that calibre-web is not going to do any insert in the database, so it would become a read-only web interface, therefore we wouldn't be able to edit the books or rate them, one of the features we'd like to have in the gallery browser. To make sure that we don't get locks, instead of using the same file, a cron job could do an rsync between the database managed by calibre and the one used by calibre-web . Calibre implements genres with tags, behind the scenes it uses the fetch-ebook-metadata command line tool, that returns all the metadata in human readable form $: fetch-ebook-metadata -i 9780061796883 -c cover.jpg Title : The Dispossessed Author ( s ) : Ursula K. le Guin Publisher : Harper Collins Tags : Fiction, Science Fiction, Space Exploration, Literary, Visionary & Metaphysical Languages : eng Published : 2009 -10-13T20:34:30.878865+00:00 Identifiers : google:tlhFtmTixvwC, isbn:9780061796883 Comments : \u201cOne of the greats\u2026.Not just a science fiction writer ; a literary icon.\u201d \u2013 Stephen KingFrom the brilliant and award-winning author Ursula K. Le Guin comes a classic tale of two planets torn apart by conflict and mistrust \u2014 and the man who risks everything to reunite them.A bleak moon settled by utopian anarchists, Anarres has long been isolated from other worlds, including its mother planet, Urras\u2014a civilization of warring nations, great poverty, and immense wealth. Now Shevek, a brilliant physicist, is determined to reunite the two planets, which have been divided by centuries of distrust. He will seek ans wers, question the unquestionable, and attempt to tear down the walls of hatred that have kept them apart.To visit Urras\u2014to learn, to teach, to share\u2014will require great sacrifice and risks, which Shevek willingly accepts. But the ambitious scientist ' s gift is soon seen as a threat, and in the profound conflict that ensues, he must reexamine his beliefs even as he ignites the fires of change. Cover : cover.jpg Or in xml if you use the -o flag. I've checked if these tags could be automatically applied to Readarr, but their tags are meant only to be attached to Authors to apply metadata profiles. I've opened an issue to see if they plan to implement tags for books. It's a pity we are not going to use calibre-web as it also had support to sync the reading stats from Kobo . In the past I used gcstar and then polar bookshelf , but decided not to use them anymore for different reasons. In conclusion, the tools reviewed don't work as I need them to, some ugly patches could be applied and maybe it would work, but it clearly shows that they are not ready yet unless you want to invest time in it, and even if you did, it will be unstable. Until a better system shows up, I'm going to use Readarr to browse the books that I want to read, and add them to an ordered markdown file with sections as genres, not ideal, but robust as hell xD. Review system \u2691 System to write reviews and rate books, if the gallery browser doesn't include it, we'll use an independent component. Until I find something better, I'm saving the title, author, genre, score, and review in a json file, so it's easy to import in the chosen component. Content discovery \u2691 Recommendation system to analyze the user taste and suggest books that might like. Right now I'm monitoring the authors with Readarr to get notifications when they release a new book. I also manually go through goodreads and similar websites looking for similar books to the ones I liked. Deprecated components \u2691 Polar bookself \u2691 It was a very promising piece of software that went wrong :(. It had a nice interface built for incremental reading and studying with anki, and a nice tag system. It was a desktop application you installed in your computer, but since Polar 2.0 they moved into a cloud hosted service, with no possibility of self-hosting it, so you give them your books and all your data, a nasty turn of events. GCStar \u2691 The first free open source application for managing collections I used, it has an old looking desktop interface and is no longer maintained.", "title": "Book Management"}, {"location": "book_management/#downloader-and-indexer", "text": "System that monitors the availability of books in a list of indexers, when they are available, they download it to a directory of the server. The best one that I've found is Readarr , it makes it easy to search for authors and books, supports a huge variety of indexers (such as Archive.org), and download clients (such as torrent clients). It can be used as a limited gallery browser, you can easily see the books of an author or series, but it doesn't yet support the automatic fetch of genres or tags. I haven't found an easy way of marking elements as read, prioritize the list of books to read, or add a user rating. Until these features are added (if they ever are), we need to use it in parallel with a better gallery browser.", "title": "Downloader and indexer"}, {"location": "book_management/#gallery-browser", "text": "System that shows the books in the library in a nice format, allowing the user to filter out the contents, prioritize them, mark them as read, rate them and optionally sync books with the ereader. Calibre-web is a beautiful solution, without trying it, it looks like it supports all of the required features, but it doesn't work well with Readarr. Readarr has support to interact with Calibre content server by defining a root folder to be managed by Calibre , but the books you want to have Readarr recognize on initial library import must already be in Calibre. Books within the folder and not in Calibre will be ignored. So you'll need to do the first import in Calibre, instead of Readarr (which is quite pleasant). Note also that you cannot add Calibre integration to a root folder after it's created. Calibre-web interacts directly with the Sqlite database of Calibre, so it doesn't expose the Calibre Content Server, therefore is not compatible with Readarr. To make it work, you'd need to have both the calibre server and the calibre-web running at the same time, which has led to database locks ( 1 , and 2 ) that the calibre-web developer has tried to avoid by controlling the database writes , and said that : If you start Calibre first and afterwards Calibre-Web, Calibre indeed locks the database and doesn't allow Calibre-Web to access the database (metadata.db) file. Starting Calibre-Web and afterwards Calibre should work. The problem comes when Readarr writes in the database through calibre to add books, and calibre-web tries to write too to add user ratings or other metadata. Another option would be to only run calibre-web and automatically import the books once they are downloaded by Readarr. calibre-web is not going to support a watch directory feature, the author recommends to use a cron script to do it. I haven't tried this path yet. Another option would be to assume that calibre-web is not going to do any insert in the database, so it would become a read-only web interface, therefore we wouldn't be able to edit the books or rate them, one of the features we'd like to have in the gallery browser. To make sure that we don't get locks, instead of using the same file, a cron job could do an rsync between the database managed by calibre and the one used by calibre-web . Calibre implements genres with tags, behind the scenes it uses the fetch-ebook-metadata command line tool, that returns all the metadata in human readable form $: fetch-ebook-metadata -i 9780061796883 -c cover.jpg Title : The Dispossessed Author ( s ) : Ursula K. le Guin Publisher : Harper Collins Tags : Fiction, Science Fiction, Space Exploration, Literary, Visionary & Metaphysical Languages : eng Published : 2009 -10-13T20:34:30.878865+00:00 Identifiers : google:tlhFtmTixvwC, isbn:9780061796883 Comments : \u201cOne of the greats\u2026.Not just a science fiction writer ; a literary icon.\u201d \u2013 Stephen KingFrom the brilliant and award-winning author Ursula K. Le Guin comes a classic tale of two planets torn apart by conflict and mistrust \u2014 and the man who risks everything to reunite them.A bleak moon settled by utopian anarchists, Anarres has long been isolated from other worlds, including its mother planet, Urras\u2014a civilization of warring nations, great poverty, and immense wealth. Now Shevek, a brilliant physicist, is determined to reunite the two planets, which have been divided by centuries of distrust. He will seek ans wers, question the unquestionable, and attempt to tear down the walls of hatred that have kept them apart.To visit Urras\u2014to learn, to teach, to share\u2014will require great sacrifice and risks, which Shevek willingly accepts. But the ambitious scientist ' s gift is soon seen as a threat, and in the profound conflict that ensues, he must reexamine his beliefs even as he ignites the fires of change. Cover : cover.jpg Or in xml if you use the -o flag. I've checked if these tags could be automatically applied to Readarr, but their tags are meant only to be attached to Authors to apply metadata profiles. I've opened an issue to see if they plan to implement tags for books. It's a pity we are not going to use calibre-web as it also had support to sync the reading stats from Kobo . In the past I used gcstar and then polar bookshelf , but decided not to use them anymore for different reasons. In conclusion, the tools reviewed don't work as I need them to, some ugly patches could be applied and maybe it would work, but it clearly shows that they are not ready yet unless you want to invest time in it, and even if you did, it will be unstable. Until a better system shows up, I'm going to use Readarr to browse the books that I want to read, and add them to an ordered markdown file with sections as genres, not ideal, but robust as hell xD.", "title": "Gallery browser"}, {"location": "book_management/#review-system", "text": "System to write reviews and rate books, if the gallery browser doesn't include it, we'll use an independent component. Until I find something better, I'm saving the title, author, genre, score, and review in a json file, so it's easy to import in the chosen component.", "title": "Review system"}, {"location": "book_management/#content-discovery", "text": "Recommendation system to analyze the user taste and suggest books that might like. Right now I'm monitoring the authors with Readarr to get notifications when they release a new book. I also manually go through goodreads and similar websites looking for similar books to the ones I liked.", "title": "Content discovery"}, {"location": "book_management/#deprecated-components", "text": "", "title": "Deprecated components"}, {"location": "book_management/#polar-bookself", "text": "It was a very promising piece of software that went wrong :(. It had a nice interface built for incremental reading and studying with anki, and a nice tag system. It was a desktop application you installed in your computer, but since Polar 2.0 they moved into a cloud hosted service, with no possibility of self-hosting it, so you give them your books and all your data, a nasty turn of events.", "title": "Polar bookself"}, {"location": "book_management/#gcstar", "text": "The first free open source application for managing collections I used, it has an old looking desktop interface and is no longer maintained.", "title": "GCStar"}, {"location": "boto3/", "text": "Boto3 is the AWS SDK for Python to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services. Installation \u2691 pip install boto3 Usage \u2691 S3 \u2691 List the files of a bucket \u2691 def list_s3_by_prefix ( bucket : str , key_prefix : str , max_results : int = 100 ) -> List [ str ]: next_token = \"\" all_keys = [] while True : if next_token : res = s3 . list_objects_v2 ( Bucket = bucket , ContinuationToken = next_token , Prefix = key_prefix ) else : res = s3 . list_objects_v2 ( Bucket = bucket , Prefix = key_prefix ) if \"Contents\" not in res : break if res [ \"IsTruncated\" ]: next_token = res [ \"NextContinuationToken\" ] else : next_token = \"\" keys = [ item [ \"Key\" ] for item in res [ \"Contents\" ]] all_keys . extend ( keys ) if not next_token : break return all_keys [ - 1 * max_results :] The boto3 doesn't have any way to sort the outputs of the bucket, you need to do them once you've loaded all the objects :S. EC2 \u2691 Run EC2 instance \u2691 Use the run_instances method of the ec2 client. Check their docs for the different configuration options. The required ones are MinCount and MaxCount . import boto3 ec2 = boto3 . client ( 'ec2' ) instance = ec2 . run_instances ( MinCount = 1 , MaxCount = 1 ) Get instance types \u2691 from pydantic import BaseModel import boto3 class InstanceType ( BaseModel ): \"\"\"Define model of the instance type. Args: id_: instance type name cpu_vcores: Number of virtual cpus (cores * threads) cpu_speed: Sustained clock speed in Ghz ram: RAM memory in MiB network_performance: price: Hourly cost \"\"\" id_ : str cpu_vcores : int cpu_speed : Optional [ int ] = None ram : int network_performance : str price : Optional [ float ] = None @property def cpu ( self ) -> int : \"\"\"Calculate the total Ghz available.\"\"\" if self . cpu_speed is None : return self . cpu_vcores return self . cpu_vcores * self . cpu_speed def get_instance_types () -> InstanceTypes : \"\"\"Get the available instance types.\"\"\" log . info ( \"Retrieving instance types\" ) instance_types : InstanceTypes = {} for type_ in _ec2_instance_types ( cpu_arch = \"x86_64\" ): instance = InstanceType ( id_ = instance_type , cpu_vcores = type_ [ \"VCpuInfo\" ][ \"DefaultVCpus\" ], ram = type_ [ \"MemoryInfo\" ][ \"SizeInMiB\" ], network_performance = type_ [ \"NetworkInfo\" ][ \"NetworkPerformance\" ], price = _ec2_price ( instance_type ), ) with suppress ( KeyError ): instance . cpu_speed = type_ [ \"ProcessorInfo\" ][ \"SustainedClockSpeedInGhz\" ] instance_types [ type_ [ \"InstanceType\" ]] = instance return instance_types Get instance prices \u2691 import json import boto3 from pkg_resources import resource_filename def _ec2_price ( instance_type : str , region_code : str = \"us-east-1\" , operating_system : str = \"Linux\" , preinstalled_software : str = \"NA\" , tenancy : str = \"Shared\" , is_byol : bool = False , ) -> Optional [ float ]: \"\"\"Get the price of an EC2 instance type.\"\"\" log . debug ( f \"Retrieving price of { instance_type } \" ) region_name = _get_region_name ( region_code ) if is_byol : license_model = \"Bring your own license\" else : license_model = \"No License required\" if tenancy == \"Host\" : capacity_status = \"AllocatedHost\" else : capacity_status = \"Used\" filters = [ { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"termType\" , \"Value\" : \"OnDemand\" }, { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"capacitystatus\" , \"Value\" : capacity_status }, { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"location\" , \"Value\" : region_name }, { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"instanceType\" , \"Value\" : instance_type }, { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"tenancy\" , \"Value\" : tenancy }, { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"operatingSystem\" , \"Value\" : operating_system }, { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"preInstalledSw\" , \"Value\" : preinstalled_software , }, { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"licenseModel\" , \"Value\" : license_model }, ] pricing_client = boto3 . client ( \"pricing\" , region_name = \"us-east-1\" ) response = pricing_client . get_products ( ServiceCode = \"AmazonEC2\" , Filters = filters ) for price in response [ \"PriceList\" ]: price = json . loads ( price ) for on_demand in price [ \"terms\" ][ \"OnDemand\" ] . values (): for price_dimensions in on_demand [ \"priceDimensions\" ] . values (): price_value = price_dimensions [ \"pricePerUnit\" ][ \"USD\" ] return float ( price_value ) return None def _get_region_name ( region_code : str ) -> str : \"\"\"Extract the region name from it's code.\"\"\" endpoint_file = resource_filename ( \"botocore\" , \"data/endpoints.json\" ) with open ( endpoint_file , \"r\" , encoding = \"UTF8\" ) as f : endpoint_data = json . load ( f ) region_name = endpoint_data [ \"partitions\" ][ 0 ][ \"regions\" ][ region_code ][ \"description\" ] return region_name . replace ( \"Europe\" , \"EU\" ) Type hints \u2691 AWS library doesn't have working type hints -.- , so you either use Any or dive into the myriad of packages that implement them. I've so far tried boto3_type_annotations , boto3-stubs , and mypy_boto3_builder without success. Any it is for now... Testing \u2691 Programs that interact with AWS through boto3 create, change or get information on real AWS resources. When developing these programs, you don't want the testing framework to actually do those changes, as it might break things and cost you money. You need to find a way to intercept the calls to AWS and substitute them with the data their API would return. I've found three ways to achieve this: Manually mocking the boto3 methods used by the program with unittest.mock . Using moto . Using Botocore's Stubber . TL;DR Try to use moto, using the stubber as fallback option. Using unittest.mock forces you to know what the API is going to return and hardcode it in your tests. If the response changes, you need to update your tests, which is not good. moto is a library that allows you to easily mock out tests based on AWS infrastructure. It works well because it mocks out all calls to AWS automatically without requiring any dependency injection. The downside is that it goes behind boto3 so some of the methods you need to test won't be still implemented, that leads us to the third option. Botocore's Stubber is a class that allows you to stub out requests so you don't have to hit an endpoint to write tests. Responses are returned first in, first out. If operations are called out of order, or are called with no remaining queued responses, an error will be raised. It's like the first option but cleaner. If you go down this path, check adamj's post on testing S3 . moto \u2691 moto's library lets you fictitiously create and change AWS resources as you normally do with the boto3 library. They mimic what the real methods do on fake objects. The Docs are awful though. Install \u2691 pip install moto Simple usage \u2691 To understand better how it works, I'm going to show you an understandable example, it's not the best way to use it though, go to the usage section for production ready usage. Imagine you have a function that you use to launch new ec2 instances: import boto3 def add_servers ( ami_id , count ): client = boto3 . client ( 'ec2' , region_name = 'us-west-1' ) client . run_instances ( ImageId = ami_id , MinCount = count , MaxCount = count ) To test it we'd use: from . import add_servers from moto import mock_ec2 @mock_ec2 def test_add_servers (): add_servers ( 'ami-1234abcd' , 2 ) client = boto3 . client ( 'ec2' , region_name = 'us-west-1' ) instances = client . describe_instances ()[ 'Reservations' ][ 0 ][ 'Instances' ] assert len ( instances ) == 2 instance1 = instances [ 0 ] assert instance1 [ 'ImageId' ] == 'ami-1234abcd' The decorator @mock_ec2 tells moto to capture all boto3 calls to AWS. When we run the add_servers function to test, it will create the fake objects on the memory (without contacting AWS servers), and the client.describe_instances boto3 method returns the data of that fake data. Isn't it awesome? Usage \u2691 You can use it with decorators , context managers , directly or with pytest fixtures . Being a pytest fan, the last option looks the cleaner to me. To make sure that you don't change the real infrastructure, ensure that your tests have dummy environmental variables. File: tests/conftest.py @pytest . fixture () def _aws_credentials () -> None : \"\"\"Mock the AWS Credentials for moto.\"\"\" os . environ [ \"AWS_ACCESS_KEY_ID\" ] = \"testing\" os . environ [ \"AWS_SECRET_ACCESS_KEY\" ] = \"testing\" os . environ [ \"AWS_SECURITY_TOKEN\" ] = \"testing\" os . environ [ \"AWS_SESSION_TOKEN\" ] = \"testing\" @pytest . fixture () def ec2 ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 EC2 client.\"\"\" with mock_ec2 (): yield boto3 . client ( \"ec2\" , region_name = \"us-east-1\" ) The ec2 fixture can then be used in the tests to setup the environment or assert results. Testing EC2 \u2691 If you want to add security groups to the tests, you need to create the resource first. def test_ec2_with_security_groups ( ec2 : Any ) -> None : security_group_id = ec2 . create_security_group ( GroupName = \"TestSecurityGroup\" , Description = \"SG description\" )[ \"GroupId\" ] instance = ec2 . run_instances ( ImageId = \"ami-xxxx\" , MinCount = 1 , MaxCount = 1 , SecurityGroupIds = [ security_group_id ], )[ \"Instances\" ][ 0 ] # Test your code here To add tags, use: def test_ec2_with_security_groups ( ec2 : Any ) -> None : instance = ec2 . run_instances ( ImageId = \"ami-xxxx\" , MinCount = 1 , MaxCount = 1 , TagSpecifications = [ { \"ResourceType\" : \"instance\" , \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"instance name\" , }, ], } ], )[ \"Instances\" ][ 0 ] # Test your code here Testing RDS \u2691 Use the rds fixture: from moto import mock_rds2 @pytest . fixture () def rds ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 RDS client.\"\"\" with mock_rds2 (): yield boto3 . client ( \"rds\" , region_name = \"us-east-1\" ) To create an instance use: instance = rds . create_db_instance ( DBInstanceIdentifier = \"db-xxxx\" , DBInstanceClass = \"db.m3.2xlarge\" , Engine = \"postgres\" , )[ \"DBInstance\" ] It won't have VPC information, if you need it, create the subnet group first (you'll need the ec2 fixture too): subnets = [ subnet [ 'SubnetId' ] for subnet in ec2 . describe_subnets ()[ \"Subnets\" ]] rds . create_db_subnet_group ( DBSubnetGroupName = \"dbsg\" , SubnetIds = subnets , DBSubnetGroupDescription = \"Text\" ) instance = rds . create_db_instance ( DBInstanceIdentifier = \"db-xxxx\" , DBInstanceClass = \"db.m3.2xlarge\" , Engine = \"postgres\" , DBSubnetGroupName = \"dbsg\" , )[ \"DBInstance\" ] Testing S3 \u2691 Use the s3_mock fixture: from moto import mock_s3 @pytest . fixture () def s3_mock ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 S3 client.\"\"\" with mock_s3 (): yield boto3 . client ( \"s3\" ) To create an instance use: s3_mock . create_bucket ( Bucket = \"mybucket\" ) instance = s3_mock . list_buckets ()[ \"Buckets\" ][ 0 ] Check the official docs to check the create_bucket arguments. Testing Route53 \u2691 Use the route53 fixture: from moto import mock_route53 @pytest . fixture ( name = 'route53' ) def route53_ ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 Route53 client.\"\"\" with mock_route53 (): yield boto3 . client ( \"route53\" ) To create an instance use: hosted_zone = route53 . create_hosted_zone ( Name = \"example.com\" , CallerReference = \"Test\" )[ \"HostedZone\" ] hosted_zone_id = re . sub ( \".hostedzone.\" , \"\" , hosted_zone [ \"Id\" ]) route53 . change_resource_record_sets ( ChangeBatch = { \"Changes\" : [ { \"Action\" : \"CREATE\" , \"ResourceRecordSet\" : { \"Name\" : \"example.com\" , \"ResourceRecords\" : [ { \"Value\" : \"192.0.2.44\" , }, ], \"TTL\" : 60 , \"Type\" : \"A\" , }, }, ], \"Comment\" : \"Web server for example.com\" , }, HostedZoneId = hosted_zone_id , ) You need to first create a hosted zone. The change_resource_record_sets order to create the instance doesn't return any data, so if you need to work on it, use the list_resource_record_sets method of the route53 client (you'll need to set the HostedZoneId argument). If you have more than 300 records, the endpoint gives you a paginated response, so if the IsTruncated attribute is True , you need to call the method again setting the StartRecordName and StartRecordType to the NextRecordName and NextRecordType response arguments. Not nice at all. Pagination is not yet supported by moto , so you won't be able to test that part of your code. Check the official docs to check the method arguments: create_hosted_zone . change_resource_record_sets . Test VPC \u2691 Use the ec2 fixture defined in the usage section . To create an instance use: instance = ec2 . create_vpc ( CidrBlock = \"172.16.0.0/16\" , )[ \"Vpc\" ] Check the official docs to check the method arguments: create_vpc . create_subnet . Testing autoscaling groups \u2691 Use the autoscaling fixture: from moto import mock_autoscaling @pytest . fixture ( name = 'autoscaling' ) def autoscaling_ ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 Autoscaling Group client.\"\"\" with mock_autoscaling (): yield boto3 . client ( \"autoscaling\" ) They don't yet support LaunchTemplates , so you'll have to use LaunchConfigurations. To create an instance use: autoscaling . create_launch_configuration ( LaunchConfigurationName = 'LaunchConfiguration' , ImageId = 'ami-xxxx' , InstanceType = 't2.medium' ) autoscaling . create_auto_scaling_group ( AutoScalingGroupName = 'ASG name' , MinSize = 1 , MaxSize = 3 , LaunchConfigurationName = 'LaunchConfiguration' , AvailabilityZones = [ 'us-east-1a' ]) instance = autoscaling . describe_auto_scaling_groups ()[ \"AutoScalingGroups\" ][ 0 ] Check the official docs to check the method arguments: create_auto_scaling_group . create_launch_configuration . describe_auto_scaling_groups . Test Security Groups \u2691 Use the ec2 fixture defined in the usage section . To create an instance use: instance_id = ec2 . create_security_group ( GroupName = \"TestSecurityGroup\" , Description = \"SG description\" )[ \"GroupId\" ] instance = ec2 . describe_security_groups ( GroupIds = [ instance_id ]) To add permissions to the security group you need to use the authorize_security_group_ingress and authorize_security_group_egress methods. ec2 . authorize_security_group_ingress ( GroupId = instance_id , IpPermissions = [ { \"IpProtocol\" : \"tcp\" , \"FromPort\" : 80 , \"ToPort\" : 80 , \"IpRanges\" : [{ \"CidrIp\" : \"0.0.0.0/0\" }], }, ], ) By default, the created security group comes with an egress rule to allow all traffic. To remove rules use the revoke_security_group_egress and revoke_security_group_ingress methods. ec2 . revoke_security_group_egress ( GroupId = instance_id , IpPermissions = [ { \"IpProtocol\" : \"-1\" , \"IpRanges\" : [{ \"CidrIp\" : \"0.0.0.0/0\" }]}, ], ) Check the official docs to check the method arguments: create_security_group . describe_security_group . authorize_security_group_ingress . authorize_security_group_egress . revoke_security_group_ingress . revoke_security_group_egress . Test IAM users \u2691 Use the iam fixture: from moto import mock_iam @pytest . fixture ( name = 'iam' ) def iam_ ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 IAM client.\"\"\" with mock_iam (): yield boto3 . client ( \"iam\" ) To create an instance use: instance = iam . create_user ( UserName = \"User\" )[ \"User\" ] Check the official docs to check the method arguments: create_user list_users Test IAM Groups \u2691 Use the iam fixture defined in the test IAM users section : To create an instance use: user = iam . create_user ( UserName = \"User\" )[ \"User\" ] instance = iam . create_group ( GroupName = \"UserGroup\" )[ \"Group\" ] iam . add_user_to_group ( GroupName = instance [ \"GroupName\" ], UserName = user [ \"UserName\" ]) Check the official docs to check the method arguments: create_group add_user_to_group Issues \u2691 Support LaunchTemplates : Once they are, test clinv autoscaling group adapter support for launch templates. Support Route53 pagination : test clinv route53 update and update the test route53 section. cn-north-1 rds and autoscaling errors : increase the timeout of clinv, and test if the coverage has changed. References \u2691 Git Docs", "title": "Boto3"}, {"location": "boto3/#installation", "text": "pip install boto3", "title": "Installation"}, {"location": "boto3/#usage", "text": "", "title": "Usage"}, {"location": "boto3/#s3", "text": "", "title": "S3"}, {"location": "boto3/#list-the-files-of-a-bucket", "text": "def list_s3_by_prefix ( bucket : str , key_prefix : str , max_results : int = 100 ) -> List [ str ]: next_token = \"\" all_keys = [] while True : if next_token : res = s3 . list_objects_v2 ( Bucket = bucket , ContinuationToken = next_token , Prefix = key_prefix ) else : res = s3 . list_objects_v2 ( Bucket = bucket , Prefix = key_prefix ) if \"Contents\" not in res : break if res [ \"IsTruncated\" ]: next_token = res [ \"NextContinuationToken\" ] else : next_token = \"\" keys = [ item [ \"Key\" ] for item in res [ \"Contents\" ]] all_keys . extend ( keys ) if not next_token : break return all_keys [ - 1 * max_results :] The boto3 doesn't have any way to sort the outputs of the bucket, you need to do them once you've loaded all the objects :S.", "title": "List the files of a bucket"}, {"location": "boto3/#ec2", "text": "", "title": "EC2"}, {"location": "boto3/#run-ec2-instance", "text": "Use the run_instances method of the ec2 client. Check their docs for the different configuration options. The required ones are MinCount and MaxCount . import boto3 ec2 = boto3 . client ( 'ec2' ) instance = ec2 . run_instances ( MinCount = 1 , MaxCount = 1 )", "title": "Run EC2 instance"}, {"location": "boto3/#get-instance-types", "text": "from pydantic import BaseModel import boto3 class InstanceType ( BaseModel ): \"\"\"Define model of the instance type. Args: id_: instance type name cpu_vcores: Number of virtual cpus (cores * threads) cpu_speed: Sustained clock speed in Ghz ram: RAM memory in MiB network_performance: price: Hourly cost \"\"\" id_ : str cpu_vcores : int cpu_speed : Optional [ int ] = None ram : int network_performance : str price : Optional [ float ] = None @property def cpu ( self ) -> int : \"\"\"Calculate the total Ghz available.\"\"\" if self . cpu_speed is None : return self . cpu_vcores return self . cpu_vcores * self . cpu_speed def get_instance_types () -> InstanceTypes : \"\"\"Get the available instance types.\"\"\" log . info ( \"Retrieving instance types\" ) instance_types : InstanceTypes = {} for type_ in _ec2_instance_types ( cpu_arch = \"x86_64\" ): instance = InstanceType ( id_ = instance_type , cpu_vcores = type_ [ \"VCpuInfo\" ][ \"DefaultVCpus\" ], ram = type_ [ \"MemoryInfo\" ][ \"SizeInMiB\" ], network_performance = type_ [ \"NetworkInfo\" ][ \"NetworkPerformance\" ], price = _ec2_price ( instance_type ), ) with suppress ( KeyError ): instance . cpu_speed = type_ [ \"ProcessorInfo\" ][ \"SustainedClockSpeedInGhz\" ] instance_types [ type_ [ \"InstanceType\" ]] = instance return instance_types", "title": "Get instance types"}, {"location": "boto3/#get-instance-prices", "text": "import json import boto3 from pkg_resources import resource_filename def _ec2_price ( instance_type : str , region_code : str = \"us-east-1\" , operating_system : str = \"Linux\" , preinstalled_software : str = \"NA\" , tenancy : str = \"Shared\" , is_byol : bool = False , ) -> Optional [ float ]: \"\"\"Get the price of an EC2 instance type.\"\"\" log . debug ( f \"Retrieving price of { instance_type } \" ) region_name = _get_region_name ( region_code ) if is_byol : license_model = \"Bring your own license\" else : license_model = \"No License required\" if tenancy == \"Host\" : capacity_status = \"AllocatedHost\" else : capacity_status = \"Used\" filters = [ { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"termType\" , \"Value\" : \"OnDemand\" }, { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"capacitystatus\" , \"Value\" : capacity_status }, { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"location\" , \"Value\" : region_name }, { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"instanceType\" , \"Value\" : instance_type }, { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"tenancy\" , \"Value\" : tenancy }, { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"operatingSystem\" , \"Value\" : operating_system }, { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"preInstalledSw\" , \"Value\" : preinstalled_software , }, { \"Type\" : \"TERM_MATCH\" , \"Field\" : \"licenseModel\" , \"Value\" : license_model }, ] pricing_client = boto3 . client ( \"pricing\" , region_name = \"us-east-1\" ) response = pricing_client . get_products ( ServiceCode = \"AmazonEC2\" , Filters = filters ) for price in response [ \"PriceList\" ]: price = json . loads ( price ) for on_demand in price [ \"terms\" ][ \"OnDemand\" ] . values (): for price_dimensions in on_demand [ \"priceDimensions\" ] . values (): price_value = price_dimensions [ \"pricePerUnit\" ][ \"USD\" ] return float ( price_value ) return None def _get_region_name ( region_code : str ) -> str : \"\"\"Extract the region name from it's code.\"\"\" endpoint_file = resource_filename ( \"botocore\" , \"data/endpoints.json\" ) with open ( endpoint_file , \"r\" , encoding = \"UTF8\" ) as f : endpoint_data = json . load ( f ) region_name = endpoint_data [ \"partitions\" ][ 0 ][ \"regions\" ][ region_code ][ \"description\" ] return region_name . replace ( \"Europe\" , \"EU\" )", "title": "Get instance prices"}, {"location": "boto3/#type-hints", "text": "AWS library doesn't have working type hints -.- , so you either use Any or dive into the myriad of packages that implement them. I've so far tried boto3_type_annotations , boto3-stubs , and mypy_boto3_builder without success. Any it is for now...", "title": "Type hints"}, {"location": "boto3/#testing", "text": "Programs that interact with AWS through boto3 create, change or get information on real AWS resources. When developing these programs, you don't want the testing framework to actually do those changes, as it might break things and cost you money. You need to find a way to intercept the calls to AWS and substitute them with the data their API would return. I've found three ways to achieve this: Manually mocking the boto3 methods used by the program with unittest.mock . Using moto . Using Botocore's Stubber . TL;DR Try to use moto, using the stubber as fallback option. Using unittest.mock forces you to know what the API is going to return and hardcode it in your tests. If the response changes, you need to update your tests, which is not good. moto is a library that allows you to easily mock out tests based on AWS infrastructure. It works well because it mocks out all calls to AWS automatically without requiring any dependency injection. The downside is that it goes behind boto3 so some of the methods you need to test won't be still implemented, that leads us to the third option. Botocore's Stubber is a class that allows you to stub out requests so you don't have to hit an endpoint to write tests. Responses are returned first in, first out. If operations are called out of order, or are called with no remaining queued responses, an error will be raised. It's like the first option but cleaner. If you go down this path, check adamj's post on testing S3 .", "title": "Testing"}, {"location": "boto3/#moto", "text": "moto's library lets you fictitiously create and change AWS resources as you normally do with the boto3 library. They mimic what the real methods do on fake objects. The Docs are awful though.", "title": "moto"}, {"location": "boto3/#install", "text": "pip install moto", "title": "Install"}, {"location": "boto3/#simple-usage", "text": "To understand better how it works, I'm going to show you an understandable example, it's not the best way to use it though, go to the usage section for production ready usage. Imagine you have a function that you use to launch new ec2 instances: import boto3 def add_servers ( ami_id , count ): client = boto3 . client ( 'ec2' , region_name = 'us-west-1' ) client . run_instances ( ImageId = ami_id , MinCount = count , MaxCount = count ) To test it we'd use: from . import add_servers from moto import mock_ec2 @mock_ec2 def test_add_servers (): add_servers ( 'ami-1234abcd' , 2 ) client = boto3 . client ( 'ec2' , region_name = 'us-west-1' ) instances = client . describe_instances ()[ 'Reservations' ][ 0 ][ 'Instances' ] assert len ( instances ) == 2 instance1 = instances [ 0 ] assert instance1 [ 'ImageId' ] == 'ami-1234abcd' The decorator @mock_ec2 tells moto to capture all boto3 calls to AWS. When we run the add_servers function to test, it will create the fake objects on the memory (without contacting AWS servers), and the client.describe_instances boto3 method returns the data of that fake data. Isn't it awesome?", "title": "Simple usage"}, {"location": "boto3/#usage_1", "text": "You can use it with decorators , context managers , directly or with pytest fixtures . Being a pytest fan, the last option looks the cleaner to me. To make sure that you don't change the real infrastructure, ensure that your tests have dummy environmental variables. File: tests/conftest.py @pytest . fixture () def _aws_credentials () -> None : \"\"\"Mock the AWS Credentials for moto.\"\"\" os . environ [ \"AWS_ACCESS_KEY_ID\" ] = \"testing\" os . environ [ \"AWS_SECRET_ACCESS_KEY\" ] = \"testing\" os . environ [ \"AWS_SECURITY_TOKEN\" ] = \"testing\" os . environ [ \"AWS_SESSION_TOKEN\" ] = \"testing\" @pytest . fixture () def ec2 ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 EC2 client.\"\"\" with mock_ec2 (): yield boto3 . client ( \"ec2\" , region_name = \"us-east-1\" ) The ec2 fixture can then be used in the tests to setup the environment or assert results.", "title": "Usage"}, {"location": "boto3/#testing-ec2", "text": "If you want to add security groups to the tests, you need to create the resource first. def test_ec2_with_security_groups ( ec2 : Any ) -> None : security_group_id = ec2 . create_security_group ( GroupName = \"TestSecurityGroup\" , Description = \"SG description\" )[ \"GroupId\" ] instance = ec2 . run_instances ( ImageId = \"ami-xxxx\" , MinCount = 1 , MaxCount = 1 , SecurityGroupIds = [ security_group_id ], )[ \"Instances\" ][ 0 ] # Test your code here To add tags, use: def test_ec2_with_security_groups ( ec2 : Any ) -> None : instance = ec2 . run_instances ( ImageId = \"ami-xxxx\" , MinCount = 1 , MaxCount = 1 , TagSpecifications = [ { \"ResourceType\" : \"instance\" , \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"instance name\" , }, ], } ], )[ \"Instances\" ][ 0 ] # Test your code here", "title": "Testing EC2"}, {"location": "boto3/#testing-rds", "text": "Use the rds fixture: from moto import mock_rds2 @pytest . fixture () def rds ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 RDS client.\"\"\" with mock_rds2 (): yield boto3 . client ( \"rds\" , region_name = \"us-east-1\" ) To create an instance use: instance = rds . create_db_instance ( DBInstanceIdentifier = \"db-xxxx\" , DBInstanceClass = \"db.m3.2xlarge\" , Engine = \"postgres\" , )[ \"DBInstance\" ] It won't have VPC information, if you need it, create the subnet group first (you'll need the ec2 fixture too): subnets = [ subnet [ 'SubnetId' ] for subnet in ec2 . describe_subnets ()[ \"Subnets\" ]] rds . create_db_subnet_group ( DBSubnetGroupName = \"dbsg\" , SubnetIds = subnets , DBSubnetGroupDescription = \"Text\" ) instance = rds . create_db_instance ( DBInstanceIdentifier = \"db-xxxx\" , DBInstanceClass = \"db.m3.2xlarge\" , Engine = \"postgres\" , DBSubnetGroupName = \"dbsg\" , )[ \"DBInstance\" ]", "title": "Testing RDS"}, {"location": "boto3/#testing-s3", "text": "Use the s3_mock fixture: from moto import mock_s3 @pytest . fixture () def s3_mock ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 S3 client.\"\"\" with mock_s3 (): yield boto3 . client ( \"s3\" ) To create an instance use: s3_mock . create_bucket ( Bucket = \"mybucket\" ) instance = s3_mock . list_buckets ()[ \"Buckets\" ][ 0 ] Check the official docs to check the create_bucket arguments.", "title": "Testing S3"}, {"location": "boto3/#testing-route53", "text": "Use the route53 fixture: from moto import mock_route53 @pytest . fixture ( name = 'route53' ) def route53_ ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 Route53 client.\"\"\" with mock_route53 (): yield boto3 . client ( \"route53\" ) To create an instance use: hosted_zone = route53 . create_hosted_zone ( Name = \"example.com\" , CallerReference = \"Test\" )[ \"HostedZone\" ] hosted_zone_id = re . sub ( \".hostedzone.\" , \"\" , hosted_zone [ \"Id\" ]) route53 . change_resource_record_sets ( ChangeBatch = { \"Changes\" : [ { \"Action\" : \"CREATE\" , \"ResourceRecordSet\" : { \"Name\" : \"example.com\" , \"ResourceRecords\" : [ { \"Value\" : \"192.0.2.44\" , }, ], \"TTL\" : 60 , \"Type\" : \"A\" , }, }, ], \"Comment\" : \"Web server for example.com\" , }, HostedZoneId = hosted_zone_id , ) You need to first create a hosted zone. The change_resource_record_sets order to create the instance doesn't return any data, so if you need to work on it, use the list_resource_record_sets method of the route53 client (you'll need to set the HostedZoneId argument). If you have more than 300 records, the endpoint gives you a paginated response, so if the IsTruncated attribute is True , you need to call the method again setting the StartRecordName and StartRecordType to the NextRecordName and NextRecordType response arguments. Not nice at all. Pagination is not yet supported by moto , so you won't be able to test that part of your code. Check the official docs to check the method arguments: create_hosted_zone . change_resource_record_sets .", "title": "Testing Route53"}, {"location": "boto3/#test-vpc", "text": "Use the ec2 fixture defined in the usage section . To create an instance use: instance = ec2 . create_vpc ( CidrBlock = \"172.16.0.0/16\" , )[ \"Vpc\" ] Check the official docs to check the method arguments: create_vpc . create_subnet .", "title": "Test VPC"}, {"location": "boto3/#testing-autoscaling-groups", "text": "Use the autoscaling fixture: from moto import mock_autoscaling @pytest . fixture ( name = 'autoscaling' ) def autoscaling_ ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 Autoscaling Group client.\"\"\" with mock_autoscaling (): yield boto3 . client ( \"autoscaling\" ) They don't yet support LaunchTemplates , so you'll have to use LaunchConfigurations. To create an instance use: autoscaling . create_launch_configuration ( LaunchConfigurationName = 'LaunchConfiguration' , ImageId = 'ami-xxxx' , InstanceType = 't2.medium' ) autoscaling . create_auto_scaling_group ( AutoScalingGroupName = 'ASG name' , MinSize = 1 , MaxSize = 3 , LaunchConfigurationName = 'LaunchConfiguration' , AvailabilityZones = [ 'us-east-1a' ]) instance = autoscaling . describe_auto_scaling_groups ()[ \"AutoScalingGroups\" ][ 0 ] Check the official docs to check the method arguments: create_auto_scaling_group . create_launch_configuration . describe_auto_scaling_groups .", "title": "Testing autoscaling groups"}, {"location": "boto3/#test-security-groups", "text": "Use the ec2 fixture defined in the usage section . To create an instance use: instance_id = ec2 . create_security_group ( GroupName = \"TestSecurityGroup\" , Description = \"SG description\" )[ \"GroupId\" ] instance = ec2 . describe_security_groups ( GroupIds = [ instance_id ]) To add permissions to the security group you need to use the authorize_security_group_ingress and authorize_security_group_egress methods. ec2 . authorize_security_group_ingress ( GroupId = instance_id , IpPermissions = [ { \"IpProtocol\" : \"tcp\" , \"FromPort\" : 80 , \"ToPort\" : 80 , \"IpRanges\" : [{ \"CidrIp\" : \"0.0.0.0/0\" }], }, ], ) By default, the created security group comes with an egress rule to allow all traffic. To remove rules use the revoke_security_group_egress and revoke_security_group_ingress methods. ec2 . revoke_security_group_egress ( GroupId = instance_id , IpPermissions = [ { \"IpProtocol\" : \"-1\" , \"IpRanges\" : [{ \"CidrIp\" : \"0.0.0.0/0\" }]}, ], ) Check the official docs to check the method arguments: create_security_group . describe_security_group . authorize_security_group_ingress . authorize_security_group_egress . revoke_security_group_ingress . revoke_security_group_egress .", "title": "Test Security Groups"}, {"location": "boto3/#test-iam-users", "text": "Use the iam fixture: from moto import mock_iam @pytest . fixture ( name = 'iam' ) def iam_ ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 IAM client.\"\"\" with mock_iam (): yield boto3 . client ( \"iam\" ) To create an instance use: instance = iam . create_user ( UserName = \"User\" )[ \"User\" ] Check the official docs to check the method arguments: create_user list_users", "title": "Test IAM users"}, {"location": "boto3/#test-iam-groups", "text": "Use the iam fixture defined in the test IAM users section : To create an instance use: user = iam . create_user ( UserName = \"User\" )[ \"User\" ] instance = iam . create_group ( GroupName = \"UserGroup\" )[ \"Group\" ] iam . add_user_to_group ( GroupName = instance [ \"GroupName\" ], UserName = user [ \"UserName\" ]) Check the official docs to check the method arguments: create_group add_user_to_group", "title": "Test IAM Groups"}, {"location": "boto3/#issues", "text": "Support LaunchTemplates : Once they are, test clinv autoscaling group adapter support for launch templates. Support Route53 pagination : test clinv route53 update and update the test route53 section. cn-north-1 rds and autoscaling errors : increase the timeout of clinv, and test if the coverage has changed.", "title": "Issues"}, {"location": "boto3/#references", "text": "Git Docs", "title": "References"}, {"location": "calendar_management/", "text": "Since the break of my taskwarrior instance I've used a physical calendar to manage the tasks that have a specific date. Can't wait for the first version of pydo to be finished. The next factors made me search for a temporal solution: It's taking longer than expected. I've started using a nextcloud calendar with some friends. I frequently use Google calendar at work. I'm sick of having to log in Nexcloud and Google to get the day's appointments. To fulfill my needs the solution needs to: Import calendar events from different sources, basically through the CalDAV protocol. Have a usable terminal user interface Optionally have a command line interface or python library so it's easy to make scripts. Optionally it can be based in python so it's easy to contribute Support having a personal calendar mixed with the shared ones. Show all calendars in the same interface Khal \u2691 Looking at the available programs I found khal , which looks like it may be up to the task. Go through the installation steps and configure the instance to have a local calendar. If you want to sync your calendar events through CalDAV, you need to set vdirsyncer .", "title": "Calendar Management"}, {"location": "calendar_management/#khal", "text": "Looking at the available programs I found khal , which looks like it may be up to the task. Go through the installation steps and configure the instance to have a local calendar. If you want to sync your calendar events through CalDAV, you need to set vdirsyncer .", "title": "Khal"}, {"location": "calendar_versioning/", "text": "Calendar Versioning is a versioning convention based on your project's release calendar, instead of arbitrary numbers. CalVer suggests version number to be in format of: YEAR.MONTH.sequence . For example, 20.1 indicates a release in 2020 January, while 20.5.2 indicates a release that occurred in 2020 May, while the 2 indicates this is the third release of the month. You can see it looks similar to semantic versioning and has the benefit that a later release qualifies as bigger than an earlier one within the semantic versioning world (which mandates that a version number must grow monotonically). This makes it easy to use in all places where semantic versioning can be used. The idea here is that if the only maintained version is the latest, then we might as well use the version number to indicate the release date to signify just how old of a version you\u2019re using. You also have the added benefit that you can make calendar-based promises. For example, Ubuntu offers five years of support, therefore given version 20.04 you can quickly determine that it will be supported up to April 2025. When to use CalVer \u2691 Check the Deciding what version system to use for your programs article section. References \u2691 Home", "title": "Calendar Versioning"}, {"location": "calendar_versioning/#when-to-use-calver", "text": "Check the Deciding what version system to use for your programs article section.", "title": "When to use CalVer"}, {"location": "calendar_versioning/#references", "text": "Home", "title": "References"}, {"location": "changelog/", "text": "A changelog is a file which contains a curated, chronologically ordered list of notable changes for each version of a project. It's purpose is to make it easier for users and contributors to see precisely what notable changes have been made between each release (or version) of the project. Types of changes \u2691 Added for new features. Changed for changes in existing functionality. Deprecated for soon-to-be removed features. Removed for now removed features. Fixed for any bug fixes. Security in case of vulnerabilities. Changelog Guidelines \u2691 Good changelogs follow the next principles: Changelogs are for humans, not machines. There should be an entry for every single version. The same types of changes should be grouped. Versions and sections should be linkable. The latest version comes first. The release date of each version is displayed. Mention your versioning strategy . Call it CHANGELOG.md . Some examples of bad changelogs are: Commit log diffs : The purpose of a changelog entry is to document the noteworthy difference, often across multiple commits, to communicate them clearly to end users. If someone wants to see the commit log diffs they can access it through the git command. Ignoring Deprecations : When people upgrade from one version to another, it should be painfully clear when something will break. It should be possible to upgrade to a version that lists deprecations, remove what's deprecated, then upgrade to the version where the deprecations become removals. Confusing Dates : Regional date formats vary throughout the world and it's often difficult to find a human-friendly date format that feels intuitive to everyone. The advantage of dates formatted like 2017-07-17 is that they follow the order of largest to smallest units: year, month, and day. This format also doesn't overlap in ambiguous ways with other date formats, unlike some regional formats that switch the position of month and day numbers. These reasons, and the fact this date format is an ISO standard, are why it is the recommended date format for changelog entries. How to reduce the effort required to maintain a changelog \u2691 There are two ways to ease the burden of maintaining a changelog: Build it automatically \u2691 If you use Semantic Versioning you can use the commitizen tool to automatically generate the changelog each time you cut a new release by running cz bump --changelog --no-verify . The --no-verify part is required if you use pre-commit hooks . Use the Unreleased section \u2691 Keep an Unreleased section at the top to track upcoming changes. This serves two purposes: People can see what changes they might expect in upcoming releases. At release time, you can move the Unreleased section changes into a new release version section. References \u2691 Keep a Changelog", "title": "Keep a Changelog"}, {"location": "changelog/#types-of-changes", "text": "Added for new features. Changed for changes in existing functionality. Deprecated for soon-to-be removed features. Removed for now removed features. Fixed for any bug fixes. Security in case of vulnerabilities.", "title": "Types of changes"}, {"location": "changelog/#changelog-guidelines", "text": "Good changelogs follow the next principles: Changelogs are for humans, not machines. There should be an entry for every single version. The same types of changes should be grouped. Versions and sections should be linkable. The latest version comes first. The release date of each version is displayed. Mention your versioning strategy . Call it CHANGELOG.md . Some examples of bad changelogs are: Commit log diffs : The purpose of a changelog entry is to document the noteworthy difference, often across multiple commits, to communicate them clearly to end users. If someone wants to see the commit log diffs they can access it through the git command. Ignoring Deprecations : When people upgrade from one version to another, it should be painfully clear when something will break. It should be possible to upgrade to a version that lists deprecations, remove what's deprecated, then upgrade to the version where the deprecations become removals. Confusing Dates : Regional date formats vary throughout the world and it's often difficult to find a human-friendly date format that feels intuitive to everyone. The advantage of dates formatted like 2017-07-17 is that they follow the order of largest to smallest units: year, month, and day. This format also doesn't overlap in ambiguous ways with other date formats, unlike some regional formats that switch the position of month and day numbers. These reasons, and the fact this date format is an ISO standard, are why it is the recommended date format for changelog entries.", "title": "Changelog Guidelines"}, {"location": "changelog/#how-to-reduce-the-effort-required-to-maintain-a-changelog", "text": "There are two ways to ease the burden of maintaining a changelog:", "title": "How to reduce the effort required to maintain a changelog"}, {"location": "changelog/#build-it-automatically", "text": "If you use Semantic Versioning you can use the commitizen tool to automatically generate the changelog each time you cut a new release by running cz bump --changelog --no-verify . The --no-verify part is required if you use pre-commit hooks .", "title": "Build it automatically"}, {"location": "changelog/#use-the-unreleased-section", "text": "Keep an Unreleased section at the top to track upcoming changes. This serves two purposes: People can see what changes they might expect in upcoming releases. At release time, you can move the Unreleased section changes into a new release version section.", "title": "Use the Unreleased section"}, {"location": "changelog/#references", "text": "Keep a Changelog", "title": "References"}, {"location": "code_learning/", "text": "Learning to code is a never ending, rewarding, frustrating, enlightening task. In this article you can see what is the generic roadmap (in my personal opinion) of a developer. As each of us is different, probably a generic roadmap won't suit your needs perfectly, if you are new to coding, I suggest you find a mentor so you can both tweak it to your case. Learning methods \u2691 Not all of us like to learn in the same way, first you need to choose how do you want to learn, for example through: Attendance-based courses. Online courses. Video courses. Reading books. Whichever you choose make sure you have regular feedback from other humans such as: Mentors . Learning communities. Friends. Roadmap \u2691 The roadmap is divided in these main phases: Beginner : You start from scratch and need to get the basic knowledge, skills and tool set to set the learning ball rolling. At the end of this phase you will be able to develop simple pieces of code and collaborate with other projects with the help of a mentor. Junior : In this phase you'll learn how to: Improve the quality of your code through the use of testing, documentation, linters, fixers and other techniques. Be more proficient with your development environment. At the end of the phase you'll become a senior developer that's able to code autonomously by: Creating a whole project from start to finish. Contribute to other open source projects by yourself. Senior : In this never ending phase you keep on improving your development skills, knowledge and tools. Don't try to rush, this is a lifetime roadmap, depending on how much time you put into learning the first steps may take from months to one or two years, the refinement phase from 2 to 8 years, and the enhancement phase never ends. Beginner \u2691 First steps are hard, you're entering a whole new world that mostly looks like magic to you. Probably you'll feel overwhelmed by the path ahead but don't fret, as every path, it's doable one step at a time. First steps are also exciting so try to channel all that energy into the learning process to overcome the obstacles you find in your way. In this section you'll learn how to start walking in the development world by: Setting up the development environment . Learning the basics Setup your development environment \u2691 Editor \u2691 TBD Git \u2691 Git is a software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development. Its goals include speed, data integrity, and support for distributed, non-linear workflows (thousands of parallel branches running on different systems). Git is a tough nut to crack, no matter how experience you are you'll frequently get surprised. Sadly it's one of the main tools to develop your code, so you must master it as soon as possible. I've listed you some resources here on how to start. From that article I think it's also interesting that you read about: Pull Request process Git workflow Language specific environment \u2691 Learn the basics \u2691 Now it's the time to study, choose your desired learning method and follow them until you get the basics of the type of developer you want to become, for example: Frontend developer . In parallel it's crucial to learn Git as soon as you can, it's the main tool to collaborate with other developers and your safety net in the development workflow. Searching for information \u2691 Search engines Github Junior \u2691 TBD Senior \u2691 TBD", "title": "Learning to code"}, {"location": "code_learning/#learning-methods", "text": "Not all of us like to learn in the same way, first you need to choose how do you want to learn, for example through: Attendance-based courses. Online courses. Video courses. Reading books. Whichever you choose make sure you have regular feedback from other humans such as: Mentors . Learning communities. Friends.", "title": "Learning methods"}, {"location": "code_learning/#roadmap", "text": "The roadmap is divided in these main phases: Beginner : You start from scratch and need to get the basic knowledge, skills and tool set to set the learning ball rolling. At the end of this phase you will be able to develop simple pieces of code and collaborate with other projects with the help of a mentor. Junior : In this phase you'll learn how to: Improve the quality of your code through the use of testing, documentation, linters, fixers and other techniques. Be more proficient with your development environment. At the end of the phase you'll become a senior developer that's able to code autonomously by: Creating a whole project from start to finish. Contribute to other open source projects by yourself. Senior : In this never ending phase you keep on improving your development skills, knowledge and tools. Don't try to rush, this is a lifetime roadmap, depending on how much time you put into learning the first steps may take from months to one or two years, the refinement phase from 2 to 8 years, and the enhancement phase never ends.", "title": "Roadmap"}, {"location": "code_learning/#beginner", "text": "First steps are hard, you're entering a whole new world that mostly looks like magic to you. Probably you'll feel overwhelmed by the path ahead but don't fret, as every path, it's doable one step at a time. First steps are also exciting so try to channel all that energy into the learning process to overcome the obstacles you find in your way. In this section you'll learn how to start walking in the development world by: Setting up the development environment . Learning the basics", "title": "Beginner"}, {"location": "code_learning/#setup-your-development-environment", "text": "", "title": "Setup your development environment"}, {"location": "code_learning/#editor", "text": "TBD", "title": "Editor"}, {"location": "code_learning/#git", "text": "Git is a software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development. Its goals include speed, data integrity, and support for distributed, non-linear workflows (thousands of parallel branches running on different systems). Git is a tough nut to crack, no matter how experience you are you'll frequently get surprised. Sadly it's one of the main tools to develop your code, so you must master it as soon as possible. I've listed you some resources here on how to start. From that article I think it's also interesting that you read about: Pull Request process Git workflow", "title": "Git"}, {"location": "code_learning/#language-specific-environment", "text": "", "title": "Language specific environment"}, {"location": "code_learning/#learn-the-basics", "text": "Now it's the time to study, choose your desired learning method and follow them until you get the basics of the type of developer you want to become, for example: Frontend developer . In parallel it's crucial to learn Git as soon as you can, it's the main tool to collaborate with other developers and your safety net in the development workflow.", "title": "Learn the basics"}, {"location": "code_learning/#searching-for-information", "text": "Search engines Github", "title": "Searching for information"}, {"location": "code_learning/#junior", "text": "TBD", "title": "Junior"}, {"location": "code_learning/#senior", "text": "TBD", "title": "Senior"}, {"location": "cone/", "text": "Cone is a mobile ledger application compatible with beancount . I use it as part of my accounting automation workflow . Installation \u2691 Download the application from F-droid . It assumes that you have a txt file to store the information . As it doesn't yet support the edition or deletion of transactions , I suggest you create the ledger.txt file with your favorite mobile editor such as Markor . Open the application and load the ledger.txt file. Usage \u2691 To be compliant with my beancount ledger: I've initialized the ledger.txt file with the open statements of the beancount accounts, so the transaction UI autocompletes them. Cone doesn't still support the beancount format by default, so in the description of the transaction I also introduce the payee. For example: * \"payee1\" \"Bought X instead of just Bought X . If I need to edit or delete a transaction, I change it with the Markor editor. To send the ledger file to the computer, I use either Share via HTTP or Termux through ssh. References \u2691 Git Docs", "title": "cone"}, {"location": "cone/#installation", "text": "Download the application from F-droid . It assumes that you have a txt file to store the information . As it doesn't yet support the edition or deletion of transactions , I suggest you create the ledger.txt file with your favorite mobile editor such as Markor . Open the application and load the ledger.txt file.", "title": "Installation"}, {"location": "cone/#usage", "text": "To be compliant with my beancount ledger: I've initialized the ledger.txt file with the open statements of the beancount accounts, so the transaction UI autocompletes them. Cone doesn't still support the beancount format by default, so in the description of the transaction I also introduce the payee. For example: * \"payee1\" \"Bought X instead of just Bought X . If I need to edit or delete a transaction, I change it with the Markor editor. To send the ledger file to the computer, I use either Share via HTTP or Termux through ssh.", "title": "Usage"}, {"location": "cone/#references", "text": "Git Docs", "title": "References"}, {"location": "contact/", "text": "I'm available through: Email: hello at spielprinzip.com", "title": "Contact"}, {"location": "cooking/", "text": "Cooking as defined in Wikipedia, is the art, science, and craft of using heat to prepare food for consumption. It sounds like an enlightening experience that brings you joy. Reality then slaps you in the face yet once again. It's very different to cook because you want to, than cooking because you need to. I love to eat, but have always hated to cook, mainly because I've always seen cooking with that second view. Being something disgusting that I had to do, pushed me to batch cook once a week as quickly as possible, and to buy prepared food from the local squat center's tavern. Now I aim to shift my point of view to enjoy the time invested preparing food. Two are the main reasons: I'm going to spend a great amount of my life in front of the stove, so I'd better enjoy it, and probably the end result would be better for my well being. One way that can help me with the switch, is to understand the science behind it and be precise with the process. Thus this section was born, I'll start with the very basics and build from there on.", "title": "Cooking"}, {"location": "cooking_basics/", "text": "All great recipes are based on the same basic principles and processes, these are the cooking snippets that I've needed. Boiling an egg \u2691 Cooking an egg well is a matter of time. Put enough water in the pot so that the eggs are completely covered. Add a pinch of salt and a dash of vinegar. Let the water boil. Use a kettle to heat it if you have one. Add the eggs. Depending on the type of egg you want, you need to wait more or less time: 5-6 minutes: You'll get soft boiled eggs, whose yolk is liquid and the white is semi-liquid. 7 minutes: mollet egs, with semi-liquid yolk and curdled white. 10-12 minutes: boiled eggs, compact white and curdled yolk. Pour off the hot water, shake it gently to crack the eggs, and add cold water, with even a few ice cubes. Wait 5 minutes if you want to serve them warm, or 15 otherwise, and then peel them under the same water. Here are some tips to improve your chances to get the perfect egg: Use fresh eggs, when they've been in the fridge for a while, they get dehydrated and the air that's inside gets expanded. That's why, when you put an egg into a glass of water, if it doesn't stay at the bottom you'd better not use it. Take them out of the fridge an hour before cooking them. (Yeah, like you're going to remember to do it :P). Cooking legumes \u2691 Legumes are wonderful, to squeeze their value remember to: Don't use old ones: If you're legumes are older than a year, they can be old . They loose water with the time up to a point that they can be impossible to cook. Soak them: Some legumes like the lentils don't need to be soaked, but for most of them it's better to be hydrated before putting them in the pot. For chickpeas and beans, the best is to soak them for 10 to 12 hours. They'll drink the water, so add it until you double the volume of the legumes. Once done, discard that water, rinse them and use new one for the pot. That way you'll prevent the acids and oligosaccharides that promotes a heavy digestion. * Don't scare them: Don't cut the cooking with cold water, they won't help you avoid farting and for some great chefs it's one of the worst errors you can do, at least with the chickpeas. Remember always to have enough water from the start to avoid this situation. * Know when to add them in the pot: chickpeas need to be added when the water is already boiling When you're using boiled legumes in your recipes, be careful, after the hydration, they weight the double! Boil chickpeas when you've forgotten to soak them \u2691 Soaked chickpeas take one or two hours to cook in a normal pot, 20 to 30 in a fast pot, which saves a ton of energy. If you forgot to soak them, add a level teaspoon of baking soda to the pot and cook them as usual. When not using a fast pot, you'll need to periodically remove the foam that will be created. The problem with this method is that you don't discard the first water round, and they can be more indigestible. Another option is to cook them for an hour, change the water and then cook them again.", "title": "Cooking Basics"}, {"location": "cooking_basics/#boiling-an-egg", "text": "Cooking an egg well is a matter of time. Put enough water in the pot so that the eggs are completely covered. Add a pinch of salt and a dash of vinegar. Let the water boil. Use a kettle to heat it if you have one. Add the eggs. Depending on the type of egg you want, you need to wait more or less time: 5-6 minutes: You'll get soft boiled eggs, whose yolk is liquid and the white is semi-liquid. 7 minutes: mollet egs, with semi-liquid yolk and curdled white. 10-12 minutes: boiled eggs, compact white and curdled yolk. Pour off the hot water, shake it gently to crack the eggs, and add cold water, with even a few ice cubes. Wait 5 minutes if you want to serve them warm, or 15 otherwise, and then peel them under the same water. Here are some tips to improve your chances to get the perfect egg: Use fresh eggs, when they've been in the fridge for a while, they get dehydrated and the air that's inside gets expanded. That's why, when you put an egg into a glass of water, if it doesn't stay at the bottom you'd better not use it. Take them out of the fridge an hour before cooking them. (Yeah, like you're going to remember to do it :P).", "title": "Boiling an egg"}, {"location": "cooking_basics/#cooking-legumes", "text": "Legumes are wonderful, to squeeze their value remember to: Don't use old ones: If you're legumes are older than a year, they can be old . They loose water with the time up to a point that they can be impossible to cook. Soak them: Some legumes like the lentils don't need to be soaked, but for most of them it's better to be hydrated before putting them in the pot. For chickpeas and beans, the best is to soak them for 10 to 12 hours. They'll drink the water, so add it until you double the volume of the legumes. Once done, discard that water, rinse them and use new one for the pot. That way you'll prevent the acids and oligosaccharides that promotes a heavy digestion. * Don't scare them: Don't cut the cooking with cold water, they won't help you avoid farting and for some great chefs it's one of the worst errors you can do, at least with the chickpeas. Remember always to have enough water from the start to avoid this situation. * Know when to add them in the pot: chickpeas need to be added when the water is already boiling When you're using boiled legumes in your recipes, be careful, after the hydration, they weight the double!", "title": "Cooking legumes"}, {"location": "cooking_basics/#boil-chickpeas-when-youve-forgotten-to-soak-them", "text": "Soaked chickpeas take one or two hours to cook in a normal pot, 20 to 30 in a fast pot, which saves a ton of energy. If you forgot to soak them, add a level teaspoon of baking soda to the pot and cook them as usual. When not using a fast pot, you'll need to periodically remove the foam that will be created. The problem with this method is that you don't discard the first water round, and they can be more indigestible. Another option is to cook them for an hour, change the water and then cook them again.", "title": "Boil chickpeas when you've forgotten to soak them"}, {"location": "cooking_software/", "text": "While using grocy to manage my recipes I've found what I want from a recipe manager: Write recipes in plaintext files. Import recipes from urls. Import other recipes in a recipe. To avoid code repetition. For example if I want to do raviolis carbonara, I want to have a basic recipe to create raviolis and another for carbonara, then a normal recipe that imports both. Define processes that can be imported as steps in recipes, for example boil water Change the number the servings Annotate lessons learned Translate common cooking units (spoonful, a piece) into metric system units of volume and weight. Helper to follow the recipe while cooking Keep track of the evolution of the recipe each time you cook it with deviations from plan, evaluation of the result dish, lessons learned and changes made to the recipe. Attach image to the recipe Specify the storage size (a ration of lentils uses 2cm of a tupper). Give rating to recipes Track the number of times you do a recipe Grade the maturity level of a recipe by the number of times done and the number of changes. Browse all available recipes with the possibility of: Search by name Search by ingredient Filter by season Filter by meal type (lunch, dinner, dessert...) Sort by rating Sort by number of times cooked Sort by last time cooked Show never cooked recipes Do a meal plan Create reusable meal plans Be able to talk to the inventory management tool to: See if there are enough ingredients. Fill up the shopping list. Select which recipes I want to cook this month, the program should show me the available ones taking into account the season. Keep track of the season of the ingredients, so you can mark an ingredient as in season or out of season, and that will allow you to select which recipes to add, or tell you which recipes are no longer going to be available. Define variations of a recipe, imagine that instead of red pepper you have green Be able to tell the brand of an ingredient Define the cooking tools to use, which will be shown when preparing a recipe Define the preparing and clean steps of a cooking tool. Show the number of inactivity interruptions, their time and the total amount of inactive time. Calculate the recipe time from the times of the processes involved and subsequent recipes Be able to define where each process is carried out, what is the distance between the places Suggest optimizations in the cooking process: Reorder of steps to reduce waiting time and movement Be able to start from the basic features and incrementally add on it Software review \u2691 Grocy \u2691 Grocy is an awesome software to manage your inventory, it's also a good one to manage your recipes, with an awesome integration with the inventory management. In fact, I've been using it for some years. Nevertheless, you can't: Write them in plaintext. Reuse recipes with other recipes in a pleasant way. Do variations of a recipe. Track the variations of a recipe. So I think whatever I choose to manage recipes needs to be able to speak to a Grocy instance at least to get an idea of the stock available and to complete the shopping list. Cooklang \u2691 Cooklang looks real good, you write the recipes in plaintext but the syntax is not as smooth as I'd like: Then add @salt and @ground black pepper{} to taste. I'd like the parser to be able to detect the ingredient salt without the need of the @ , and I don't either like how to specify the measurements: Place @bacon strips{1%kg} on a baking sheet and glaze with @syrup{1/2%tbsp}. On the other side there is more less popular: Their spec has their spec 400 stars. It has a cli, and an Android and ios apps There is a vim plugin for the syntax. You can import recipes from urls. There are some recipe books , some of them look nice with a mkdocs frontend. It could be used as part of the system, but it falls short in many of my desired features. KookBook \u2691 KookBook is KDE solution for plaintext recipe management. Their documentation is sparse and not popular at all. I don't feel like using it. RecipeSage \u2691 RecipeSage is free personal recipe keeper, meal planner, and shopping list manager for Web, IOS, and Android. Quickly capture and save recipes from any website simply by entering the website URL. Sync your recipes, meal plans, and shopping lists between all of your devices. Share your recipes, shopping lists, and meal plans with family and friends. It looks good, but I'd use grocy instead. Mealie \u2691 Mealie is a self hosted recipe manager and meal planner with a RestAPI backend and a reactive frontend application built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the url and mealie will automatically import the relevant data or add a family recipe with the UI editor. It does have an API , but it looks too complex. Maybe to be used as a backend to retrieve recipes from the internet, but no plaintext recipes. Chowdown \u2691 Chowdown is a simple, plaintext recipe database for hackers. It has nice features: You write your recipes in Markdown. You can easily import recipes in other recipes. An example would be: --- layout: recipe title: \"Broccoli Beer Cheese Soup\" image: broccoli-beer-cheese-soup.jpg tags: sides, soups ingredients: - 4 tablespoons butter - 1 cup diced onion - 1/2 cup shredded carrot - 1/2 cup diced celery - 1 tablespoon garlic - 1/4 cup flour - 1 quart chicken broth - 1 cup heavy cream - 10 ounces muenster cheese - 1 cup white white wine - 1 cup pale beer - 1 teaspoon Worcestershire sauce - 1/2 teaspoon hot sauce directions: - Start with butter, onions, carrots, celery, garlic until cooked down - Add flour, stir well, cook for 4-5 mins - Add chicken broth, bring to a boil - Add wine and reduce to a simmer - Add cream, cheese, Worcestershire, and hot sauce - Serve with croutons --- This recipe is inspired by one of my favorites, Gourmand's Beer Cheese Soup, which uses Shiner Bock. Feel free to use whatever you want, then go to [Gourmand's](http://lovethysandwich.com) to have the real thing. Or using other recipes: --- layout: recipe title: \"Red Berry Tart\" image: red-berry-tart.jpg tags: desserts directions: - Bake the crust and let it cool - Make the custard, pour into crust - Make the red berry topping, spread over the top components: - Graham Cracker Crust - Vanilla Custard Filling - Red Berry Dessert Topping --- A favorite when I go to BBQs (parties, hackathons, your folks' place), this red berry tart is fairly easy to make and packs a huge wow factor. Where Graham Cracker Crust is another recipe. The outcome is nice too . Downsides are: It's written in HTML and javascript. They don't answer issues so it looks unmaintained. It redirects to an interesting schema of a recipe . https://github.com/clarklab/chowdown https://raw.githubusercontent.com/clarklab/chowdown/gh-pages/_recipes/broccoli-cheese-soup.md https://www.paprikaapp.com/ Recipes \u2691 https://docs.tandoor.dev/features/authentication/ Chef \u2691", "title": "Cooking software"}, {"location": "cooking_software/#software-review", "text": "", "title": "Software review"}, {"location": "cooking_software/#grocy", "text": "Grocy is an awesome software to manage your inventory, it's also a good one to manage your recipes, with an awesome integration with the inventory management. In fact, I've been using it for some years. Nevertheless, you can't: Write them in plaintext. Reuse recipes with other recipes in a pleasant way. Do variations of a recipe. Track the variations of a recipe. So I think whatever I choose to manage recipes needs to be able to speak to a Grocy instance at least to get an idea of the stock available and to complete the shopping list.", "title": "Grocy"}, {"location": "cooking_software/#cooklang", "text": "Cooklang looks real good, you write the recipes in plaintext but the syntax is not as smooth as I'd like: Then add @salt and @ground black pepper{} to taste. I'd like the parser to be able to detect the ingredient salt without the need of the @ , and I don't either like how to specify the measurements: Place @bacon strips{1%kg} on a baking sheet and glaze with @syrup{1/2%tbsp}. On the other side there is more less popular: Their spec has their spec 400 stars. It has a cli, and an Android and ios apps There is a vim plugin for the syntax. You can import recipes from urls. There are some recipe books , some of them look nice with a mkdocs frontend. It could be used as part of the system, but it falls short in many of my desired features.", "title": "Cooklang"}, {"location": "cooking_software/#kookbook", "text": "KookBook is KDE solution for plaintext recipe management. Their documentation is sparse and not popular at all. I don't feel like using it.", "title": "KookBook"}, {"location": "cooking_software/#recipesage", "text": "RecipeSage is free personal recipe keeper, meal planner, and shopping list manager for Web, IOS, and Android. Quickly capture and save recipes from any website simply by entering the website URL. Sync your recipes, meal plans, and shopping lists between all of your devices. Share your recipes, shopping lists, and meal plans with family and friends. It looks good, but I'd use grocy instead.", "title": "RecipeSage"}, {"location": "cooking_software/#mealie", "text": "Mealie is a self hosted recipe manager and meal planner with a RestAPI backend and a reactive frontend application built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the url and mealie will automatically import the relevant data or add a family recipe with the UI editor. It does have an API , but it looks too complex. Maybe to be used as a backend to retrieve recipes from the internet, but no plaintext recipes.", "title": "Mealie"}, {"location": "cooking_software/#chowdown", "text": "Chowdown is a simple, plaintext recipe database for hackers. It has nice features: You write your recipes in Markdown. You can easily import recipes in other recipes. An example would be: --- layout: recipe title: \"Broccoli Beer Cheese Soup\" image: broccoli-beer-cheese-soup.jpg tags: sides, soups ingredients: - 4 tablespoons butter - 1 cup diced onion - 1/2 cup shredded carrot - 1/2 cup diced celery - 1 tablespoon garlic - 1/4 cup flour - 1 quart chicken broth - 1 cup heavy cream - 10 ounces muenster cheese - 1 cup white white wine - 1 cup pale beer - 1 teaspoon Worcestershire sauce - 1/2 teaspoon hot sauce directions: - Start with butter, onions, carrots, celery, garlic until cooked down - Add flour, stir well, cook for 4-5 mins - Add chicken broth, bring to a boil - Add wine and reduce to a simmer - Add cream, cheese, Worcestershire, and hot sauce - Serve with croutons --- This recipe is inspired by one of my favorites, Gourmand's Beer Cheese Soup, which uses Shiner Bock. Feel free to use whatever you want, then go to [Gourmand's](http://lovethysandwich.com) to have the real thing. Or using other recipes: --- layout: recipe title: \"Red Berry Tart\" image: red-berry-tart.jpg tags: desserts directions: - Bake the crust and let it cool - Make the custard, pour into crust - Make the red berry topping, spread over the top components: - Graham Cracker Crust - Vanilla Custard Filling - Red Berry Dessert Topping --- A favorite when I go to BBQs (parties, hackathons, your folks' place), this red berry tart is fairly easy to make and packs a huge wow factor. Where Graham Cracker Crust is another recipe. The outcome is nice too . Downsides are: It's written in HTML and javascript. They don't answer issues so it looks unmaintained. It redirects to an interesting schema of a recipe . https://github.com/clarklab/chowdown https://raw.githubusercontent.com/clarklab/chowdown/gh-pages/_recipes/broccoli-cheese-soup.md https://www.paprikaapp.com/", "title": "Chowdown"}, {"location": "cooking_software/#recipes", "text": "https://docs.tandoor.dev/features/authentication/", "title": "Recipes"}, {"location": "cooking_software/#chef", "text": "", "title": "Chef"}, {"location": "cpu/", "text": "A central processing unit or CPU , also known as the brain of the server, is the electronic circuitry that executes instructions comprising a computer program. The CPU performs basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions in the program. The main processor factors you should consider the most while buying a server are: Clock speed: Is measured in gigahertz (GHz). The higher the clock speed, the faster the processor will calculate. The clock speed along with the bit width conveys us that in a single second how much data can flow. If a processor has a speed of 2.92 GHz and the bit width of 32 bits, then it means that it can process almost 3 billion units of 32 bits of data per second. Cores: Every processor core handles individual processing tasks. A multi-core processor can process multiple computing instructions in parallel.. Threads: Threads refer to the number of processors a chip can handle simultaneously. The difference between threads and cores is that cores are physical processing units in charge of computing processes. In contrast, threads are virtual components that run tasks as software would. Cache: Used by the processor to speed up the access to instructions and data between the processor and the RAM. There are three types of cache you will come across while looking at the specification sheet: L1: Is the fastest, but cramped. L2: Is roomier, but slower. L3: Is spacious, but comparatively slower than above two. Speed \u2691 To achieve the same speed you can play with the speed of a core and the number of cores. Having more cores and lesser clock speed has the next advantages: Processors with higher cores deliver higher performance and are cost-effective. Applications supporting multi-threading would benefit from a higher number of cores. Multi-threading support for applications will continue to enhance over time. Easily run more applications without experiencing the performance drop. Great for virtualization and running multiple virtual machines. With the disadvantages that it offers lower single threaded performance. On the other hand, using fewer cores and higher clock speed gives the next advantages: Offers better single threaded performance. Lower cost. And the next disadvantages Due to the lower number of cores, it becomes difficult to split between applications. Not so strong as multi-threading performance. Providers \u2691 AMD \u2691 The Ryzen family is broken down into four distinct branches: Ryzen 3: entry-level. Ryzen 5: mainstream. Ryzen 7: performance Ryzen 9: high-end They're all great chips in their own ways, but some certainly offer more value than others, and for many, the most powerful chips will be complete overkill. Market analysis \u2691 Property Ryzen 7 5800x Ryzen 5 5600x Ryzen 7 5700x Ryzen 5 5600G Cores 8 6 8 6 Threads 16 12 16 12 Clock 3.8 3.7 3.4 3.9 Socket AM4 AM4 AM4 AM4 PCI 4.0 4.0 4.0 3.0 Thermal Not included Wraith Stealth Not included Wraith Stealth Default TDP 105W 65W 65W 65W System Mem spec >= 3200 MHz >= 3200 MHz >= 3200 MHz >= 3200 MHz Mem type DDR4 DDR4 DDR4 DDR4 Price 315 232 279 179 The data was extracted from AMD's official page . They all support the chosen RAM and the motherboard. I'm ruling out Ryzen 7 5800x because it's too expensive both on monetary and power consumption terms. Also ruling out Ryzen 5 5600G because it has comparatively bad properties. Between Ryzen 5 5600x and Ryzen 7 5700x, after checking these comparisons ( 1 , 2 ) it looks like: Single core performance is similar. 7 wins when all cores are involved. 7 is more power efficient. 7 is better rated. 7 is newer (1.5 years). 7 has around 3.52 GB/s (7%) higher theoretical RAM memory bandwidth They have the same cache 7 has 5 degrees less of max temperature They both support ECC 5 has a greater market share 5 is 47$ cheaper I think that for 47$ it's work the increase on cores and theoretical RAM memory bandwidth. Therefore I'd go with the Ryzen 7 5700x . CPU coolers \u2691 One of the most important decisions when building your PC, especially if you plan on overclocking, is choosing the best CPU cooler. The cooler is often a limiting factor to your overclocking potential, especially under sustained loads. Your cooler choice can also make a substantial difference in noise output. So buying a cooler that can handle your best CPU\u2019s thermal output/heat, (be it at stock settings or when overclocked) is critical to avoiding throttling and achieving your system\u2019s full potential, while keeping the whole system quiet. CPU Coolers come in dozens of shapes and sizes, but most fall into these categories: Air: made of some combination of metal heatsinks and fans, come in all shapes and sizes and varying thermal dissipation capacities (sometimes listed as TDP). High-end air coolers these days rival many all-in-one (AIO) liquid coolers that have become popular in the market over the past several years. Closed-loop All-in one (AIO)custom AIO or closed-loop coolers can be (but aren\u2019t always) quieter than air coolers, without requiring the complications of cutting and fitting custom tubes and maintaining coolant levels after setup. AIOs have also become increasingly resistant to leaks over the years, and are easier to install. But they require room for a radiator, so may require a larger case than some air coolers. Here\u2019s a quick comparison of some of the pros and cons of air and liquid cooling. Liquid Cooling Pros: Highest cooling potential . Fewer clearance issues around the socket. Liquid Cooling Cons: Price is generally higher (and price to performance ratio is typically lower as well). (Slim) possibility of component-damaging leaks. Air Cooling Pros: Price is generally lower (better price to performance ratio). No maintenance required. Zero chance for leaks. Air Cooling Cons: Limited cooling potential. Increased fitment issues around the socket with memory, fans, etc). Can be heavy/difficult to mount. Quick shopping tips \u2691 Own a recent Ryzen CPU?: You may not need to buy a cooler, even for overclocking. All Ryzen 300- and 2000-series processors and some older Ryzen models ship with coolers, and many of them can handle moderate overclocks. If you want the best CPU clock speed possible, you\u2019ll still want to buy an aftermarket cooler, but for many Ryzen owners, that won\u2019t be necessary. Check clearances before buying: Big air coolers and low-profile models can bump up against tall RAM and even VRM heat sinks sometimes. And tall coolers can butt up against your case door or window. Be sure to check the dimensions and advertised clearances of any cooler and your case before buying. More fans=better cooling, but more noise: The coolers that do the absolute best job of moving warm air away from your CPU and out of your case are also often the loudest. If fan noise is a problem for you, you\u2019ll want a cooler that does a good job of balancing noise and cooling. Make sure you can turn off RGB: Many coolers these days include RGB fans and / or lighting. This can be a fun way to customize the look of your PC. But be sure there\u2019s a way, either via a built-in controller or when plugging the cooler into a compatible RGB motherboard header, to turn the lights off without turning off the PC. Check that the CPU has GPU if you don't want to use an external graphic card. Otherwise the BIOS won't start. Market analysis \u2691 After a quick review I'm deciding between the Dark Rock 4 and the Enermax ETS-T50 Axe . The analysis is done after reading Tomshardware reviews ( 1 , 2 ). They are equal in: CPU core and motherboard temperatures. Have installation videos. The Enermax has the advantages: Is much more silent (between 2 and 4 dB). It has better acoustic efficiency (Relative temperature against Relative Noise) between 15% and 29%. More TDP Much cheaper (25 EUR) If you have a Phillips screwdriver (normal cross screwdriver) you don't get another one. All in all the Enermax ETS-T50 Axe is a better one, but after checking the sizes, my case limit on the height of the CPU cooler is 160mm and the Enermax is 163mm... The Cooler Master Masterair ma610p has 166mm, so it's out of the question too. The Dark Rock 4 max height is 159mm. I don't know if I should bargain. To be in the safe side I'll go with the Dark Rock 4 Ryzen recommended coolers \u2691 CPU Thermal paste \u2691 Thermal paste is designed to minimize microscopic air gaps and irregularities between the surface of the cooler and the CPU's IHS (integrated heat spreader), the piece of metal which is built into the top of the processor. Good thermal paste can have a profound impact on your performance, because it will allow your processor to transfer more of its waste heat to your cooler, keeping your processor running cool. Most pastes are comprised of ceramic or metallic materials suspended within a proprietary binder which allows for easy application and spread as well as simple cleanup. These thermal pastes can be electrically conductive or non-conductive, depending on their specific formula. Electrically conductive thermal pastes can carry current between two points, meaning that if the paste squeezes out onto other components, it can cause damage to motherboards and CPUs when you switch on the power. A single drop out of place can lead to a dead PC, so extra care is imperative. Liquid metal compounds are almost always electrically conductive, so while these compounds provide better performance than their paste counterparts, they require more focus and attention during application. They are very hard to remove if you get some in the wrong place, which would fry your system. In contrast, traditional thermal paste compounds are relatively simple for every experience level. Most, but not all, traditional pastes are electrically non-conductive. Most cpu coolers come with their own thermal paste, so check yours before buying another one. Market analysis \u2691 Model ProlimaTech PK-3 Thermal Grizzly Kryonaut Cooler Master MasterGel Pro v2 Electrical conductive No No No Thermal Conductivity 11.2 W/mk 12.5 W/mk 9 W/mk Ease of Use 4.5 4.5 4.5 Relative Performance 4.0 4.0 3.5 Price per gram 6.22 9.48 2.57 The best choice would be the ProlimaTech but the package sold are expensive because it has many grams. In my case, my cooler comes with the thermal paste so I'd start with that before spending 20$ more. Installation \u2691 When installing an AM4 CPU in the motherboard, rotate the CPU so that the small arrow on one of the corners of the chip matches the arrow on the corner of the motherboard socket. References \u2691 Tom's hardware CPU guide Tom's hardware CPU cooling guide How to select the best processor for your server Cloudzy best server processor", "title": "CPU"}, {"location": "cpu/#speed", "text": "To achieve the same speed you can play with the speed of a core and the number of cores. Having more cores and lesser clock speed has the next advantages: Processors with higher cores deliver higher performance and are cost-effective. Applications supporting multi-threading would benefit from a higher number of cores. Multi-threading support for applications will continue to enhance over time. Easily run more applications without experiencing the performance drop. Great for virtualization and running multiple virtual machines. With the disadvantages that it offers lower single threaded performance. On the other hand, using fewer cores and higher clock speed gives the next advantages: Offers better single threaded performance. Lower cost. And the next disadvantages Due to the lower number of cores, it becomes difficult to split between applications. Not so strong as multi-threading performance.", "title": "Speed"}, {"location": "cpu/#providers", "text": "", "title": "Providers"}, {"location": "cpu/#amd", "text": "The Ryzen family is broken down into four distinct branches: Ryzen 3: entry-level. Ryzen 5: mainstream. Ryzen 7: performance Ryzen 9: high-end They're all great chips in their own ways, but some certainly offer more value than others, and for many, the most powerful chips will be complete overkill.", "title": "AMD"}, {"location": "cpu/#market-analysis", "text": "Property Ryzen 7 5800x Ryzen 5 5600x Ryzen 7 5700x Ryzen 5 5600G Cores 8 6 8 6 Threads 16 12 16 12 Clock 3.8 3.7 3.4 3.9 Socket AM4 AM4 AM4 AM4 PCI 4.0 4.0 4.0 3.0 Thermal Not included Wraith Stealth Not included Wraith Stealth Default TDP 105W 65W 65W 65W System Mem spec >= 3200 MHz >= 3200 MHz >= 3200 MHz >= 3200 MHz Mem type DDR4 DDR4 DDR4 DDR4 Price 315 232 279 179 The data was extracted from AMD's official page . They all support the chosen RAM and the motherboard. I'm ruling out Ryzen 7 5800x because it's too expensive both on monetary and power consumption terms. Also ruling out Ryzen 5 5600G because it has comparatively bad properties. Between Ryzen 5 5600x and Ryzen 7 5700x, after checking these comparisons ( 1 , 2 ) it looks like: Single core performance is similar. 7 wins when all cores are involved. 7 is more power efficient. 7 is better rated. 7 is newer (1.5 years). 7 has around 3.52 GB/s (7%) higher theoretical RAM memory bandwidth They have the same cache 7 has 5 degrees less of max temperature They both support ECC 5 has a greater market share 5 is 47$ cheaper I think that for 47$ it's work the increase on cores and theoretical RAM memory bandwidth. Therefore I'd go with the Ryzen 7 5700x .", "title": "Market analysis"}, {"location": "cpu/#cpu-coolers", "text": "One of the most important decisions when building your PC, especially if you plan on overclocking, is choosing the best CPU cooler. The cooler is often a limiting factor to your overclocking potential, especially under sustained loads. Your cooler choice can also make a substantial difference in noise output. So buying a cooler that can handle your best CPU\u2019s thermal output/heat, (be it at stock settings or when overclocked) is critical to avoiding throttling and achieving your system\u2019s full potential, while keeping the whole system quiet. CPU Coolers come in dozens of shapes and sizes, but most fall into these categories: Air: made of some combination of metal heatsinks and fans, come in all shapes and sizes and varying thermal dissipation capacities (sometimes listed as TDP). High-end air coolers these days rival many all-in-one (AIO) liquid coolers that have become popular in the market over the past several years. Closed-loop All-in one (AIO)custom AIO or closed-loop coolers can be (but aren\u2019t always) quieter than air coolers, without requiring the complications of cutting and fitting custom tubes and maintaining coolant levels after setup. AIOs have also become increasingly resistant to leaks over the years, and are easier to install. But they require room for a radiator, so may require a larger case than some air coolers. Here\u2019s a quick comparison of some of the pros and cons of air and liquid cooling. Liquid Cooling Pros: Highest cooling potential . Fewer clearance issues around the socket. Liquid Cooling Cons: Price is generally higher (and price to performance ratio is typically lower as well). (Slim) possibility of component-damaging leaks. Air Cooling Pros: Price is generally lower (better price to performance ratio). No maintenance required. Zero chance for leaks. Air Cooling Cons: Limited cooling potential. Increased fitment issues around the socket with memory, fans, etc). Can be heavy/difficult to mount.", "title": "CPU coolers"}, {"location": "cpu/#quick-shopping-tips", "text": "Own a recent Ryzen CPU?: You may not need to buy a cooler, even for overclocking. All Ryzen 300- and 2000-series processors and some older Ryzen models ship with coolers, and many of them can handle moderate overclocks. If you want the best CPU clock speed possible, you\u2019ll still want to buy an aftermarket cooler, but for many Ryzen owners, that won\u2019t be necessary. Check clearances before buying: Big air coolers and low-profile models can bump up against tall RAM and even VRM heat sinks sometimes. And tall coolers can butt up against your case door or window. Be sure to check the dimensions and advertised clearances of any cooler and your case before buying. More fans=better cooling, but more noise: The coolers that do the absolute best job of moving warm air away from your CPU and out of your case are also often the loudest. If fan noise is a problem for you, you\u2019ll want a cooler that does a good job of balancing noise and cooling. Make sure you can turn off RGB: Many coolers these days include RGB fans and / or lighting. This can be a fun way to customize the look of your PC. But be sure there\u2019s a way, either via a built-in controller or when plugging the cooler into a compatible RGB motherboard header, to turn the lights off without turning off the PC. Check that the CPU has GPU if you don't want to use an external graphic card. Otherwise the BIOS won't start.", "title": "Quick shopping tips"}, {"location": "cpu/#market-analysis_1", "text": "After a quick review I'm deciding between the Dark Rock 4 and the Enermax ETS-T50 Axe . The analysis is done after reading Tomshardware reviews ( 1 , 2 ). They are equal in: CPU core and motherboard temperatures. Have installation videos. The Enermax has the advantages: Is much more silent (between 2 and 4 dB). It has better acoustic efficiency (Relative temperature against Relative Noise) between 15% and 29%. More TDP Much cheaper (25 EUR) If you have a Phillips screwdriver (normal cross screwdriver) you don't get another one. All in all the Enermax ETS-T50 Axe is a better one, but after checking the sizes, my case limit on the height of the CPU cooler is 160mm and the Enermax is 163mm... The Cooler Master Masterair ma610p has 166mm, so it's out of the question too. The Dark Rock 4 max height is 159mm. I don't know if I should bargain. To be in the safe side I'll go with the Dark Rock 4", "title": "Market analysis"}, {"location": "cpu/#ryzen-recommended-coolers", "text": "", "title": "Ryzen recommended coolers"}, {"location": "cpu/#cpu-thermal-paste", "text": "Thermal paste is designed to minimize microscopic air gaps and irregularities between the surface of the cooler and the CPU's IHS (integrated heat spreader), the piece of metal which is built into the top of the processor. Good thermal paste can have a profound impact on your performance, because it will allow your processor to transfer more of its waste heat to your cooler, keeping your processor running cool. Most pastes are comprised of ceramic or metallic materials suspended within a proprietary binder which allows for easy application and spread as well as simple cleanup. These thermal pastes can be electrically conductive or non-conductive, depending on their specific formula. Electrically conductive thermal pastes can carry current between two points, meaning that if the paste squeezes out onto other components, it can cause damage to motherboards and CPUs when you switch on the power. A single drop out of place can lead to a dead PC, so extra care is imperative. Liquid metal compounds are almost always electrically conductive, so while these compounds provide better performance than their paste counterparts, they require more focus and attention during application. They are very hard to remove if you get some in the wrong place, which would fry your system. In contrast, traditional thermal paste compounds are relatively simple for every experience level. Most, but not all, traditional pastes are electrically non-conductive. Most cpu coolers come with their own thermal paste, so check yours before buying another one.", "title": "CPU Thermal paste"}, {"location": "cpu/#market-analysis_2", "text": "Model ProlimaTech PK-3 Thermal Grizzly Kryonaut Cooler Master MasterGel Pro v2 Electrical conductive No No No Thermal Conductivity 11.2 W/mk 12.5 W/mk 9 W/mk Ease of Use 4.5 4.5 4.5 Relative Performance 4.0 4.0 3.5 Price per gram 6.22 9.48 2.57 The best choice would be the ProlimaTech but the package sold are expensive because it has many grams. In my case, my cooler comes with the thermal paste so I'd start with that before spending 20$ more.", "title": "Market analysis"}, {"location": "cpu/#installation", "text": "When installing an AM4 CPU in the motherboard, rotate the CPU so that the small arrow on one of the corners of the chip matches the arrow on the corner of the motherboard socket.", "title": "Installation"}, {"location": "cpu/#references", "text": "Tom's hardware CPU guide Tom's hardware CPU cooling guide How to select the best processor for your server Cloudzy best server processor", "title": "References"}, {"location": "css/", "text": "CSS stands for Cascading Style Sheets and is used to format the layout of a webpage. With CSS, you can control the color, font, the size of text, the spacing between elements, how elements are positioned and laid out, what background images or background colors are to be used, different displays for different devices and screen sizes, and much more! Using CSS in HTML \u2691 CSS can be added to HTML documents in 3 ways: Inline : by using the style attribute inside HTML elements. Internal : by using a", "title": "CSS"}, {"location": "css/#using-css-in-html", "text": "CSS can be added to HTML documents in 3 ways: Inline : by using the style attribute inside HTML elements. Internal : by using a", "title": "Using CSS in HTML"}, {"location": "cypress/", "text": "Cypress is a next generation front end testing tool built for the modern web. Cypress enables you to write all types of tests: End-to-end tests Integration tests Unit tests Cypress can test anything that runs in a browser. Features \u2691 Time Travel : Cypress takes snapshots as your tests run. Hover over commands in the Command Log to see exactly what happened at each step. Debuggability : Stop guessing why your tests are failing. Debug directly from familiar tools like Developer Tools. Our readable errors and stack traces make debugging lightning fast. Automatic Waiting : Never add waits or sleeps to your tests. Cypress automatically waits for commands and assertions before moving on. No more async hell. Spies, Stubs, and Clocks : Verify and control the behavior of functions, server responses, or timers. The same functionality you love from unit testing is right at your fingertips. Network Traffic Control : Easily control, stub, and test edge cases without involving your server. You can stub network traffic however you like. Consistent Results : Our architecture doesn\u2019t use Selenium or WebDriver. Say hello to fast, consistent and reliable tests that are flake-free. Screenshots and Videos : View screenshots taken automatically on failure, or videos of your entire test suite when run from the CLI. Cross browser Testing : Run tests within Firefox and Chrome-family browsers (including Edge and Electron) locally and optimally in a Continuous Integration pipeline. Check the key differences page to see more benefits of using the tool. Installation \u2691 npm install cypress --save-dev Usage \u2691 You first need to open cypress with npx cypress open . To get an overview of cypress' workflow follow the Writing your first test tutorial Tests live in the cypress directory, if you create a new file in the cypress/integration directory it will automatically show up in the UI. Cypress monitors your spec files for any changes and automatically displays any changes. Writing tests is meant to be simple, for example: describe ( 'My First Test' , () => { it ( 'Does not do much!' , () => { expect ( true ). to . equal ( true ) }) }) Test structure \u2691 The test interface, borrowed from Mocha , provides describe() , context() , it() and specify() . context() is identical to describe() and specify() is identical to it() . describe ( 'Unit test our math functions' , () => { context ( 'math' , () => { it ( 'can add numbers' , () => { expect ( add ( 1 , 2 )). to . eq ( 3 ) }) it ( 'can subtract numbers' , () => { expect ( subtract ( 5 , 12 )). to . eq ( - 7 ) }) specify ( 'can divide numbers' , () => { expect ( divide ( 27 , 9 )). to . eq ( 3 ) }) specify ( 'can multiply numbers' , () => { expect ( multiply ( 5 , 4 )). to . eq ( 20 ) }) }) }) Hooks \u2691 Hooks are helpful to set conditions that you want to run before a set of tests or before each test. They're also helpful to clean up conditions after a set of tests or after each test. before (() => { // root-level hook // runs once before all tests }) beforeEach (() => { // root-level hook // runs before every test block }) afterEach (() => { // runs after each test block }) after (() => { // runs once all tests are done }) describe ( 'Hooks' , () => { before (() => { // runs once before all tests in the block }) beforeEach (() => { // runs before each test in the block }) afterEach (() => { // runs after each test in the block }) after (() => { // runs once after all tests in the block }) }) !!! warning \"Before writing after() or afterEach() hooks, read the anti-pattern of cleaning up state with after() or afterEach() \" Skipping tests \u2691 You can skip tests in the next ways: describe ( 'TodoMVC' , () => { it ( 'is not written yet' ) it . skip ( 'adds 2 todos' , function () { cy . visit ( '/' ) cy . get ( '.new-todo' ). type ( 'learn testing{enter}' ). type ( 'be cool{enter}' ) cy . get ( '.todo-list li' ). should ( 'have.length' , 100 ) }) xit ( 'another test' , () => { expect ( false ). to . true }) }) Querying elements \u2691 Cypress automatically retries the query until either the element is found or a set timeout is reached. This makes Cypress robust and immune to dozens of common problems that occur in other testing tools. Query by HTML properties \u2691 You need to find the elements to act upon, usually you do it with the cy.get() function. For example: cy . get ( '.my-selector' ) Cypress leverages jQuery's powerful selector engine and exposes many of its DOM traversal methods to you so you can work with complex HTML structures. For example: cy . get ( '#main-content' ). find ( '.article' ). children ( 'img[src^=\"/static\"]' ). first () If you follow the Write testable code guide , you'll select elements by the data-cy element. cy . get ( '[data-cy=submit]' ) You'll probably write that a lot, that's why it's useful to define the next commands in /cypress/support/commands.ts . Cypress . Commands . add ( 'getById' , ( selector , ... args ) => { return cy . get ( `[data-cy= ${ selector } ]` , ... args ) }) Cypress . Commands . add ( 'getByIdLike' , ( selector , ... args ) => { return cy . get ( `[data-cy*= ${ selector } ]` , ... args ) }) Cypress . Commands . add ( 'findById' , { prevSubject : true }, ( subject , selector , ... args ) => { return subject . find ( `[data-cy= ${ selector } ]` , ... args ) }) So you can now do cy . getById ( 'submit' ) Query by content \u2691 Another way to locate things -- a more human way -- is to look them up by their content, by what the user would see on the page. For this, there's the handy cy.contains() command, for example: // Find an element in the document containing the text 'New Post' cy . contains ( 'New Post' ) // Find an element within '.main' containing the text 'New Post' cy . get ( '.main' ). contains ( 'New Post' ) This is helpful when writing tests from the perspective of a user interacting with your app. They only know that they want to click the button labeled \"Submit\". They have no idea that it has a type attribute of submit, or a CSS class of my-submit-button . Changing the timeout \u2691 The querying methods accept the timeout argument to change the default timeout. // Give this element 10 seconds to appear cy . get ( '.my-slow-selector' , { timeout : 10000 }) Select by position in list \u2691 Inside our list, we can select elements based on their position in the list, using .first() , .last() or .eq() selector. cy . get ( 'li' ) . first (); // select \"red\" cy . get ( 'li' ) . last (); // select \"violet\" cy . get ( 'li' ) . eq ( 2 ); // select \"yellow\" You can also use .next() and .prev() to navigate through the elements. Select elements by filtering \u2691 Once you select multiple elements, you can filter within these based on another selector. cy . get ( 'li' ) . filter ( '.primary' ) // select all elements with the class .primary To do the exact opposite, you can use .not() command. cy .get('li') .not('.primary') // select all elements without the class .primary Finding elements \u2691 You can specify your selector by first selecting an element you want to search within, and then look down the DOM structure to find a specific element you are looking for. cy . get ( '.list' ) . find ( '.violet' ) // finds an element with class .violet inside .list element Instead of looking down the DOM structure and finding an element within another element, we can look up. In this example, we first select our list item, and then try to find an element with a .list class. cy . get ( '.violet' ) . parent ( '.list' ) // finds an element with class .list that is above our .violet element Interacting with elements \u2691 Cypress allows you to click on and type into elements on the page by using .click() and .type() commands with a cy.get() or cy.contains() command. This is a great example of chaining in action. cy . get ( 'textarea.post-body' ). type ( 'This is an excellent post.' ) We're chaining the .type() onto the cy.get() , telling it to type into the subject yielded from the cy.get() command, which will be a DOM element. Here are even more action commands Cypress provides to interact with your app: .blur() : Make a focused DOM element blur. .focus() : Focus on a DOM element. .clear() : Clear the value of an input or textarea . .check() : Check checkbox(es) or radio(s). .uncheck() : Uncheck checkbox(es). .select() : Select an <option> within a <select> . .dblclick() : Double-click a DOM element. .rightclick() : Right-click a DOM element. These commands ensure some guarantees about what the state of the elements should be prior to performing their actions. For example, when writing a .click() command, Cypress ensures that the element is able to be interacted with (like a real user would). It will automatically wait until the element reaches an \"actionable\" state by: Not being hidden Not being covered Not being disabled Not animating This also helps prevent flake when interacting with your application in tests. If you want to jump into the command flow and use a custom function use .then() . When the previous command resolves, it will call your callback function with the yielded subject as the first argument. If you wish to continue chaining commands after your .then() , you'll need to specify the subject you want to yield to those commands, which you can achieve with a return value other than null or undefined . Cypress will yield that to the next command for you. cy // Find the el with id 'some-link' . get ( '#some-link' ) . then (( $myElement ) => { // ...massage the subject with some arbitrary code // grab its href property const href = $myElement . prop ( 'href' ) // strip out the 'hash' character and everything after it return href . replace ( /(#.*)/ , '' ) }) . then (( href ) => { // href is now the new subject // which we can work with now }) Setting aliases \u2691 Cypress has some added functionality for quickly referring back to past subjects called Aliases. It looks something like this: cy . get ( '.my-selector' ) . as ( 'myElement' ) // sets the alias . click () /* many more actions */ cy . get ( '@myElement' ) // re-queries the DOM as before (only if necessary) . click () This lets us reuse our DOM queries for faster tests when the element is still in the DOM, and it automatically handles re-querying the DOM for us when it is not immediately found in the DOM. This is particularly helpful when dealing with front end frameworks that do a lot of re-rendering. It can be used to share context between tests, for example with fixtures: beforeEach (() => { // alias the users fixtures cy . fixture ( 'users.json' ). as ( 'users' ) }) it ( 'utilize users in some way' , function () { // access the users property const user = this . users [ 0 ] // make sure the header contains the first // user's name cy . get ( 'header' ). should ( 'contain' , user . name ) }) Asserting about elements \u2691 Assertions let you do things like ensuring an element is visible or has a particular attribute, CSS class, or state. Assertions are commands that enable you to describe the desired state of your application. Cypress will automatically wait until your elements reach this state, or fail the test if the assertions don't pass. For example: cy . get ( ':checkbox' ). should ( 'be.disabled' ) cy . get ( 'form' ). should ( 'have.class' , 'form-horizontal' ) cy . get ( 'input' ). should ( 'not.have.value' , 'US' ) Cypress bundles Chai , Chai-jQuery , and Sinon-Chai to provide built-in assertions. You can see a comprehensive list of them in the list of assertions reference . You can also write your own assertions as Chai plugins and use them in Cypress. Default assertions \u2691 Many commands have a default, built-in assertion, or rather have requirements that may cause it to fail without needing an explicit assertion you've added. cy.visit() : Expects the page to send text/html content with a 200 status code. cy.request() : Expects the remote server to exist and provide a response. cy.contains() : Expects the element with content to eventually exist in the DOM. cy.get() : Expects the element to eventually exist in the DOM. .find() : Also expects the element to eventually exist in the DOM. .type() : Expects the element to eventually be in a typeable state. .click() : Expects the element to eventually be in an actionable state. .its() : Expects to eventually find a property on the current subject. Certain commands may have a specific requirement that causes them to immediately fail without retrying: such as cy.request() . Others, such as DOM based commands will automatically retry and wait for their corresponding elements to exist before failing. Writing assertions \u2691 There are two ways to write assertions in Cypress: Implicit Subjects: Using .should() or .and() . Explicit Subjects: Using expect . The implicit form is much shorter, so only use the explicit form in the next cases: Assert multiple things about the same subject. Massage the subject in some way prior to making the assertion. Implicit Subjects \u2691 Using .should() or .and() commands is the preferred way of making assertions in Cypress. // the implicit subject here is the first <tr> // this asserts that the <tr> has an .active class cy . get ( 'tbody tr:first' ). should ( 'have.class' , 'active' ) You can chain multiple assertions together using .and() , which is another name for .should() that makes things more readable: cy . get ( '#header a' ) . should ( 'have.class' , 'active' ) . and ( 'have.attr' , 'href' , '/users' ) Because .should('have.class') does not change the subject, .and('have.attr') is executed against the same element. This is handy when you need to assert multiple things against a single subject quickly. Explicit Subjects \u2691 Using expect allows you to pass in a specific subject and make an assertion about it. // the explicit subject here is the boolean: true expect ( true ). to . be . true Common Assertions \u2691 Length : // retry until we find 3 matching <li.selected> cy . get ( 'li.selected' ). should ( 'have.length' , 3 ) Attribute : For example to test links // check the content of an attribute cy . get ( 'a' ) . should ( 'have.attr' , 'href' , 'https://docs.cypress.io' ) . and ( 'have.attr' , 'target' , '_blank' ) // Test it's meant to be opened // another tab Class : // retry until this input does not have class disabled cy . get ( 'form' ). find ( 'input' ). should ( 'not.have.class' , 'disabled' ) Value : // retry until this textarea has the correct value cy . get ( 'textarea' ). should ( 'have.value' , 'foo bar baz' ) Text Content : // assert the element's text content is exactly the given text cy . get ( '#user-name' ). should ( 'have.text' , 'Joe Smith' ) // assert the element's text includes the given substring cy . get ( '#address' ). should ( 'include.text' , 'Atlanta' ) // retry until this span does not contain 'click me' cy . get ( 'a' ). parent ( 'span.help' ). should ( 'not.contain' , 'click me' ) // the element's text should start with \"Hello\" cy . get ( '#greeting' ) . invoke ( 'text' ) . should ( 'match' , /^Hello/ ) // tip: use cy.contains to find element with its text // matching the given regular expression cy . contains ( '#a-greeting' , /^Hello/ ) Visibility : // retry until the button with id \"form-submit\" is visible cy . get ( 'button#form-submit' ). should ( 'be.visible' ) // retry until the list item with text \"write tests\" is visible cy . contains ( '.todo li' , 'write tests' ). should ( 'be.visible' ) Note : if there are multiple elements, the assertions be.visible and not.be.visible act differently: // retry until SOME elements are visible cy . get ( 'li' ). should ( 'be.visible' ) // retry until EVERY element is invisible cy . get ( 'li.hidden' ). should ( 'not.be.visible' ) Existence : // retry until loading spinner no longer exists cy . get ( '#loading' ). should ( 'not.exist' ) State : // retry until our radio is checked cy . get ( ':radio' ). should ( 'be.checked' ) CSS : // retry until .completed has matching css cy . get ( '.completed' ). should ( 'have.css' , 'text-decoration' , 'line-through' ) // retry while .accordion css has the \"display: none\" property cy . get ( '#accordion' ). should ( 'not.have.css' , 'display' , 'none' ) Disabled property : < input type = \"text\" id = \"example-input\" disabled /> cy . get ( '#example-input' ) . should ( 'be.disabled' ) // let's enable this element from the test . invoke ( 'prop' , 'disabled' , false ) cy . get ( '#example-input' ) // we can use \"enabled\" assertion . should ( 'be.enabled' ) // or negate the \"disabled\" assertion . and ( 'not.be.disabled' ) Negative assertions \u2691 There are positive and negative assertions. Examples of positive assertions are: cy . get ( '.todo-item' ). should ( 'have.length' , 2 ). and ( 'have.class' , 'completed' ) The negative assertions have the not chainer prefixed to the assertion. For example: cy . contains ( 'first todo' ). should ( 'not.have.class' , 'completed' ) cy . get ( '#loading' ). should ( 'not.be.visible' ) We recommend using negative assertions to verify that a specific condition is no longer present after the application performs an action. For example, when a previously completed item is unchecked, we might verify that a CSS class is removed. // at first the item is marked completed cy . contains ( 'li.todo' , 'Write tests' ) . should ( 'have.class' , 'completed' ) . find ( '.toggle' ) . click () // the CSS class has been removed cy . contains ( 'li.todo' , 'Write tests' ). should ( 'not.have.class' , 'completed' ) Read more on the topic in the blog post Be Careful With Negative Assertions . Custom assertions \u2691 You can write your own assertion function and pass it as a callback to the .should() command. cy . get ( 'div' ). should (( $div ) => { expect ( $div ). to . have . length ( 1 ) const className = $div [ 0 ]. className // className will be a string like \"main-abc123 heading-xyz987\" expect ( className ). to . match ( /heading-/ ) }) Setting up the tests \u2691 Depending on how your application is built - it's likely that your web application is going to be affected and controlled by the server. Traditionally when writing e2e tests using Selenium, before you automate the browser you do some kind of set up and tear down on the server. You generally have three ways to facilitate this with Cypress: cy.exec() : To run system commands. cy.task() : To run code in Node via the pluginsFile . cy.request() : To make HTTP requests. If you're running node.js on your server, you might add a before or beforeEach hook that executes an npm task. describe ( 'The Home Page' , () => { beforeEach (() => { // reset and seed the database prior to every test cy . exec ( 'npm run db:reset && npm run db:seed' ) }) it ( 'successfully loads' , () => { cy . visit ( '/' ) }) }) Instead of just executing a system command, you may want more flexibility and could expose a series of routes only when running in a test environment. For instance, you could compose several requests together to tell your server exactly the state you want to create. describe ( 'The Home Page' , () => { beforeEach (() => { // reset and seed the database prior to every test cy . exec ( 'npm run db:reset && npm run db:seed' ) // seed a post in the DB that we control from our tests cy . request ( 'POST' , '/test/seed/post' , { title : 'First Post' , authorId : 1 , body : '...' , }) // seed a user in the DB that we can control from our tests cy . request ( 'POST' , '/test/seed/user' , { name : 'Jane' }) . its ( 'body' ) . as ( 'currentUser' ) }) it ( 'successfully loads' , () => { // this.currentUser will now point to the response // body of the cy.request() that we could use // to log in or work with in some way cy . visit ( '/' ) }) }) While there's nothing really wrong with this approach, it does add a lot of complexity. You will be battling synchronizing the state between your server and your browser - and you'll always need to set up / tear down this state before tests (which is slow). The good news is that we aren't Selenium, nor are we a traditional e2e testing tool. That means we're not bound to the same restrictions. With Cypress, there are several other approaches that can offer an arguably better and faster experience. Stubbing the server \u2691 Another valid approach opposed to seeding and talking to your server is to bypass it altogether. While you'll still receive all of the regular HTML / JS / CSS assets from your server and you'll continue to cy.visit() it in the same way - you can instead stub the JSON responses coming from it. This means that instead of resetting the database, or seeding it with the state we want, you can force the server to respond with whatever you want it to. In this way, we not only prevent needing to synchronize the state between the server and browser, but we also prevent mutating state from our tests. That means tests won't build up state that may affect other tests. Another upside is that this enables you to build out your application without needing the contract of the server to exist. You can build it the way you want the data to be structured, and even test all of the edge cases, without needing a server. However - there is likely still a balance here where both strategies are valid (and you should likely do them). While stubbing is great, it means that you don't have the guarantees that these response payloads actually match what the server will send. However, there are still many valid ways to get around this: Generate the fixture stubs ahead of time : You could have the server generate all of the fixture stubs for you ahead of time. This means their data will reflect what the server will actually send. Write a single e2e test without stubs, and then stub the rest : Another more balanced approach is to integrate both strategies. You likely want to have a single test that takes a true e2e approach and stubs nothing. It'll use the feature for real - including seeding the database and setting up state. Once you've established it's working you can then use stubs to test all of the edge cases and additional scenarios. There are no benefits to using real data in the vast majority of cases. We recommend that the vast majority of tests use stub data. They will be orders of magnitude faster, and much less complex. cy.intercept() is used to control the behavior of HTTP requests. You can statically define the body, HTTP status code, headers, and other response characteristics. cy . intercept ( { method : 'GET' , // Route all GET requests url : '/users/*' , // that have a URL that matches '/users/*' }, [] // and force the response to be: [] ). as ( 'getUsers' ) // and assign an alias Fixtures \u2691 A fixture is a fixed set of data located in a file that is used in your tests. The purpose of a test fixture is to ensure that there is a well known and fixed environment in which tests are run so that results are repeatable. Fixtures are accessed within tests by calling the cy.fixture() command. When stubbing a response, you typically need to manage potentially large and complex JSON objects. Cypress allows you to integrate fixture syntax directly into responses. // we set the response to be the activites.json fixture cy . intercept ( 'GET' , '/activities/*' , { fixture : 'activities.json' }) Fixtures live in /cypress/fixtures/ and can be further organized within additional directories. For instance, you could create another folder called images and add images: /cypress/fixtures/images/cats.png /cypress/fixtures/images/dogs.png /cypress/fixtures/images/birds.png To access the fixtures nested within the images folder, include the folder in your cy.fixture() command. cy . fixture ( 'images/dogs.png' ) // yields dogs.png as Base64 Use the content of a fixture set in a hook in a test \u2691 If you store and access the fixture data using this test context object, make sure to use function () { ... } callbacks both for the hook and the test. Otherwise the test engine will NOT have this pointing at the test context. describe ( 'User page' , () => { beforeEach ( function () { // \"this\" points at the test context object cy . fixture ( 'user' ). then (( user ) => { // \"this\" is still the test context object this . user = user }) }) // the test callback is in \"function () { ... }\" form it ( 'has user' , function () { // this.user exists expect ( this . user . firstName ). to . equal ( 'Jane' ) }) }) Logging in \u2691 One of the first (and arguably one of the hardest) hurdles you'll have to overcome in testing is logging into your application. It's a great idea to get your signup and login flow under test coverage since it is very important to all of your users and you never want it to break. Logging in is one of those features that are mission critical and should likely involve your server. We recommend you test signup and login using your UI as a real user would. For example: describe ( 'The Login Page' , () => { beforeEach (() => { // reset and seed the database prior to every test cy . exec ( 'npm run db:reset && npm run db:seed' ) // seed a user in the DB that we can control from our tests // assuming it generates a random password for us cy . request ( 'POST' , '/test/seed/user' , { username : 'jane.lane' }) . its ( 'body' ) . as ( 'currentUser' ) }) it ( 'sets auth cookie when logging in via form submission' , function () { // destructuring assignment of the this.currentUser object const { username , password } = this . currentUser cy . visit ( '/login' ) cy . get ( 'input[name=username]' ). type ( username ) // {enter} causes the form to submit cy . get ( 'input[name=password]' ). type ( ` ${ password } {enter}` ) // we should be redirected to /dashboard cy . url (). should ( 'include' , '/dashboard' ) // our auth cookie should be present cy . getCookie ( 'your-session-cookie' ). should ( 'exist' ) // UI should reflect this user being logged in cy . get ( 'h1' ). should ( 'contain' , 'jane.lane' ) }) }) You'll likely also want to test your login UI for: Invalid username / password. Username taken. Password complexity requirements. Edge cases like locked / deleted accounts. Each of these likely requires a full blown e2e test, and it makes sense to go through the login process. But when you're testing another area of the system that relies on a state from a previous feature: do not use your UI to set up this state. So for these cases you'd do: describe ( 'The Dashboard Page' , () => { beforeEach (() => { // reset and seed the database prior to every test cy . exec ( 'npm run db:reset && npm run db:seed' ) // seed a user in the DB that we can control from our tests // assuming it generates a random password for us cy . request ( 'POST' , '/test/seed/user' , { username : 'jane.lane' }) . its ( 'body' ) . as ( 'currentUser' ) }) it ( 'logs in programmatically without using the UI' , function () { // destructuring assignment of the this.currentUser object const { username , password } = this . currentUser // programmatically log us in without needing the UI cy . request ( 'POST' , '/login' , { username , password , }) // now that we're logged in, we can visit // any kind of restricted route! cy . visit ( '/dashboard' ) // our auth cookie should be present cy . getCookie ( 'your-session-cookie' ). should ( 'exist' ) // UI should reflect this user being logged in cy . get ( 'h1' ). should ( 'contain' , 'jane.lane' ) }) }) This saves an enormous amount of time visiting the login page, filling out the username, password, and waiting for the server to redirect us before every test. Because we previously tested the login system end-to-end without using any shortcuts, we already have 100% confidence it's working correctly. Here are other login recipes. Setting up backend servers for E2E tests \u2691 Cypress team does NOT recommend trying to start your back end web server from within Cypress. Any command run by cy.exec() or cy.task() has to exit eventually. Otherwise, Cypress will not continue running any other commands. Trying to start a web server from cy.exec() or cy.task() causes all kinds of problems because: You have to background the process. You lose access to it via terminal. You don't have access to its stdout or logs. Every time your tests run, you'd have to work out the complexity around starting an already running web server. You would likely encounter constant port conflicts. Therefore you should start your web server before running Cypress and kill it after it completes. They have examples showing you how to start and stop your web server in a CI environment . Waiting \u2691 Cypress enables you to declaratively cy.wait() for requests and their responses. cy . intercept ( '/activities/*' , { fixture : 'activities' }). as ( 'getActivities' ) cy . intercept ( '/messages/*' , { fixture : 'messages' }). as ( 'getMessages' ) // visit the dashboard, which should make requests that match // the two routes above cy . visit ( 'http://localhost:8888/dashboard' ) // pass an array of Route Aliases that forces Cypress to wait // until it sees a response for each request that matches // each of these aliases cy . wait ([ '@getActivities' , '@getMessages' ]) // these commands will not run until the wait command resolves above cy . get ( 'h1' ). should ( 'contain' , 'Dashboard' ) If you would like to check the response data of each response of an aliased route, you can use several cy.wait() calls. cy . intercept ({ method : 'POST' , url : '/myApi' , }). as ( 'apiCheck' ) cy . visit ( '/' ) cy . wait ( '@apiCheck' ). then (( interception ) => { assert . isNotNull ( interception . response . body , '1st API call has data' ) }) cy . wait ( '@apiCheck' ). then (( interception ) => { assert . isNotNull ( interception . response . body , '2nd API call has data' ) }) cy . wait ( '@apiCheck' ). then (( interception ) => { assert . isNotNull ( interception . response . body , '3rd API call has data' ) }) Waiting on an aliased route has big advantages: Tests are more robust with much less flake. Failure messages are much more precise. You can assert about the underlying request object. Avoiding Flake tests \u2691 One advantage of declaratively waiting for responses is that it decreases test flake. You can think of cy.wait() as a guard that indicates to Cypress when you expect a request to be made that matches a specific routing alias. This prevents the next commands from running until responses come back and it guards against situations where your requests are initially delayed. cy . intercept ( '/search*' , [{ item : 'Book 1' }, { item : 'Book 2' }]). as ( 'getSearch' ) // our autocomplete field is throttled // meaning it only makes a request after // 500ms from the last keyPress cy . get ( '#autocomplete' ). type ( 'Book' ) // wait for the request + response // thus insulating us from the // throttled request cy . wait ( '@getSearch' ) cy . get ( '#results' ). should ( 'contain' , 'Book 1' ). and ( 'contain' , 'Book 2' ) Assert on wait content \u2691 Another benefit of using cy.wait() on requests is that it allows you to access the actual request object. This is useful when you want to make assertions about this object. In our example above we can assert about the request object to verify that it sent data as a query string in the URL. Although we're mocking the response, we can still verify that our application sends the correct request. // any request to \"/search/*\" endpoint will automatically receive // an array with two book objects cy . intercept ( '/search/*' , [{ item : 'Book 1' }, { item : 'Book 2' }]). as ( 'getSearch' ) cy . get ( '#autocomplete' ). type ( 'Book' ) // this yields us the interception cycle object which includes // fields for the request and response cy . wait ( '@getSearch' ). its ( 'request.url' ). should ( 'include' , '/search?query=Book' ) cy . get ( '#results' ). should ( 'contain' , 'Book 1' ). and ( 'contain' , 'Book 2' ) Of the intercepted object you can check: URL. Method. Status Code. Request Body. Request Headers. Response Body. Response Headers. // spy on POST requests to /users endpoint cy . intercept ( 'POST' , '/users' ). as ( 'new-user' ) // trigger network calls by manipulating web app's user interface, then cy . wait ( '@new-user' ). should ( 'have.property' , 'response.statusCode' , 201 ) // we can grab the completed interception object again to run more assertions // using cy.get(<alias>) cy . get ( '@new-user' ) // yields the same interception object . its ( 'request.body' ) . should ( 'deep.equal' , JSON . stringify ({ id : '101' , firstName : 'Joe' , lastName : 'Black' , }) ) // and we can place multiple assertions in a single \"should\" callback cy . get ( '@new-user' ). should (({ request , response }) => { expect ( request . url ). to . match ( /\\/users$/ ) expect ( request . method ). to . equal ( 'POST' ) // it is a good practice to add assertion messages // as the 2nd argument to expect() expect ( response . headers , 'response headers' ). to . include ({ 'cache-control' : 'no-cache' , expires : '-1' , 'content-type' : 'application/json; charset=utf-8' , location : '<domain>/users/101' , }) }) You can inspect the full request cycle object by logging it to the console cy . wait ( '@new-user' ). then ( console . log ) Don't repeat yourself \u2691 Share code before each test \u2691 describe ( 'my form' , () => { beforeEach (() => { cy . visit ( '/users/new' ) cy . get ( '#first' ). type ( 'Johnny' ) cy . get ( '#last' ). type ( 'Appleseed' ) }) it ( 'displays form validation' , () => { cy . get ( '#first' ). clear () // clear out first name cy . get ( 'form' ). submit () cy . get ( '#errors' ). should ( 'contain' , 'First name is required' ) }) it ( 'can submit a valid form' , () => { cy . get ( 'form' ). submit () }) }) Parametrization \u2691 If you want to run similar tests with different data, you can use parametrization. For example to test the same pages for different screen sizes use: const sizes = [ 'iphone-6' , 'ipad-2' , [ 1024 , 768 ]] describe ( 'Logo' , () => { sizes . forEach (( size ) => { // make assertions on the logo using // an array of different viewports it ( `Should display logo on ${ size } screen` , () => { if ( Cypress . _ . isArray ( size )) { cy . viewport ( size [ 0 ], size [ 1 ]) } else { cy . viewport ( size ) } cy . visit ( 'https://www.cypress.io' ) cy . get ( '#logo' ). should ( 'be.visible' ) }) }) }) Use functions \u2691 Sometimes, the piece of code is redundant and we don't we don't require it in all the test cases. We can create utility functions and move such code there. We can create a separate folder as utils in support folder and store our functions in a file in that folder. Consider the following example of utility function for login. //cypress/support/utils/common.js export const loginViaUI = ( username , password ) => { cy . get ( \"[data-cy='login-email-field']\" ). type ( username ); cy . get ( \"[data-cy='login-password-field']\" ). type ( password ); cy . get ( \"[data-cy='submit-button']\" ). submit () } This is how we can use utility function in our test case: import { loginViaUI } from '../support/utils/common.js' ; describe ( \"Login\" , () => { it ( 'should allow user to log in' , () => { cy . visit ( '/login' ); loginViaUI ( 'username' , 'password' ); }); }); Utility functions are similar to Cypress commands. If the code being used in almost every test suite, we can create a custom command for it. The benefit of this is that we don't have to import the js file to use the command, it is available directly on cy object i.e. cy.loginViaUI() . But, this doesn't mean that we should use commands for everything. If the code is used in only some of the test suite, we can create a utility function and import it whenever needed. Setting up time of the tests \u2691 Specify a now timestamp // your app code $ ( '#date' ). text ( new Date (). toJSON ()) const now = new Date ( 2017 , 3 , 14 ). getTime () // April 14, 2017 timestamp cy . clock ( now ) cy . visit ( '/index.html' ) cy . get ( '#date' ). contains ( '2017-04-14' ) Simulate errors \u2691 End-to-end tests are excellent for testing \u201chappy path\u201d scenarios and the most important application features. However, there are unexpected situations, and when they occur, the application cannot completely \"break\". Such situations can occur due to errors on the server or the network, to name a few. With Cypress, we can easily simulate error situations. Below are examples of tests for server and network errors. context ( 'Errors' , () => { const errorMsg = 'Oops! Try again later' it ( 'simulates a server error' , () => { cy . intercept ( 'GET' , '**/search?query=cypress' , { statusCode : 500 } ). as ( 'getServerFailure' ) cy . visit ( 'https://example.com/search' ) cy . get ( '[data-cy=\"search-field\"]' ) . should ( 'be.visible' ) . type ( 'cypress{enter}' ) cy . wait ( '@getServerFailure' ) cy . contains ( errorMsg ) . should ( 'be.visible' ) }) it ( 'simulates a network failure' , () => { cy . intercept ( 'GET' , '**/search?query=cypressio' , { forceNetworkError : true } ). as ( 'getNetworkFailure' ) cy . visit ( 'https://example.com/search' ) cy . get ( '[data-cy=\"search-field\"]' ) . should ( 'be.visible' ) . type ( 'cypressio{enter}' ) cy . wait ( '@getNetworkFailure' ) cy . contais ( errorMsg ) . should ( 'be.visible' ) }) }) In the above tests, the HTTP request of type GET to the search endpoint is intercepted. In the first test, we use the statusCode option with the value 500 . In the second test, we use the forceNewtworkError option with the value of true . After that, you can test that the correct message is visible to the user. Sending different responses \u2691 To return different responses from a single GET /todos intercept, you can place all prepared responses into an array, and then use Array.prototype.shift to return and remove the first item. it ( 'returns list with more items on page reload' , () => { const replies = [{ fixture : 'articles.json' }, { statusCode : 404 }] cy . intercept ( 'GET' , '/api/inbox' , req => req . reply ( replies . shift ())) }) Component testing \u2691 Component testing in Cypress is similar to end-to-end testing. The notable differences are: There's no need to navigate to a URL. You don't need to call cy.visit() in your test. Cypress provides a blank canvas where we can mount components in isolation. For example: import { mount } from '@cypress/vue' import TodoList from './components/TodoList' describe ( 'TodoList' , () => { it ( 'renders the todo list' , () => { mount ( < TodoList /> ) cy . get ( '[data-testid=todo-list]' ). should ( 'exist' ) }) it ( 'contains the correct number of todos' , () => { const todos = [ { text : 'Buy milk' , id : 1 }, { text : 'Learn Component Testing' , id : 2 }, ] mount ( < TodoList todos = { todos } /> ) cy . get ( '[data-testid=todos]' ). should ( 'have.length' , todos . length ) }) }) If you are using Cypress Component Testing in a project that also has tests written with the Cypress End-to-End test runner, you may want to configure some Component Testing specific defaults . It doesn't yet work with vuetify Install \u2691 Run: npm install --save-dev cypress @cypress/vue @cypress/webpack-dev-server webpack-dev-server You will also need to configure the component testing framework of your choice by installing the corresponding component testing plugin. // cypress/plugins/index.js module . exports = ( on , config ) => { if ( config . testingType === 'component' ) { const { startDevServer } = require ( '@cypress/webpack-dev-server' ) // Vue's Webpack configuration const webpackConfig = require ( '@vue/cli-service/webpack.config.js' ) on ( 'dev-server:start' , ( options ) => startDevServer ({ options , webpackConfig }) ) } } Usage \u2691 // components/HelloWorld.spec.js import { mount } from '@cypress/vue' import { HelloWorld } from './HelloWorld.vue' describe ( 'HelloWorld component' , () => { it ( 'works' , () => { mount ( HelloWorld ) // now use standard Cypress commands cy . contains ( 'Hello World!' ). should ( 'be.visible' ) }) }) You can pass additional styles, css files and external stylesheets to load, see docs/styles.md for full list. import Todo from './Todo.vue' const todo = { id : '123' , title : 'Write more tests' , } mount ( Todo , { propsData : { todo }, stylesheets : [ 'https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.2/css/bulma.css' , ], }) Visual testing \u2691 Cypress is a functional Test Runner. It drives the web application the way a user would, and checks if the app functions as expected: if the expected message appears, an element is removed, or a CSS class is added after the appropriate user action. Cypress does NOT see how the page actually looks though. You could technically write a functional test asserting the CSS properties using the have.css assertion, but these may quickly become cumbersome to write and maintain, especially when visual styles rely on a lot of CSS styles. Visual testing can be done through plugins that do visual regression testing, which is to take an image snapshot of the entire application under test or a specific element, and then compare the image to a previously approved baseline image. If the images are the same (within a set pixel tolerance), it is determined that the web application looks the same to the user. If there are differences, then there has been some change to the DOM layout, fonts, colors or other visual properties that needs to be investigated. If you want to test if your app is responsive use parametrization to have maintainable tests. For more information on how to do visual regression testing read this article . As of 2022-04-23 the most popular tools that don't depend on third party servers are: cypress-plugin-snapshots : It looks to be the best plugin as it allows you to update the screenshots directly through the Cypress interface, but it is unmaintained cypress-visual-regression : Maintained but it doesn't show the differences in the cypress interface and you have to interact with them through the command line. cypress-image-snapshot : Most popular but it looks unmaintained ( 1 , 2 ) Check the Visual testing plugins list to see all available solutions. Beware of the third party solutions like Percy and Applitools as they send your pictures to their servers on each test. cypress-visual-regression \u2691 Installation \u2691 npm install --save-dev cypress-visual-regression Add the following config to your cypress.json file: { \"screenshotsFolder\" : \"./cypress/snapshots/actual\" , \"trashAssetsBeforeRuns\" : true } Add the plugin to cypress/plugins/index.js : const getCompareSnapshotsPlugin = require ( 'cypress-visual-regression/dist/plugin' ); module . exports = ( on , config ) => { getCompareSnapshotsPlugin ( on , config ); }; Add the command to cypress/support/commands.js : const compareSnapshotCommand = require ( 'cypress-visual-regression/dist/command' ); compareSnapshotCommand (); Make sure you import commands.js in cypress/support/index.js : import './commands' Use \u2691 Add cy.compareSnapshot('home') in your tests specs whenever you want to test for visual regressions, making sure to replace home with a relevant name. You can also add an optional error threshold: Value can range from 0.00 (no difference) to 1.00 (every pixel is different). So, if you enter an error threshold of 0.51, the test would fail only if > 51% of pixels are different. For example: it ( 'should display the login page correctly' , () => { cy . visit ( '/03.html' ); cy . get ( 'H1' ). contains ( 'Login' ); cy . compareSnapshot ( 'login' , 0.0 ); cy . compareSnapshot ( 'login' , 0.1 ); }); You can target a single HTML element as well: cy . get ( '#my-header' ). compareSnapshot ( 'just-header' ) Check more examples here You need to take or update the base images, do it with: npx cypress run \\ --env type = base \\ --config screenshotsFolder = cypress/snapshots/base,testFiles = \\\" **/*regression-tests.js \\\" To find regressions run: npx cypress run --env type = actual Or if you want to just check a subset of tests use: npx cypress run --env type = actual --spec \"cypress\\integration\\visual-tests.spec.js\" npx cypress run --env type = actual --spec \"cypress\\integration\\test1.spec.js\" , \"cypress\\integration\\test2.spec.js\" npx cypress run --env type = actual --spec \"cypress\\integration\\**\\*.spec.js Third party component testing \u2691 Other examples of testing third party components Testing HTML emails Configuration \u2691 Cypress saves it's configuration in the cypress.json file. { \"baseUrl\" : \"http://localhost:8080\" } Where: baseUrl : Will be prefixed on cy.visit() and cy.requests() . Environment variables \u2691 Environment variables are useful when: Values are different across developer machines. Values are different across multiple environments: (dev, staging, qa, prod). Values change frequently and are highly dynamic. Instead of hard coding this in your tests: cy . request ( 'https://api.acme.corp' ) // this will break on other environments We can move this into a Cypress environment variable: cy . request ( Cypress . env ( 'EXTERNAL_API' )) // points to a dynamic env var Any key/value you set in your configuration file under the env key will become an environment variable. { \"projectId\" : \"128076ed-9868-4e98-9cef-98dd8b705d75\" , \"env\" : { \"login_url\" : \"/login\" , \"products_url\" : \"/products\" } } To access it use: Cypress . env () // {login_url: '/login', products_url: '/products'} Cypress . env ( 'login_url' ) // '/login' Cypress . env ( 'products_url' ) // '/products' Configure component testing \u2691 You can configure or override Component Testing defaults in your configuration file using the component key. { \"testFiles\" : \"cypress/integration/*.spec.js\" , \"component\" : { \"componentFolder\" : \"src\" , \"testFiles\" : \".*/__tests__/.*spec.tsx\" , \"viewportHeight\" : 500 , \"viewportWidth\" : 700 } } Debugging \u2691 Using the debugger \u2691 Use the .debug() command directly BEFORE the action. // break on a debugger before the action command cy . get ( 'button' ). debug (). click () Step through test commands \u2691 You can run the test command by command using the .pause() command. it ( 'adds items' , () => { cy . pause () cy . get ( '.new-todo' ) // more commands }) This allows you to inspect the web application, the DOM, the network, and any storage after each command to make sure everything happens as expected. Issues \u2691 Allow rerun only failed tests : Until it's ready use it.only on the test you want to run. References \u2691 Home Git Examples of usage Cypress API Real World Application Cypress testing example Tutorial on writing tests Video tutorials", "title": "Cypress"}, {"location": "cypress/#features", "text": "Time Travel : Cypress takes snapshots as your tests run. Hover over commands in the Command Log to see exactly what happened at each step. Debuggability : Stop guessing why your tests are failing. Debug directly from familiar tools like Developer Tools. Our readable errors and stack traces make debugging lightning fast. Automatic Waiting : Never add waits or sleeps to your tests. Cypress automatically waits for commands and assertions before moving on. No more async hell. Spies, Stubs, and Clocks : Verify and control the behavior of functions, server responses, or timers. The same functionality you love from unit testing is right at your fingertips. Network Traffic Control : Easily control, stub, and test edge cases without involving your server. You can stub network traffic however you like. Consistent Results : Our architecture doesn\u2019t use Selenium or WebDriver. Say hello to fast, consistent and reliable tests that are flake-free. Screenshots and Videos : View screenshots taken automatically on failure, or videos of your entire test suite when run from the CLI. Cross browser Testing : Run tests within Firefox and Chrome-family browsers (including Edge and Electron) locally and optimally in a Continuous Integration pipeline. Check the key differences page to see more benefits of using the tool.", "title": "Features"}, {"location": "cypress/#installation", "text": "npm install cypress --save-dev", "title": "Installation"}, {"location": "cypress/#usage", "text": "You first need to open cypress with npx cypress open . To get an overview of cypress' workflow follow the Writing your first test tutorial Tests live in the cypress directory, if you create a new file in the cypress/integration directory it will automatically show up in the UI. Cypress monitors your spec files for any changes and automatically displays any changes. Writing tests is meant to be simple, for example: describe ( 'My First Test' , () => { it ( 'Does not do much!' , () => { expect ( true ). to . equal ( true ) }) })", "title": "Usage"}, {"location": "cypress/#test-structure", "text": "The test interface, borrowed from Mocha , provides describe() , context() , it() and specify() . context() is identical to describe() and specify() is identical to it() . describe ( 'Unit test our math functions' , () => { context ( 'math' , () => { it ( 'can add numbers' , () => { expect ( add ( 1 , 2 )). to . eq ( 3 ) }) it ( 'can subtract numbers' , () => { expect ( subtract ( 5 , 12 )). to . eq ( - 7 ) }) specify ( 'can divide numbers' , () => { expect ( divide ( 27 , 9 )). to . eq ( 3 ) }) specify ( 'can multiply numbers' , () => { expect ( multiply ( 5 , 4 )). to . eq ( 20 ) }) }) })", "title": "Test structure"}, {"location": "cypress/#hooks", "text": "Hooks are helpful to set conditions that you want to run before a set of tests or before each test. They're also helpful to clean up conditions after a set of tests or after each test. before (() => { // root-level hook // runs once before all tests }) beforeEach (() => { // root-level hook // runs before every test block }) afterEach (() => { // runs after each test block }) after (() => { // runs once all tests are done }) describe ( 'Hooks' , () => { before (() => { // runs once before all tests in the block }) beforeEach (() => { // runs before each test in the block }) afterEach (() => { // runs after each test in the block }) after (() => { // runs once after all tests in the block }) }) !!! warning \"Before writing after() or afterEach() hooks, read the anti-pattern of cleaning up state with after() or afterEach() \"", "title": "Hooks"}, {"location": "cypress/#skipping-tests", "text": "You can skip tests in the next ways: describe ( 'TodoMVC' , () => { it ( 'is not written yet' ) it . skip ( 'adds 2 todos' , function () { cy . visit ( '/' ) cy . get ( '.new-todo' ). type ( 'learn testing{enter}' ). type ( 'be cool{enter}' ) cy . get ( '.todo-list li' ). should ( 'have.length' , 100 ) }) xit ( 'another test' , () => { expect ( false ). to . true }) })", "title": "Skipping tests"}, {"location": "cypress/#querying-elements", "text": "Cypress automatically retries the query until either the element is found or a set timeout is reached. This makes Cypress robust and immune to dozens of common problems that occur in other testing tools.", "title": "Querying elements"}, {"location": "cypress/#query-by-html-properties", "text": "You need to find the elements to act upon, usually you do it with the cy.get() function. For example: cy . get ( '.my-selector' ) Cypress leverages jQuery's powerful selector engine and exposes many of its DOM traversal methods to you so you can work with complex HTML structures. For example: cy . get ( '#main-content' ). find ( '.article' ). children ( 'img[src^=\"/static\"]' ). first () If you follow the Write testable code guide , you'll select elements by the data-cy element. cy . get ( '[data-cy=submit]' ) You'll probably write that a lot, that's why it's useful to define the next commands in /cypress/support/commands.ts . Cypress . Commands . add ( 'getById' , ( selector , ... args ) => { return cy . get ( `[data-cy= ${ selector } ]` , ... args ) }) Cypress . Commands . add ( 'getByIdLike' , ( selector , ... args ) => { return cy . get ( `[data-cy*= ${ selector } ]` , ... args ) }) Cypress . Commands . add ( 'findById' , { prevSubject : true }, ( subject , selector , ... args ) => { return subject . find ( `[data-cy= ${ selector } ]` , ... args ) }) So you can now do cy . getById ( 'submit' )", "title": "Query by HTML properties"}, {"location": "cypress/#query-by-content", "text": "Another way to locate things -- a more human way -- is to look them up by their content, by what the user would see on the page. For this, there's the handy cy.contains() command, for example: // Find an element in the document containing the text 'New Post' cy . contains ( 'New Post' ) // Find an element within '.main' containing the text 'New Post' cy . get ( '.main' ). contains ( 'New Post' ) This is helpful when writing tests from the perspective of a user interacting with your app. They only know that they want to click the button labeled \"Submit\". They have no idea that it has a type attribute of submit, or a CSS class of my-submit-button .", "title": "Query by content"}, {"location": "cypress/#changing-the-timeout", "text": "The querying methods accept the timeout argument to change the default timeout. // Give this element 10 seconds to appear cy . get ( '.my-slow-selector' , { timeout : 10000 })", "title": "Changing the timeout"}, {"location": "cypress/#select-by-position-in-list", "text": "Inside our list, we can select elements based on their position in the list, using .first() , .last() or .eq() selector. cy . get ( 'li' ) . first (); // select \"red\" cy . get ( 'li' ) . last (); // select \"violet\" cy . get ( 'li' ) . eq ( 2 ); // select \"yellow\" You can also use .next() and .prev() to navigate through the elements.", "title": "Select by position in list"}, {"location": "cypress/#select-elements-by-filtering", "text": "Once you select multiple elements, you can filter within these based on another selector. cy . get ( 'li' ) . filter ( '.primary' ) // select all elements with the class .primary To do the exact opposite, you can use .not() command. cy .get('li') .not('.primary') // select all elements without the class .primary", "title": "Select elements by filtering"}, {"location": "cypress/#finding-elements", "text": "You can specify your selector by first selecting an element you want to search within, and then look down the DOM structure to find a specific element you are looking for. cy . get ( '.list' ) . find ( '.violet' ) // finds an element with class .violet inside .list element Instead of looking down the DOM structure and finding an element within another element, we can look up. In this example, we first select our list item, and then try to find an element with a .list class. cy . get ( '.violet' ) . parent ( '.list' ) // finds an element with class .list that is above our .violet element", "title": "Finding elements"}, {"location": "cypress/#interacting-with-elements", "text": "Cypress allows you to click on and type into elements on the page by using .click() and .type() commands with a cy.get() or cy.contains() command. This is a great example of chaining in action. cy . get ( 'textarea.post-body' ). type ( 'This is an excellent post.' ) We're chaining the .type() onto the cy.get() , telling it to type into the subject yielded from the cy.get() command, which will be a DOM element. Here are even more action commands Cypress provides to interact with your app: .blur() : Make a focused DOM element blur. .focus() : Focus on a DOM element. .clear() : Clear the value of an input or textarea . .check() : Check checkbox(es) or radio(s). .uncheck() : Uncheck checkbox(es). .select() : Select an <option> within a <select> . .dblclick() : Double-click a DOM element. .rightclick() : Right-click a DOM element. These commands ensure some guarantees about what the state of the elements should be prior to performing their actions. For example, when writing a .click() command, Cypress ensures that the element is able to be interacted with (like a real user would). It will automatically wait until the element reaches an \"actionable\" state by: Not being hidden Not being covered Not being disabled Not animating This also helps prevent flake when interacting with your application in tests. If you want to jump into the command flow and use a custom function use .then() . When the previous command resolves, it will call your callback function with the yielded subject as the first argument. If you wish to continue chaining commands after your .then() , you'll need to specify the subject you want to yield to those commands, which you can achieve with a return value other than null or undefined . Cypress will yield that to the next command for you. cy // Find the el with id 'some-link' . get ( '#some-link' ) . then (( $myElement ) => { // ...massage the subject with some arbitrary code // grab its href property const href = $myElement . prop ( 'href' ) // strip out the 'hash' character and everything after it return href . replace ( /(#.*)/ , '' ) }) . then (( href ) => { // href is now the new subject // which we can work with now })", "title": "Interacting with elements"}, {"location": "cypress/#setting-aliases", "text": "Cypress has some added functionality for quickly referring back to past subjects called Aliases. It looks something like this: cy . get ( '.my-selector' ) . as ( 'myElement' ) // sets the alias . click () /* many more actions */ cy . get ( '@myElement' ) // re-queries the DOM as before (only if necessary) . click () This lets us reuse our DOM queries for faster tests when the element is still in the DOM, and it automatically handles re-querying the DOM for us when it is not immediately found in the DOM. This is particularly helpful when dealing with front end frameworks that do a lot of re-rendering. It can be used to share context between tests, for example with fixtures: beforeEach (() => { // alias the users fixtures cy . fixture ( 'users.json' ). as ( 'users' ) }) it ( 'utilize users in some way' , function () { // access the users property const user = this . users [ 0 ] // make sure the header contains the first // user's name cy . get ( 'header' ). should ( 'contain' , user . name ) })", "title": "Setting aliases"}, {"location": "cypress/#asserting-about-elements", "text": "Assertions let you do things like ensuring an element is visible or has a particular attribute, CSS class, or state. Assertions are commands that enable you to describe the desired state of your application. Cypress will automatically wait until your elements reach this state, or fail the test if the assertions don't pass. For example: cy . get ( ':checkbox' ). should ( 'be.disabled' ) cy . get ( 'form' ). should ( 'have.class' , 'form-horizontal' ) cy . get ( 'input' ). should ( 'not.have.value' , 'US' ) Cypress bundles Chai , Chai-jQuery , and Sinon-Chai to provide built-in assertions. You can see a comprehensive list of them in the list of assertions reference . You can also write your own assertions as Chai plugins and use them in Cypress.", "title": "Asserting about elements"}, {"location": "cypress/#default-assertions", "text": "Many commands have a default, built-in assertion, or rather have requirements that may cause it to fail without needing an explicit assertion you've added. cy.visit() : Expects the page to send text/html content with a 200 status code. cy.request() : Expects the remote server to exist and provide a response. cy.contains() : Expects the element with content to eventually exist in the DOM. cy.get() : Expects the element to eventually exist in the DOM. .find() : Also expects the element to eventually exist in the DOM. .type() : Expects the element to eventually be in a typeable state. .click() : Expects the element to eventually be in an actionable state. .its() : Expects to eventually find a property on the current subject. Certain commands may have a specific requirement that causes them to immediately fail without retrying: such as cy.request() . Others, such as DOM based commands will automatically retry and wait for their corresponding elements to exist before failing.", "title": "Default assertions"}, {"location": "cypress/#writing-assertions", "text": "There are two ways to write assertions in Cypress: Implicit Subjects: Using .should() or .and() . Explicit Subjects: Using expect . The implicit form is much shorter, so only use the explicit form in the next cases: Assert multiple things about the same subject. Massage the subject in some way prior to making the assertion.", "title": "Writing assertions"}, {"location": "cypress/#implicit-subjects", "text": "Using .should() or .and() commands is the preferred way of making assertions in Cypress. // the implicit subject here is the first <tr> // this asserts that the <tr> has an .active class cy . get ( 'tbody tr:first' ). should ( 'have.class' , 'active' ) You can chain multiple assertions together using .and() , which is another name for .should() that makes things more readable: cy . get ( '#header a' ) . should ( 'have.class' , 'active' ) . and ( 'have.attr' , 'href' , '/users' ) Because .should('have.class') does not change the subject, .and('have.attr') is executed against the same element. This is handy when you need to assert multiple things against a single subject quickly.", "title": "Implicit Subjects"}, {"location": "cypress/#explicit-subjects", "text": "Using expect allows you to pass in a specific subject and make an assertion about it. // the explicit subject here is the boolean: true expect ( true ). to . be . true", "title": "Explicit Subjects"}, {"location": "cypress/#common-assertions", "text": "Length : // retry until we find 3 matching <li.selected> cy . get ( 'li.selected' ). should ( 'have.length' , 3 ) Attribute : For example to test links // check the content of an attribute cy . get ( 'a' ) . should ( 'have.attr' , 'href' , 'https://docs.cypress.io' ) . and ( 'have.attr' , 'target' , '_blank' ) // Test it's meant to be opened // another tab Class : // retry until this input does not have class disabled cy . get ( 'form' ). find ( 'input' ). should ( 'not.have.class' , 'disabled' ) Value : // retry until this textarea has the correct value cy . get ( 'textarea' ). should ( 'have.value' , 'foo bar baz' ) Text Content : // assert the element's text content is exactly the given text cy . get ( '#user-name' ). should ( 'have.text' , 'Joe Smith' ) // assert the element's text includes the given substring cy . get ( '#address' ). should ( 'include.text' , 'Atlanta' ) // retry until this span does not contain 'click me' cy . get ( 'a' ). parent ( 'span.help' ). should ( 'not.contain' , 'click me' ) // the element's text should start with \"Hello\" cy . get ( '#greeting' ) . invoke ( 'text' ) . should ( 'match' , /^Hello/ ) // tip: use cy.contains to find element with its text // matching the given regular expression cy . contains ( '#a-greeting' , /^Hello/ ) Visibility : // retry until the button with id \"form-submit\" is visible cy . get ( 'button#form-submit' ). should ( 'be.visible' ) // retry until the list item with text \"write tests\" is visible cy . contains ( '.todo li' , 'write tests' ). should ( 'be.visible' ) Note : if there are multiple elements, the assertions be.visible and not.be.visible act differently: // retry until SOME elements are visible cy . get ( 'li' ). should ( 'be.visible' ) // retry until EVERY element is invisible cy . get ( 'li.hidden' ). should ( 'not.be.visible' ) Existence : // retry until loading spinner no longer exists cy . get ( '#loading' ). should ( 'not.exist' ) State : // retry until our radio is checked cy . get ( ':radio' ). should ( 'be.checked' ) CSS : // retry until .completed has matching css cy . get ( '.completed' ). should ( 'have.css' , 'text-decoration' , 'line-through' ) // retry while .accordion css has the \"display: none\" property cy . get ( '#accordion' ). should ( 'not.have.css' , 'display' , 'none' ) Disabled property : < input type = \"text\" id = \"example-input\" disabled /> cy . get ( '#example-input' ) . should ( 'be.disabled' ) // let's enable this element from the test . invoke ( 'prop' , 'disabled' , false ) cy . get ( '#example-input' ) // we can use \"enabled\" assertion . should ( 'be.enabled' ) // or negate the \"disabled\" assertion . and ( 'not.be.disabled' )", "title": "Common Assertions"}, {"location": "cypress/#negative-assertions", "text": "There are positive and negative assertions. Examples of positive assertions are: cy . get ( '.todo-item' ). should ( 'have.length' , 2 ). and ( 'have.class' , 'completed' ) The negative assertions have the not chainer prefixed to the assertion. For example: cy . contains ( 'first todo' ). should ( 'not.have.class' , 'completed' ) cy . get ( '#loading' ). should ( 'not.be.visible' ) We recommend using negative assertions to verify that a specific condition is no longer present after the application performs an action. For example, when a previously completed item is unchecked, we might verify that a CSS class is removed. // at first the item is marked completed cy . contains ( 'li.todo' , 'Write tests' ) . should ( 'have.class' , 'completed' ) . find ( '.toggle' ) . click () // the CSS class has been removed cy . contains ( 'li.todo' , 'Write tests' ). should ( 'not.have.class' , 'completed' ) Read more on the topic in the blog post Be Careful With Negative Assertions .", "title": "Negative assertions"}, {"location": "cypress/#custom-assertions", "text": "You can write your own assertion function and pass it as a callback to the .should() command. cy . get ( 'div' ). should (( $div ) => { expect ( $div ). to . have . length ( 1 ) const className = $div [ 0 ]. className // className will be a string like \"main-abc123 heading-xyz987\" expect ( className ). to . match ( /heading-/ ) })", "title": "Custom assertions"}, {"location": "cypress/#setting-up-the-tests", "text": "Depending on how your application is built - it's likely that your web application is going to be affected and controlled by the server. Traditionally when writing e2e tests using Selenium, before you automate the browser you do some kind of set up and tear down on the server. You generally have three ways to facilitate this with Cypress: cy.exec() : To run system commands. cy.task() : To run code in Node via the pluginsFile . cy.request() : To make HTTP requests. If you're running node.js on your server, you might add a before or beforeEach hook that executes an npm task. describe ( 'The Home Page' , () => { beforeEach (() => { // reset and seed the database prior to every test cy . exec ( 'npm run db:reset && npm run db:seed' ) }) it ( 'successfully loads' , () => { cy . visit ( '/' ) }) }) Instead of just executing a system command, you may want more flexibility and could expose a series of routes only when running in a test environment. For instance, you could compose several requests together to tell your server exactly the state you want to create. describe ( 'The Home Page' , () => { beforeEach (() => { // reset and seed the database prior to every test cy . exec ( 'npm run db:reset && npm run db:seed' ) // seed a post in the DB that we control from our tests cy . request ( 'POST' , '/test/seed/post' , { title : 'First Post' , authorId : 1 , body : '...' , }) // seed a user in the DB that we can control from our tests cy . request ( 'POST' , '/test/seed/user' , { name : 'Jane' }) . its ( 'body' ) . as ( 'currentUser' ) }) it ( 'successfully loads' , () => { // this.currentUser will now point to the response // body of the cy.request() that we could use // to log in or work with in some way cy . visit ( '/' ) }) }) While there's nothing really wrong with this approach, it does add a lot of complexity. You will be battling synchronizing the state between your server and your browser - and you'll always need to set up / tear down this state before tests (which is slow). The good news is that we aren't Selenium, nor are we a traditional e2e testing tool. That means we're not bound to the same restrictions. With Cypress, there are several other approaches that can offer an arguably better and faster experience.", "title": "Setting up the tests"}, {"location": "cypress/#stubbing-the-server", "text": "Another valid approach opposed to seeding and talking to your server is to bypass it altogether. While you'll still receive all of the regular HTML / JS / CSS assets from your server and you'll continue to cy.visit() it in the same way - you can instead stub the JSON responses coming from it. This means that instead of resetting the database, or seeding it with the state we want, you can force the server to respond with whatever you want it to. In this way, we not only prevent needing to synchronize the state between the server and browser, but we also prevent mutating state from our tests. That means tests won't build up state that may affect other tests. Another upside is that this enables you to build out your application without needing the contract of the server to exist. You can build it the way you want the data to be structured, and even test all of the edge cases, without needing a server. However - there is likely still a balance here where both strategies are valid (and you should likely do them). While stubbing is great, it means that you don't have the guarantees that these response payloads actually match what the server will send. However, there are still many valid ways to get around this: Generate the fixture stubs ahead of time : You could have the server generate all of the fixture stubs for you ahead of time. This means their data will reflect what the server will actually send. Write a single e2e test without stubs, and then stub the rest : Another more balanced approach is to integrate both strategies. You likely want to have a single test that takes a true e2e approach and stubs nothing. It'll use the feature for real - including seeding the database and setting up state. Once you've established it's working you can then use stubs to test all of the edge cases and additional scenarios. There are no benefits to using real data in the vast majority of cases. We recommend that the vast majority of tests use stub data. They will be orders of magnitude faster, and much less complex. cy.intercept() is used to control the behavior of HTTP requests. You can statically define the body, HTTP status code, headers, and other response characteristics. cy . intercept ( { method : 'GET' , // Route all GET requests url : '/users/*' , // that have a URL that matches '/users/*' }, [] // and force the response to be: [] ). as ( 'getUsers' ) // and assign an alias", "title": "Stubbing the server"}, {"location": "cypress/#fixtures", "text": "A fixture is a fixed set of data located in a file that is used in your tests. The purpose of a test fixture is to ensure that there is a well known and fixed environment in which tests are run so that results are repeatable. Fixtures are accessed within tests by calling the cy.fixture() command. When stubbing a response, you typically need to manage potentially large and complex JSON objects. Cypress allows you to integrate fixture syntax directly into responses. // we set the response to be the activites.json fixture cy . intercept ( 'GET' , '/activities/*' , { fixture : 'activities.json' }) Fixtures live in /cypress/fixtures/ and can be further organized within additional directories. For instance, you could create another folder called images and add images: /cypress/fixtures/images/cats.png /cypress/fixtures/images/dogs.png /cypress/fixtures/images/birds.png To access the fixtures nested within the images folder, include the folder in your cy.fixture() command. cy . fixture ( 'images/dogs.png' ) // yields dogs.png as Base64", "title": "Fixtures"}, {"location": "cypress/#use-the-content-of-a-fixture-set-in-a-hook-in-a-test", "text": "If you store and access the fixture data using this test context object, make sure to use function () { ... } callbacks both for the hook and the test. Otherwise the test engine will NOT have this pointing at the test context. describe ( 'User page' , () => { beforeEach ( function () { // \"this\" points at the test context object cy . fixture ( 'user' ). then (( user ) => { // \"this\" is still the test context object this . user = user }) }) // the test callback is in \"function () { ... }\" form it ( 'has user' , function () { // this.user exists expect ( this . user . firstName ). to . equal ( 'Jane' ) }) })", "title": "Use the content of a fixture set in a hook in a test"}, {"location": "cypress/#logging-in", "text": "One of the first (and arguably one of the hardest) hurdles you'll have to overcome in testing is logging into your application. It's a great idea to get your signup and login flow under test coverage since it is very important to all of your users and you never want it to break. Logging in is one of those features that are mission critical and should likely involve your server. We recommend you test signup and login using your UI as a real user would. For example: describe ( 'The Login Page' , () => { beforeEach (() => { // reset and seed the database prior to every test cy . exec ( 'npm run db:reset && npm run db:seed' ) // seed a user in the DB that we can control from our tests // assuming it generates a random password for us cy . request ( 'POST' , '/test/seed/user' , { username : 'jane.lane' }) . its ( 'body' ) . as ( 'currentUser' ) }) it ( 'sets auth cookie when logging in via form submission' , function () { // destructuring assignment of the this.currentUser object const { username , password } = this . currentUser cy . visit ( '/login' ) cy . get ( 'input[name=username]' ). type ( username ) // {enter} causes the form to submit cy . get ( 'input[name=password]' ). type ( ` ${ password } {enter}` ) // we should be redirected to /dashboard cy . url (). should ( 'include' , '/dashboard' ) // our auth cookie should be present cy . getCookie ( 'your-session-cookie' ). should ( 'exist' ) // UI should reflect this user being logged in cy . get ( 'h1' ). should ( 'contain' , 'jane.lane' ) }) }) You'll likely also want to test your login UI for: Invalid username / password. Username taken. Password complexity requirements. Edge cases like locked / deleted accounts. Each of these likely requires a full blown e2e test, and it makes sense to go through the login process. But when you're testing another area of the system that relies on a state from a previous feature: do not use your UI to set up this state. So for these cases you'd do: describe ( 'The Dashboard Page' , () => { beforeEach (() => { // reset and seed the database prior to every test cy . exec ( 'npm run db:reset && npm run db:seed' ) // seed a user in the DB that we can control from our tests // assuming it generates a random password for us cy . request ( 'POST' , '/test/seed/user' , { username : 'jane.lane' }) . its ( 'body' ) . as ( 'currentUser' ) }) it ( 'logs in programmatically without using the UI' , function () { // destructuring assignment of the this.currentUser object const { username , password } = this . currentUser // programmatically log us in without needing the UI cy . request ( 'POST' , '/login' , { username , password , }) // now that we're logged in, we can visit // any kind of restricted route! cy . visit ( '/dashboard' ) // our auth cookie should be present cy . getCookie ( 'your-session-cookie' ). should ( 'exist' ) // UI should reflect this user being logged in cy . get ( 'h1' ). should ( 'contain' , 'jane.lane' ) }) }) This saves an enormous amount of time visiting the login page, filling out the username, password, and waiting for the server to redirect us before every test. Because we previously tested the login system end-to-end without using any shortcuts, we already have 100% confidence it's working correctly. Here are other login recipes.", "title": "Logging in"}, {"location": "cypress/#setting-up-backend-servers-for-e2e-tests", "text": "Cypress team does NOT recommend trying to start your back end web server from within Cypress. Any command run by cy.exec() or cy.task() has to exit eventually. Otherwise, Cypress will not continue running any other commands. Trying to start a web server from cy.exec() or cy.task() causes all kinds of problems because: You have to background the process. You lose access to it via terminal. You don't have access to its stdout or logs. Every time your tests run, you'd have to work out the complexity around starting an already running web server. You would likely encounter constant port conflicts. Therefore you should start your web server before running Cypress and kill it after it completes. They have examples showing you how to start and stop your web server in a CI environment .", "title": "Setting up backend servers for E2E tests"}, {"location": "cypress/#waiting", "text": "Cypress enables you to declaratively cy.wait() for requests and their responses. cy . intercept ( '/activities/*' , { fixture : 'activities' }). as ( 'getActivities' ) cy . intercept ( '/messages/*' , { fixture : 'messages' }). as ( 'getMessages' ) // visit the dashboard, which should make requests that match // the two routes above cy . visit ( 'http://localhost:8888/dashboard' ) // pass an array of Route Aliases that forces Cypress to wait // until it sees a response for each request that matches // each of these aliases cy . wait ([ '@getActivities' , '@getMessages' ]) // these commands will not run until the wait command resolves above cy . get ( 'h1' ). should ( 'contain' , 'Dashboard' ) If you would like to check the response data of each response of an aliased route, you can use several cy.wait() calls. cy . intercept ({ method : 'POST' , url : '/myApi' , }). as ( 'apiCheck' ) cy . visit ( '/' ) cy . wait ( '@apiCheck' ). then (( interception ) => { assert . isNotNull ( interception . response . body , '1st API call has data' ) }) cy . wait ( '@apiCheck' ). then (( interception ) => { assert . isNotNull ( interception . response . body , '2nd API call has data' ) }) cy . wait ( '@apiCheck' ). then (( interception ) => { assert . isNotNull ( interception . response . body , '3rd API call has data' ) }) Waiting on an aliased route has big advantages: Tests are more robust with much less flake. Failure messages are much more precise. You can assert about the underlying request object.", "title": "Waiting"}, {"location": "cypress/#avoiding-flake-tests", "text": "One advantage of declaratively waiting for responses is that it decreases test flake. You can think of cy.wait() as a guard that indicates to Cypress when you expect a request to be made that matches a specific routing alias. This prevents the next commands from running until responses come back and it guards against situations where your requests are initially delayed. cy . intercept ( '/search*' , [{ item : 'Book 1' }, { item : 'Book 2' }]). as ( 'getSearch' ) // our autocomplete field is throttled // meaning it only makes a request after // 500ms from the last keyPress cy . get ( '#autocomplete' ). type ( 'Book' ) // wait for the request + response // thus insulating us from the // throttled request cy . wait ( '@getSearch' ) cy . get ( '#results' ). should ( 'contain' , 'Book 1' ). and ( 'contain' , 'Book 2' )", "title": "Avoiding Flake tests"}, {"location": "cypress/#assert-on-wait-content", "text": "Another benefit of using cy.wait() on requests is that it allows you to access the actual request object. This is useful when you want to make assertions about this object. In our example above we can assert about the request object to verify that it sent data as a query string in the URL. Although we're mocking the response, we can still verify that our application sends the correct request. // any request to \"/search/*\" endpoint will automatically receive // an array with two book objects cy . intercept ( '/search/*' , [{ item : 'Book 1' }, { item : 'Book 2' }]). as ( 'getSearch' ) cy . get ( '#autocomplete' ). type ( 'Book' ) // this yields us the interception cycle object which includes // fields for the request and response cy . wait ( '@getSearch' ). its ( 'request.url' ). should ( 'include' , '/search?query=Book' ) cy . get ( '#results' ). should ( 'contain' , 'Book 1' ). and ( 'contain' , 'Book 2' ) Of the intercepted object you can check: URL. Method. Status Code. Request Body. Request Headers. Response Body. Response Headers. // spy on POST requests to /users endpoint cy . intercept ( 'POST' , '/users' ). as ( 'new-user' ) // trigger network calls by manipulating web app's user interface, then cy . wait ( '@new-user' ). should ( 'have.property' , 'response.statusCode' , 201 ) // we can grab the completed interception object again to run more assertions // using cy.get(<alias>) cy . get ( '@new-user' ) // yields the same interception object . its ( 'request.body' ) . should ( 'deep.equal' , JSON . stringify ({ id : '101' , firstName : 'Joe' , lastName : 'Black' , }) ) // and we can place multiple assertions in a single \"should\" callback cy . get ( '@new-user' ). should (({ request , response }) => { expect ( request . url ). to . match ( /\\/users$/ ) expect ( request . method ). to . equal ( 'POST' ) // it is a good practice to add assertion messages // as the 2nd argument to expect() expect ( response . headers , 'response headers' ). to . include ({ 'cache-control' : 'no-cache' , expires : '-1' , 'content-type' : 'application/json; charset=utf-8' , location : '<domain>/users/101' , }) }) You can inspect the full request cycle object by logging it to the console cy . wait ( '@new-user' ). then ( console . log )", "title": "Assert on wait content"}, {"location": "cypress/#dont-repeat-yourself", "text": "", "title": "Don't repeat yourself"}, {"location": "cypress/#share-code-before-each-test", "text": "describe ( 'my form' , () => { beforeEach (() => { cy . visit ( '/users/new' ) cy . get ( '#first' ). type ( 'Johnny' ) cy . get ( '#last' ). type ( 'Appleseed' ) }) it ( 'displays form validation' , () => { cy . get ( '#first' ). clear () // clear out first name cy . get ( 'form' ). submit () cy . get ( '#errors' ). should ( 'contain' , 'First name is required' ) }) it ( 'can submit a valid form' , () => { cy . get ( 'form' ). submit () }) })", "title": "Share code before each test"}, {"location": "cypress/#parametrization", "text": "If you want to run similar tests with different data, you can use parametrization. For example to test the same pages for different screen sizes use: const sizes = [ 'iphone-6' , 'ipad-2' , [ 1024 , 768 ]] describe ( 'Logo' , () => { sizes . forEach (( size ) => { // make assertions on the logo using // an array of different viewports it ( `Should display logo on ${ size } screen` , () => { if ( Cypress . _ . isArray ( size )) { cy . viewport ( size [ 0 ], size [ 1 ]) } else { cy . viewport ( size ) } cy . visit ( 'https://www.cypress.io' ) cy . get ( '#logo' ). should ( 'be.visible' ) }) }) })", "title": "Parametrization"}, {"location": "cypress/#use-functions", "text": "Sometimes, the piece of code is redundant and we don't we don't require it in all the test cases. We can create utility functions and move such code there. We can create a separate folder as utils in support folder and store our functions in a file in that folder. Consider the following example of utility function for login. //cypress/support/utils/common.js export const loginViaUI = ( username , password ) => { cy . get ( \"[data-cy='login-email-field']\" ). type ( username ); cy . get ( \"[data-cy='login-password-field']\" ). type ( password ); cy . get ( \"[data-cy='submit-button']\" ). submit () } This is how we can use utility function in our test case: import { loginViaUI } from '../support/utils/common.js' ; describe ( \"Login\" , () => { it ( 'should allow user to log in' , () => { cy . visit ( '/login' ); loginViaUI ( 'username' , 'password' ); }); }); Utility functions are similar to Cypress commands. If the code being used in almost every test suite, we can create a custom command for it. The benefit of this is that we don't have to import the js file to use the command, it is available directly on cy object i.e. cy.loginViaUI() . But, this doesn't mean that we should use commands for everything. If the code is used in only some of the test suite, we can create a utility function and import it whenever needed.", "title": "Use functions"}, {"location": "cypress/#setting-up-time-of-the-tests", "text": "Specify a now timestamp // your app code $ ( '#date' ). text ( new Date (). toJSON ()) const now = new Date ( 2017 , 3 , 14 ). getTime () // April 14, 2017 timestamp cy . clock ( now ) cy . visit ( '/index.html' ) cy . get ( '#date' ). contains ( '2017-04-14' )", "title": "Setting up time of the tests"}, {"location": "cypress/#simulate-errors", "text": "End-to-end tests are excellent for testing \u201chappy path\u201d scenarios and the most important application features. However, there are unexpected situations, and when they occur, the application cannot completely \"break\". Such situations can occur due to errors on the server or the network, to name a few. With Cypress, we can easily simulate error situations. Below are examples of tests for server and network errors. context ( 'Errors' , () => { const errorMsg = 'Oops! Try again later' it ( 'simulates a server error' , () => { cy . intercept ( 'GET' , '**/search?query=cypress' , { statusCode : 500 } ). as ( 'getServerFailure' ) cy . visit ( 'https://example.com/search' ) cy . get ( '[data-cy=\"search-field\"]' ) . should ( 'be.visible' ) . type ( 'cypress{enter}' ) cy . wait ( '@getServerFailure' ) cy . contains ( errorMsg ) . should ( 'be.visible' ) }) it ( 'simulates a network failure' , () => { cy . intercept ( 'GET' , '**/search?query=cypressio' , { forceNetworkError : true } ). as ( 'getNetworkFailure' ) cy . visit ( 'https://example.com/search' ) cy . get ( '[data-cy=\"search-field\"]' ) . should ( 'be.visible' ) . type ( 'cypressio{enter}' ) cy . wait ( '@getNetworkFailure' ) cy . contais ( errorMsg ) . should ( 'be.visible' ) }) }) In the above tests, the HTTP request of type GET to the search endpoint is intercepted. In the first test, we use the statusCode option with the value 500 . In the second test, we use the forceNewtworkError option with the value of true . After that, you can test that the correct message is visible to the user.", "title": "Simulate errors"}, {"location": "cypress/#sending-different-responses", "text": "To return different responses from a single GET /todos intercept, you can place all prepared responses into an array, and then use Array.prototype.shift to return and remove the first item. it ( 'returns list with more items on page reload' , () => { const replies = [{ fixture : 'articles.json' }, { statusCode : 404 }] cy . intercept ( 'GET' , '/api/inbox' , req => req . reply ( replies . shift ())) })", "title": "Sending different responses"}, {"location": "cypress/#component-testing", "text": "Component testing in Cypress is similar to end-to-end testing. The notable differences are: There's no need to navigate to a URL. You don't need to call cy.visit() in your test. Cypress provides a blank canvas where we can mount components in isolation. For example: import { mount } from '@cypress/vue' import TodoList from './components/TodoList' describe ( 'TodoList' , () => { it ( 'renders the todo list' , () => { mount ( < TodoList /> ) cy . get ( '[data-testid=todo-list]' ). should ( 'exist' ) }) it ( 'contains the correct number of todos' , () => { const todos = [ { text : 'Buy milk' , id : 1 }, { text : 'Learn Component Testing' , id : 2 }, ] mount ( < TodoList todos = { todos } /> ) cy . get ( '[data-testid=todos]' ). should ( 'have.length' , todos . length ) }) }) If you are using Cypress Component Testing in a project that also has tests written with the Cypress End-to-End test runner, you may want to configure some Component Testing specific defaults . It doesn't yet work with vuetify", "title": "Component testing"}, {"location": "cypress/#install", "text": "Run: npm install --save-dev cypress @cypress/vue @cypress/webpack-dev-server webpack-dev-server You will also need to configure the component testing framework of your choice by installing the corresponding component testing plugin. // cypress/plugins/index.js module . exports = ( on , config ) => { if ( config . testingType === 'component' ) { const { startDevServer } = require ( '@cypress/webpack-dev-server' ) // Vue's Webpack configuration const webpackConfig = require ( '@vue/cli-service/webpack.config.js' ) on ( 'dev-server:start' , ( options ) => startDevServer ({ options , webpackConfig }) ) } }", "title": "Install"}, {"location": "cypress/#usage_1", "text": "// components/HelloWorld.spec.js import { mount } from '@cypress/vue' import { HelloWorld } from './HelloWorld.vue' describe ( 'HelloWorld component' , () => { it ( 'works' , () => { mount ( HelloWorld ) // now use standard Cypress commands cy . contains ( 'Hello World!' ). should ( 'be.visible' ) }) }) You can pass additional styles, css files and external stylesheets to load, see docs/styles.md for full list. import Todo from './Todo.vue' const todo = { id : '123' , title : 'Write more tests' , } mount ( Todo , { propsData : { todo }, stylesheets : [ 'https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.2/css/bulma.css' , ], })", "title": "Usage"}, {"location": "cypress/#visual-testing", "text": "Cypress is a functional Test Runner. It drives the web application the way a user would, and checks if the app functions as expected: if the expected message appears, an element is removed, or a CSS class is added after the appropriate user action. Cypress does NOT see how the page actually looks though. You could technically write a functional test asserting the CSS properties using the have.css assertion, but these may quickly become cumbersome to write and maintain, especially when visual styles rely on a lot of CSS styles. Visual testing can be done through plugins that do visual regression testing, which is to take an image snapshot of the entire application under test or a specific element, and then compare the image to a previously approved baseline image. If the images are the same (within a set pixel tolerance), it is determined that the web application looks the same to the user. If there are differences, then there has been some change to the DOM layout, fonts, colors or other visual properties that needs to be investigated. If you want to test if your app is responsive use parametrization to have maintainable tests. For more information on how to do visual regression testing read this article . As of 2022-04-23 the most popular tools that don't depend on third party servers are: cypress-plugin-snapshots : It looks to be the best plugin as it allows you to update the screenshots directly through the Cypress interface, but it is unmaintained cypress-visual-regression : Maintained but it doesn't show the differences in the cypress interface and you have to interact with them through the command line. cypress-image-snapshot : Most popular but it looks unmaintained ( 1 , 2 ) Check the Visual testing plugins list to see all available solutions. Beware of the third party solutions like Percy and Applitools as they send your pictures to their servers on each test.", "title": "Visual testing"}, {"location": "cypress/#cypress-visual-regression", "text": "", "title": "cypress-visual-regression"}, {"location": "cypress/#installation_1", "text": "npm install --save-dev cypress-visual-regression Add the following config to your cypress.json file: { \"screenshotsFolder\" : \"./cypress/snapshots/actual\" , \"trashAssetsBeforeRuns\" : true } Add the plugin to cypress/plugins/index.js : const getCompareSnapshotsPlugin = require ( 'cypress-visual-regression/dist/plugin' ); module . exports = ( on , config ) => { getCompareSnapshotsPlugin ( on , config ); }; Add the command to cypress/support/commands.js : const compareSnapshotCommand = require ( 'cypress-visual-regression/dist/command' ); compareSnapshotCommand (); Make sure you import commands.js in cypress/support/index.js : import './commands'", "title": "Installation"}, {"location": "cypress/#use", "text": "Add cy.compareSnapshot('home') in your tests specs whenever you want to test for visual regressions, making sure to replace home with a relevant name. You can also add an optional error threshold: Value can range from 0.00 (no difference) to 1.00 (every pixel is different). So, if you enter an error threshold of 0.51, the test would fail only if > 51% of pixels are different. For example: it ( 'should display the login page correctly' , () => { cy . visit ( '/03.html' ); cy . get ( 'H1' ). contains ( 'Login' ); cy . compareSnapshot ( 'login' , 0.0 ); cy . compareSnapshot ( 'login' , 0.1 ); }); You can target a single HTML element as well: cy . get ( '#my-header' ). compareSnapshot ( 'just-header' ) Check more examples here You need to take or update the base images, do it with: npx cypress run \\ --env type = base \\ --config screenshotsFolder = cypress/snapshots/base,testFiles = \\\" **/*regression-tests.js \\\" To find regressions run: npx cypress run --env type = actual Or if you want to just check a subset of tests use: npx cypress run --env type = actual --spec \"cypress\\integration\\visual-tests.spec.js\" npx cypress run --env type = actual --spec \"cypress\\integration\\test1.spec.js\" , \"cypress\\integration\\test2.spec.js\" npx cypress run --env type = actual --spec \"cypress\\integration\\**\\*.spec.js", "title": "Use"}, {"location": "cypress/#third-party-component-testing", "text": "Other examples of testing third party components Testing HTML emails", "title": "Third party component testing"}, {"location": "cypress/#configuration", "text": "Cypress saves it's configuration in the cypress.json file. { \"baseUrl\" : \"http://localhost:8080\" } Where: baseUrl : Will be prefixed on cy.visit() and cy.requests() .", "title": "Configuration"}, {"location": "cypress/#environment-variables", "text": "Environment variables are useful when: Values are different across developer machines. Values are different across multiple environments: (dev, staging, qa, prod). Values change frequently and are highly dynamic. Instead of hard coding this in your tests: cy . request ( 'https://api.acme.corp' ) // this will break on other environments We can move this into a Cypress environment variable: cy . request ( Cypress . env ( 'EXTERNAL_API' )) // points to a dynamic env var Any key/value you set in your configuration file under the env key will become an environment variable. { \"projectId\" : \"128076ed-9868-4e98-9cef-98dd8b705d75\" , \"env\" : { \"login_url\" : \"/login\" , \"products_url\" : \"/products\" } } To access it use: Cypress . env () // {login_url: '/login', products_url: '/products'} Cypress . env ( 'login_url' ) // '/login' Cypress . env ( 'products_url' ) // '/products'", "title": "Environment variables"}, {"location": "cypress/#configure-component-testing", "text": "You can configure or override Component Testing defaults in your configuration file using the component key. { \"testFiles\" : \"cypress/integration/*.spec.js\" , \"component\" : { \"componentFolder\" : \"src\" , \"testFiles\" : \".*/__tests__/.*spec.tsx\" , \"viewportHeight\" : 500 , \"viewportWidth\" : 700 } }", "title": "Configure component testing"}, {"location": "cypress/#debugging", "text": "", "title": "Debugging"}, {"location": "cypress/#using-the-debugger", "text": "Use the .debug() command directly BEFORE the action. // break on a debugger before the action command cy . get ( 'button' ). debug (). click ()", "title": "Using the debugger"}, {"location": "cypress/#step-through-test-commands", "text": "You can run the test command by command using the .pause() command. it ( 'adds items' , () => { cy . pause () cy . get ( '.new-todo' ) // more commands }) This allows you to inspect the web application, the DOM, the network, and any storage after each command to make sure everything happens as expected.", "title": "Step through test commands"}, {"location": "cypress/#issues", "text": "Allow rerun only failed tests : Until it's ready use it.only on the test you want to run.", "title": "Issues"}, {"location": "cypress/#references", "text": "Home Git Examples of usage Cypress API Real World Application Cypress testing example Tutorial on writing tests Video tutorials", "title": "References"}, {"location": "digital_garden/", "text": "Digital Garden is a method of storing and maintaining knowledge in an maintainable, scalable and searchable way. They are also known as second brains. Unlike in common blogging where you write an article and forget about it, posts are treated as plants in various stages of growth and nurturing. Some might wither and die, and others will flourish and provide a source of continued knowledge for the gardener and folks in the community that visit. The content is diverse, you can find ideas, articles, investigations, snippets, resources, thoughts, collections, and other bits and pieces that I find interesting and useful. It's my personal Stock , the content that\u2019s as interesting in two months (or two years) as it is today. It\u2019s what people discover via search. It\u2019s what spreads slowly but surely, building fans over time. They are a metaphor for thinking about writing and creating that focuses less on the resulting \"showpiece\" and more on the process, care, and craft it takes to get there. Existing digital gardens \u2691 If you look for inspiration check my favourite digital gardens: Nikita's Everything I know : It's awesome both in content quality and length, as the way he presents it. Gwern's site : The way he presents content is unique and gorgeous. I've found myself not very hooked to the content, but when you find something you like it's awesome, such as the about page , his article on spaced repetition or the essay of Death Note: L, Anonymity & Eluding Entropy . Or browse the following lists: Best-of Digital gardens Maggie Appleton's compilation Nikita's compilation Richard Litt's compilation KasperZutterman's compilation Or the digital garden 's reddit. References \u2691 Joel Hooks article on Digital Gardens Tom Critchlow article on Digital Gardens", "title": "Digital Garden"}, {"location": "digital_garden/#existing-digital-gardens", "text": "If you look for inspiration check my favourite digital gardens: Nikita's Everything I know : It's awesome both in content quality and length, as the way he presents it. Gwern's site : The way he presents content is unique and gorgeous. I've found myself not very hooked to the content, but when you find something you like it's awesome, such as the about page , his article on spaced repetition or the essay of Death Note: L, Anonymity & Eluding Entropy . Or browse the following lists: Best-of Digital gardens Maggie Appleton's compilation Nikita's compilation Richard Litt's compilation KasperZutterman's compilation Or the digital garden 's reddit.", "title": "Existing digital gardens"}, {"location": "digital_garden/#references", "text": "Joel Hooks article on Digital Gardens Tom Critchlow article on Digital Gardens", "title": "References"}, {"location": "diversity/", "text": "Diversity, equity, and inclusion (DEI) can be defined as : Diversity is the representation and acknowledgement of the multitudes of identities, experiences, and ways of moving through the world. This includes\u2014but is not limited to\u2014ability, age, citizenship status, criminal record and/or incarceration, educational attainment, ethnicity, gender, geographical location, language, nationality, political affiliation, religion, race, sexuality, socioeconomic status, and veteran status. Further, we recognize that each individual's experience is informed by intersections across multiple identities. Equity seeks to ensure respect and equal opportunity for all, using all resources and tools to elevate the voices of under-represented and/or disadvantaged groups. Inclusion is fostering an environment in which people of all identities are welcome, valued, and supported. An inclusive organization solicits, listens to, learns from, and acts on the contributions of all its stakeholders. References \u2691 Pulitzer center DEI page Journalist's Toolbox DEI links", "title": "Diversity"}, {"location": "diversity/#references", "text": "Pulitzer center DEI page Journalist's Toolbox DEI links", "title": "References"}, {"location": "docker/", "text": "Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. Because all of the containers share the services of a single operating system kernel, they use fewer resources than virtual machines. How to keep containers updated \u2691 With Renovate \u2691 Renovate is a program that does automated dependency updates. Multi-platform and multi-language. With Watchtower \u2691 With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially. Run the watchtower container with the next command: docker run -d \\ --name watchtower \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /etc/localtime:/etc/localtime:ro \\ -e WATCHTOWER_NOTIFICATIONS = email \\ -e WATCHTOWER_NOTIFICATION_EMAIL_FROM ={{ email.from }} \\ -e WATCHTOWER_NOTIFICATION_EMAIL_TO ={{ email.to }} \\\\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER = mail.riseup.net \\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PORT = 587 \\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_USER ={{ email.user }} \\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PASSWORD ={{ email.password }} \\ -e WATCHTOWER_NOTIFICATION_EMAIL_DELAY = 2 \\ containrrr/watchtower:latest --no-restart --no-startup-message Use the --no-restart flag if you use systemd to manage the dockers, and --no-startup-message if you don't want watchtower to send you an email each time it starts the update process. Keep in mind that if the containers don't have good migration scripts, upgrading may break the service. To enable this feature, make sure you have frequent backups and a tested rollback process. If you're not sure one of the containers is going to behave well, you can only monitor it or disable it by using docker labels. The first check will be done by default in the next 24 hours, to check that everything works use the --run-once flag. Another alternative is Diun , which is a CLI application written in Go and delivered as a single executable (and a Docker image) to receive notifications when a Docker image is updated on a Docker registry. They don't yet support Prometheus metrics but it surely looks promising. [Logging in \u2691 automatically]( https://docs.docker.com/engine/reference/commandline/login/#provide-a-password-using-stdin ) To log in automatically without entering the password, you need to have the password stored in your personal password store (not in root's!), imagine it's in the dockerhub entry. Then you can use: pass show dockerhub | docker login --username foo --password-stdin Snippets \u2691 Attach a docker to many networks \u2691 You can't do it through the docker run command, there you can only specify one network. However, you can attach a docker to a network with the command: docker network attach network-name docker-name Get the output of docker ps as a json \u2691 To get the complete json for reference. docker ps -a --format \"{{json .}}\" | jq -s To get only the required columns in the output with tab separated version docker ps -a --format \"{{json .}}\" | jq -r -c '[.ID, .State, .Names, .Image]' To get also the image's ID you can use: docker inspect --format = '{{json .}}' $( docker ps -aq ) | jq -r -c '[.Id, .Name, .Config.Image, .Image]' Connect multiple docker compose files \u2691 You can connect services defined across multiple docker-compose.yml files. In order to do this you\u2019ll need to: Create an external network with docker network create <network name> In each of your docker-compose.yml configure the default network to use your externally created network with the networks top-level key. You can use either the service name or container name to connect between containers. Let's do it with an example: Creating the network $ docker network create external-example 2af4d92c2054e9deb86edaea8bb55ecb74f84a62aec7614c9f09fee386f248a6 Create the first docker-compose file version : '3' services : service1 : image : busybox command : sleep infinity networks : default : external : name : external-example Bring the service up $ docker-compose up -d Creating compose1_service1_1 ... done Create the second docker-compose file with network configured version : '3' services : service2 : image : busybox command : sleep infinity networks : default : external : name : external-example Bring the service up $ docker-compose up -d Creating compose2_service2_1 ... done After running docker-compose up -d on both docker-compose.yml files, we see that no new networks were created. $ docker network ls NETWORK ID NAME DRIVER SCOPE 25e0c599d5e5 bridge bridge local 2af4d92c2054 external-example bridge local 7df4631e9cff host host local 194d4156d7ab none null local With the containers using the external-example network, they are able to ping one another. # By service name $ docker exec -it compose1_service1_1 ping service2 PING service2 ( 172 .24.0.3 ) : 56 data bytes 64 bytes from 172 .24.0.3: seq = 0 ttl = 64 time = 0 .054 ms ^C --- service2 ping statistics --- 1 packets transmitted, 1 packets received, 0 % packet loss round-trip min/avg/max = 0 .054/0.054/0.054 ms # By container name $ docker exec -it compose1_service1_1 ping compose2_service2_1 PING compose2_service2_1 ( 172 .24.0.2 ) : 56 data bytes 64 bytes from 172 .24.0.2: seq = 0 ttl = 64 time = 0 .042 ms ^C --- compose2_service2_1 ping statistics --- 1 packets transmitted, 1 packets received, 0 % packet loss round-trip min/avg/max = 0 .042/0.042/0.042 ms The other way around works too. Troubleshooting \u2691 If you are using a VPN and docker, you're going to have a hard time. The docker systemd service logs systemctl status docker.service usually doesn't give much information. Try to start the daemon directly with sudo /usr/bin/dockerd . Don't store credentials in plaintext \u2691 It doesn't work, don't go this painful road and assume that docker is broken. The official steps are horrible, and once you've spent two hours debugging it, you won't be able to push or pull images with your user . When you use docker login and introduce the user and password you get the next warning: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store I got a nice surprise when I saw that pass was suggested in the link of the warning, to be used as a backend to store the password. But that feeling soon faded. To make docker understand that you want to use pass you need to use the docker-credential-pass script. A Go script \"maintained\" by docker, whose last commit was two years ago , has the CI broken and many old unanswered issues. Setting it up it's not easy either and it's ill documented . Furthermore, the script doesn't do what I expected, which is to store the password of your registry user in a pass entry. Instead, you need to create an empty pass entry in docker-credential-helpers/docker-pass-initialized-check , and when you use docker login , manually introducing your data, it creates another entry, as you can see in the next pass output: Password Store \u2514\u2500\u2500 docker-credential-helpers \u251c\u2500\u2500 aHR0cHM6Ly9pbmRleC5kb2NrZXIuaW8vdjEv \u2502 \u2514\u2500\u2500 lyz \u2514\u2500\u2500 docker-pass-initialized-check That entry is removed when you use docker logout so the next time you log in you need to introduce the user and password (\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b . Installing docker-credential-pass \u2691 You first need to install the script: # Check for later releases at https://github.com/docker/docker-credential-helpers/releases version = \"v0.6.3\" archive = \"docker-credential-pass- $version -amd64.tar.gz\" url = \"https://github.com/docker/docker-credential-helpers/releases/download/ $version / $archive \" # Download cred helper, unpack, make executable, and move it where Docker will find it. wget $url \\ && tar -xf $archive \\ && chmod +x docker-credential-pass \\ && mv -f docker-credential-pass /usr/local/bin/ Another tricky issue is that even if you use a non-root user who's part of the docker group, the script is not aware of that, so it will look in the password store of root instead of the user's. This means that additionally to your own, you need to create a new password store for root. Follow the next steps with the root user: Create the password with gpg --full-gen , and copy the key id. Use a non empty password, otherwise you are getting the same security as with the password in cleartext. Initialize the password store pass init gpg_id , changing gpg_id for the one of the last step. Create the empty docker-credential-helpers/docker-pass-initialized-check entry: pass insert docker-credential-helpers/docker-pass-initialized-check And press enter twice. Finally we need to specify in the root's docker configuration that we want to use the pass credential storage. File: /root/.docker/config.json { \"credsStore\" : \"pass\" } Testing it works \u2691 To test that docker is able to use pass as backend to store the credentials, run docker login and introduce the user and password. You should see the Login Succeeded message without any warning. Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one. Username: lyz Password: Login Succeeded Awful experience, wasn't it? Don't worry it gets worse. Now that you're logged in, whenever you try to push an image you're probably going to get an denied: requested access to the resource is denied error. That's because docker is not able to use the password it has stored in the root's password store. If you're using root to push the image (bad idea anyway), you will need to export GPG_TTY=$(tty) so that docker can ask you for your password to unlock root's pass entry. If you're like me that uses a non-root user belonging to the docker group, not even that works, so you've spent all this time reading and trying to fix everything for nothing... Thank you Docker -.- . Start request repeated too quickly \u2691 Shutdown the VPN and it will work. If it doesn't inspect the output of journalctl -eu docker .", "title": "Docker"}, {"location": "docker/#how-to-keep-containers-updated", "text": "", "title": "How to keep containers updated"}, {"location": "docker/#with-renovate", "text": "Renovate is a program that does automated dependency updates. Multi-platform and multi-language.", "title": "With Renovate"}, {"location": "docker/#with-watchtower", "text": "With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially. Run the watchtower container with the next command: docker run -d \\ --name watchtower \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /etc/localtime:/etc/localtime:ro \\ -e WATCHTOWER_NOTIFICATIONS = email \\ -e WATCHTOWER_NOTIFICATION_EMAIL_FROM ={{ email.from }} \\ -e WATCHTOWER_NOTIFICATION_EMAIL_TO ={{ email.to }} \\\\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER = mail.riseup.net \\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PORT = 587 \\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_USER ={{ email.user }} \\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PASSWORD ={{ email.password }} \\ -e WATCHTOWER_NOTIFICATION_EMAIL_DELAY = 2 \\ containrrr/watchtower:latest --no-restart --no-startup-message Use the --no-restart flag if you use systemd to manage the dockers, and --no-startup-message if you don't want watchtower to send you an email each time it starts the update process. Keep in mind that if the containers don't have good migration scripts, upgrading may break the service. To enable this feature, make sure you have frequent backups and a tested rollback process. If you're not sure one of the containers is going to behave well, you can only monitor it or disable it by using docker labels. The first check will be done by default in the next 24 hours, to check that everything works use the --run-once flag. Another alternative is Diun , which is a CLI application written in Go and delivered as a single executable (and a Docker image) to receive notifications when a Docker image is updated on a Docker registry. They don't yet support Prometheus metrics but it surely looks promising.", "title": "With Watchtower"}, {"location": "docker/#logging-in", "text": "automatically]( https://docs.docker.com/engine/reference/commandline/login/#provide-a-password-using-stdin ) To log in automatically without entering the password, you need to have the password stored in your personal password store (not in root's!), imagine it's in the dockerhub entry. Then you can use: pass show dockerhub | docker login --username foo --password-stdin", "title": "[Logging in"}, {"location": "docker/#snippets", "text": "", "title": "Snippets"}, {"location": "docker/#attach-a-docker-to-many-networks", "text": "You can't do it through the docker run command, there you can only specify one network. However, you can attach a docker to a network with the command: docker network attach network-name docker-name", "title": "Attach a docker to many networks"}, {"location": "docker/#get-the-output-of-docker-ps-as-a-json", "text": "To get the complete json for reference. docker ps -a --format \"{{json .}}\" | jq -s To get only the required columns in the output with tab separated version docker ps -a --format \"{{json .}}\" | jq -r -c '[.ID, .State, .Names, .Image]' To get also the image's ID you can use: docker inspect --format = '{{json .}}' $( docker ps -aq ) | jq -r -c '[.Id, .Name, .Config.Image, .Image]'", "title": "Get the output of docker ps as a json"}, {"location": "docker/#connect-multiple-docker-compose-files", "text": "You can connect services defined across multiple docker-compose.yml files. In order to do this you\u2019ll need to: Create an external network with docker network create <network name> In each of your docker-compose.yml configure the default network to use your externally created network with the networks top-level key. You can use either the service name or container name to connect between containers. Let's do it with an example: Creating the network $ docker network create external-example 2af4d92c2054e9deb86edaea8bb55ecb74f84a62aec7614c9f09fee386f248a6 Create the first docker-compose file version : '3' services : service1 : image : busybox command : sleep infinity networks : default : external : name : external-example Bring the service up $ docker-compose up -d Creating compose1_service1_1 ... done Create the second docker-compose file with network configured version : '3' services : service2 : image : busybox command : sleep infinity networks : default : external : name : external-example Bring the service up $ docker-compose up -d Creating compose2_service2_1 ... done After running docker-compose up -d on both docker-compose.yml files, we see that no new networks were created. $ docker network ls NETWORK ID NAME DRIVER SCOPE 25e0c599d5e5 bridge bridge local 2af4d92c2054 external-example bridge local 7df4631e9cff host host local 194d4156d7ab none null local With the containers using the external-example network, they are able to ping one another. # By service name $ docker exec -it compose1_service1_1 ping service2 PING service2 ( 172 .24.0.3 ) : 56 data bytes 64 bytes from 172 .24.0.3: seq = 0 ttl = 64 time = 0 .054 ms ^C --- service2 ping statistics --- 1 packets transmitted, 1 packets received, 0 % packet loss round-trip min/avg/max = 0 .054/0.054/0.054 ms # By container name $ docker exec -it compose1_service1_1 ping compose2_service2_1 PING compose2_service2_1 ( 172 .24.0.2 ) : 56 data bytes 64 bytes from 172 .24.0.2: seq = 0 ttl = 64 time = 0 .042 ms ^C --- compose2_service2_1 ping statistics --- 1 packets transmitted, 1 packets received, 0 % packet loss round-trip min/avg/max = 0 .042/0.042/0.042 ms The other way around works too.", "title": "Connect multiple docker compose files"}, {"location": "docker/#troubleshooting", "text": "If you are using a VPN and docker, you're going to have a hard time. The docker systemd service logs systemctl status docker.service usually doesn't give much information. Try to start the daemon directly with sudo /usr/bin/dockerd .", "title": "Troubleshooting"}, {"location": "docker/#dont-store-credentials-in-plaintext", "text": "It doesn't work, don't go this painful road and assume that docker is broken. The official steps are horrible, and once you've spent two hours debugging it, you won't be able to push or pull images with your user . When you use docker login and introduce the user and password you get the next warning: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store I got a nice surprise when I saw that pass was suggested in the link of the warning, to be used as a backend to store the password. But that feeling soon faded. To make docker understand that you want to use pass you need to use the docker-credential-pass script. A Go script \"maintained\" by docker, whose last commit was two years ago , has the CI broken and many old unanswered issues. Setting it up it's not easy either and it's ill documented . Furthermore, the script doesn't do what I expected, which is to store the password of your registry user in a pass entry. Instead, you need to create an empty pass entry in docker-credential-helpers/docker-pass-initialized-check , and when you use docker login , manually introducing your data, it creates another entry, as you can see in the next pass output: Password Store \u2514\u2500\u2500 docker-credential-helpers \u251c\u2500\u2500 aHR0cHM6Ly9pbmRleC5kb2NrZXIuaW8vdjEv \u2502 \u2514\u2500\u2500 lyz \u2514\u2500\u2500 docker-pass-initialized-check That entry is removed when you use docker logout so the next time you log in you need to introduce the user and password (\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b .", "title": "Don't store credentials in plaintext"}, {"location": "docker/#installing-docker-credential-pass", "text": "You first need to install the script: # Check for later releases at https://github.com/docker/docker-credential-helpers/releases version = \"v0.6.3\" archive = \"docker-credential-pass- $version -amd64.tar.gz\" url = \"https://github.com/docker/docker-credential-helpers/releases/download/ $version / $archive \" # Download cred helper, unpack, make executable, and move it where Docker will find it. wget $url \\ && tar -xf $archive \\ && chmod +x docker-credential-pass \\ && mv -f docker-credential-pass /usr/local/bin/ Another tricky issue is that even if you use a non-root user who's part of the docker group, the script is not aware of that, so it will look in the password store of root instead of the user's. This means that additionally to your own, you need to create a new password store for root. Follow the next steps with the root user: Create the password with gpg --full-gen , and copy the key id. Use a non empty password, otherwise you are getting the same security as with the password in cleartext. Initialize the password store pass init gpg_id , changing gpg_id for the one of the last step. Create the empty docker-credential-helpers/docker-pass-initialized-check entry: pass insert docker-credential-helpers/docker-pass-initialized-check And press enter twice. Finally we need to specify in the root's docker configuration that we want to use the pass credential storage. File: /root/.docker/config.json { \"credsStore\" : \"pass\" }", "title": "Installing docker-credential-pass"}, {"location": "docker/#testing-it-works", "text": "To test that docker is able to use pass as backend to store the credentials, run docker login and introduce the user and password. You should see the Login Succeeded message without any warning. Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one. Username: lyz Password: Login Succeeded Awful experience, wasn't it? Don't worry it gets worse. Now that you're logged in, whenever you try to push an image you're probably going to get an denied: requested access to the resource is denied error. That's because docker is not able to use the password it has stored in the root's password store. If you're using root to push the image (bad idea anyway), you will need to export GPG_TTY=$(tty) so that docker can ask you for your password to unlock root's pass entry. If you're like me that uses a non-root user belonging to the docker group, not even that works, so you've spent all this time reading and trying to fix everything for nothing... Thank you Docker -.- .", "title": "Testing it works"}, {"location": "docker/#start-request-repeated-too-quickly", "text": "Shutdown the VPN and it will work. If it doesn't inspect the output of journalctl -eu docker .", "title": "Start request repeated too quickly"}, {"location": "documentation/", "text": "It doesn't matter how good your program is, because if its documentation is not good enough, people will not use it. Even if they have to use it because they have no choice, without good documentation, they won\u2019t use it effectively or the way you\u2019d like them to. People working with software need different kinds of documentation at different times, in different circumstances, so good software documentation needs them all. They first need to get started and see how to solve specific problems. Then they need a way to search the software possibilities in a reference. Finally when they hit a road block, they need to understand how everything works so they can solve it. Each of these sections must be clearly differenced and the writing style must be adapted. The five types of documentation are: Introduction : A short description with optional pictures or screen casts, that catches the user's attention and makes them want to use it. Like the advertisement of your program. Get started : Lessons that allows the newcomer learn how to start using the software. Like teaching a small child how to cook. How-to guides : Series of steps that show how to solve a specific problem. Like a recipe in a cookery book. Technical reference : Searchable and organized dry description of the software's machinery. Like a reference encyclopedia article. Background information : Discursive explanations that makes the user understand how the software works and how has it evolved. Like an article on culinary social history. This division makes it obvious to both author and reader what material, and what kind of material, goes where. It tells the author how to write, what to write, and where to write it. Introduction \u2691 The introduction is the first gateway for the users to your program, as such, it needs to be eye-catching, otherwise they will walk pass it to one of the other thousand programs or libraries out there. It needs to start with a short phrase that defines the whole project in a way that catches the user's attention. If the short phrase doesn't give enough context, you can add a small paragraph with further information. But don't make it too long, human's attention is weak. It's also a good idea to add a screenshot or screencast showing the usage of the program. Optionally, you can also add a list of features that differentiate your solution from the rest. Get started \u2691 Made of tutorials that take the reader by the hand through a series of steps to complete a meaningful project achievable for a complete beginner. They are what your project needs in order to show a beginner that they can achieve something with it. Tutorials are what will turn your learners into users. A bad or missing tutorial will prevent your project from acquiring new users. They need to be useful for the beginner, easy to follow, meaningful, extremely robust, and kept up-to-date. You might well find that writing and maintaining your tutorials can occupy as much time and energy as the other four parts put together. How to write good tutorials \u2691 Allow the user to learn by doing \u2691 Your learner needs to do things. The different things that they do while following your tutorial need to cover a wide range of tools and operations, building up from the simplest ones at the start to more complex ones. Get the user started \u2691 It\u2019s perfectly acceptable if your beginner\u2019s first steps are hand-held baby steps. It\u2019s also good if what you get the beginner to do is not the way an experienced person would, or even if it\u2019s not the \u2018correct\u2019 way. The point of a tutorial is to get your learner started on their journey, not to get them to a final destination. Make sure that your tutorial works \u2691 One of your jobs as a tutor is to inspire the beginner\u2019s confidence: in the software, in the tutorial, in the tutor, and in their own ability to achieve what\u2019s being asked of them. There are many things that contribute to this. A friendly tone helps, as does consistent use of language, and a logical progression through the material. But the single most important thing is that what you ask the beginner to do must work. If the learner\u2019s actions produce an error or unexpected results, your tutorial has failed. When your students are there with you, you can rescue them; if they\u2019re reading your documentation on their own you can\u2019t. So you have to prevent that from happening in advance. One way of achieving this is by adding the snippets in your documentation to the test suite. Ensure the user sees results immediately \u2691 Everything the learner does should accomplish something comprehensible, however small. If your student has to do strange and incomprehensible things for two pages before they even see a result, that\u2019s much too long. The effect of every action should be visible and evident as soon as possible, and the connection to the action should be clear. The conclusion of each section of a tutorial, or the tutorial as a whole, must be a meaningful accomplishment. Focus on concrete steps, not abstract concepts \u2691 Tutorials need to be concrete, built around specific, particular actions and outcomes. The temptation to introduce abstraction is huge; it is after all how most computing derives its power. But all learning proceeds from the particular and concrete to the general and abstract, and asking the learner to appreciate levels of abstraction before they have even had a chance to grasp the concrete is poor teaching. Provide the minimum necessary explanation \u2691 Don\u2019t explain anything the learner doesn\u2019t need to know in order to complete the tutorial. Extended discussion is important, just not in a tutorial. In a tutorial, it is an obstruction and a distraction. Only the bare minimum is appropriate. Instead, link to explanations elsewhere in the documentation. Focus only on the steps the user needs to take \u2691 Your tutorial needs to be focused on the task in hand. Maybe the command you\u2019re introducing has many other options, or maybe there are different ways to access a certain API. It doesn\u2019t matter: right now, your learner does not need to know about those in order to make progress. How-to guides \u2691 Technical reference \u2691 Background information \u2691 References \u2691 divio's documentation wiki Vue's guidelines FastAPI awesome docs", "title": "Writing good documentation"}, {"location": "documentation/#introduction", "text": "The introduction is the first gateway for the users to your program, as such, it needs to be eye-catching, otherwise they will walk pass it to one of the other thousand programs or libraries out there. It needs to start with a short phrase that defines the whole project in a way that catches the user's attention. If the short phrase doesn't give enough context, you can add a small paragraph with further information. But don't make it too long, human's attention is weak. It's also a good idea to add a screenshot or screencast showing the usage of the program. Optionally, you can also add a list of features that differentiate your solution from the rest.", "title": "Introduction"}, {"location": "documentation/#get-started", "text": "Made of tutorials that take the reader by the hand through a series of steps to complete a meaningful project achievable for a complete beginner. They are what your project needs in order to show a beginner that they can achieve something with it. Tutorials are what will turn your learners into users. A bad or missing tutorial will prevent your project from acquiring new users. They need to be useful for the beginner, easy to follow, meaningful, extremely robust, and kept up-to-date. You might well find that writing and maintaining your tutorials can occupy as much time and energy as the other four parts put together.", "title": "Get started"}, {"location": "documentation/#how-to-write-good-tutorials", "text": "", "title": "How to write good tutorials"}, {"location": "documentation/#allow-the-user-to-learn-by-doing", "text": "Your learner needs to do things. The different things that they do while following your tutorial need to cover a wide range of tools and operations, building up from the simplest ones at the start to more complex ones.", "title": "Allow the user to learn by doing"}, {"location": "documentation/#get-the-user-started", "text": "It\u2019s perfectly acceptable if your beginner\u2019s first steps are hand-held baby steps. It\u2019s also good if what you get the beginner to do is not the way an experienced person would, or even if it\u2019s not the \u2018correct\u2019 way. The point of a tutorial is to get your learner started on their journey, not to get them to a final destination.", "title": "Get the user started"}, {"location": "documentation/#make-sure-that-your-tutorial-works", "text": "One of your jobs as a tutor is to inspire the beginner\u2019s confidence: in the software, in the tutorial, in the tutor, and in their own ability to achieve what\u2019s being asked of them. There are many things that contribute to this. A friendly tone helps, as does consistent use of language, and a logical progression through the material. But the single most important thing is that what you ask the beginner to do must work. If the learner\u2019s actions produce an error or unexpected results, your tutorial has failed. When your students are there with you, you can rescue them; if they\u2019re reading your documentation on their own you can\u2019t. So you have to prevent that from happening in advance. One way of achieving this is by adding the snippets in your documentation to the test suite.", "title": "Make sure that your tutorial works"}, {"location": "documentation/#ensure-the-user-sees-results-immediately", "text": "Everything the learner does should accomplish something comprehensible, however small. If your student has to do strange and incomprehensible things for two pages before they even see a result, that\u2019s much too long. The effect of every action should be visible and evident as soon as possible, and the connection to the action should be clear. The conclusion of each section of a tutorial, or the tutorial as a whole, must be a meaningful accomplishment.", "title": "Ensure the user sees results immediately"}, {"location": "documentation/#focus-on-concrete-steps-not-abstract-concepts", "text": "Tutorials need to be concrete, built around specific, particular actions and outcomes. The temptation to introduce abstraction is huge; it is after all how most computing derives its power. But all learning proceeds from the particular and concrete to the general and abstract, and asking the learner to appreciate levels of abstraction before they have even had a chance to grasp the concrete is poor teaching.", "title": "Focus on concrete steps, not abstract concepts"}, {"location": "documentation/#provide-the-minimum-necessary-explanation", "text": "Don\u2019t explain anything the learner doesn\u2019t need to know in order to complete the tutorial. Extended discussion is important, just not in a tutorial. In a tutorial, it is an obstruction and a distraction. Only the bare minimum is appropriate. Instead, link to explanations elsewhere in the documentation.", "title": "Provide the minimum necessary explanation"}, {"location": "documentation/#focus-only-on-the-steps-the-user-needs-to-take", "text": "Your tutorial needs to be focused on the task in hand. Maybe the command you\u2019re introducing has many other options, or maybe there are different ways to access a certain API. It doesn\u2019t matter: right now, your learner does not need to know about those in order to make progress.", "title": "Focus only on the steps the user needs to take"}, {"location": "documentation/#how-to-guides", "text": "", "title": "How-to guides"}, {"location": "documentation/#technical-reference", "text": "", "title": "Technical reference"}, {"location": "documentation/#background-information", "text": "", "title": "Background information"}, {"location": "documentation/#references", "text": "divio's documentation wiki Vue's guidelines FastAPI awesome docs", "title": "References"}, {"location": "drone/", "text": "Drone is a modern Continuous Integration platform that empowers busy teams to automate their build, test and release workflows using a powerful, cloud native pipeline engine. Installation \u2691 This section explains how to install the Drone server for Gitea. Note They explicitly recommend not to use Gitea and Drone in the same instance, and even less using docker-compose due to network complications. But if you have only a small instance as I do, you'll have to try :P. Create a Gitea user to be used by the CI. Log in with the drone Gitea user. Create a Gitea OAuth application. The Consumer Key and Consumer Secret are used to authorize access to Gitea resources. Create a shared secret to authenticate communication between runners and your central Drone server. openssl rand -hex 16 Create the required docker networks: docker network create continuous-delivery docker network create drone docker network create swag Create the docker-compose file for the server --- version : '3' services : server : image : drone/drone:2 environment : - DRONE_GITEA_SERVER=https://try.gitea.io - DRONE_GITEA_CLIENT_ID=05136e57d80189bef462 - DRONE_GITEA_CLIENT_SECRET=7c229228a77d2cbddaa61ddc78d45e - DRONE_RPC_SECRET=super-duper-secret - DRONE_SERVER_HOST=drone.company.com - DRONE_SERVER_PROTO=https container_name : drone restart : always networks : - swag - drone - continuous-delivery volumes : - drone-data:/data networks : continuous-delivery : external : name : continuous-delivery drone : external : name : drone swag : external : name : swag volumes : drone-data : driver : local driver_opts : type : none o : bind device : /data/drone Where we specify where we want the data to be stored at, and the networks to use. We're assuming that you're going to use the linuxserver swag proxy to end the ssl connection (which is accessible through the swag network), and that gitea is in the continuous-delivery network. Add the runners you want to install. Configure your proxy to forward the requests to the correct dockers. Run docker-compose up from the file where your docker-compose.yaml file is to test everything works. If it does, run docker-compose down . Create a systemd service to start and stop the whole service. For example create the /etc/systemd/system/drone.service file with the content: Description=drone [Unit] Description=drone Requires=gitea.service After=gitea.service [Service] Restart=always User=root Group=docker WorkingDirectory=/data/config/continuous-delivery/drone # Shutdown container (if running) when unit is started TimeoutStartSec=100 RestartSec=2s # Start container when unit is started ExecStart=/usr/bin/docker-compose -f docker-compose.yml up # Stop container when unit is stopped ExecStop=/usr/bin/docker-compose -f docker-compose.yml down [Install] WantedBy=multi-user.target Drone Runners \u2691 Docker Runner \u2691 Merge the next docker-compose with the one of the server above: --- version : '3' services : docker_runner : image : drone/drone-runner-docker:1 environment : - DRONE_RPC_PROTO=https - DRONE_RPC_HOST=drone.company.com - DRONE_RPC_SECRET=super-duper-secret - DRONE_RUNNER_CAPACITY=2 - DRONE_RUNNER_NAME=docker-runner container_name : drone-docker-runner restart : always networks : - drone volumes : - /var/run/docker.sock:/var/run/docker.sock expose : - \"3000\" networks : drone : external : name : drone Use the docker logs command to view the logs and verify the runner successfully established a connection with the Drone server. $ docker logs runner INFO [ 0000 ] starting the server INFO [ 0000 ] successfully pinged the remote server SSH Runner \u2691 Merge the next docker-compose with the one of the server above: --- version : '3' services : ssh_runner : image : drone/drone-runner-ssh:latest environment : - DRONE_RPC_PROTO=https - DRONE_RPC_HOST=drone.company.com - DRONE_RPC_SECRET=super-duper-secret container_name : drone-ssh-runner restart : always networks : - drone expose : - \"3000\" networks : drone : external : name : drone Use the docker logs command to view the logs and verify the runner successfully established a connection with the Drone server. $ docker logs runner INFO [ 0000 ] starting the server INFO [ 0000 ] successfully pinged the remote server References \u2691 Docs Home", "title": "Drone"}, {"location": "drone/#installation", "text": "This section explains how to install the Drone server for Gitea. Note They explicitly recommend not to use Gitea and Drone in the same instance, and even less using docker-compose due to network complications. But if you have only a small instance as I do, you'll have to try :P. Create a Gitea user to be used by the CI. Log in with the drone Gitea user. Create a Gitea OAuth application. The Consumer Key and Consumer Secret are used to authorize access to Gitea resources. Create a shared secret to authenticate communication between runners and your central Drone server. openssl rand -hex 16 Create the required docker networks: docker network create continuous-delivery docker network create drone docker network create swag Create the docker-compose file for the server --- version : '3' services : server : image : drone/drone:2 environment : - DRONE_GITEA_SERVER=https://try.gitea.io - DRONE_GITEA_CLIENT_ID=05136e57d80189bef462 - DRONE_GITEA_CLIENT_SECRET=7c229228a77d2cbddaa61ddc78d45e - DRONE_RPC_SECRET=super-duper-secret - DRONE_SERVER_HOST=drone.company.com - DRONE_SERVER_PROTO=https container_name : drone restart : always networks : - swag - drone - continuous-delivery volumes : - drone-data:/data networks : continuous-delivery : external : name : continuous-delivery drone : external : name : drone swag : external : name : swag volumes : drone-data : driver : local driver_opts : type : none o : bind device : /data/drone Where we specify where we want the data to be stored at, and the networks to use. We're assuming that you're going to use the linuxserver swag proxy to end the ssl connection (which is accessible through the swag network), and that gitea is in the continuous-delivery network. Add the runners you want to install. Configure your proxy to forward the requests to the correct dockers. Run docker-compose up from the file where your docker-compose.yaml file is to test everything works. If it does, run docker-compose down . Create a systemd service to start and stop the whole service. For example create the /etc/systemd/system/drone.service file with the content: Description=drone [Unit] Description=drone Requires=gitea.service After=gitea.service [Service] Restart=always User=root Group=docker WorkingDirectory=/data/config/continuous-delivery/drone # Shutdown container (if running) when unit is started TimeoutStartSec=100 RestartSec=2s # Start container when unit is started ExecStart=/usr/bin/docker-compose -f docker-compose.yml up # Stop container when unit is stopped ExecStop=/usr/bin/docker-compose -f docker-compose.yml down [Install] WantedBy=multi-user.target", "title": "Installation"}, {"location": "drone/#drone-runners", "text": "", "title": "Drone Runners"}, {"location": "drone/#docker-runner", "text": "Merge the next docker-compose with the one of the server above: --- version : '3' services : docker_runner : image : drone/drone-runner-docker:1 environment : - DRONE_RPC_PROTO=https - DRONE_RPC_HOST=drone.company.com - DRONE_RPC_SECRET=super-duper-secret - DRONE_RUNNER_CAPACITY=2 - DRONE_RUNNER_NAME=docker-runner container_name : drone-docker-runner restart : always networks : - drone volumes : - /var/run/docker.sock:/var/run/docker.sock expose : - \"3000\" networks : drone : external : name : drone Use the docker logs command to view the logs and verify the runner successfully established a connection with the Drone server. $ docker logs runner INFO [ 0000 ] starting the server INFO [ 0000 ] successfully pinged the remote server", "title": "Docker Runner"}, {"location": "drone/#ssh-runner", "text": "Merge the next docker-compose with the one of the server above: --- version : '3' services : ssh_runner : image : drone/drone-runner-ssh:latest environment : - DRONE_RPC_PROTO=https - DRONE_RPC_HOST=drone.company.com - DRONE_RPC_SECRET=super-duper-secret container_name : drone-ssh-runner restart : always networks : - drone expose : - \"3000\" networks : drone : external : name : drone Use the docker logs command to view the logs and verify the runner successfully established a connection with the Drone server. $ docker logs runner INFO [ 0000 ] starting the server INFO [ 0000 ] successfully pinged the remote server", "title": "SSH Runner"}, {"location": "drone/#references", "text": "Docs Home", "title": "References"}, {"location": "dunst/", "text": "Dunst is a lightweight replacement for the notification daemons provided by most desktop environments. It\u2019s very customizable, isn\u2019t dependent on any toolkits, and therefore fits into those window manager centric setups we all love to customize to perfection. Installation \u2691 sudo apt-get install dunst Test it's working with: notify-send \"Notification Title\" \"Notification Messages\" If your distro version is too old that doesn't have dunstctl or dunstify , you can install it manually : git clone https://github.com/dunst-project/dunst.git cd dunst # Install dependencies sudo apt-get install libgdk-pixbuf2.0-0 libnotify-dev librust-pangocairo-dev # Build the program and install make WAYLAND = 0 SYSTEMD = 1 sudo make WAYLAND = 0 SYSTEMD = 1 install Read and tweak the ~/.dunst/dunstrc file to your liking. References \u2691 Git Home Archwiki page on dunst", "title": "dunst"}, {"location": "dunst/#installation", "text": "sudo apt-get install dunst Test it's working with: notify-send \"Notification Title\" \"Notification Messages\" If your distro version is too old that doesn't have dunstctl or dunstify , you can install it manually : git clone https://github.com/dunst-project/dunst.git cd dunst # Install dependencies sudo apt-get install libgdk-pixbuf2.0-0 libnotify-dev librust-pangocairo-dev # Build the program and install make WAYLAND = 0 SYSTEMD = 1 sudo make WAYLAND = 0 SYSTEMD = 1 install Read and tweak the ~/.dunst/dunstrc file to your liking.", "title": "Installation"}, {"location": "dunst/#references", "text": "Git Home Archwiki page on dunst", "title": "References"}, {"location": "dynamicdns/", "text": "Dynamic DNS (DDNS) is a method of automatically updating a name server in the Domain Name Server (DNS), often in real time, with the active DDNS configuration of its configured hostnames, addresses or other information. There are different DDNS providers, I use Duckdns as it is easy to setup and the Linuxserver people have a docker that makes it work .", "title": "Dynamic DNS"}, {"location": "elasticsearch_exporter/", "text": "The elasticsearch exporter allows monitoring Elasticsearch clusters with Prometheus . Installation \u2691 To install the exporter we'll use helmfile to install the prometheus-elasticsearch-exporter chart . Add the following lines to your helmfile.yaml . - name : prometheus-elasticsearch-exporter namespace : monitoring chart : prometheus-community/prometheus-elasticsearch-exporter values : - prometheus-elasticsearch-exporter/values.yaml Edit the chart values. mkdir prometheus-elasticsearch-exporter helm inspect values prometheus-community/prometheus-elasticsearch-exporter > prometheus-elasticsearch-exporter/values.yaml vi prometheus-elasticsearch-exporter/values.yaml Comment out all the values you don't edit, so that the chart doesn't break when you upgrade it. Make sure that the serviceMonitor labels match your Prometheus serviceMonitorSelector otherwise they won't be added to the configuration. es : ## Address (host and port) of the Elasticsearch node we should connect to. ## This could be a local node (localhost:9200, for instance), or the address ## of a remote Elasticsearch server. When basic auth is needed, ## specify as: <proto>://<user>:<password>@<host>:<port>. e.g., http://admin:pass@localhost:9200. ## uri : http://localhost:9200 serviceMonitor : ## If true, a ServiceMonitor CRD is created for a prometheus operator ## https://github.com/coreos/prometheus-operator ## enabled : true # namespace: monitoring labels : release : prometheus-operator interval : 30s # scrapeTimeout: 10s # scheme: http # relabelings: [] # targetLabels: [] metricRelabelings : - sourceLabels : [ cluster ] targetLabel : cluster_name regex : '.*:(.*)' # sampleLimit: 0 You can build the cluster label following this instructions , I didn't find the required meta tags, so I've built the cluster_name label for alerting purposes. The grafana dashboard I chose is 2322 . Taking as reference the grafana helm chart values, add the next yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : elasticsearch : # Ref: https://grafana.com/dashboards/2322 gnetId : 2322 revision : 4 datasource : Prometheus And install. helmfile diff helmfile apply Elasticsearch exporter alerts \u2691 Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. Availability alerts \u2691 The most basic probes, test if the service is healthy - alert : ElasticsearchClusterRed expr : elasticsearch_cluster_health_status{color=\"red\"} == 1 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Cluster Red (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster Red status VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchClusterYellow expr : elasticsearch_cluster_health_status{color=\"yellow\"} == 1 for : 0m labels : severity : warning annotations : summary : > Elasticsearch Cluster Yellow (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster Yellow status VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHealthyNodes expr : elasticsearch_cluster_health_number_of_nodes < 3 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Healthy Nodes (cluster {{ $labels.cluster_name }}) description : | Missing node in Elasticsearch cluster VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHealthyMasterNodes expr : > elasticsearch_cluster_health_number_of_nodes - elasticsearch_cluster_health_number_of_data_nodes > 0 < 3 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Healthy Master Nodes < 3 (cluster {{ $labels.cluster_name }}) description : | Missing master node in Elasticsearch cluster VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHealthyDataNodes expr : elasticsearch_cluster_health_number_of_data_nodes < 3 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Healthy Data Nodes (cluster {{ $labels.cluster_name }}) description : | Missing data node in Elasticsearch cluster VALUE = {{ $value }} LABELS = {{ $labels }} Performance alerts \u2691 - alert : ElasticsearchCPUUsageTooHigh expr : elasticsearch_os_cpu_percent > 90 for : 2m labels : severity : critical annotations : summary : > Elasticsearch Node CPU Usage Too High (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The CPU usage of node {{ $labels.name }} is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCPUUsageWarning expr : elasticsearch_os_cpu_percent > 80 for : 2m labels : severity : warning annotations : summary : > Elasticsearch Node CPU Usage Too High (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The CPU usage of node {{ $labels.name }} is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHeapUsageTooHigh expr : > ( elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"} ) * 100 > 90 for : 2m labels : severity : critical annotations : summary : > Elasticsearch Node Heap Usage Critical (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The heap usage of node {{ $labels.name }} is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHeapUsageWarning expr : > ( elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"} ) * 100 > 80 for : 2m labels : severity : warning annotations : summary : > Elasticsearch Node Heap Usage Warning (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The heap usage of node {{ $labels.name }} is over 80% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchDiskOutOfSpace expr : > elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 10 for : 0m labels : severity : critical annotations : summary : > Elasticsearch disk out of space (cluster {{ $labels.cluster_name }}) description : | The disk usage is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchDiskSpaceLow expr : > elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 20 for : 2m labels : severity : warning annotations : summary : > Elasticsearch disk space low (cluster {{ $labels.cluster_name }}) description : | The disk usage is over 80% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchRelocatingShardsTooLong expr : elasticsearch_cluster_health_relocating_shards > 0 for : 15m labels : severity : warning annotations : summary : > Elasticsearch relocating shards too long (cluster {{ $labels.cluster_name }}) description : | Elasticsearch has been relocating shards for 15min VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchInitializingShardsTooLong expr : elasticsearch_cluster_health_initializing_shards > 0 for : 15m labels : severity : warning annotations : summary : > Elasticsearch initializing shards too long (cluster_name {{ $labels.cluster }}) description : | Elasticsearch has been initializing shards for 15 min VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchUnassignedShards expr : elasticsearch_cluster_health_unassigned_shards > 0 for : 0m labels : severity : critical annotations : summary : > Elasticsearch unassigned shards (cluster {{ $labels.cluster_name }}) description : | Elasticsearch has unassigned shards VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchPendingTasks expr : elasticsearch_cluster_health_number_of_pending_tasks > 0 for : 15m labels : severity : warning annotations : summary : > Elasticsearch pending tasks (cluster {{ $labels.cluster_name }}) description : | Elasticsearch has pending tasks. Cluster works slowly. VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCountOfJVMGarbageCollectorRuns expr : rate(elasticsearch_jvm_gc_collection_seconds_count{}[5m]) > 5 for : 1m labels : severity : warning annotations : summary : > Elasticsearch JVM Garbage Collector runs > 5 (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster JVM Garbage Collector runs > 5 VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCountOfJVMGarbageCollectorTime expr : rate(elasticsearch_jvm_gc_collection_seconds_sum[5m]) > 0.3 for : 1m labels : severity : warning annotations : summary : > Elasticsearch JVM Garbage Collector time > 0.3 (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster JVM Garbage Collector runs > 0.3 VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchJSONParseErrors expr : elasticsearch_cluster_health_json_parse_failures > 0 for : 1m labels : severity : warning annotations : summary : > Elasticsearch json parse error (cluster {{ $labels.cluster_name }}) description : | Elasticsearch json parse error VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCircuitBreakerTripped expr : rate(elasticsearch_breakers_tripped{}[5m])>0 for : 1m labels : severity : warning annotations : summary : > Elasticsearch breaker {{ $labels.breaker }} tripped (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) description : | Elasticsearch breaker {{ $labels.breaker }} tripped (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) VALUE = {{ $value }} LABELS = {{ $labels }} Snapshot alerts \u2691 - alert : ElasticsearchMonthlySnapshot expr : > time() - elasticsearch_snapshot_stats_snapshot_end_time_timestamp{state=\"SUCCESS\"} > (3600 * 24 * 32) for : 15m labels : severity : warning annotations : summary : > Elasticsearch monthly snapshot failed (cluster {{ $labels.cluster_name }}, snapshot {{ $labels.repository }}) description : | Last successful elasticsearch snapshot of repository {{ $labels.repository}} is older than 32 days. VALUE = {{ $value }} LABELS = {{ $labels }} - record : elasticsearch_indices_search_latency:rate1m expr : | increase(elasticsearch_indices_search_query_time_seconds[1m])/ increase(elasticsearch_indices_search_query_total[1m]) - record : elasticsearch_indices_search_rate:rate1m expr : increase(elasticsearch_indices_search_query_total[1m])/60 - alert : ElasticsearchSlowSearchLatency expr : elasticsearch_indices_search_latency:rate1m > 1 for : 2m labels : severity : warning annotations : summary : > Elasticsearch search latency is greater than 1 s (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) description : | Elasticsearch search latency is greater than 1 s (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) VALUE = {{ $value }} LABELS = {{ $labels }} Links \u2691 Git", "title": "Elasticsearch Exporter"}, {"location": "elasticsearch_exporter/#installation", "text": "To install the exporter we'll use helmfile to install the prometheus-elasticsearch-exporter chart . Add the following lines to your helmfile.yaml . - name : prometheus-elasticsearch-exporter namespace : monitoring chart : prometheus-community/prometheus-elasticsearch-exporter values : - prometheus-elasticsearch-exporter/values.yaml Edit the chart values. mkdir prometheus-elasticsearch-exporter helm inspect values prometheus-community/prometheus-elasticsearch-exporter > prometheus-elasticsearch-exporter/values.yaml vi prometheus-elasticsearch-exporter/values.yaml Comment out all the values you don't edit, so that the chart doesn't break when you upgrade it. Make sure that the serviceMonitor labels match your Prometheus serviceMonitorSelector otherwise they won't be added to the configuration. es : ## Address (host and port) of the Elasticsearch node we should connect to. ## This could be a local node (localhost:9200, for instance), or the address ## of a remote Elasticsearch server. When basic auth is needed, ## specify as: <proto>://<user>:<password>@<host>:<port>. e.g., http://admin:pass@localhost:9200. ## uri : http://localhost:9200 serviceMonitor : ## If true, a ServiceMonitor CRD is created for a prometheus operator ## https://github.com/coreos/prometheus-operator ## enabled : true # namespace: monitoring labels : release : prometheus-operator interval : 30s # scrapeTimeout: 10s # scheme: http # relabelings: [] # targetLabels: [] metricRelabelings : - sourceLabels : [ cluster ] targetLabel : cluster_name regex : '.*:(.*)' # sampleLimit: 0 You can build the cluster label following this instructions , I didn't find the required meta tags, so I've built the cluster_name label for alerting purposes. The grafana dashboard I chose is 2322 . Taking as reference the grafana helm chart values, add the next yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : elasticsearch : # Ref: https://grafana.com/dashboards/2322 gnetId : 2322 revision : 4 datasource : Prometheus And install. helmfile diff helmfile apply", "title": "Installation"}, {"location": "elasticsearch_exporter/#elasticsearch-exporter-alerts", "text": "Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection.", "title": "Elasticsearch exporter alerts"}, {"location": "elasticsearch_exporter/#availability-alerts", "text": "The most basic probes, test if the service is healthy - alert : ElasticsearchClusterRed expr : elasticsearch_cluster_health_status{color=\"red\"} == 1 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Cluster Red (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster Red status VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchClusterYellow expr : elasticsearch_cluster_health_status{color=\"yellow\"} == 1 for : 0m labels : severity : warning annotations : summary : > Elasticsearch Cluster Yellow (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster Yellow status VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHealthyNodes expr : elasticsearch_cluster_health_number_of_nodes < 3 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Healthy Nodes (cluster {{ $labels.cluster_name }}) description : | Missing node in Elasticsearch cluster VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHealthyMasterNodes expr : > elasticsearch_cluster_health_number_of_nodes - elasticsearch_cluster_health_number_of_data_nodes > 0 < 3 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Healthy Master Nodes < 3 (cluster {{ $labels.cluster_name }}) description : | Missing master node in Elasticsearch cluster VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHealthyDataNodes expr : elasticsearch_cluster_health_number_of_data_nodes < 3 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Healthy Data Nodes (cluster {{ $labels.cluster_name }}) description : | Missing data node in Elasticsearch cluster VALUE = {{ $value }} LABELS = {{ $labels }}", "title": "Availability alerts"}, {"location": "elasticsearch_exporter/#performance-alerts", "text": "- alert : ElasticsearchCPUUsageTooHigh expr : elasticsearch_os_cpu_percent > 90 for : 2m labels : severity : critical annotations : summary : > Elasticsearch Node CPU Usage Too High (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The CPU usage of node {{ $labels.name }} is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCPUUsageWarning expr : elasticsearch_os_cpu_percent > 80 for : 2m labels : severity : warning annotations : summary : > Elasticsearch Node CPU Usage Too High (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The CPU usage of node {{ $labels.name }} is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHeapUsageTooHigh expr : > ( elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"} ) * 100 > 90 for : 2m labels : severity : critical annotations : summary : > Elasticsearch Node Heap Usage Critical (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The heap usage of node {{ $labels.name }} is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHeapUsageWarning expr : > ( elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"} ) * 100 > 80 for : 2m labels : severity : warning annotations : summary : > Elasticsearch Node Heap Usage Warning (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The heap usage of node {{ $labels.name }} is over 80% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchDiskOutOfSpace expr : > elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 10 for : 0m labels : severity : critical annotations : summary : > Elasticsearch disk out of space (cluster {{ $labels.cluster_name }}) description : | The disk usage is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchDiskSpaceLow expr : > elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 20 for : 2m labels : severity : warning annotations : summary : > Elasticsearch disk space low (cluster {{ $labels.cluster_name }}) description : | The disk usage is over 80% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchRelocatingShardsTooLong expr : elasticsearch_cluster_health_relocating_shards > 0 for : 15m labels : severity : warning annotations : summary : > Elasticsearch relocating shards too long (cluster {{ $labels.cluster_name }}) description : | Elasticsearch has been relocating shards for 15min VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchInitializingShardsTooLong expr : elasticsearch_cluster_health_initializing_shards > 0 for : 15m labels : severity : warning annotations : summary : > Elasticsearch initializing shards too long (cluster_name {{ $labels.cluster }}) description : | Elasticsearch has been initializing shards for 15 min VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchUnassignedShards expr : elasticsearch_cluster_health_unassigned_shards > 0 for : 0m labels : severity : critical annotations : summary : > Elasticsearch unassigned shards (cluster {{ $labels.cluster_name }}) description : | Elasticsearch has unassigned shards VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchPendingTasks expr : elasticsearch_cluster_health_number_of_pending_tasks > 0 for : 15m labels : severity : warning annotations : summary : > Elasticsearch pending tasks (cluster {{ $labels.cluster_name }}) description : | Elasticsearch has pending tasks. Cluster works slowly. VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCountOfJVMGarbageCollectorRuns expr : rate(elasticsearch_jvm_gc_collection_seconds_count{}[5m]) > 5 for : 1m labels : severity : warning annotations : summary : > Elasticsearch JVM Garbage Collector runs > 5 (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster JVM Garbage Collector runs > 5 VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCountOfJVMGarbageCollectorTime expr : rate(elasticsearch_jvm_gc_collection_seconds_sum[5m]) > 0.3 for : 1m labels : severity : warning annotations : summary : > Elasticsearch JVM Garbage Collector time > 0.3 (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster JVM Garbage Collector runs > 0.3 VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchJSONParseErrors expr : elasticsearch_cluster_health_json_parse_failures > 0 for : 1m labels : severity : warning annotations : summary : > Elasticsearch json parse error (cluster {{ $labels.cluster_name }}) description : | Elasticsearch json parse error VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCircuitBreakerTripped expr : rate(elasticsearch_breakers_tripped{}[5m])>0 for : 1m labels : severity : warning annotations : summary : > Elasticsearch breaker {{ $labels.breaker }} tripped (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) description : | Elasticsearch breaker {{ $labels.breaker }} tripped (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) VALUE = {{ $value }} LABELS = {{ $labels }}", "title": "Performance alerts"}, {"location": "elasticsearch_exporter/#snapshot-alerts", "text": "- alert : ElasticsearchMonthlySnapshot expr : > time() - elasticsearch_snapshot_stats_snapshot_end_time_timestamp{state=\"SUCCESS\"} > (3600 * 24 * 32) for : 15m labels : severity : warning annotations : summary : > Elasticsearch monthly snapshot failed (cluster {{ $labels.cluster_name }}, snapshot {{ $labels.repository }}) description : | Last successful elasticsearch snapshot of repository {{ $labels.repository}} is older than 32 days. VALUE = {{ $value }} LABELS = {{ $labels }} - record : elasticsearch_indices_search_latency:rate1m expr : | increase(elasticsearch_indices_search_query_time_seconds[1m])/ increase(elasticsearch_indices_search_query_total[1m]) - record : elasticsearch_indices_search_rate:rate1m expr : increase(elasticsearch_indices_search_query_total[1m])/60 - alert : ElasticsearchSlowSearchLatency expr : elasticsearch_indices_search_latency:rate1m > 1 for : 2m labels : severity : warning annotations : summary : > Elasticsearch search latency is greater than 1 s (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) description : | Elasticsearch search latency is greater than 1 s (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) VALUE = {{ $value }} LABELS = {{ $labels }}", "title": "Snapshot alerts"}, {"location": "elasticsearch_exporter/#links", "text": "Git", "title": "Links"}, {"location": "email_automation/", "text": "Most of the received emails require repetitive actions that can be automated, and you may also want to access your emails through a command line interface and be able to search through them. One of the ways to achieve that goals is to use a combination of tools to synchronize the mailboxes, tag them, and run scripts automatically based on the tags. Installation \u2691 First you need a program that syncs your mailboxes, following pazz's advice , I'll use mbsync . Follow the steps under installation to configure your accounts, taking as an example an account called lyz you should be able to sync all your emails with: mbsync -V lyz Now we need to install notmuch a tool to index, search, read, and tag large collections of email messages. Follow the steps under installation under you have created the database that indexes your emails. Once we have that, we need a tool to tag the emails following our desired rules. afew is one way to go. Follow the steps under installation . The remaining step to keep the inboxes synced and tagged is to run all the steps above in a cron. Particularize pazz's script for your usecase: #!/bin/bash # # Download and index new mail. # # Copyright (c) 2017 Patrick Totzke # Dependencies: flock, nm-online, mbsync, notmuch, afew # Example crontab entry: # # */2 * * * * /usr/bin/flock -n /home/pazz/.pullmail.lock /home/pazz/bin/pullmail.sh > /home/pazz/.pullmail.log # PATH = /home/pazz/.local/bin:/usr/local/bin/: $PATH ACCOUNTDIR = /home/pazz/.pullmail/ # this makes the keyring daemon accessible function keyring-control () { local -a vars =( \\ DBUS_SESSION_BUS_ADDRESS \\ GNOME_KEYRING_CONTROL \\ GNOME_KEYRING_PID \\ XDG_SESSION_COOKIE \\ GPG_AGENT_INFO \\ SSH_AUTH_SOCK \\ ) local pid = $( ps -C i3 -o pid --no-heading ) eval \"unset ${ vars [@] } ; $( printf \"export %s;\" $( sed 's/\\x00/\\n/g' /proc/ ${ pid //[^0-9]/ } /environ | grep $( printf -- \"-e ^%s= \" \" ${ vars [@] } \" )) ) \" } function log () { notify-send -t 2000 'mail sync:' \" $@ \" } function die () { notify-send -t 2000 -u critical 'mail sync:' \" $@ \" exit 1 } # Let's Do stuff keyring-control # abort as soon as something fails set -e # abort if not online nm-online -x -t 0 echo --------------------------------------------------------- date for accfile in ` ls $ACCOUNTDIR ` ; do ACC = $( basename $accfile ) echo ------------------------ $ACC ------------------------ mbsync -V $ACC || log \" $ACC failed\" done # index and tag new mails echo ------------------------ NOTMUCH ------------------------ notmuch new 2 >/dev/null || die \"NOTMUCH new failed\" echo ------------------------ AFEW ------------------------ afew -v --tag --new || die \"AFEW died\" echo --------------------------------------------------------- echo \"all done, goodbye.\" Where flock is a tool to manage locks from shell scripts. And add the entry in your crontab -e . If you want to process your emails with this system through a command line interface, you can configure alot .", "title": "Email automation"}, {"location": "email_automation/#installation", "text": "First you need a program that syncs your mailboxes, following pazz's advice , I'll use mbsync . Follow the steps under installation to configure your accounts, taking as an example an account called lyz you should be able to sync all your emails with: mbsync -V lyz Now we need to install notmuch a tool to index, search, read, and tag large collections of email messages. Follow the steps under installation under you have created the database that indexes your emails. Once we have that, we need a tool to tag the emails following our desired rules. afew is one way to go. Follow the steps under installation . The remaining step to keep the inboxes synced and tagged is to run all the steps above in a cron. Particularize pazz's script for your usecase: #!/bin/bash # # Download and index new mail. # # Copyright (c) 2017 Patrick Totzke # Dependencies: flock, nm-online, mbsync, notmuch, afew # Example crontab entry: # # */2 * * * * /usr/bin/flock -n /home/pazz/.pullmail.lock /home/pazz/bin/pullmail.sh > /home/pazz/.pullmail.log # PATH = /home/pazz/.local/bin:/usr/local/bin/: $PATH ACCOUNTDIR = /home/pazz/.pullmail/ # this makes the keyring daemon accessible function keyring-control () { local -a vars =( \\ DBUS_SESSION_BUS_ADDRESS \\ GNOME_KEYRING_CONTROL \\ GNOME_KEYRING_PID \\ XDG_SESSION_COOKIE \\ GPG_AGENT_INFO \\ SSH_AUTH_SOCK \\ ) local pid = $( ps -C i3 -o pid --no-heading ) eval \"unset ${ vars [@] } ; $( printf \"export %s;\" $( sed 's/\\x00/\\n/g' /proc/ ${ pid //[^0-9]/ } /environ | grep $( printf -- \"-e ^%s= \" \" ${ vars [@] } \" )) ) \" } function log () { notify-send -t 2000 'mail sync:' \" $@ \" } function die () { notify-send -t 2000 -u critical 'mail sync:' \" $@ \" exit 1 } # Let's Do stuff keyring-control # abort as soon as something fails set -e # abort if not online nm-online -x -t 0 echo --------------------------------------------------------- date for accfile in ` ls $ACCOUNTDIR ` ; do ACC = $( basename $accfile ) echo ------------------------ $ACC ------------------------ mbsync -V $ACC || log \" $ACC failed\" done # index and tag new mails echo ------------------------ NOTMUCH ------------------------ notmuch new 2 >/dev/null || die \"NOTMUCH new failed\" echo ------------------------ AFEW ------------------------ afew -v --tag --new || die \"AFEW died\" echo --------------------------------------------------------- echo \"all done, goodbye.\" Where flock is a tool to manage locks from shell scripts. And add the entry in your crontab -e . If you want to process your emails with this system through a command line interface, you can configure alot .", "title": "Installation"}, {"location": "email_management/", "text": "Email can be one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification backend of services that don't need to be acted upon immediately or when more powerful mechanisms are not available. If not used wisely, it can be a sink of productivity. Analyze how often you need to check it \u2691 Follow the interruption analysis to discover how often you need to check it and if you need the notifications. Once you've decided the frequency, try to respect it!. If you want an example, check my work or personal analysis. Workflow \u2691 Each time I decide to go through my emails I follow the inbox processing guidelines . I understand the email inbox are items that need to be taken care of. If an email doesn't fall in that category I either archive or delete it. That way the inbox has the smallest number of items, and if everything went well, it is empty. Having an empty inbox helps you a lot to reduce the mental load for many reasons: When you look at it and don't see any mail, you get the small satisfaction that you have done everything. When there is something new, it stands out, without the distraction of other email subjects that can drift your attention. Accounts shared by many people \u2691 On email accounts managed by many people, I delete/archive emails that I know that need no interaction by any of them. If there is nothing for me to do, I mark them as read and wait for them to archive/delete them. If an email is left unread for 3 or 4 days I ask by other channels what should we do with that event. Use email to transport information, not to store it \u2691 Email was envisioned as a protocol for person A to send information to person B. The fact that the \"free email providers\" such as Google allow users to have almost no limit on their inbox has driven people to store all their emails and use it as a knowledge repository. This approach has many problems: As most people don't use end to end encryption (GPG), the data of their emails is available for the email provider to read. This is a privacy violation that leads to scary behaviours, such as targeted adds or google suggestions based on the content of recent emails. You could improve the situation by using POP3 instead of IMAP, but that'll force you to only use one device to check your email, something that's becoming uncommon. The decent email providers that respect you, such as RiseUp , Autistici or Disroot , are maintained by communities and can only offer a limited storage, so you're forced to empty your emails periodically to be able to receive new ones. If you don't spend time and effort classifying your emails, searching between them is a nightmare. It is even if you classify them. There are more efficient knowledge repositories to store your information. On my personal emails, I forward the information to my archive, task manager or knowledge manager, deleting the email afterwards. At work, they use an indecent provider, encrypts most of emails with GPG and trust the provider to hold the rest of the data. I try to leak the least amount of personal information and I archive every email because you don't know when you're going to need them. Use key bindings \u2691 Using the mouse to interact with the email client graphical interface is not efficient, try to learn the key bindings and use them as much as possible. Environment setup \u2691 Account management \u2691 It's common to have more than one account to check. For example, at work, I have my own account and another for each team I'm part of, the last ones are managed by all the team members. On the personal level, I've got many accounts for the different OpSec profiles or identities. For efficiency reasons, you need to be able to check all of them on one place. You can use an email manager such as Thunderbird . Once you choose one, try to master it. Isolate your work and personal environments \u2691 Make sure that you set your environment so that you can't check your personal email when you're working and the other way around. For example, you could set two Thunderbird profiles, or you could avoid configuring the work email in your personal phone. Automatic filtering and processing \u2691 Inbox management is time consuming, so you want to reduce the number of emails to process. From the interruption analysis you'll know which ones don't give you any value, our goal is to make them disappear before we open our inbox. You can get rid of them by: Preventing the sender to send them: Unsubscribe from the newsletters you no longer read or fix the configuration of the services that send you notifications that don't want. Tweak your spam filter: If you have no control on the source, tweak your spam filter so that it filters them out for you. Use your email client filtering and processing features: If you want to receive the emails for archival purposes, configure your email client to match them by regular expressions on the sender or subject, mark them as read and move them to the desired directory. Use email automation software: If you want to run automatic processes triggered by emails, use email automation solutions . Use your preferred editor to write the emails \u2691 You'll probably be less efficient with the email client's editor in comparison with your own. If you use vim or emacs, there's a good chance that the email client has a plugin that allows you to use it. Or you can always migrate to a command line client. I'll probably do that once I set up the email automation system .", "title": "Email management"}, {"location": "email_management/#analyze-how-often-you-need-to-check-it", "text": "Follow the interruption analysis to discover how often you need to check it and if you need the notifications. Once you've decided the frequency, try to respect it!. If you want an example, check my work or personal analysis.", "title": "Analyze how often you need to check it"}, {"location": "email_management/#workflow", "text": "Each time I decide to go through my emails I follow the inbox processing guidelines . I understand the email inbox are items that need to be taken care of. If an email doesn't fall in that category I either archive or delete it. That way the inbox has the smallest number of items, and if everything went well, it is empty. Having an empty inbox helps you a lot to reduce the mental load for many reasons: When you look at it and don't see any mail, you get the small satisfaction that you have done everything. When there is something new, it stands out, without the distraction of other email subjects that can drift your attention.", "title": "Workflow"}, {"location": "email_management/#accounts-shared-by-many-people", "text": "On email accounts managed by many people, I delete/archive emails that I know that need no interaction by any of them. If there is nothing for me to do, I mark them as read and wait for them to archive/delete them. If an email is left unread for 3 or 4 days I ask by other channels what should we do with that event.", "title": "Accounts shared by many people"}, {"location": "email_management/#use-email-to-transport-information-not-to-store-it", "text": "Email was envisioned as a protocol for person A to send information to person B. The fact that the \"free email providers\" such as Google allow users to have almost no limit on their inbox has driven people to store all their emails and use it as a knowledge repository. This approach has many problems: As most people don't use end to end encryption (GPG), the data of their emails is available for the email provider to read. This is a privacy violation that leads to scary behaviours, such as targeted adds or google suggestions based on the content of recent emails. You could improve the situation by using POP3 instead of IMAP, but that'll force you to only use one device to check your email, something that's becoming uncommon. The decent email providers that respect you, such as RiseUp , Autistici or Disroot , are maintained by communities and can only offer a limited storage, so you're forced to empty your emails periodically to be able to receive new ones. If you don't spend time and effort classifying your emails, searching between them is a nightmare. It is even if you classify them. There are more efficient knowledge repositories to store your information. On my personal emails, I forward the information to my archive, task manager or knowledge manager, deleting the email afterwards. At work, they use an indecent provider, encrypts most of emails with GPG and trust the provider to hold the rest of the data. I try to leak the least amount of personal information and I archive every email because you don't know when you're going to need them.", "title": "Use email to transport information, not to store it"}, {"location": "email_management/#use-key-bindings", "text": "Using the mouse to interact with the email client graphical interface is not efficient, try to learn the key bindings and use them as much as possible.", "title": "Use key bindings"}, {"location": "email_management/#environment-setup", "text": "", "title": "Environment setup"}, {"location": "email_management/#account-management", "text": "It's common to have more than one account to check. For example, at work, I have my own account and another for each team I'm part of, the last ones are managed by all the team members. On the personal level, I've got many accounts for the different OpSec profiles or identities. For efficiency reasons, you need to be able to check all of them on one place. You can use an email manager such as Thunderbird . Once you choose one, try to master it.", "title": "Account management"}, {"location": "email_management/#isolate-your-work-and-personal-environments", "text": "Make sure that you set your environment so that you can't check your personal email when you're working and the other way around. For example, you could set two Thunderbird profiles, or you could avoid configuring the work email in your personal phone.", "title": "Isolate your work and personal environments"}, {"location": "email_management/#automatic-filtering-and-processing", "text": "Inbox management is time consuming, so you want to reduce the number of emails to process. From the interruption analysis you'll know which ones don't give you any value, our goal is to make them disappear before we open our inbox. You can get rid of them by: Preventing the sender to send them: Unsubscribe from the newsletters you no longer read or fix the configuration of the services that send you notifications that don't want. Tweak your spam filter: If you have no control on the source, tweak your spam filter so that it filters them out for you. Use your email client filtering and processing features: If you want to receive the emails for archival purposes, configure your email client to match them by regular expressions on the sender or subject, mark them as read and move them to the desired directory. Use email automation software: If you want to run automatic processes triggered by emails, use email automation solutions .", "title": "Automatic filtering and processing"}, {"location": "email_management/#use-your-preferred-editor-to-write-the-emails", "text": "You'll probably be less efficient with the email client's editor in comparison with your own. If you use vim or emacs, there's a good chance that the email client has a plugin that allows you to use it. Or you can always migrate to a command line client. I'll probably do that once I set up the email automation system .", "title": "Use your preferred editor to write the emails"}, {"location": "emojis/", "text": "Curated list of emojis to copy paste. Angry \u2691 (\u0482\u2323\u0300_\u2323\u0301) ( >\u0434<) \u0295\u2022\u0300o\u2022\u0301\u0294 \u30fd(\u2267\u0414\u2266)\u30ce \u1559(\u21c0\u2038\u21bc\u2036)\u1557 \u0669(\u256c\u0298\u76ca\u0298\u256c)\u06f6 Annoyed \u2691 (\u2256\u035e_\u2256\u0325) (>_<) Awesome \u2691 ( \u00b7_\u00b7) ( \u00b7_\u00b7) --\u25a0-\u25a0 ( \u00b7_\u00b7)--\u25a0-\u25a0 (-\u25a0_\u25a0) YEAAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHH Conforting \u2691 (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) Congratulations \u2691 ( \u141b )\u0648 \uff3c\\ \u0669( \u141b )\u0648 /\uff0f Crying \u2691 (\u2565\ufe4f\u2565) (\u0ca5\ufe4f\u0ca5) Excited \u2691 (((o(*\uff9f\u25bd\uff9f*)o))) o(\u2267\u2207\u2266o) Dance \u2691 (~\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e~) \u250c(\u30fb\u3002\u30fb)\u2518 \u266a \u2514(\u30fb\u3002\u30fb)\u2510 \u266a \u250c(\u30fb\u3002\u30fb)\u2518 \u01aa(\u02d8\u2323\u02d8)\u2510 \u01aa(\u02d8\u2323\u02d8)\u0283 \u250c(\u02d8\u2323\u02d8)\u0283 (>'-')> <('-'<) ^('-')^ v('-')v (>'-')> (^-^) Happy \u2691 \u1555( \u141b )\u1557 \u0295\u2022\u1d25\u2022\u0294 (\u2022\u203f\u2022) (\u25e1\u203f\u25e1\u273f) (\u273f\u25e0\u203f\u25e0) \u266a(\u0e51\u1d16\u25e1\u1d16\u0e51)\u266a Kisses \u2691 (\u3065\uffe3 \u00b3\uffe3)\u3065 ( \u02d8 \u00b3\u02d8)\u2665 Love \u2691 \u2764 Pride \u2691 <(\uffe3\uff3e\uffe3)> Relax \u2691 _\u3078__(\u203e\u25e1\u25dd )> Sad \u2691 \uff61\uff9f(*\u00b4\u25a1`)\uff9f\uff61 (\u25de\u2038\u25df\uff1b) Scared \u2691 \u30fd(\uff9f\u0414\uff9f)\uff89 \u30fd\u3014\uff9f\u0414\uff9f\u3015\u4e3f Sleepy \u2691 (\u1d17\u02f3\u1d17) Smug \u2691 \uff08\uffe3\uff5e\uffe3\uff09 Whyyyy? \u2691 (/\uff9f\u0414\uff9f)/ Surprised \u2691 (\\_/) (O.o) (> <) (\u2299_\u2609) (\u00ac\u00ba-\u00b0)\u00ac (\u2609_\u2609) (\u2022 \u0325\u0306\u2006\u2022) \u00af\\(\u00b0_o)/\u00af (\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002) (*\uff9f\u25ef\uff9f*) Who cares \u2691 \u00af\\_(\u30c4)_/\u00af WTF \u2691 (\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b \u30d8\uff08\u3002\u25a1\u00b0\uff09\u30d8 Links \u2691 Japanese Emoticons", "title": "Emojis"}, {"location": "emojis/#angry", "text": "(\u0482\u2323\u0300_\u2323\u0301) ( >\u0434<) \u0295\u2022\u0300o\u2022\u0301\u0294 \u30fd(\u2267\u0414\u2266)\u30ce \u1559(\u21c0\u2038\u21bc\u2036)\u1557 \u0669(\u256c\u0298\u76ca\u0298\u256c)\u06f6", "title": "Angry"}, {"location": "emojis/#annoyed", "text": "(\u2256\u035e_\u2256\u0325) (>_<)", "title": "Annoyed"}, {"location": "emojis/#awesome", "text": "( \u00b7_\u00b7) ( \u00b7_\u00b7) --\u25a0-\u25a0 ( \u00b7_\u00b7)--\u25a0-\u25a0 (-\u25a0_\u25a0) YEAAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHH", "title": "Awesome"}, {"location": "emojis/#conforting", "text": "(\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002)", "title": "Conforting"}, {"location": "emojis/#congratulations", "text": "( \u141b )\u0648 \uff3c\\ \u0669( \u141b )\u0648 /\uff0f", "title": "Congratulations"}, {"location": "emojis/#crying", "text": "(\u2565\ufe4f\u2565) (\u0ca5\ufe4f\u0ca5)", "title": "Crying"}, {"location": "emojis/#excited", "text": "(((o(*\uff9f\u25bd\uff9f*)o))) o(\u2267\u2207\u2266o)", "title": "Excited"}, {"location": "emojis/#dance", "text": "(~\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e~) \u250c(\u30fb\u3002\u30fb)\u2518 \u266a \u2514(\u30fb\u3002\u30fb)\u2510 \u266a \u250c(\u30fb\u3002\u30fb)\u2518 \u01aa(\u02d8\u2323\u02d8)\u2510 \u01aa(\u02d8\u2323\u02d8)\u0283 \u250c(\u02d8\u2323\u02d8)\u0283 (>'-')> <('-'<) ^('-')^ v('-')v (>'-')> (^-^)", "title": "Dance"}, {"location": "emojis/#happy", "text": "\u1555( \u141b )\u1557 \u0295\u2022\u1d25\u2022\u0294 (\u2022\u203f\u2022) (\u25e1\u203f\u25e1\u273f) (\u273f\u25e0\u203f\u25e0) \u266a(\u0e51\u1d16\u25e1\u1d16\u0e51)\u266a", "title": "Happy"}, {"location": "emojis/#kisses", "text": "(\u3065\uffe3 \u00b3\uffe3)\u3065 ( \u02d8 \u00b3\u02d8)\u2665", "title": "Kisses"}, {"location": "emojis/#love", "text": "\u2764", "title": "Love"}, {"location": "emojis/#pride", "text": "<(\uffe3\uff3e\uffe3)>", "title": "Pride"}, {"location": "emojis/#relax", "text": "_\u3078__(\u203e\u25e1\u25dd )>", "title": "Relax"}, {"location": "emojis/#sad", "text": "\uff61\uff9f(*\u00b4\u25a1`)\uff9f\uff61 (\u25de\u2038\u25df\uff1b)", "title": "Sad"}, {"location": "emojis/#scared", "text": "\u30fd(\uff9f\u0414\uff9f)\uff89 \u30fd\u3014\uff9f\u0414\uff9f\u3015\u4e3f", "title": "Scared"}, {"location": "emojis/#sleepy", "text": "(\u1d17\u02f3\u1d17)", "title": "Sleepy"}, {"location": "emojis/#smug", "text": "\uff08\uffe3\uff5e\uffe3\uff09", "title": "Smug"}, {"location": "emojis/#whyyyy", "text": "(/\uff9f\u0414\uff9f)/", "title": "Whyyyy?"}, {"location": "emojis/#surprised", "text": "(\\_/) (O.o) (> <) (\u2299_\u2609) (\u00ac\u00ba-\u00b0)\u00ac (\u2609_\u2609) (\u2022 \u0325\u0306\u2006\u2022) \u00af\\(\u00b0_o)/\u00af (\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002) (*\uff9f\u25ef\uff9f*)", "title": "Surprised"}, {"location": "emojis/#who-cares", "text": "\u00af\\_(\u30c4)_/\u00af", "title": "Who cares"}, {"location": "emojis/#wtf", "text": "(\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b \u30d8\uff08\u3002\u25a1\u00b0\uff09\u30d8", "title": "WTF"}, {"location": "emojis/#links", "text": "Japanese Emoticons", "title": "Links"}, {"location": "environmentalism/", "text": "Measure the carbon footprint of your travels \u2691 https://www.carbonfootprint.com/ There are also some calculators for events itself: https://co2.myclimate.org/en/event_calculators/new https://psci.princeton.edu/events-emissions-calculator Saving water \u2691 Here are some small things I'm doing to save some water each day: Use the watering can or a bucket to gather the shower water until it's warm enough. I use this water to flush the toilet. It would be best if it were possible to fill up the toilet's deposit, but it's not easy. Use a glass of water to wet the toothbrush and rinse my mouth instead of using running water.", "title": "Environmentalism"}, {"location": "environmentalism/#measure-the-carbon-footprint-of-your-travels", "text": "https://www.carbonfootprint.com/ There are also some calculators for events itself: https://co2.myclimate.org/en/event_calculators/new https://psci.princeton.edu/events-emissions-calculator", "title": "Measure the carbon footprint of your travels"}, {"location": "environmentalism/#saving-water", "text": "Here are some small things I'm doing to save some water each day: Use the watering can or a bucket to gather the shower water until it's warm enough. I use this water to flush the toilet. It would be best if it were possible to fill up the toilet's deposit, but it's not easy. Use a glass of water to wet the toothbrush and rinse my mouth instead of using running water.", "title": "Saving water"}, {"location": "fastapi/", "text": "FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints. The key features are: Fast: Very high performance, on par with NodeJS and Go (thanks to Starlette and Pydantic). One of the fastest Python frameworks available. Fast to code: Increase the speed to develop features by about 200% to 300%. Fewer bugs: Reduce about 40% of human (developer) induced errors. Intuitive: Great editor support. Completion everywhere. Less time debugging. Easy: Designed to be easy to use and learn. Less time reading docs. Short: Minimize code duplication. Multiple features from each parameter declaration. Fewer bugs. Robust: Get production-ready code. With automatic interactive documentation. Standards-based: Based on (and fully compatible with) the open standards for APIs: OpenAPI (previously known as Swagger) and JSON Schema. Authentication with JWT : with a super nice tutorial on how to set it up. Installation \u2691 pip install fastapi You will also need an ASGI server, for production such as Uvicorn or Hypercorn. pip install uvicorn [ standard ] Simple example \u2691 Create a file main.py with: from typing import Optional from fastapi import FastAPI app = FastAPI () @app . get ( \"/\" ) def read_root (): return { \"Hello\" : \"World\" } @app . get ( \"/items/ {item_id} \" ) def read_item ( item_id : int , q : Optional [ str ] = None ): return { \"item_id\" : item_id , \"q\" : q } Run the server: uvicorn main:app --reload Open your browser at http://127.0.0.1:8000/items/5?q=somequery . You will see the JSON response as: { \"item_id\" : 5 , \"q\" : \"somequery\" } You already created an API that: Receives HTTP requests in the paths / and /items/{item_id} . Both paths take GET operations (also known as HTTP methods). The path /items/{item_id} has a path parameter item_id that should be an int . The path /items/{item_id} has an optional str query parameter q . Has interactive API docs made for you: Swagger: http://127.0.0.1:8000/docs . Redoc: http://127.0.0.1:8000/redoc . You will see the automatic interactive API documentation (provided by Swagger UI): Sending data to the server \u2691 When you need to send data from a client (let's say, a browser) to your API, you have three basic options: As path parameters in the URL ( /items/2 ). As query parameters in the URL ( /items/2?skip=true ). In the body of a POST request. To send simple data use the first two, to send complex or sensitive data, use the last. It also supports sending data through cookies and headers . Path Parameters \u2691 You can declare path \"parameters\" or \"variables\" with the same syntax used by Python format strings: @app . get ( \"/items/ {item_id} \" ) def read_item ( item_id : int ): return { \"item_id\" : item_id } If you define the type hints of the function arguments, FastAPI will use pydantic data validation. If you need to use a Linux path as an argument, check this workaround , but be aware that it's not supported by OpenAPI. Order matters \u2691 Because path operations are evaluated in order, you need to make sure that the path for the fixed endpoint /users/me is declared before the variable one /users/{user_id} : @app . get ( \"/users/me\" ) async def read_user_me (): return { \"user_id\" : \"the current user\" } @app . get ( \"/users/ {user_id} \" ) async def read_user ( user_id : str ): return { \"user_id\" : user_id } Otherwise, the path for /users/{user_id} would match also for /users/me , \"thinking\" that it's receiving a parameter user_id with a value of \"me\". Predefined values \u2691 If you want the possible valid path parameter values to be predefined, you can use a standard Python Enum . from enum import Enum class ModelName ( str , Enum ): alexnet = \"alexnet\" resnet = \"resnet\" lenet = \"lenet\" @app . get ( \"/models/ {model_name} \" ) def get_model ( model_name : ModelName ): if model_name == ModelName . alexnet : return { \"model_name\" : model_name , \"message\" : \"Deep Learning FTW!\" } if model_name . value == \"lenet\" : return { \"model_name\" : model_name , \"message\" : \"LeCNN all the images\" } return { \"model_name\" : model_name , \"message\" : \"Have some residuals\" } These are the basics, FastAPI supports more complex path parameters and string validations . Query Parameters \u2691 When you declare other function parameters that are not part of the path parameters, they are automatically interpreted as \"query\" parameters. fake_items_db = [{ \"item_name\" : \"Foo\" }, { \"item_name\" : \"Bar\" }, { \"item_name\" : \"Baz\" }] @app . get ( \"/items/\" ) async def read_item ( skip : int = 0 , limit : int = 10 ): return fake_items_db [ skip : skip + limit ] The query is the set of key-value pairs that go after the ? in a URL, separated by & characters. For example, in the URL: http://127.0.0.1:8000/items/?skip=0&limit=10 These are the basics, FastAPI supports more complex query parameters and string validations . Request Body \u2691 To declare a request body, you use Pydantic models with all their power and benefits. from typing import Optional from pydantic import BaseModel class Item ( BaseModel ): name : str description : Optional [ str ] = None price : float tax : Optional [ float ] = None @app . post ( \"/items/\" ) async def create_item ( item : Item ): return item With just that Python type declaration, FastAPI will: Read the body of the request as JSON. Convert the corresponding types (if needed). Validate the data: If the data is invalid, it will return a nice and clear error, indicating exactly where and what was the incorrect data. Give you the received data in the parameter item . Generate JSON Schema definitions for your model. Those schemas will be part of the generated OpenAPI schema, and used by the automatic documentation UIs. These are the basics, FastAPI supports more complex patterns such as: Using multiple models in the same query . Additional validations of the pydantic models . Nested models . Sending data to the client \u2691 When you create a FastAPI path operation you can normally return any data from it: a dict , a list , a Pydantic model, a database model, etc. By default, FastAPI would automatically convert that return value to JSON using the jsonable_encoder . To return custom responses such as a direct string, xml or html use Response : from fastapi import FastAPI , Response app = FastAPI () @app . get ( \"/legacy/\" ) def get_legacy_data (): data = \"\"\"<?xml version=\"1.0\"?> <shampoo> <Header> Apply shampoo here. </Header> <Body> You'll have to use soap here. </Body> </shampoo> \"\"\" return Response ( content = data , media_type = \"application/xml\" ) Handling errors \u2691 There are many situations in where you need to notify an error to a client that is using your API. In these cases, you would normally return an HTTP status code in the range of 400 (from 400 to 499). This is similar to the 200 HTTP status codes (from 200 to 299). Those \"200\" status codes mean that somehow there was a \"success\" in the request. To return HTTP responses with errors to the client you use HTTPException . from fastapi import HTTPException items = { \"foo\" : \"The Foo Wrestlers\" } @app . get ( \"/items/ {item_id} \" ) async def read_item ( item_id : str ): if item_id not in items : raise HTTPException ( status_code = 404 , detail = \"Item not found\" ) return { \"item\" : items [ item_id ]} Updating data \u2691 Update replacing with PUT \u2691 To update an item you can use the HTTP PUT operation. You can use the jsonable_encoder to convert the input data to data that can be stored as JSON (e.g. with a NoSQL database). For example, converting datetime to str. from typing import List , Optional from fastapi.encoders import jsonable_encoder from pydantic import BaseModel class Item ( BaseModel ): name : Optional [ str ] = None description : Optional [ str ] = None price : Optional [ float ] = None tax : float = 10.5 tags : List [ str ] = [] items = { \"foo\" : { \"name\" : \"Foo\" , \"price\" : 50.2 }, \"bar\" : { \"name\" : \"Bar\" , \"description\" : \"The bartenders\" , \"price\" : 62 , \"tax\" : 20.2 }, \"baz\" : { \"name\" : \"Baz\" , \"description\" : None , \"price\" : 50.2 , \"tax\" : 10.5 , \"tags\" : []}, } @app . get ( \"/items/ {item_id} \" , response_model = Item ) async def read_item ( item_id : str ): return items [ item_id ] @app . put ( \"/items/ {item_id} \" , response_model = Item ) async def update_item ( item_id : str , item : Item ): update_item_encoded = jsonable_encoder ( item ) items [ item_id ] = update_item_encoded return update_item_encoded Partial updates with PATCH \u2691 You can also use the HTTP PATCH operation to partially update data. This means that you can send only the data that you want to update, leaving the rest intact. Configuration \u2691 Application configuration \u2691 In many cases your application could need some external settings or configurations, for example secret keys, database credentials, credentials for email services, etc. You can load these configurations through environmental variables , or you can use the awesome Pydantic settings management , whose advantages are: Do Pydantic's type validation on the fields. Automatically reads the missing values from environmental variables . Supports reading variables from Dotenv files . Supports secrets . First you define the Settings class with all the fields: File: config.py : from pydantic import BaseSettings class Settings ( BaseSettings ): verbose : bool = True database_url : str = \"tinydb://~/.local/share/pyscrobbler/database.tinydb\" Then in the api definition, set the dependency . File: api.py : from functools import lru_cache from fastapi import Depends , FastAPI app = FastAPI () @lru_cache () def get_settings () -> Settings : \"\"\"Configure the program settings.\"\"\" return Settings () @app . get ( \"/verbose\" ) def verbose ( settings : Settings = Depends ( get_settings )) -> bool : return settings . verbose Where: get_settings is the dependency function that configures the Settings object. The endpoint verbose is dependant of get_settings . The @lru_cache decorator changes the function it decorates to return the same value that was returned the first time, instead of computing it again, executing the code of the function every time. So, the function will be executed once for each combination of arguments. And then the values returned by each of those combinations of arguments will be used again and again whenever the function is called with exactly the same combination of arguments. Creating the Settings object is a costly operation as it needs to check the environment variables or read a file, so we want to do it just once, not on each request. This setup makes it easy to inject testing configuration so as not to break production code. OpenAPI configuration \u2691 Define title, description and version \u2691 from fastapi import FastAPI app = FastAPI ( title = \"My Super Project\" , description = \"This is a very fancy project, with auto docs for the API and everything\" , version = \"2.5.0\" , ) Define path tags \u2691 You can add tags to your path operation, pass the parameter tags with a list of str (commonly just one str ): from typing import Optional , Set from pydantic import BaseModel class Item ( BaseModel ): name : str description : Optional [ str ] = None price : float tax : Optional [ float ] = None tags : Set [ str ] = [] @app . post ( \"/items/\" , response_model = Item , tags = [ \"items\" ]) async def create_item ( item : Item ): return item @app . get ( \"/items/\" , tags = [ \"items\" ]) async def read_items (): return [{ \"name\" : \"Foo\" , \"price\" : 42 }] @app . get ( \"/users/\" , tags = [ \"users\" ]) async def read_users (): return [{ \"username\" : \"johndoe\" }] They will be added to the OpenAPI schema and used by the automatic documentation interfaces. Add metadata to the tags \u2691 tags_metadata = [ { \"name\" : \"users\" , \"description\" : \"Operations with users. The **login** logic is also here.\" , }, { \"name\" : \"items\" , \"description\" : \"Manage items. So _fancy_ they have their own docs.\" , \"externalDocs\" : { \"description\" : \"Items external docs\" , \"url\" : \"https://fastapi.tiangolo.com/\" , }, }, ] app = FastAPI(openapi_tags=tags_metadata) Add a summary and description \u2691 @app . post ( \"/items/\" , response_model = Item , summary = \"Create an item\" ) async def create_item ( item : Item ): \"\"\" Create an item with all the information: - **name**: each item must have a name - **description**: a long description - **price**: required - **tax**: if the item doesn't have tax, you can omit this - **tags**: a set of unique tag strings for this item \"\"\" return item Response description \u2691 @app . post ( \"/items/\" , response_description = \"The created item\" , ) async def create_item ( item : Item ): return item Deprecate a path operation \u2691 When you need to mark a path operation as deprecated, but without removing it @app . get ( \"/elements/\" , tags = [ \"items\" ], deprecated = True ) async def read_elements (): return [{ \"item_id\" : \"Foo\" }] Deploy with Docker . \u2691 FastAPI has it's own optimized docker , which makes the deployment of your applications really easy. In your project directory create the Dockerfile file: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 COPY ./app /app Go to the project directory (in where your Dockerfile is, containing your app directory). Build your FastAPI image: docker build -t myimage . Run a container based on your image: docker run -d --name mycontainer -p 80 :80 myimage Now you have an optimized FastAPI server in a Docker container. Auto-tuned for your current server (and number of CPU cores). Installing dependencies \u2691 If your program needs other dependencies, use the next dockerfile: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 COPY ./requirements.txt /app RUN pip install -r requirements.txt COPY ./app /app Other project structures \u2691 The previous examples assume that you have followed the FastAPI project structure. If instead you've used mine your application will be defined in the app variable in the src/program_name/entrypoints/api.py file. To make things simpler make the app variable available on the root of your package, so you can do from program_name import app instead of from program_name.entrypoints.api import app . To do that we need to add app to the __all__ internal python variable of the __init__.py file of our package. File: src/program_name/__init__.py : from .entrypoints.ap import app __all__ : List [ str ] = [ 'app' ] The image is configured through environmental variables So we will need to use: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 ENV MODULE_NAME = \"program_name\" COPY ./src/program_name /app/program_name Testing \u2691 FastAPI gives a TestClient object borrowed from Starlette to do the integration tests on your application. from fastapi import FastAPI from fastapi.testclient import TestClient app = FastAPI () @app . get ( \"/\" ) async def read_main (): return { \"msg\" : \"Hello World\" } @pytest . fixture ( name = \"client\" ) def client_ () -> TestClient : \"\"\"Configure FastAPI TestClient.\"\"\" return TestClient ( app ) def test_read_main ( client : TestClient ): response = client . get ( \"/\" ) assert response . status_code == 200 assert response . json () == { \"msg\" : \"Hello World\" } Test a POST request \u2691 result = client . post ( \"/items/\" , headers = { \"X-Token\" : \"coneofsilence\" }, json = { \"id\" : \"foobar\" , \"title\" : \"Foo Bar\" , \"description\" : \"The Foo Barters\" }, ) Inject testing configuration \u2691 If your application follows the application configuration section , injecting testing configuration is easy with dependency injection . Imagine you have a db_tinydb fixture that sets up the testing database: @pytest . fixture ( name = \"db_tinydb\" ) def db_tinydb_ ( tmp_path : Path ) -> str : \"\"\"Create an TinyDB database engine. Returns: database_url: Url used to connect to the database. \"\"\" tinydb_file_path = str ( tmp_path / \"tinydb.db\" ) return f \"tinydb:/// { tinydb_file_path } \" You can override the default database_url with: @pytest . fixture ( name = \"client\" ) def client_ ( db_tinydb : str ) -> TestClient : \"\"\"Configure FastAPI TestClient.\"\"\" def override_settings () -> Settings : \"\"\"Inject the testing database in the application settings.\"\"\" return Settings ( database_url = db_tinydb ) app . dependency_overrides [ get_settings ] = override_settings return TestClient ( app ) Add endpoints only on testing environment \u2691 Sometimes you want to have some API endpoints to populate the database for end to end testing the frontend. If your app config has the environment attribute, you could try to do: app = FastAPI () @lru_cache () def get_config () -> Config : \"\"\"Configure the program settings.\"\"\" # no cover: the dependency are injected in the tests log . info ( \"Loading the config\" ) return Config () # pragma: no cover if get_config () . environment == \"testing\" : @app . get ( \"/seed\" , status_code = 201 ) def seed_data ( repo : Repository = Depends ( get_repo ), empty : bool = True , num_articles : int = 3 , num_sources : int = 2 , ) -> None : \"\"\"Add seed data for the end to end tests. Args: repo: Repository to store the data. \"\"\" services . seed ( repo = repo , empty = empty , num_articles = num_articles , num_sources = num_sources ) repo . close () But the injection of the dependencies is only done inside the functions, so get_config().environment will always be the default value. I ended up doing that check inside the endpoint, which is not ideal. @app . get ( \"/seed\" , status_code = 201 ) def seed_data ( config : Config = Depends ( get_config ), repo : Repository = Depends ( get_repo ), empty : bool = True , num_articles : int = 3 , num_sources : int = 2 , ) -> None : \"\"\"Add seed data for the end to end tests. Args: repo: Repository to store the data. \"\"\" if config . environment != \"testing\" : repo . close () raise HTTPException ( status_code = 404 ) ... Tips and tricks \u2691 Create redirections \u2691 Returns an HTTP redirect. Uses a 307 status code (Temporary Redirect) by default. from fastapi import FastAPI from fastapi.responses import RedirectResponse app = FastAPI () @app . get ( \"/typer\" ) async def read_typer (): return RedirectResponse ( \"https://typer.tiangolo.com\" ) Test that your application works locally \u2691 Once you have your application built and tested , everything should work right? well, sometimes it don't. If you need to use pdb to debug what's going on, you can't use the docker as you won't be able to interact with the debugger. Instead, launch an uvicorn application directly with: uvicorn program_name:app --reload Note: The command is assuming that your app is available at the root of your package, look at the deploy section if you feel lost. Resolve the 307 error \u2691 Probably you've introduced an ending / to the endpoint, so instead of asking for /my/endpoint you tried to do /my/endpoint/ . Resolve the 409 error \u2691 Probably an exception was raised in the backend, use pdb to follow the trace and catch where it happened. Resolve the 422 error \u2691 You're probably passing the wrong arguments to the POST request, to solve it see the text attribute of the result. For example: # client: TestClient result = client . post ( \"/source/add\" , json = { \"body\" : body }, ) result . text # '{\"detail\":[{\"loc\":[\"query\",\"url\"],\"msg\":\"field required\",\"type\":\"value_error.missing\"}]}' The error is telling us that the required url parameter is missing. Logging \u2691 By default the application log messages are not shown in the uvicorn log , you need to add the next lines to the file where your app is defined: File: src/program_name/entrypoints/api.py : from fastapi import FastAPI from fastapi.logger import logger import logging log = logging . getLogger ( \"gunicorn.error\" ) logger . handlers = log . handlers if __name__ != \"main\" : logger . setLevel ( log . level ) else : logger . setLevel ( logging . DEBUG ) app = FastAPI () # rest of the application... Logging to Sentry \u2691 FastAPI can integrate with Sentry or similar application loggers through the ASGI middleware . Run a FastAPI server in the background for testing purposes \u2691 Sometimes you want to launch a web server with a simple API to test a program that can't use the testing client . First define the API to launch with: File: tests/api_server.py : from fastapi import FastAPI , HTTPException app = FastAPI () @app . get ( \"/existent\" ) async def existent (): return { \"msg\" : \"exists!\" } @app . get ( \"/inexistent\" ) async def inexistent (): raise HTTPException ( status_code = 404 , detail = \"It doesn't exist\" ) Then create the fixture: File: tests/conftest.py : from multiprocessing import Process from typing import Generator import pytest import uvicorn from .api_server import app def run_server () -> None : \"\"\"Command to run the fake api server.\"\"\" uvicorn . run ( app ) @pytest . fixture () def _server () -> Generator [ None , None , None ]: \"\"\"Start the fake api server.\"\"\" proc = Process ( target = run_server , args = (), daemon = True ) proc . start () yield proc . kill () # Cleanup after test Now you can use the server: None fixture in your tests and run your queries against http://localhost:8000 . Interesting features to explore \u2691 Structure big applications . Dependency injection . Running background tasks after the request is finished . Return a different response model . Upload files . Set authentication . Host behind a proxy . Static files . Issues \u2691 FastAPI does not log messages : update pyscrobbler and any other maintained applications and remove the snippet defined in the logging section . References \u2691 Docs Git Awesome FastAPI Testdriven.io course : suggested by the developer.", "title": "FastAPI"}, {"location": "fastapi/#installation", "text": "pip install fastapi You will also need an ASGI server, for production such as Uvicorn or Hypercorn. pip install uvicorn [ standard ]", "title": "Installation"}, {"location": "fastapi/#simple-example", "text": "Create a file main.py with: from typing import Optional from fastapi import FastAPI app = FastAPI () @app . get ( \"/\" ) def read_root (): return { \"Hello\" : \"World\" } @app . get ( \"/items/ {item_id} \" ) def read_item ( item_id : int , q : Optional [ str ] = None ): return { \"item_id\" : item_id , \"q\" : q } Run the server: uvicorn main:app --reload Open your browser at http://127.0.0.1:8000/items/5?q=somequery . You will see the JSON response as: { \"item_id\" : 5 , \"q\" : \"somequery\" } You already created an API that: Receives HTTP requests in the paths / and /items/{item_id} . Both paths take GET operations (also known as HTTP methods). The path /items/{item_id} has a path parameter item_id that should be an int . The path /items/{item_id} has an optional str query parameter q . Has interactive API docs made for you: Swagger: http://127.0.0.1:8000/docs . Redoc: http://127.0.0.1:8000/redoc . You will see the automatic interactive API documentation (provided by Swagger UI):", "title": "Simple example"}, {"location": "fastapi/#sending-data-to-the-server", "text": "When you need to send data from a client (let's say, a browser) to your API, you have three basic options: As path parameters in the URL ( /items/2 ). As query parameters in the URL ( /items/2?skip=true ). In the body of a POST request. To send simple data use the first two, to send complex or sensitive data, use the last. It also supports sending data through cookies and headers .", "title": "Sending data to the server"}, {"location": "fastapi/#path-parameters", "text": "You can declare path \"parameters\" or \"variables\" with the same syntax used by Python format strings: @app . get ( \"/items/ {item_id} \" ) def read_item ( item_id : int ): return { \"item_id\" : item_id } If you define the type hints of the function arguments, FastAPI will use pydantic data validation. If you need to use a Linux path as an argument, check this workaround , but be aware that it's not supported by OpenAPI.", "title": "Path Parameters"}, {"location": "fastapi/#order-matters", "text": "Because path operations are evaluated in order, you need to make sure that the path for the fixed endpoint /users/me is declared before the variable one /users/{user_id} : @app . get ( \"/users/me\" ) async def read_user_me (): return { \"user_id\" : \"the current user\" } @app . get ( \"/users/ {user_id} \" ) async def read_user ( user_id : str ): return { \"user_id\" : user_id } Otherwise, the path for /users/{user_id} would match also for /users/me , \"thinking\" that it's receiving a parameter user_id with a value of \"me\".", "title": "Order matters"}, {"location": "fastapi/#predefined-values", "text": "If you want the possible valid path parameter values to be predefined, you can use a standard Python Enum . from enum import Enum class ModelName ( str , Enum ): alexnet = \"alexnet\" resnet = \"resnet\" lenet = \"lenet\" @app . get ( \"/models/ {model_name} \" ) def get_model ( model_name : ModelName ): if model_name == ModelName . alexnet : return { \"model_name\" : model_name , \"message\" : \"Deep Learning FTW!\" } if model_name . value == \"lenet\" : return { \"model_name\" : model_name , \"message\" : \"LeCNN all the images\" } return { \"model_name\" : model_name , \"message\" : \"Have some residuals\" } These are the basics, FastAPI supports more complex path parameters and string validations .", "title": "Predefined values"}, {"location": "fastapi/#query-parameters", "text": "When you declare other function parameters that are not part of the path parameters, they are automatically interpreted as \"query\" parameters. fake_items_db = [{ \"item_name\" : \"Foo\" }, { \"item_name\" : \"Bar\" }, { \"item_name\" : \"Baz\" }] @app . get ( \"/items/\" ) async def read_item ( skip : int = 0 , limit : int = 10 ): return fake_items_db [ skip : skip + limit ] The query is the set of key-value pairs that go after the ? in a URL, separated by & characters. For example, in the URL: http://127.0.0.1:8000/items/?skip=0&limit=10 These are the basics, FastAPI supports more complex query parameters and string validations .", "title": "Query Parameters"}, {"location": "fastapi/#request-body", "text": "To declare a request body, you use Pydantic models with all their power and benefits. from typing import Optional from pydantic import BaseModel class Item ( BaseModel ): name : str description : Optional [ str ] = None price : float tax : Optional [ float ] = None @app . post ( \"/items/\" ) async def create_item ( item : Item ): return item With just that Python type declaration, FastAPI will: Read the body of the request as JSON. Convert the corresponding types (if needed). Validate the data: If the data is invalid, it will return a nice and clear error, indicating exactly where and what was the incorrect data. Give you the received data in the parameter item . Generate JSON Schema definitions for your model. Those schemas will be part of the generated OpenAPI schema, and used by the automatic documentation UIs. These are the basics, FastAPI supports more complex patterns such as: Using multiple models in the same query . Additional validations of the pydantic models . Nested models .", "title": "Request Body"}, {"location": "fastapi/#sending-data-to-the-client", "text": "When you create a FastAPI path operation you can normally return any data from it: a dict , a list , a Pydantic model, a database model, etc. By default, FastAPI would automatically convert that return value to JSON using the jsonable_encoder . To return custom responses such as a direct string, xml or html use Response : from fastapi import FastAPI , Response app = FastAPI () @app . get ( \"/legacy/\" ) def get_legacy_data (): data = \"\"\"<?xml version=\"1.0\"?> <shampoo> <Header> Apply shampoo here. </Header> <Body> You'll have to use soap here. </Body> </shampoo> \"\"\" return Response ( content = data , media_type = \"application/xml\" )", "title": "Sending data to the client"}, {"location": "fastapi/#handling-errors", "text": "There are many situations in where you need to notify an error to a client that is using your API. In these cases, you would normally return an HTTP status code in the range of 400 (from 400 to 499). This is similar to the 200 HTTP status codes (from 200 to 299). Those \"200\" status codes mean that somehow there was a \"success\" in the request. To return HTTP responses with errors to the client you use HTTPException . from fastapi import HTTPException items = { \"foo\" : \"The Foo Wrestlers\" } @app . get ( \"/items/ {item_id} \" ) async def read_item ( item_id : str ): if item_id not in items : raise HTTPException ( status_code = 404 , detail = \"Item not found\" ) return { \"item\" : items [ item_id ]}", "title": "Handling errors"}, {"location": "fastapi/#updating-data", "text": "", "title": "Updating data"}, {"location": "fastapi/#update-replacing-with-put", "text": "To update an item you can use the HTTP PUT operation. You can use the jsonable_encoder to convert the input data to data that can be stored as JSON (e.g. with a NoSQL database). For example, converting datetime to str. from typing import List , Optional from fastapi.encoders import jsonable_encoder from pydantic import BaseModel class Item ( BaseModel ): name : Optional [ str ] = None description : Optional [ str ] = None price : Optional [ float ] = None tax : float = 10.5 tags : List [ str ] = [] items = { \"foo\" : { \"name\" : \"Foo\" , \"price\" : 50.2 }, \"bar\" : { \"name\" : \"Bar\" , \"description\" : \"The bartenders\" , \"price\" : 62 , \"tax\" : 20.2 }, \"baz\" : { \"name\" : \"Baz\" , \"description\" : None , \"price\" : 50.2 , \"tax\" : 10.5 , \"tags\" : []}, } @app . get ( \"/items/ {item_id} \" , response_model = Item ) async def read_item ( item_id : str ): return items [ item_id ] @app . put ( \"/items/ {item_id} \" , response_model = Item ) async def update_item ( item_id : str , item : Item ): update_item_encoded = jsonable_encoder ( item ) items [ item_id ] = update_item_encoded return update_item_encoded", "title": "Update replacing with PUT"}, {"location": "fastapi/#partial-updates-with-patch", "text": "You can also use the HTTP PATCH operation to partially update data. This means that you can send only the data that you want to update, leaving the rest intact.", "title": "Partial updates with PATCH"}, {"location": "fastapi/#configuration", "text": "", "title": "Configuration"}, {"location": "fastapi/#application-configuration", "text": "In many cases your application could need some external settings or configurations, for example secret keys, database credentials, credentials for email services, etc. You can load these configurations through environmental variables , or you can use the awesome Pydantic settings management , whose advantages are: Do Pydantic's type validation on the fields. Automatically reads the missing values from environmental variables . Supports reading variables from Dotenv files . Supports secrets . First you define the Settings class with all the fields: File: config.py : from pydantic import BaseSettings class Settings ( BaseSettings ): verbose : bool = True database_url : str = \"tinydb://~/.local/share/pyscrobbler/database.tinydb\" Then in the api definition, set the dependency . File: api.py : from functools import lru_cache from fastapi import Depends , FastAPI app = FastAPI () @lru_cache () def get_settings () -> Settings : \"\"\"Configure the program settings.\"\"\" return Settings () @app . get ( \"/verbose\" ) def verbose ( settings : Settings = Depends ( get_settings )) -> bool : return settings . verbose Where: get_settings is the dependency function that configures the Settings object. The endpoint verbose is dependant of get_settings . The @lru_cache decorator changes the function it decorates to return the same value that was returned the first time, instead of computing it again, executing the code of the function every time. So, the function will be executed once for each combination of arguments. And then the values returned by each of those combinations of arguments will be used again and again whenever the function is called with exactly the same combination of arguments. Creating the Settings object is a costly operation as it needs to check the environment variables or read a file, so we want to do it just once, not on each request. This setup makes it easy to inject testing configuration so as not to break production code.", "title": "Application configuration"}, {"location": "fastapi/#openapi-configuration", "text": "", "title": "OpenAPI configuration"}, {"location": "fastapi/#define-title-description-and-version", "text": "from fastapi import FastAPI app = FastAPI ( title = \"My Super Project\" , description = \"This is a very fancy project, with auto docs for the API and everything\" , version = \"2.5.0\" , )", "title": "Define title, description and version"}, {"location": "fastapi/#define-path-tags", "text": "You can add tags to your path operation, pass the parameter tags with a list of str (commonly just one str ): from typing import Optional , Set from pydantic import BaseModel class Item ( BaseModel ): name : str description : Optional [ str ] = None price : float tax : Optional [ float ] = None tags : Set [ str ] = [] @app . post ( \"/items/\" , response_model = Item , tags = [ \"items\" ]) async def create_item ( item : Item ): return item @app . get ( \"/items/\" , tags = [ \"items\" ]) async def read_items (): return [{ \"name\" : \"Foo\" , \"price\" : 42 }] @app . get ( \"/users/\" , tags = [ \"users\" ]) async def read_users (): return [{ \"username\" : \"johndoe\" }] They will be added to the OpenAPI schema and used by the automatic documentation interfaces.", "title": "Define path tags"}, {"location": "fastapi/#add-metadata-to-the-tags", "text": "tags_metadata = [ { \"name\" : \"users\" , \"description\" : \"Operations with users. The **login** logic is also here.\" , }, { \"name\" : \"items\" , \"description\" : \"Manage items. So _fancy_ they have their own docs.\" , \"externalDocs\" : { \"description\" : \"Items external docs\" , \"url\" : \"https://fastapi.tiangolo.com/\" , }, }, ] app = FastAPI(openapi_tags=tags_metadata)", "title": "Add metadata to the tags"}, {"location": "fastapi/#add-a-summary-and-description", "text": "@app . post ( \"/items/\" , response_model = Item , summary = \"Create an item\" ) async def create_item ( item : Item ): \"\"\" Create an item with all the information: - **name**: each item must have a name - **description**: a long description - **price**: required - **tax**: if the item doesn't have tax, you can omit this - **tags**: a set of unique tag strings for this item \"\"\" return item", "title": "Add a summary and description"}, {"location": "fastapi/#response-description", "text": "@app . post ( \"/items/\" , response_description = \"The created item\" , ) async def create_item ( item : Item ): return item", "title": "Response description"}, {"location": "fastapi/#deprecate-a-path-operation", "text": "When you need to mark a path operation as deprecated, but without removing it @app . get ( \"/elements/\" , tags = [ \"items\" ], deprecated = True ) async def read_elements (): return [{ \"item_id\" : \"Foo\" }]", "title": "Deprecate a path operation"}, {"location": "fastapi/#deploy-with-docker", "text": "FastAPI has it's own optimized docker , which makes the deployment of your applications really easy. In your project directory create the Dockerfile file: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 COPY ./app /app Go to the project directory (in where your Dockerfile is, containing your app directory). Build your FastAPI image: docker build -t myimage . Run a container based on your image: docker run -d --name mycontainer -p 80 :80 myimage Now you have an optimized FastAPI server in a Docker container. Auto-tuned for your current server (and number of CPU cores).", "title": "Deploy with Docker."}, {"location": "fastapi/#installing-dependencies", "text": "If your program needs other dependencies, use the next dockerfile: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 COPY ./requirements.txt /app RUN pip install -r requirements.txt COPY ./app /app", "title": "Installing dependencies"}, {"location": "fastapi/#other-project-structures", "text": "The previous examples assume that you have followed the FastAPI project structure. If instead you've used mine your application will be defined in the app variable in the src/program_name/entrypoints/api.py file. To make things simpler make the app variable available on the root of your package, so you can do from program_name import app instead of from program_name.entrypoints.api import app . To do that we need to add app to the __all__ internal python variable of the __init__.py file of our package. File: src/program_name/__init__.py : from .entrypoints.ap import app __all__ : List [ str ] = [ 'app' ] The image is configured through environmental variables So we will need to use: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 ENV MODULE_NAME = \"program_name\" COPY ./src/program_name /app/program_name", "title": "Other project structures"}, {"location": "fastapi/#testing", "text": "FastAPI gives a TestClient object borrowed from Starlette to do the integration tests on your application. from fastapi import FastAPI from fastapi.testclient import TestClient app = FastAPI () @app . get ( \"/\" ) async def read_main (): return { \"msg\" : \"Hello World\" } @pytest . fixture ( name = \"client\" ) def client_ () -> TestClient : \"\"\"Configure FastAPI TestClient.\"\"\" return TestClient ( app ) def test_read_main ( client : TestClient ): response = client . get ( \"/\" ) assert response . status_code == 200 assert response . json () == { \"msg\" : \"Hello World\" }", "title": "Testing"}, {"location": "fastapi/#test-a-post-request", "text": "result = client . post ( \"/items/\" , headers = { \"X-Token\" : \"coneofsilence\" }, json = { \"id\" : \"foobar\" , \"title\" : \"Foo Bar\" , \"description\" : \"The Foo Barters\" }, )", "title": "Test a POST request"}, {"location": "fastapi/#inject-testing-configuration", "text": "If your application follows the application configuration section , injecting testing configuration is easy with dependency injection . Imagine you have a db_tinydb fixture that sets up the testing database: @pytest . fixture ( name = \"db_tinydb\" ) def db_tinydb_ ( tmp_path : Path ) -> str : \"\"\"Create an TinyDB database engine. Returns: database_url: Url used to connect to the database. \"\"\" tinydb_file_path = str ( tmp_path / \"tinydb.db\" ) return f \"tinydb:/// { tinydb_file_path } \" You can override the default database_url with: @pytest . fixture ( name = \"client\" ) def client_ ( db_tinydb : str ) -> TestClient : \"\"\"Configure FastAPI TestClient.\"\"\" def override_settings () -> Settings : \"\"\"Inject the testing database in the application settings.\"\"\" return Settings ( database_url = db_tinydb ) app . dependency_overrides [ get_settings ] = override_settings return TestClient ( app )", "title": "Inject testing configuration"}, {"location": "fastapi/#add-endpoints-only-on-testing-environment", "text": "Sometimes you want to have some API endpoints to populate the database for end to end testing the frontend. If your app config has the environment attribute, you could try to do: app = FastAPI () @lru_cache () def get_config () -> Config : \"\"\"Configure the program settings.\"\"\" # no cover: the dependency are injected in the tests log . info ( \"Loading the config\" ) return Config () # pragma: no cover if get_config () . environment == \"testing\" : @app . get ( \"/seed\" , status_code = 201 ) def seed_data ( repo : Repository = Depends ( get_repo ), empty : bool = True , num_articles : int = 3 , num_sources : int = 2 , ) -> None : \"\"\"Add seed data for the end to end tests. Args: repo: Repository to store the data. \"\"\" services . seed ( repo = repo , empty = empty , num_articles = num_articles , num_sources = num_sources ) repo . close () But the injection of the dependencies is only done inside the functions, so get_config().environment will always be the default value. I ended up doing that check inside the endpoint, which is not ideal. @app . get ( \"/seed\" , status_code = 201 ) def seed_data ( config : Config = Depends ( get_config ), repo : Repository = Depends ( get_repo ), empty : bool = True , num_articles : int = 3 , num_sources : int = 2 , ) -> None : \"\"\"Add seed data for the end to end tests. Args: repo: Repository to store the data. \"\"\" if config . environment != \"testing\" : repo . close () raise HTTPException ( status_code = 404 ) ...", "title": "Add endpoints only on testing environment"}, {"location": "fastapi/#tips-and-tricks", "text": "", "title": "Tips and tricks"}, {"location": "fastapi/#create-redirections", "text": "Returns an HTTP redirect. Uses a 307 status code (Temporary Redirect) by default. from fastapi import FastAPI from fastapi.responses import RedirectResponse app = FastAPI () @app . get ( \"/typer\" ) async def read_typer (): return RedirectResponse ( \"https://typer.tiangolo.com\" )", "title": "Create redirections"}, {"location": "fastapi/#test-that-your-application-works-locally", "text": "Once you have your application built and tested , everything should work right? well, sometimes it don't. If you need to use pdb to debug what's going on, you can't use the docker as you won't be able to interact with the debugger. Instead, launch an uvicorn application directly with: uvicorn program_name:app --reload Note: The command is assuming that your app is available at the root of your package, look at the deploy section if you feel lost.", "title": "Test that your application works locally"}, {"location": "fastapi/#resolve-the-307-error", "text": "Probably you've introduced an ending / to the endpoint, so instead of asking for /my/endpoint you tried to do /my/endpoint/ .", "title": "Resolve the 307 error"}, {"location": "fastapi/#resolve-the-409-error", "text": "Probably an exception was raised in the backend, use pdb to follow the trace and catch where it happened.", "title": "Resolve the 409 error"}, {"location": "fastapi/#resolve-the-422-error", "text": "You're probably passing the wrong arguments to the POST request, to solve it see the text attribute of the result. For example: # client: TestClient result = client . post ( \"/source/add\" , json = { \"body\" : body }, ) result . text # '{\"detail\":[{\"loc\":[\"query\",\"url\"],\"msg\":\"field required\",\"type\":\"value_error.missing\"}]}' The error is telling us that the required url parameter is missing.", "title": "Resolve the 422 error"}, {"location": "fastapi/#logging", "text": "By default the application log messages are not shown in the uvicorn log , you need to add the next lines to the file where your app is defined: File: src/program_name/entrypoints/api.py : from fastapi import FastAPI from fastapi.logger import logger import logging log = logging . getLogger ( \"gunicorn.error\" ) logger . handlers = log . handlers if __name__ != \"main\" : logger . setLevel ( log . level ) else : logger . setLevel ( logging . DEBUG ) app = FastAPI () # rest of the application...", "title": "Logging"}, {"location": "fastapi/#logging-to-sentry", "text": "FastAPI can integrate with Sentry or similar application loggers through the ASGI middleware .", "title": "Logging to Sentry"}, {"location": "fastapi/#run-a-fastapi-server-in-the-background-for-testing-purposes", "text": "Sometimes you want to launch a web server with a simple API to test a program that can't use the testing client . First define the API to launch with: File: tests/api_server.py : from fastapi import FastAPI , HTTPException app = FastAPI () @app . get ( \"/existent\" ) async def existent (): return { \"msg\" : \"exists!\" } @app . get ( \"/inexistent\" ) async def inexistent (): raise HTTPException ( status_code = 404 , detail = \"It doesn't exist\" ) Then create the fixture: File: tests/conftest.py : from multiprocessing import Process from typing import Generator import pytest import uvicorn from .api_server import app def run_server () -> None : \"\"\"Command to run the fake api server.\"\"\" uvicorn . run ( app ) @pytest . fixture () def _server () -> Generator [ None , None , None ]: \"\"\"Start the fake api server.\"\"\" proc = Process ( target = run_server , args = (), daemon = True ) proc . start () yield proc . kill () # Cleanup after test Now you can use the server: None fixture in your tests and run your queries against http://localhost:8000 .", "title": "Run a FastAPI server in the background for testing purposes"}, {"location": "fastapi/#interesting-features-to-explore", "text": "Structure big applications . Dependency injection . Running background tasks after the request is finished . Return a different response model . Upload files . Set authentication . Host behind a proxy . Static files .", "title": "Interesting features to explore"}, {"location": "fastapi/#issues", "text": "FastAPI does not log messages : update pyscrobbler and any other maintained applications and remove the snippet defined in the logging section .", "title": "Issues"}, {"location": "fastapi/#references", "text": "Docs Git Awesome FastAPI Testdriven.io course : suggested by the developer.", "title": "References"}, {"location": "ferdium/", "text": "Ferdium is a desktop application to have all your services in one place. It's similar to Rambox, Franz or Ferdi only that it's maintained by the community and respects your privacy. Installation \u2691 Download the deb package and run sudo dpkg -i /path/to/your/file.deb Security \u2691 In terms of security the Ferdium master password lock will only prevent an attacker from accessing your passwords if it has very few time to do the attack. They encrypt the password and save it in the config file along with a property lockingFeatureEnabled which is set to true when you activate this feature. Nevertheless if an attacker were to change this value to false , then they'll be able to access your Ferdium instance. Therefore I think that it's better to rely on locking your computer when leaving it and encrypting your hard drive. Adding the master password will only make the life harder for you for no substantial increase in security. Keep in mind that Ferdium stores the cookies to automatically log in the sites and that the information is accessible by an attacker that has access to your device. So only add services that are not critical to you. References \u2691 Homepage", "title": "ferdium"}, {"location": "ferdium/#installation", "text": "Download the deb package and run sudo dpkg -i /path/to/your/file.deb", "title": "Installation"}, {"location": "ferdium/#security", "text": "In terms of security the Ferdium master password lock will only prevent an attacker from accessing your passwords if it has very few time to do the attack. They encrypt the password and save it in the config file along with a property lockingFeatureEnabled which is set to true when you activate this feature. Nevertheless if an attacker were to change this value to false , then they'll be able to access your Ferdium instance. Therefore I think that it's better to rely on locking your computer when leaving it and encrypting your hard drive. Adding the master password will only make the life harder for you for no substantial increase in security. Keep in mind that Ferdium stores the cookies to automatically log in the sites and that the information is accessible by an attacker that has access to your device. So only add services that are not critical to you.", "title": "Security"}, {"location": "ferdium/#references", "text": "Homepage", "title": "References"}, {"location": "ffmpeg/", "text": "ffmpeg is a complete, cross-platform solution to record, convert and stream audio and video You can run ffmpeg -formats to get a list of every format that is supported. Cut \u2691 Cut video file into a shorter clip \u2691 You can use the time offset parameter -ss to specify the start time stamp in HH:MM:SS.ms format while the -t parameter is for specifying the actual duration of the clip in seconds. ffmpeg -i input.mp4 -ss 00 :00:50.0 -codec copy -t 20 output.mp4 Split a video into multiple parts \u2691 The next command will split the source video into 2 parts. One ending at 50s from the start and the other beginning at 50s and ending at the end of the input video. ffmpeg -i video.mp4 -t 00 :00:50 -c copy small-1.mp4 -ss 00 :00:50 -codec copy small-2.mp4 Crop an audio file \u2691 To create a 30 second audio file starting at 90 seconds from the original audio file without transcoding use: ffmpeg -ss 00 :01:30 -t 30 -acodec copy -i inputfile.mp3 outputfile.mp3 Join \u2691 Join (concatenate) video files \u2691 If you have multiple audio or video files encoded with the same codecs, you can join them into a single file. Create a input file with a list of all source files that you wish to concatenate and then run this command. Create first the file list with a Bash for loop: for f in ./*.wav ; do echo \"file ' $f '\" >> mylist.txt ; done Then convert ffmpeg -f concat -safe 0 -i mylist.txt -c copy output.mp4 Merge an audio and video file \u2691 You can also specify the -shortest switch to finish the encoding when the shortest clip ends. ffmpeg -i video.mp4 -i audio.mp3 -c:v copy -c:a aac -strict experimental output.mp4 ffmpeg -i video.mp4 -i audio.mp3 -c:v copy -c:a aac -strict experimental -shortest output.mp4 Mute \u2691 Use the -an parameter to disable the audio portion of a video stream. ffmpeg -i video.mp4 -an mute-video.mp4 Convert \u2691 Convert video from one format to another \u2691 You can use the -vcodec parameter to specify the encoding format to be used for the output video. Encoding a video takes time but you can speed up the process by forcing a preset though it would degrade the quality of the output video. ffmpeg -i youtube.flv -c:v libx264 filename.mp4 ffmpeg -i video.wmv -c:v libx264 -preset ultrafast video.mp4 Convert a x265 file into x264 \u2691 for i in *.mkv ; do ffmpeg -i \" $i \" -bsf:v h264_mp4toannexb -vcodec libx264 \" $i .x264.mkv\" done ffmpeg -i \"$i\" : Executes the program ffmpeg and calls for files to be processed. -bsf:v : Activates the video bit stream filter to be used. h264_mp4toannexb : Is the bit stream filter that is activated. Convert an H.264 bitstream from length prefixed mode to start code prefixed mode (as defined in the Annex B of the ITU-T H.264 specification). This is required by some streaming formats, typically the MPEG-2 transport stream format (mpegts) processing MKV h.264 (currently)requires this, if is not included you will get an error in the terminal window instructing you to use it. * -vcodec libx264 This tells ffmpeg to encode the output to H.264. * \"$i.ts\" Saves the output to .ts format, this is useful so as not to overwrite your source files. Convert VOB to mkv \u2691 Unify your VOBs cat *.VOB > output.vob Identify the streams ffmpeg -analyzeduration 100M -probesize 100M -i output.vob Select the streams that you are interested in, imagine that is 1, 3, 4, 5 and 6. Encoding ffmpeg \\ -analyzeduration 100M -probesize 100M \\ -i output.vob \\ -map 0 :1 -map 0 :3 -map 0 :4 -map 0 :5 -map 0 :6 \\ -metadata:s:a:0 language = ita -metadata:s:a:0 title = \"Italian stereo\" \\ -metadata:s:a:1 language = eng -metadata:s:a:1 title = \"English stereo\" \\ -metadata:s:s:0 language = ita -metadata:s:s:0 title = \"Italian\" \\ -metadata:s:s:1 language = eng -metadata:s:s:1 title = \"English\" \\ -codec:v libx264 -crf 21 \\ -codec:a libmp3lame -qscale:a 2 \\ -codec:s copy \\ output.mkv Convert a video into animated GIF \u2691 ffmpeg -ss 30 -t 3 -i input.mp4 -vf \"fps=10,scale=480:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -loop 0 output.gif This example will skip the first 30 seconds (-ss 30) of the input and create a 3 second output (-t 3). fps filter sets the frame rate. A rate of 10 frames per second is used in the example. Scale filter will resize the output to 320 pixels wide and automatically determine the height while preserving the aspect ratio. The lanczos scaling algorithm is used in this example. Palettegen and paletteuse filters will generate and use a custom palette generated from your input. These filters have many options, so refer to the links for a list of all available options and values. Also see the Advanced options section below. Split filter will allow everything to be done in one command and avoids having to create a temporary PNG file of the palette. Control looping with -loop output option but the values are confusing. A value of 0 is infinite looping, -1 is no looping, and 1 will loop once meaning it will play twice. So a value of 10 will cause the GIF to play 11 times. Convert video into images \u2691 You can use FFmpeg to automatically extract image frames from a video every n seconds and the images are saved in a sequence. This command saves image frame after every 4 seconds. ffmpeg -i movie.mp4 -r 0 .25 frames_%04d.png Convert a single image into a video \u2691 Use the -t parameter to specify the duration of the video. ffmpeg -loop 1 -i image.png -c:v libx264 -t 30 -pix_fmt yuv420p video.mp4 Convert opus or wav to mp3 \u2691 ffmpeg -i input.wav -vn -ar 44100 -ac 2 -b:a 320k output.mp3 -i : input file. -vn : Disable video, to make sure no video (including album cover image) is included if the source would be a video file. -ar : Set the audio sampling frequency. For output streams it is set by default to the frequency of the corresponding input stream. For input streams this option only makes sense for audio grabbing devices and raw demuxers and is mapped to the corresponding demuxer options. -ac : Set the number of audio channels. For output streams it is set by default to the number of input audio channels. For input streams this option only makes sense for audio grabbing devices and raw demuxers and is mapped to the corresponding demuxer options. So used here to make sure it is stereo (2 channels). -b:a : Converts the audio bitrate to be exact 320kbit per second. Extract \u2691 Extract the audio from video \u2691 The -vn switch extracts the audio portion from a video and we are using the -ab switch to save the audio as a 256kbps MP3 audio file. ffmpeg -i video.mp4 -vn -ab 256 audio.mp3 Extract image frames from a video \u2691 This command will extract the video frame at the 15s mark and saves it as a 800px wide JPEG image. You can also use the -s switch (like -s 400\u00d7300) to specify the exact dimensions of the image file though it will probably create a stretched image if the image size doesn\u2019t follow the aspect ratio of the original video file. ffmpeg -ss 00 :00:15 -i video.mp4 -vf scale = 800 :-1 -vframes 1 image.jpg Extract metadata of video \u2691 ffprobe {{ file }} Resize \u2691 Resize a video \u2691 Change the Constat Rate Factor \u2691 Setting the Constant Rate Factor, which lowers the average bit rate, but retains better quality. Vary the CRF between around 18 and 24 \u2014 the lower, the higher the bitrate. ffmpeg -i input.mp4 -vcodec libx265 -crf 24 output.mp4 Change the codec as needed - libx264 may be available if libx265 is not, at the cost of a slightly larger resultant file size. Change video resolution \u2691 Use the size -s switch with ffmpeg to resize a video while maintaining the aspect ratio. ffmpeg -i input.mp4 -s 480x320 -c:a copy output.mp4 Presentation \u2691 Create video slideshow from images \u2691 This command creates a video slideshow using a series of images that are named as img001.png , img002.png , etc. Each image will have a duration of 5 seconds (-r \u2155). ffmpeg -r 1 /5 -i img%03d.png -c:v libx264 -r 30 -pix_fmt yuv420p slideshow.mp4 Add a poster image to audio \u2691 You can add a cover image to an audio file and the length of the output video will be the same as that of the input audio stream. This may come handy for uploading MP3s to YouTube. ffmpeg -loop 1 -i image.jpg -i audio.mp3 -c:v libx264 -c:a aac -strict experimental -b:a 192k -shortest output.mp4 Add subtitles to a movie \u2691 This will take the subtitles from the .srt file. FFmpeg can decode most common subtitle formats. ffmpeg -i movie.mp4 -i subtitles.srt -map 0 -map 1 -c copy -c:v libx264 -crf 23 -preset veryfast output.mkv Change the audio volume \u2691 You can use the volume filter to alter the volume of a media file using FFmpeg. This command will half the volume of the audio file. ffmpeg -i input.wav -af 'volume=0.5' output.wav Rotate a video \u2691 This command will rotate a video clip 90\u00b0 clockwise. You can set transpose to 2 to rotate the video 90\u00b0 anti-clockwise. ffmpeg -i input.mp4 -filter:v 'transpose=1' rotated-video.mp4 This will rotate the video 180\u00b0 counter-clockwise. ffmpeg -i input.mp4 -filter:v 'transpose=2,transpose=2' rotated-video.mp4 Speed up or Slow down the video \u2691 You can change the speed of your video using the setpts (set presentation time stamp) filter of FFmpeg. This command will make the video 8x (\u215b) faster or use setpts=4*PTS to make the video 4x slower. ffmpeg -i input.mp4 -filter:v \"setpts=0.125*PTS\" output.mp4 Speed up or Slow down the audio \u2691 For changing the speed of audio, use the atempo audio filter. This command will double the speed of audio. You can use any value between 0.5 and 2.0 for audio. bash ffmpeg -i input.mkv -filter:a \"atempo=2.0\" -vn output.mkv Stack Exchange has a good overview to get you started with FFmpeg. You should also check out the official documentation at ffmpeg.org or the wiki at trac.ffmpeg.org to know about all the possible things you can do with FFmpeg. References \u2691 Home", "title": "ffmpeg"}, {"location": "ffmpeg/#cut", "text": "", "title": "Cut"}, {"location": "ffmpeg/#cut-video-file-into-a-shorter-clip", "text": "You can use the time offset parameter -ss to specify the start time stamp in HH:MM:SS.ms format while the -t parameter is for specifying the actual duration of the clip in seconds. ffmpeg -i input.mp4 -ss 00 :00:50.0 -codec copy -t 20 output.mp4", "title": "Cut video file into a shorter clip"}, {"location": "ffmpeg/#split-a-video-into-multiple-parts", "text": "The next command will split the source video into 2 parts. One ending at 50s from the start and the other beginning at 50s and ending at the end of the input video. ffmpeg -i video.mp4 -t 00 :00:50 -c copy small-1.mp4 -ss 00 :00:50 -codec copy small-2.mp4", "title": "Split a video into multiple parts"}, {"location": "ffmpeg/#crop-an-audio-file", "text": "To create a 30 second audio file starting at 90 seconds from the original audio file without transcoding use: ffmpeg -ss 00 :01:30 -t 30 -acodec copy -i inputfile.mp3 outputfile.mp3", "title": "Crop an audio file"}, {"location": "ffmpeg/#join", "text": "", "title": "Join"}, {"location": "ffmpeg/#join-concatenate-video-files", "text": "If you have multiple audio or video files encoded with the same codecs, you can join them into a single file. Create a input file with a list of all source files that you wish to concatenate and then run this command. Create first the file list with a Bash for loop: for f in ./*.wav ; do echo \"file ' $f '\" >> mylist.txt ; done Then convert ffmpeg -f concat -safe 0 -i mylist.txt -c copy output.mp4", "title": "Join (concatenate) video files"}, {"location": "ffmpeg/#merge-an-audio-and-video-file", "text": "You can also specify the -shortest switch to finish the encoding when the shortest clip ends. ffmpeg -i video.mp4 -i audio.mp3 -c:v copy -c:a aac -strict experimental output.mp4 ffmpeg -i video.mp4 -i audio.mp3 -c:v copy -c:a aac -strict experimental -shortest output.mp4", "title": "Merge an audio and video file"}, {"location": "ffmpeg/#mute", "text": "Use the -an parameter to disable the audio portion of a video stream. ffmpeg -i video.mp4 -an mute-video.mp4", "title": "Mute"}, {"location": "ffmpeg/#convert", "text": "", "title": "Convert"}, {"location": "ffmpeg/#convert-video-from-one-format-to-another", "text": "You can use the -vcodec parameter to specify the encoding format to be used for the output video. Encoding a video takes time but you can speed up the process by forcing a preset though it would degrade the quality of the output video. ffmpeg -i youtube.flv -c:v libx264 filename.mp4 ffmpeg -i video.wmv -c:v libx264 -preset ultrafast video.mp4", "title": "Convert video from one format to another"}, {"location": "ffmpeg/#convert-a-x265-file-into-x264", "text": "for i in *.mkv ; do ffmpeg -i \" $i \" -bsf:v h264_mp4toannexb -vcodec libx264 \" $i .x264.mkv\" done ffmpeg -i \"$i\" : Executes the program ffmpeg and calls for files to be processed. -bsf:v : Activates the video bit stream filter to be used. h264_mp4toannexb : Is the bit stream filter that is activated. Convert an H.264 bitstream from length prefixed mode to start code prefixed mode (as defined in the Annex B of the ITU-T H.264 specification). This is required by some streaming formats, typically the MPEG-2 transport stream format (mpegts) processing MKV h.264 (currently)requires this, if is not included you will get an error in the terminal window instructing you to use it. * -vcodec libx264 This tells ffmpeg to encode the output to H.264. * \"$i.ts\" Saves the output to .ts format, this is useful so as not to overwrite your source files.", "title": "Convert a x265 file into x264"}, {"location": "ffmpeg/#convert-vob-to-mkv", "text": "Unify your VOBs cat *.VOB > output.vob Identify the streams ffmpeg -analyzeduration 100M -probesize 100M -i output.vob Select the streams that you are interested in, imagine that is 1, 3, 4, 5 and 6. Encoding ffmpeg \\ -analyzeduration 100M -probesize 100M \\ -i output.vob \\ -map 0 :1 -map 0 :3 -map 0 :4 -map 0 :5 -map 0 :6 \\ -metadata:s:a:0 language = ita -metadata:s:a:0 title = \"Italian stereo\" \\ -metadata:s:a:1 language = eng -metadata:s:a:1 title = \"English stereo\" \\ -metadata:s:s:0 language = ita -metadata:s:s:0 title = \"Italian\" \\ -metadata:s:s:1 language = eng -metadata:s:s:1 title = \"English\" \\ -codec:v libx264 -crf 21 \\ -codec:a libmp3lame -qscale:a 2 \\ -codec:s copy \\ output.mkv", "title": "Convert VOB to mkv"}, {"location": "ffmpeg/#convert-a-video-into-animated-gif", "text": "ffmpeg -ss 30 -t 3 -i input.mp4 -vf \"fps=10,scale=480:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -loop 0 output.gif This example will skip the first 30 seconds (-ss 30) of the input and create a 3 second output (-t 3). fps filter sets the frame rate. A rate of 10 frames per second is used in the example. Scale filter will resize the output to 320 pixels wide and automatically determine the height while preserving the aspect ratio. The lanczos scaling algorithm is used in this example. Palettegen and paletteuse filters will generate and use a custom palette generated from your input. These filters have many options, so refer to the links for a list of all available options and values. Also see the Advanced options section below. Split filter will allow everything to be done in one command and avoids having to create a temporary PNG file of the palette. Control looping with -loop output option but the values are confusing. A value of 0 is infinite looping, -1 is no looping, and 1 will loop once meaning it will play twice. So a value of 10 will cause the GIF to play 11 times.", "title": "Convert a video into animated GIF"}, {"location": "ffmpeg/#convert-video-into-images", "text": "You can use FFmpeg to automatically extract image frames from a video every n seconds and the images are saved in a sequence. This command saves image frame after every 4 seconds. ffmpeg -i movie.mp4 -r 0 .25 frames_%04d.png", "title": "Convert video into images"}, {"location": "ffmpeg/#convert-a-single-image-into-a-video", "text": "Use the -t parameter to specify the duration of the video. ffmpeg -loop 1 -i image.png -c:v libx264 -t 30 -pix_fmt yuv420p video.mp4", "title": "Convert a single image into a video"}, {"location": "ffmpeg/#convert-opus-or-wav-to-mp3", "text": "ffmpeg -i input.wav -vn -ar 44100 -ac 2 -b:a 320k output.mp3 -i : input file. -vn : Disable video, to make sure no video (including album cover image) is included if the source would be a video file. -ar : Set the audio sampling frequency. For output streams it is set by default to the frequency of the corresponding input stream. For input streams this option only makes sense for audio grabbing devices and raw demuxers and is mapped to the corresponding demuxer options. -ac : Set the number of audio channels. For output streams it is set by default to the number of input audio channels. For input streams this option only makes sense for audio grabbing devices and raw demuxers and is mapped to the corresponding demuxer options. So used here to make sure it is stereo (2 channels). -b:a : Converts the audio bitrate to be exact 320kbit per second.", "title": "Convert opus or wav to mp3"}, {"location": "ffmpeg/#extract", "text": "", "title": "Extract"}, {"location": "ffmpeg/#extract-the-audio-from-video", "text": "The -vn switch extracts the audio portion from a video and we are using the -ab switch to save the audio as a 256kbps MP3 audio file. ffmpeg -i video.mp4 -vn -ab 256 audio.mp3", "title": "Extract the audio from video"}, {"location": "ffmpeg/#extract-image-frames-from-a-video", "text": "This command will extract the video frame at the 15s mark and saves it as a 800px wide JPEG image. You can also use the -s switch (like -s 400\u00d7300) to specify the exact dimensions of the image file though it will probably create a stretched image if the image size doesn\u2019t follow the aspect ratio of the original video file. ffmpeg -ss 00 :00:15 -i video.mp4 -vf scale = 800 :-1 -vframes 1 image.jpg", "title": "Extract image frames from a video"}, {"location": "ffmpeg/#extract-metadata-of-video", "text": "ffprobe {{ file }}", "title": "Extract metadata of video"}, {"location": "ffmpeg/#resize", "text": "", "title": "Resize"}, {"location": "ffmpeg/#resize-a-video", "text": "", "title": "Resize a video"}, {"location": "ffmpeg/#change-the-constat-rate-factor", "text": "Setting the Constant Rate Factor, which lowers the average bit rate, but retains better quality. Vary the CRF between around 18 and 24 \u2014 the lower, the higher the bitrate. ffmpeg -i input.mp4 -vcodec libx265 -crf 24 output.mp4 Change the codec as needed - libx264 may be available if libx265 is not, at the cost of a slightly larger resultant file size.", "title": "Change the Constat Rate Factor"}, {"location": "ffmpeg/#change-video-resolution", "text": "Use the size -s switch with ffmpeg to resize a video while maintaining the aspect ratio. ffmpeg -i input.mp4 -s 480x320 -c:a copy output.mp4", "title": "Change video resolution"}, {"location": "ffmpeg/#presentation", "text": "", "title": "Presentation"}, {"location": "ffmpeg/#create-video-slideshow-from-images", "text": "This command creates a video slideshow using a series of images that are named as img001.png , img002.png , etc. Each image will have a duration of 5 seconds (-r \u2155). ffmpeg -r 1 /5 -i img%03d.png -c:v libx264 -r 30 -pix_fmt yuv420p slideshow.mp4", "title": "Create video slideshow from images"}, {"location": "ffmpeg/#add-a-poster-image-to-audio", "text": "You can add a cover image to an audio file and the length of the output video will be the same as that of the input audio stream. This may come handy for uploading MP3s to YouTube. ffmpeg -loop 1 -i image.jpg -i audio.mp3 -c:v libx264 -c:a aac -strict experimental -b:a 192k -shortest output.mp4", "title": "Add a poster image to audio"}, {"location": "ffmpeg/#add-subtitles-to-a-movie", "text": "This will take the subtitles from the .srt file. FFmpeg can decode most common subtitle formats. ffmpeg -i movie.mp4 -i subtitles.srt -map 0 -map 1 -c copy -c:v libx264 -crf 23 -preset veryfast output.mkv", "title": "Add subtitles to a movie"}, {"location": "ffmpeg/#change-the-audio-volume", "text": "You can use the volume filter to alter the volume of a media file using FFmpeg. This command will half the volume of the audio file. ffmpeg -i input.wav -af 'volume=0.5' output.wav", "title": "Change the audio volume"}, {"location": "ffmpeg/#rotate-a-video", "text": "This command will rotate a video clip 90\u00b0 clockwise. You can set transpose to 2 to rotate the video 90\u00b0 anti-clockwise. ffmpeg -i input.mp4 -filter:v 'transpose=1' rotated-video.mp4 This will rotate the video 180\u00b0 counter-clockwise. ffmpeg -i input.mp4 -filter:v 'transpose=2,transpose=2' rotated-video.mp4", "title": "Rotate a video"}, {"location": "ffmpeg/#speed-up-or-slow-down-the-video", "text": "You can change the speed of your video using the setpts (set presentation time stamp) filter of FFmpeg. This command will make the video 8x (\u215b) faster or use setpts=4*PTS to make the video 4x slower. ffmpeg -i input.mp4 -filter:v \"setpts=0.125*PTS\" output.mp4", "title": "Speed up or Slow down the video"}, {"location": "ffmpeg/#speed-up-or-slow-down-the-audio", "text": "For changing the speed of audio, use the atempo audio filter. This command will double the speed of audio. You can use any value between 0.5 and 2.0 for audio. bash ffmpeg -i input.mkv -filter:a \"atempo=2.0\" -vn output.mkv Stack Exchange has a good overview to get you started with FFmpeg. You should also check out the official documentation at ffmpeg.org or the wiki at trac.ffmpeg.org to know about all the possible things you can do with FFmpeg.", "title": "Speed up or Slow down the audio"}, {"location": "ffmpeg/#references", "text": "Home", "title": "References"}, {"location": "finnix/", "text": "Finnix is a live Linux distribution specialized in the recovery, maintenance, testing of systems. Installation \u2691 Download the latest version from the web Load it into a usb: dd if = /path/to/your/finnix.iso of = /dev/path/to/your/disk !!! warning \"Be sure that the /dev/path/to/your/disk is the one you want to overwrite, you may end up fucking your device hard drive instead!\" References \u2691 Home", "title": "finnix"}, {"location": "finnix/#installation", "text": "Download the latest version from the web Load it into a usb: dd if = /path/to/your/finnix.iso of = /dev/path/to/your/disk !!! warning \"Be sure that the /dev/path/to/your/disk is the one you want to overwrite, you may end up fucking your device hard drive instead!\"", "title": "Installation"}, {"location": "finnix/#references", "text": "Home", "title": "References"}, {"location": "fitness_band/", "text": "Fitness tracker or activity trackers are devices or applications for monitoring and tracking fitness-related metrics such as distance walked or run, calorie consumption, and in some cases heartbeat. It is a type of wearable computer. As with anything that can be bought, I usually first try a cheap model to see if I need the advanced features that the expensive ones offer. After a quick model review, I went for the Amazfit band 5 . I've now discovered wasp-os an open source firmware for smart watches that are based on the nRF52 family of microcontrollers. Fully supported by gadgetbridge , Wasp-os features full heart rate monitoring and step counting support together with multiple clock faces, a stopwatch, an alarm clock, a countdown timer, a calculator and lots of other games and utilities. All of this, and still with access to the MicroPython REPL for interactive tweaking, development and testing. Currently it support the following devices: Colmi P8 Senbono K9 Pine64 PineTime Pinetime seems to be a work in progress, Colmi P8 looks awesome, and the Senbono K9 looks good too but wasp-os is lacking touch screen support. So if I had to choose now, I'd try the Colmi P8, the only thing that I'd miss is the possible voice assistant support. They say that you can take one for 18$ in aliexpress.", "title": "Fitness tracker"}, {"location": "flakeheaven/", "text": "Flakeheaven is a Flake8 wrapper to make it cool. Some of it's features are: Lint md, rst, ipynb, and more . Shareable and remote configs . Legacy-friendly : ability to get report only about new errors. Caching for much better performance. Use only specified plugins , not everything installed. Make output beautiful . pyproject.toml support. Check that all required plugins are installed . Syntax highlighting in messages and code snippets . PyLint integration. Remove unused noqa . Powerful GitLab support . Codes management: Manage codes per plugin. Enable and disable plugins and codes by wildcard. Show codes for installed plugins . Show all messages and codes for a plugin . Allow codes intersection for different plugins. You can use this cookiecutter template to create a python project with flakeheaven already configured. Installation \u2691 pip install flakeheaven Configuration \u2691 Flakeheaven can be configured in pyproject.toml . You can specify any Flake8 options and Flakeheaven-specific parameters. Plugins \u2691 In pyproject.toml you can specify [tool.flakeheaven.plugins] table. It's a list of flake8 plugins and associated to them rules. Key can be exact plugin name or wildcard template. For example \"flake8-commas\" or \"flake8-*\" . Flakeheaven will choose the longest match for every plugin if possible. In the previous example, flake8-commas will match to the first pattern, flake8-bandit and flake8-bugbear to the second, and pycodestyle will not match to any pattern. Value is a list of templates for error codes for this plugin. First symbol in every template must be + (include) or - (exclude). The latest matched pattern wins. For example, [\"+*\", \"-F*\", \"-E30?\", \"-E401\"] means \"Include everything except all checks that starts with F , check from E301 to E310 , and E401 \". Example: pyproject.toml [tool.flakeheaven] # specify any flake8 options. For example, exclude \"example.py\": exclude = [\"example.py\"] # make output nice format = \"grouped\" # don't limit yourself max_line_length = 120 # show line of source code in output show_source = true # list of plugins and rules for them [tool.flakeheaven.plugins] # include everything in pyflakes except F401 pyflakes = [\"+*\", \"-F401\"] # enable only codes from S100 to S199 flake8-bandit = [\"-*\", \"+S1??\"] # enable everything that starts from `flake8-` \"flake8-*\" = [\"+*\"] # explicitly disable plugin flake8-docstrings = [\"-*\"] # disable some checks for tests [tool.flakeheaven.exceptions.\"tests/\"] pycodestyle = [\"-F401\"] # disable a check pyflakes = [\"-*\"] # disable a plugin # do not disable `pyflakes` for one file in tests [tool.flakeheaven.exceptions.\"tests/test_example.py\"] pyflakes = [\"+*\"] # enable a plugin Check a complete list of flake8 extensions. flake8-bugbear : Finding likely bugs and design problems in your program. Contains warnings that don't belong in pyflakes and pycodestyle. flake8-fixme : Check for FIXME, TODO and other temporary developer notes. flake8-debugger : Check for pdb or idbp imports and set traces. flake8-mutable : Checks for mutable default arguments anti-pattern. flake8-pytest : Check for uses of Django-style assert-statements in tests. So no more self.assertEqual(a, b) , but instead assert a == b . flake8-pytest-style : Checks common style issues or inconsistencies with pytest-based tests. flake8-simplify : Helps you to simplify code. flake8-variables-names : Helps to make more readable variables names. pep8-naming : Check your code against PEP 8 naming conventions. flake8-expression-complexity : Check expression complexity. flake8-use-fstring : Checks you're using f-strings. flake8-docstrings : adds an extension for the fantastic pydocstyle tool to Flake8 . flake8-markdown : lints GitHub-style Python code blocks in Markdown files using flake8. pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. dlint : Encourage best coding practices and helping ensure Python code is secure. flake8-aaa : Checks Python tests follow the Arrange-Act-Assert pattern . flake8-annotations-complexity : Report on too complex type annotations. flake8-annotations : Detects the absence of PEP 3107-style function annotations and PEP 484-style type comments. flake8-typing-imports : Checks that typing imports are properly guarded. flake8-comprehensions : Help you write better list/set/dict comprehensions. flake8-eradicate : find commented out (or so called \"dead\") code. Usage \u2691 When using Flakeheaven, I frequently use the following commands: flakeheaven lint Runs the linter, similar to the flake8 command. flakeheaven plugins Lists all the plugins used, and their configuration status. flakeheaven missed Shows any plugins that are in the configuration but not installed properly. flakeheaven code S322 (or any other code) Shows the explanation for that specific warning code. flakeheaven yesqa Removes unused codes from # noqa and removes bare noqa that says \u201cignore everything on this line\u201d as is a bad practice. Integrations \u2691 Flakeheaven checks can be run in: In Vim though the ALE plugin . Through a pre-commit: - repo : https://github.com/flakeheaven/flakeheaven rev : master hooks : - name : Run flakeheaven static analysis tool id : flakeheaven In the CI: - name : Test linters run : make lint Assuming you're using a Makefile like the one in my cookiecutter-python-project . Issues \u2691 ImportError: cannot import name 'MergedConfigParser' from 'flake8.options.config' : remove the dependency pin in cookiecutter template and propagate to all projects. 'Namespace' object has no attribute 'extended_default_ignore' error : Until it's fixed either use a version below or equal to 3.9.0, or add to your pyproject.toml : [tool.flakeheaven] extended_default_ignore = [] # add this Once it's fixed, remove the patch from the maintained projects. Troubleshooting \u2691 ['Namespace' object has no attribute \u2691 'extended_default_ignore']( https://githubmemory.com/repo/flakeheaven/flakeheaven/issues/10 ) Add to your pyproject.toml : [tool.flakeheaven] extended_default_ignore = [] References \u2691 Git Docs Using Flake8 and pyproject.toml with Flakeheaven article by Jonathan Bowman", "title": "Flakeheaven"}, {"location": "flakeheaven/#installation", "text": "pip install flakeheaven", "title": "Installation"}, {"location": "flakeheaven/#configuration", "text": "Flakeheaven can be configured in pyproject.toml . You can specify any Flake8 options and Flakeheaven-specific parameters.", "title": "Configuration"}, {"location": "flakeheaven/#plugins", "text": "In pyproject.toml you can specify [tool.flakeheaven.plugins] table. It's a list of flake8 plugins and associated to them rules. Key can be exact plugin name or wildcard template. For example \"flake8-commas\" or \"flake8-*\" . Flakeheaven will choose the longest match for every plugin if possible. In the previous example, flake8-commas will match to the first pattern, flake8-bandit and flake8-bugbear to the second, and pycodestyle will not match to any pattern. Value is a list of templates for error codes for this plugin. First symbol in every template must be + (include) or - (exclude). The latest matched pattern wins. For example, [\"+*\", \"-F*\", \"-E30?\", \"-E401\"] means \"Include everything except all checks that starts with F , check from E301 to E310 , and E401 \". Example: pyproject.toml [tool.flakeheaven] # specify any flake8 options. For example, exclude \"example.py\": exclude = [\"example.py\"] # make output nice format = \"grouped\" # don't limit yourself max_line_length = 120 # show line of source code in output show_source = true # list of plugins and rules for them [tool.flakeheaven.plugins] # include everything in pyflakes except F401 pyflakes = [\"+*\", \"-F401\"] # enable only codes from S100 to S199 flake8-bandit = [\"-*\", \"+S1??\"] # enable everything that starts from `flake8-` \"flake8-*\" = [\"+*\"] # explicitly disable plugin flake8-docstrings = [\"-*\"] # disable some checks for tests [tool.flakeheaven.exceptions.\"tests/\"] pycodestyle = [\"-F401\"] # disable a check pyflakes = [\"-*\"] # disable a plugin # do not disable `pyflakes` for one file in tests [tool.flakeheaven.exceptions.\"tests/test_example.py\"] pyflakes = [\"+*\"] # enable a plugin Check a complete list of flake8 extensions. flake8-bugbear : Finding likely bugs and design problems in your program. Contains warnings that don't belong in pyflakes and pycodestyle. flake8-fixme : Check for FIXME, TODO and other temporary developer notes. flake8-debugger : Check for pdb or idbp imports and set traces. flake8-mutable : Checks for mutable default arguments anti-pattern. flake8-pytest : Check for uses of Django-style assert-statements in tests. So no more self.assertEqual(a, b) , but instead assert a == b . flake8-pytest-style : Checks common style issues or inconsistencies with pytest-based tests. flake8-simplify : Helps you to simplify code. flake8-variables-names : Helps to make more readable variables names. pep8-naming : Check your code against PEP 8 naming conventions. flake8-expression-complexity : Check expression complexity. flake8-use-fstring : Checks you're using f-strings. flake8-docstrings : adds an extension for the fantastic pydocstyle tool to Flake8 . flake8-markdown : lints GitHub-style Python code blocks in Markdown files using flake8. pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. dlint : Encourage best coding practices and helping ensure Python code is secure. flake8-aaa : Checks Python tests follow the Arrange-Act-Assert pattern . flake8-annotations-complexity : Report on too complex type annotations. flake8-annotations : Detects the absence of PEP 3107-style function annotations and PEP 484-style type comments. flake8-typing-imports : Checks that typing imports are properly guarded. flake8-comprehensions : Help you write better list/set/dict comprehensions. flake8-eradicate : find commented out (or so called \"dead\") code.", "title": "Plugins"}, {"location": "flakeheaven/#usage", "text": "When using Flakeheaven, I frequently use the following commands: flakeheaven lint Runs the linter, similar to the flake8 command. flakeheaven plugins Lists all the plugins used, and their configuration status. flakeheaven missed Shows any plugins that are in the configuration but not installed properly. flakeheaven code S322 (or any other code) Shows the explanation for that specific warning code. flakeheaven yesqa Removes unused codes from # noqa and removes bare noqa that says \u201cignore everything on this line\u201d as is a bad practice.", "title": "Usage"}, {"location": "flakeheaven/#integrations", "text": "Flakeheaven checks can be run in: In Vim though the ALE plugin . Through a pre-commit: - repo : https://github.com/flakeheaven/flakeheaven rev : master hooks : - name : Run flakeheaven static analysis tool id : flakeheaven In the CI: - name : Test linters run : make lint Assuming you're using a Makefile like the one in my cookiecutter-python-project .", "title": "Integrations"}, {"location": "flakeheaven/#issues", "text": "ImportError: cannot import name 'MergedConfigParser' from 'flake8.options.config' : remove the dependency pin in cookiecutter template and propagate to all projects. 'Namespace' object has no attribute 'extended_default_ignore' error : Until it's fixed either use a version below or equal to 3.9.0, or add to your pyproject.toml : [tool.flakeheaven] extended_default_ignore = [] # add this Once it's fixed, remove the patch from the maintained projects.", "title": "Issues"}, {"location": "flakeheaven/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "flakeheaven/#namespace-object-has-no-attribute", "text": "'extended_default_ignore']( https://githubmemory.com/repo/flakeheaven/flakeheaven/issues/10 ) Add to your pyproject.toml : [tool.flakeheaven] extended_default_ignore = []", "title": "['Namespace' object has no attribute"}, {"location": "flakeheaven/#references", "text": "Git Docs Using Flake8 and pyproject.toml with Flakeheaven article by Jonathan Bowman", "title": "References"}, {"location": "food_management/", "text": "As humans diet is an important factor in our health, we need to eat daily around three times a day, as such, each week we need to invest time into managing how to get food in front of us. Tasks like thinking what do you want to eat, buying the ingredients and cooking them make use a non negligible amount of time. Also something to keep in mind, is that eating is one of the great pleasures in our lives, so doing it poorly is a waste. The last part of the equation is that to eat good you either need time or money. This article explores my thoughts and findings on how to optimize the use of time, money and mental load in food management while keeping the desired level of quality to enjoy each meal, being healthy and following the principles of ecology and sustainability. I'm no expert at all on either of these topics. I'm learning and making my mind while writing these lines. Choosing your diet \u2691 The broad picture of diet \u2691 Seasonal, vegetarian, proximity, responsible The details of diet \u2691 How to choose what to eat this week Buying the products \u2691 Check grocy management for more details. Cooking \u2691 Week batch cooking, yearly batch cooking", "title": "Food management"}, {"location": "food_management/#choosing-your-diet", "text": "", "title": "Choosing your diet"}, {"location": "food_management/#the-broad-picture-of-diet", "text": "Seasonal, vegetarian, proximity, responsible", "title": "The broad picture of diet"}, {"location": "food_management/#the-details-of-diet", "text": "How to choose what to eat this week", "title": "The details of diet"}, {"location": "food_management/#buying-the-products", "text": "Check grocy management for more details.", "title": "Buying the products"}, {"location": "food_management/#cooking", "text": "Week batch cooking, yearly batch cooking", "title": "Cooking"}, {"location": "free_knowledge/", "text": "One of the early principles of the internet has been to make knowledge free to everyone. Alexandra Elbakyan of Sci-Hub , bookwarrior of Library Genesis , Aaron Swartz , and countless unnamed others have fought to free science from the grips of for-profit publishers. Today, they do it working in hiding, alone, without acknowledgment, in fear of imprisonment, and even now wiretapped by the FBI. They sacrifice everything for one vision: Open Science. Some days ago, a post appeared on reddit to rescue Sci-Hub by increasing the seeders of the 850 scihub torrents . The plan is to follow the steps done last year to move Libgen to IPFS to make it more difficult for the states to bring down this marvelous collection. A good way to start is to look at the most ill torrents and fix their state. If you follow this path, take care of IP leaking, they're surely monitoring who's sharing. Another way to contribute is by following the guidelines of freeread.org and contribute to the IPFS free library . Beware though, the guidelines don't explain how to install IPFS behind a VPN or Tor. This could be contributed to the site. Something that is needed is a command line tool that reads the list of ill torrents , and downloads the torrents that have a low number of seeders and DHT peers. The number of torrents to download could be limited by the amount the user wants to share. A second version could have an interaction with the torrent client so that when a torrent is no longer ill, it's automatically replaced with one that is. References \u2691 FreeRead.org Libgen reddit Sci-Hub reddit DataHoarder reddit", "title": "Free Knowledge"}, {"location": "free_knowledge/#references", "text": "FreeRead.org Libgen reddit Sci-Hub reddit DataHoarder reddit", "title": "References"}, {"location": "frontend_development/", "text": "I've recently started learning how to make web frontends, two years ago I learned a bit of HTML , CSS , Javascript and React , but it didn't stick that much. This time I'm full in with Vue which in my opinion is by far prettier than React. Newbie tips \u2691 I feel completely lost xD, I don't know even how to search well what I need, it's like going back to programming 101. Funnily though, it's bringing me closer to the people I mentor on Python , I'm getting frustrated with similar things that they do, those things that you don't see when you already are proficient in a language, so here are some tips. Don't resize your browser window \u2691 As I use i3wm, I've caught myself resizing the browser by adding terminals above and besides the browser to see how does the site react to different screen sizes. I needed the facepalm of a work colleague, which kindly suggested to spawn the Developer Tools and move that around. If you need to resize in the other direction, change the position of the Developer tools and grow it in that direction. A better feature yet is to use the Responsive Design mode, which lets you select screen sizes of existent devices, and it's easy to resize the screen. Your frontend probably doesn't talk to your backend \u2691 If you're using Vue or a similar framework, your frontend is just a webserver (like nginx) that has some html, js and css static files whose only purpose is to serve those static files to the user. It is the user's browser the one that does all the queries, even to the backend. Imagine that we have a frontend application that uses a backend API behind the scenes. In the front application you'll do the queries on /api and depending on the environment two different things will happen: In your development environment, when you run the development server to manually interact with it with the browser, you configure it so that whatever request you do to /api is redirected to the backend endpoint, which usually is listening on another port on localhost . If you are doing unit or integration tests, you'll probably use your test runner to intercept those calls and mock the result. If you are doing E2E tests, your test runner will probably understand your development configuration and forward the requests to the backend service. In production you'll have an SSL proxy, for example linuxserver's swag , that will forward /api to the backend and the rest to the frontend. UX design \u2691 The most popular tool out there is Figma but it's closed sourced, the alternative (quite popular in github) is penpot . Testing \u2691 Write testable code \u2691 Every test you write will include selectors for elements. To save yourself a lot of headaches, you should write selectors that are resilient to changes. Oftentimes we see users run into problems targeting their elements because: Your application may use dynamic classes or ID's that change. Your selectors break from development changes to CSS styles or JS behavior. Luckily, it is possible to avoid both of these problems. Don't target elements based on CSS attributes such as: id, class, tag. Don't target elements that may change their textContent . Add data-* attributes to make it easier to target elements. Given a button that we want to interact with: < button id = \"main\" class = \"btn btn-large\" name = \"submission\" role = \"button\" data-cy = \"submit\" > Submit </ button > Let's investigate how we could target it: | Selector | Recommended | Notes | | cy.get('button').click() | Never | Worst - too generic, no context. | | cy.get('.btn.btn-large').click() | Never | Bad. Coupled to styling. Highly subject to change. | | cy.get('#main').click() | Sparingly | Better. But still coupled to styling or JS event listeners. | | cy.get('[name=submission]').click() | Sparingly | Coupled to the name attribute which has HTML semantics. | | cy.contains('Submit').click() | Depends | Much better. But still coupled to text content that may change. | | cy.get('[data-cy=submit]').click() | Always | Best. Isolated from all changes. | Conditional testing \u2691 Conditional testing refers to the common programming pattern: If X, then Y, else Z Here are some examples: How do I do something different whether an element does or doesn't exist? My application does A/B testing, how do I account for that? My users receive a \"welcome wizard\", but existing ones don't. Can I always close the wizard in case it's shown, and ignore it when it's not? I want to automatically find all elements and based on which ones I find, I want to check that each link works. The problem is - while first appearing simple, writing tests in this fashion often leads to flaky tests, random failures, and difficult to track down edge cases. Some interesting cases and their solutions: Welcome wizard A/B Campaign", "title": "Frontend Development"}, {"location": "frontend_development/#newbie-tips", "text": "I feel completely lost xD, I don't know even how to search well what I need, it's like going back to programming 101. Funnily though, it's bringing me closer to the people I mentor on Python , I'm getting frustrated with similar things that they do, those things that you don't see when you already are proficient in a language, so here are some tips.", "title": "Newbie tips"}, {"location": "frontend_development/#dont-resize-your-browser-window", "text": "As I use i3wm, I've caught myself resizing the browser by adding terminals above and besides the browser to see how does the site react to different screen sizes. I needed the facepalm of a work colleague, which kindly suggested to spawn the Developer Tools and move that around. If you need to resize in the other direction, change the position of the Developer tools and grow it in that direction. A better feature yet is to use the Responsive Design mode, which lets you select screen sizes of existent devices, and it's easy to resize the screen.", "title": "Don't resize your browser window"}, {"location": "frontend_development/#your-frontend-probably-doesnt-talk-to-your-backend", "text": "If you're using Vue or a similar framework, your frontend is just a webserver (like nginx) that has some html, js and css static files whose only purpose is to serve those static files to the user. It is the user's browser the one that does all the queries, even to the backend. Imagine that we have a frontend application that uses a backend API behind the scenes. In the front application you'll do the queries on /api and depending on the environment two different things will happen: In your development environment, when you run the development server to manually interact with it with the browser, you configure it so that whatever request you do to /api is redirected to the backend endpoint, which usually is listening on another port on localhost . If you are doing unit or integration tests, you'll probably use your test runner to intercept those calls and mock the result. If you are doing E2E tests, your test runner will probably understand your development configuration and forward the requests to the backend service. In production you'll have an SSL proxy, for example linuxserver's swag , that will forward /api to the backend and the rest to the frontend.", "title": "Your frontend probably doesn't talk to your backend"}, {"location": "frontend_development/#ux-design", "text": "The most popular tool out there is Figma but it's closed sourced, the alternative (quite popular in github) is penpot .", "title": "UX design"}, {"location": "frontend_development/#testing", "text": "", "title": "Testing"}, {"location": "frontend_development/#write-testable-code", "text": "Every test you write will include selectors for elements. To save yourself a lot of headaches, you should write selectors that are resilient to changes. Oftentimes we see users run into problems targeting their elements because: Your application may use dynamic classes or ID's that change. Your selectors break from development changes to CSS styles or JS behavior. Luckily, it is possible to avoid both of these problems. Don't target elements based on CSS attributes such as: id, class, tag. Don't target elements that may change their textContent . Add data-* attributes to make it easier to target elements. Given a button that we want to interact with: < button id = \"main\" class = \"btn btn-large\" name = \"submission\" role = \"button\" data-cy = \"submit\" > Submit </ button > Let's investigate how we could target it: | Selector | Recommended | Notes | | cy.get('button').click() | Never | Worst - too generic, no context. | | cy.get('.btn.btn-large').click() | Never | Bad. Coupled to styling. Highly subject to change. | | cy.get('#main').click() | Sparingly | Better. But still coupled to styling or JS event listeners. | | cy.get('[name=submission]').click() | Sparingly | Coupled to the name attribute which has HTML semantics. | | cy.contains('Submit').click() | Depends | Much better. But still coupled to text content that may change. | | cy.get('[data-cy=submit]').click() | Always | Best. Isolated from all changes. |", "title": "Write testable code"}, {"location": "frontend_development/#conditional-testing", "text": "Conditional testing refers to the common programming pattern: If X, then Y, else Z Here are some examples: How do I do something different whether an element does or doesn't exist? My application does A/B testing, how do I account for that? My users receive a \"welcome wizard\", but existing ones don't. Can I always close the wizard in case it's shown, and ignore it when it's not? I want to automatically find all elements and based on which ones I find, I want to check that each link works. The problem is - while first appearing simple, writing tests in this fashion often leads to flaky tests, random failures, and difficult to track down edge cases. Some interesting cases and their solutions: Welcome wizard A/B Campaign", "title": "Conditional testing"}, {"location": "frontend_learning/", "text": "This section is the particularization of the Development learning article for a frontend developer, in particular a Vue developer. What is a Frontend developer? \u2691 A Front-End Developer is someone who creates websites and web applications. It's main responsibility is to create what the user sees. The basic languages for Front-End Development are HTML , CSS , and JavaScript . Nowadays writing interfaces with only the basic languages makes no sense as there are other languages and frameworks that make better and quicker solutions. One of them is Vue , which is the one I learnt, so the whole document will be focused on this path, nevertheless there are others popular ones like: Bootstrap , React , jQuery or Angular . The difference between Front-End and Back-End is that Front-End refers to how a web page looks, while back-end refers to how it works. Roadmap \u2691 First steps \u2691 Setup your development environment \u2691 Learn the basics \u2691 In order to write Vue code you first need to understand the foundations it's built upon, that means learning the basics of: HTML : For example following the W3 tutorial , at least until the HTML Forms section. CSS : For example following the W3 tutorial until CSS Advanced. Javascript : For example using the W3 tutorial until JS Versions. If you follow other learning methods , make sure that they cover more less the same concepts.", "title": "Frontend developer"}, {"location": "frontend_learning/#what-is-a-frontend-developer", "text": "A Front-End Developer is someone who creates websites and web applications. It's main responsibility is to create what the user sees. The basic languages for Front-End Development are HTML , CSS , and JavaScript . Nowadays writing interfaces with only the basic languages makes no sense as there are other languages and frameworks that make better and quicker solutions. One of them is Vue , which is the one I learnt, so the whole document will be focused on this path, nevertheless there are others popular ones like: Bootstrap , React , jQuery or Angular . The difference between Front-End and Back-End is that Front-End refers to how a web page looks, while back-end refers to how it works.", "title": "What is a Frontend developer?"}, {"location": "frontend_learning/#roadmap", "text": "", "title": "Roadmap"}, {"location": "frontend_learning/#first-steps", "text": "", "title": "First steps"}, {"location": "frontend_learning/#setup-your-development-environment", "text": "", "title": "Setup your development environment"}, {"location": "frontend_learning/#learn-the-basics", "text": "In order to write Vue code you first need to understand the foundations it's built upon, that means learning the basics of: HTML : For example following the W3 tutorial , at least until the HTML Forms section. CSS : For example following the W3 tutorial until CSS Advanced. Javascript : For example using the W3 tutorial until JS Versions. If you follow other learning methods , make sure that they cover more less the same concepts.", "title": "Learn the basics"}, {"location": "fun/", "text": "Coding \u2691 (source)", "title": "Fun Stuff"}, {"location": "fun/#coding", "text": "(source)", "title": "Coding"}, {"location": "gadgetbridge/", "text": "Gadgetbridge is an Android (4.4+) application which will allow you to use your Pebble, Mi Band, Amazfit Bip and HPlus device (and more) without the vendor's closed source application and without the need to create an account and transmit any of your data to the vendor's servers. It wont be ever be as good as the proprietary application, but it supports a good range of features , and supports a huge range of bands . Data extraction \u2691 You can use the Data export or Data Auto export to get copy of your data. Here is an example of a Python program that post processes the data. Also is this post explaining how to reverse engineer the miband2 with this or this scripts. If you start the path of reverse engineering the Bluetooth protocol look at gadgetbridge guidelines . If you start to think on how to avoid the connection with an android phone and directly extract or interact from a linux device through python, I'd go with pybluez for the bluetooth interface, understand the band code of Gadgetbridge porting the logic to the python module, and reverse engineering the call you want to process. There isn't much in the internet following this approach, I've found an implementation for the Mi Band 4 though , which can be a good start. Heartrate measurement \u2691 Follow the official instructions . Sleep \u2691 It looks that they don't yet support smart alarms . Weather \u2691 Follow the official instructions Events \u2691 I haven't figured out yet how to let the events show in the \"events\" tab. Mobile calendar events show up as notifications, but you can't see the list of the next ones. For the Amazfit band 5, there is a bug that prevents events from showing in the reminders tab . Notifications work well though. Setup \u2691 In the case of the Amazfit band 5 , we need to use the Huami server pairing : Install the Zepp application Create an account through the application. Pair your band and wait for the firmware update Use the python script to extract the credentials git clone https://github.com/argrento/huami-token.git pip install -r requirements.txt Run script with your credentials: python huami_token.py --method amazfit --email youemail@example.com --password your_password --bt_keys Do not unpair the band/watch from MiFit/Amazfit/Zepp app Kill or uninstall the MiFit/Amazfit/Zepp app Ensure GPS/location services are enabled The official instructions tell you to unpair the band/watch from your phone's bluetooth but I didn't have to do it. Add the band in gadgetbridge. Under Auth key add your key. References \u2691 Git Home Issue tracker Blog , although the RSS is not working .", "title": "GadgetBridge"}, {"location": "gadgetbridge/#data-extraction", "text": "You can use the Data export or Data Auto export to get copy of your data. Here is an example of a Python program that post processes the data. Also is this post explaining how to reverse engineer the miband2 with this or this scripts. If you start the path of reverse engineering the Bluetooth protocol look at gadgetbridge guidelines . If you start to think on how to avoid the connection with an android phone and directly extract or interact from a linux device through python, I'd go with pybluez for the bluetooth interface, understand the band code of Gadgetbridge porting the logic to the python module, and reverse engineering the call you want to process. There isn't much in the internet following this approach, I've found an implementation for the Mi Band 4 though , which can be a good start.", "title": "Data extraction"}, {"location": "gadgetbridge/#heartrate-measurement", "text": "Follow the official instructions .", "title": "Heartrate measurement"}, {"location": "gadgetbridge/#sleep", "text": "It looks that they don't yet support smart alarms .", "title": "Sleep"}, {"location": "gadgetbridge/#weather", "text": "Follow the official instructions", "title": "Weather"}, {"location": "gadgetbridge/#events", "text": "I haven't figured out yet how to let the events show in the \"events\" tab. Mobile calendar events show up as notifications, but you can't see the list of the next ones. For the Amazfit band 5, there is a bug that prevents events from showing in the reminders tab . Notifications work well though.", "title": "Events"}, {"location": "gadgetbridge/#setup", "text": "In the case of the Amazfit band 5 , we need to use the Huami server pairing : Install the Zepp application Create an account through the application. Pair your band and wait for the firmware update Use the python script to extract the credentials git clone https://github.com/argrento/huami-token.git pip install -r requirements.txt Run script with your credentials: python huami_token.py --method amazfit --email youemail@example.com --password your_password --bt_keys Do not unpair the band/watch from MiFit/Amazfit/Zepp app Kill or uninstall the MiFit/Amazfit/Zepp app Ensure GPS/location services are enabled The official instructions tell you to unpair the band/watch from your phone's bluetooth but I didn't have to do it. Add the band in gadgetbridge. Under Auth key add your key.", "title": "Setup"}, {"location": "gadgetbridge/#references", "text": "Git Home Issue tracker Blog , although the RSS is not working .", "title": "References"}, {"location": "gajim/", "text": "Gajim is the best Linux XMPP client in terms of end-to-end encryption support as it's able to speak OMEMO. Installation \u2691 sudo apt-get install gajim gajim-omemo Once you open it, you need to enable the plugin in the main program dropdown. The only problem I've encountered so far is that OMEMO is not enabled by default , they made a PR but closed it because it encountered some errors that was not able to solve . It's a crucial feature, so if you have some spare time and know a bit of Python please try to fix it! Developing \u2691 I've found the Developing section in the wiki to get started. Issues \u2691 Enable encryption by default : Nothing to do. References \u2691 Homepage", "title": "Gajim"}, {"location": "gajim/#installation", "text": "sudo apt-get install gajim gajim-omemo Once you open it, you need to enable the plugin in the main program dropdown. The only problem I've encountered so far is that OMEMO is not enabled by default , they made a PR but closed it because it encountered some errors that was not able to solve . It's a crucial feature, so if you have some spare time and know a bit of Python please try to fix it!", "title": "Installation"}, {"location": "gajim/#developing", "text": "I've found the Developing section in the wiki to get started.", "title": "Developing"}, {"location": "gajim/#issues", "text": "Enable encryption by default : Nothing to do.", "title": "Issues"}, {"location": "gajim/#references", "text": "Homepage", "title": "References"}, {"location": "gettext/", "text": "Gettext is the defacto universal solution for internationalization (I18N) and localization (L10N), offering a set of tools that provides a framework to help other packages produce multi-lingual messages. It gives an opinionated way of how programs should be written to support translated message strings and a directory and file naming organisation for the messages that need to be translated. In regards to directory conventions, we need to have a place to put our localised translations based on the specified locale language. For example, let\u2019s say we need to support 2 languages English and Greek. Their language codes are en and el respectively. We can create a directory named locales and inside we need to create directories for each language code and each folder will contain another directory named each LC_MESSAGES with one or multiple .po files. So, the file structure should look like this: locales/ \u251c\u2500\u2500 el \u2502 \u2514\u2500\u2500 LC_MESSAGES \u2502 \u2514\u2500\u2500 base.po \u2514\u2500\u2500 en \u2514\u2500\u2500 LC_MESSAGES \u2514\u2500\u2500 base.po A PO file contains a number of messages, partly independent text segments to be translated, which have been grouped into one file according to some logical division of what is being translated. Those groups are called domains. In the example above, we have only one domain named as base . The PO files themselves are also called message catalogs. The PO format is a plain text format. Apart from PO files, you might sometimes encounter .mo files. MO, or Machine Object is a binary data file that contains object data referenced by a program. It is typically used to translate program code, and can be loaded or imported into the GNU gettext program. In addition, there are also .pot files. These are the template files for PO files. They will have all the translation strings left empty. A POT file is essentially an empty PO file without the translations, with just the original strings. In practice we have the .pot files be generated from some tools and we should not modify them directly. Usage \u2691 The gettext module comes shipped with Python. It exposes two APIs. The first one is the basic API that supports the GNU gettext catalog API. The second one is the higher level one, class-based API that may be more appropriate for Python files. The class bases API offers more flexibility and greater convenience than the GNU gettext API and it is the recommended way of localizing your Python applications and modules. In order to provide multilingual messages for your Python programs, you need to take the next steps: Mark all translatable strings in your program with a wrapper function. Run a suite of tools over your marked files to generate raw messages catalogs or POT files. Duplicate the POT files into specific locale folders and write the translations. Import and use the gettext module so that message strings are properly translated. Let\u2019s start with a function that prints some strings. # main.py def print_some_strings (): print ( \"Hello world\" ) print ( \"This is a translatable string\" ) if __name__ == '__main__' : print_some_strings () Now as it is you cannot provide localization options using gettext . The first step is to specially mark all translatable strings in the program. To do that we need to wrap all the translatable strings inside _() . # main.py import gettext _ = gettext . gettext def print_some_strings (): print ( _ ( \"Hello world\" )) print ( _ ( \"This is a translatable string\" )) if __name__ == '__main__' : print_some_strings () Notice that we imported gettext and assigned _ as gettext.gettext . This is to ensure that our program compiles as well. If you run the program, you will see that nothing has changed: $: python main.py Hello world This is a translatable string However, now we are able to proceed to the next steps which are extracting the translatable messages in a POT file. Create the POT files \u2691 For the purpose of automating the process of generating raw translatable messages from wrapped strings throughout the applications, the gettext library authors have provided a set to tools that help to parse the source files and to extract the messages in a general message catalog. The Python distribution includes some specific programs called pygettext.py and msgfmt.py that recognize only python source code and not other languages. Call it specifying the file you want to parse the strings for: $: pygettext -d base -o locales/base.pot src/main.py If you want to search for other strings than _ , use the -k flag, for example -k gettext . That will generate a base.pot file in the locales directory taken from our main.py program. Remember that POT files are just templates and we should not touch them. Let us inspect the contents of the base.pot file: # SOME DESCRIPTIVE TITLE. # Copyright (C) YEAR ORGANIZATION # FIRST AUTHOR <EMAIL@ADDRESS>, YEAR. # msgid \"\" msgstr \"\" \"Project-Id-Version: PACKAGE VERSION\\n\" \"POT-Creation-Date: 2018-01-28 16:47+0000\\n\" \"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\\n\" \"Last-Translator: FULL NAME <EMAIL@ADDRESS>\\n\" \"Language-Team: LANGUAGE <LL@li.org>\\n\" \"MIME-Version: 1.0\\n\" \"Content-Type: text/plain; charset=UTF-8\\n\" \"Content-Transfer-Encoding: 8bit\\n\" \"Generated-By: pygettext.py 1.5\\n\" #: src/main.py:5 msgid \"Hello world\" msgstr \"\" #: src/main.py:6 msgid \"This is a translatable string\" msgstr \"\" In a bigger program, we would have many translatable strings following. Here we specified a domain called base because the application is only one file. In bigger ones, I would use multiple domains in order to logically separate the different messages based on the application scope. Notice that we have a simple convention for our translatable strings. msgid is the original string wrapped in _() . msgstr is the translation we need to provide. Create the PO files \u2691 Now we are ready to create our translations. Because we have the template generated for us, the next step is to create the required directory structure and copy the template into the right spot. We\u2019ve seen the recommended file structure before. We are going to create 2 additional directories inside locales with the structure locales/$language/LC_MESSAGES/$domain.po Where: $language is the language identifier such as en or el $domain is base . Copy and rename the base.pot into the following directories locales/en/LC_MESSAGES/base.po and locales/el/LC_MESSAGES/base.po . Then modify their headers to include more information about the locale. For example, this is the Greek translation. # My App. # Copyright (C) 2018 # msgid \"\" msgstr \"\" \"Project-Id-Version: 1.0\\n\" \"POT-Creation-Date: 2018-01-28 16:47+0000\\n\" \"PO-Revision-Date: 2018-01-28 16:48+0000\\n\" \"Last-Translator: me <johndoe@example.com>\\n\" \"Language-Team: Greek <yourteam@example.com>\\n\" \"MIME-Version: 1.0\\n\" \"Content-Type: text/plain; charset=UTF-8\\n\" \"Content-Transfer-Encoding: 8bit\\n\" \"Generated-By: pygettext.py 1.5\\n\" #: main.py:5 msgid \"Hello world\" msgstr \"\u03a7\u03ad\u03c1\u03b5 \u039a\u03cc\u03c3\u03bc\u03b5\" #: main.py:6 msgid \"This is a translatable string\" msgstr \"\u0391\u03c5\u03c4\u03cc \u03b5\u03af\u03bd\u03b1\u03b9 \u03ad\u03bd\u03b1 \u03bc\u03b5\u03c4\u03b1\u03c6\u03c1\u03b1\u03b6\u03cc\u03bc\u03b5\u03bd\u03bf \u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03bf\" Updating POT and PO files \u2691 Once you add more strings or change some strings in your program, you execute again pygettext which regenerates the template file: pygettext main.py -o po/hello.pot Then you can update individual translation files to match newly created templates (this includes reordering the strings to match new template) with msgmerge : msgmerge --previous --update po/cs.po po/hello.pot Create the MO files \u2691 The catalog is built from the .po file using a tool called msgformat.py . This tool will parse the .po file and generate an equivalent .mo file. $: msgfmt -o base.mo base This command will generate a base.mo file in the same folder as the base.po file. So, the final file structure should look like this: locales \u251c\u2500\u2500 el \u2502 \u2514\u2500\u2500 LC_MESSAGES \u2502 \u251c\u2500\u2500 base.mo \u2502 \u2514\u2500\u2500 base.po \u251c\u2500\u2500 en \u2502 \u2514\u2500\u2500 LC_MESSAGES \u2502 \u251c\u2500\u2500 base.mo \u2502 \u2514\u2500\u2500 base.po \u2514\u2500\u2500 base.pot Switching Locale \u2691 To have the ability to switch locales in our program we need to actually use the Class based gettext API. One of it's methods is gettext.translation , it accepts some parameters that can be used to load the associated .mo files of a particular language. If no .mo file is found, it raises an error. Add the following code to the program: import gettext el = gettext . translation ( 'base' , localedir = 'locales' , languages = [ 'el' ]) el . install () _ = el . gettext # Greek The first argument base is the domain and the method will look for a .po file with the same name in our locale directory. If you don\u2019t specify a domain it will fallback to the messages domain. The localedir parameter is the directory location of the locales directory you created. The languages parameter is a hint for the searching mechanism to load particular language code more resiliently. If you run the program again you will see the translations happening: $ python main.py \u03a7\u03b1\u03af\u03c1\u03b5 \u039a\u03cc\u03c3\u03bc\u03b5 \u0391\u03c5\u03c4\u03cc \u03b5\u03af\u03bd\u03b1\u03b9 \u03ad\u03bd\u03b1 \u03bc\u03b5\u03c4\u03b1\u03c6\u03c1\u03b1\u03b6\u03cc\u03bc\u03b5\u03bd\u03bf \u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03bf The install method will cause all the _() calls to return the Greek translated strings globally into the built-in namespace. This is because we assigned _ to point to the Greek dictionary of translations. To go back to the English just assign _ to be the original gettext object. _ = gettext . gettext Finding Message Catalogs \u2691 When there are cases where you need to locate all translation files at runtime, you can use the find function as provided by the class-based API. This function takes a few parameters in order to retrieve from the disk a list of .mo files available. You can pass a localedir , a domain and a list of languages . If you don\u2019t, the library module will use the respective defaults, which is not what you intended to do in most cases. For example, if you don\u2019t specify a localdir parameter, it will fallback to sys.prefix + \u2018/share/locale\u2019 which is a global locale dir that can contain a lot of random files. The language portion of the path is taken from one of several environment variables that can be used to configure localization features (LANGUAGE, LC_ALL, LC_MESSAGES, and LANG). The first variable found to be set is used. Multiple languages can be selected by separating the values with a colon :. >>> os . environ [ 'LANGUAGE' ] = 'el:en' >>> gettext . find ( 'base' , 'locales' ) 'locales/el/LC_MESSAGES/base.mo' >>> gettext . find ( 'base' , 'locales' , all = True ) [ 'locales/el/LC_MESSAGES/base.mo' , 'locales/en/LC_MESSAGES/base.mo' ] Using f-strings \u2691 You can't use f-strings inside gettext , you'll get an Seen unexpected token \"f\" error, you need to use the old format method: _ ( 'Hey {} ,' ) . format ( username ) Integrations \u2691 You can use it with weblate . References \u2691 Homepage Reference Phrase blog on Localizing with GNU gettext", "title": "Gettext"}, {"location": "gettext/#usage", "text": "The gettext module comes shipped with Python. It exposes two APIs. The first one is the basic API that supports the GNU gettext catalog API. The second one is the higher level one, class-based API that may be more appropriate for Python files. The class bases API offers more flexibility and greater convenience than the GNU gettext API and it is the recommended way of localizing your Python applications and modules. In order to provide multilingual messages for your Python programs, you need to take the next steps: Mark all translatable strings in your program with a wrapper function. Run a suite of tools over your marked files to generate raw messages catalogs or POT files. Duplicate the POT files into specific locale folders and write the translations. Import and use the gettext module so that message strings are properly translated. Let\u2019s start with a function that prints some strings. # main.py def print_some_strings (): print ( \"Hello world\" ) print ( \"This is a translatable string\" ) if __name__ == '__main__' : print_some_strings () Now as it is you cannot provide localization options using gettext . The first step is to specially mark all translatable strings in the program. To do that we need to wrap all the translatable strings inside _() . # main.py import gettext _ = gettext . gettext def print_some_strings (): print ( _ ( \"Hello world\" )) print ( _ ( \"This is a translatable string\" )) if __name__ == '__main__' : print_some_strings () Notice that we imported gettext and assigned _ as gettext.gettext . This is to ensure that our program compiles as well. If you run the program, you will see that nothing has changed: $: python main.py Hello world This is a translatable string However, now we are able to proceed to the next steps which are extracting the translatable messages in a POT file.", "title": "Usage"}, {"location": "gettext/#create-the-pot-files", "text": "For the purpose of automating the process of generating raw translatable messages from wrapped strings throughout the applications, the gettext library authors have provided a set to tools that help to parse the source files and to extract the messages in a general message catalog. The Python distribution includes some specific programs called pygettext.py and msgfmt.py that recognize only python source code and not other languages. Call it specifying the file you want to parse the strings for: $: pygettext -d base -o locales/base.pot src/main.py If you want to search for other strings than _ , use the -k flag, for example -k gettext . That will generate a base.pot file in the locales directory taken from our main.py program. Remember that POT files are just templates and we should not touch them. Let us inspect the contents of the base.pot file: # SOME DESCRIPTIVE TITLE. # Copyright (C) YEAR ORGANIZATION # FIRST AUTHOR <EMAIL@ADDRESS>, YEAR. # msgid \"\" msgstr \"\" \"Project-Id-Version: PACKAGE VERSION\\n\" \"POT-Creation-Date: 2018-01-28 16:47+0000\\n\" \"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\\n\" \"Last-Translator: FULL NAME <EMAIL@ADDRESS>\\n\" \"Language-Team: LANGUAGE <LL@li.org>\\n\" \"MIME-Version: 1.0\\n\" \"Content-Type: text/plain; charset=UTF-8\\n\" \"Content-Transfer-Encoding: 8bit\\n\" \"Generated-By: pygettext.py 1.5\\n\" #: src/main.py:5 msgid \"Hello world\" msgstr \"\" #: src/main.py:6 msgid \"This is a translatable string\" msgstr \"\" In a bigger program, we would have many translatable strings following. Here we specified a domain called base because the application is only one file. In bigger ones, I would use multiple domains in order to logically separate the different messages based on the application scope. Notice that we have a simple convention for our translatable strings. msgid is the original string wrapped in _() . msgstr is the translation we need to provide.", "title": "Create the POT files"}, {"location": "gettext/#create-the-po-files", "text": "Now we are ready to create our translations. Because we have the template generated for us, the next step is to create the required directory structure and copy the template into the right spot. We\u2019ve seen the recommended file structure before. We are going to create 2 additional directories inside locales with the structure locales/$language/LC_MESSAGES/$domain.po Where: $language is the language identifier such as en or el $domain is base . Copy and rename the base.pot into the following directories locales/en/LC_MESSAGES/base.po and locales/el/LC_MESSAGES/base.po . Then modify their headers to include more information about the locale. For example, this is the Greek translation. # My App. # Copyright (C) 2018 # msgid \"\" msgstr \"\" \"Project-Id-Version: 1.0\\n\" \"POT-Creation-Date: 2018-01-28 16:47+0000\\n\" \"PO-Revision-Date: 2018-01-28 16:48+0000\\n\" \"Last-Translator: me <johndoe@example.com>\\n\" \"Language-Team: Greek <yourteam@example.com>\\n\" \"MIME-Version: 1.0\\n\" \"Content-Type: text/plain; charset=UTF-8\\n\" \"Content-Transfer-Encoding: 8bit\\n\" \"Generated-By: pygettext.py 1.5\\n\" #: main.py:5 msgid \"Hello world\" msgstr \"\u03a7\u03ad\u03c1\u03b5 \u039a\u03cc\u03c3\u03bc\u03b5\" #: main.py:6 msgid \"This is a translatable string\" msgstr \"\u0391\u03c5\u03c4\u03cc \u03b5\u03af\u03bd\u03b1\u03b9 \u03ad\u03bd\u03b1 \u03bc\u03b5\u03c4\u03b1\u03c6\u03c1\u03b1\u03b6\u03cc\u03bc\u03b5\u03bd\u03bf \u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03bf\"", "title": "Create the PO files"}, {"location": "gettext/#updating-pot-and-po-files", "text": "Once you add more strings or change some strings in your program, you execute again pygettext which regenerates the template file: pygettext main.py -o po/hello.pot Then you can update individual translation files to match newly created templates (this includes reordering the strings to match new template) with msgmerge : msgmerge --previous --update po/cs.po po/hello.pot", "title": "Updating POT and PO files"}, {"location": "gettext/#create-the-mo-files", "text": "The catalog is built from the .po file using a tool called msgformat.py . This tool will parse the .po file and generate an equivalent .mo file. $: msgfmt -o base.mo base This command will generate a base.mo file in the same folder as the base.po file. So, the final file structure should look like this: locales \u251c\u2500\u2500 el \u2502 \u2514\u2500\u2500 LC_MESSAGES \u2502 \u251c\u2500\u2500 base.mo \u2502 \u2514\u2500\u2500 base.po \u251c\u2500\u2500 en \u2502 \u2514\u2500\u2500 LC_MESSAGES \u2502 \u251c\u2500\u2500 base.mo \u2502 \u2514\u2500\u2500 base.po \u2514\u2500\u2500 base.pot", "title": "Create the MO files"}, {"location": "gettext/#switching-locale", "text": "To have the ability to switch locales in our program we need to actually use the Class based gettext API. One of it's methods is gettext.translation , it accepts some parameters that can be used to load the associated .mo files of a particular language. If no .mo file is found, it raises an error. Add the following code to the program: import gettext el = gettext . translation ( 'base' , localedir = 'locales' , languages = [ 'el' ]) el . install () _ = el . gettext # Greek The first argument base is the domain and the method will look for a .po file with the same name in our locale directory. If you don\u2019t specify a domain it will fallback to the messages domain. The localedir parameter is the directory location of the locales directory you created. The languages parameter is a hint for the searching mechanism to load particular language code more resiliently. If you run the program again you will see the translations happening: $ python main.py \u03a7\u03b1\u03af\u03c1\u03b5 \u039a\u03cc\u03c3\u03bc\u03b5 \u0391\u03c5\u03c4\u03cc \u03b5\u03af\u03bd\u03b1\u03b9 \u03ad\u03bd\u03b1 \u03bc\u03b5\u03c4\u03b1\u03c6\u03c1\u03b1\u03b6\u03cc\u03bc\u03b5\u03bd\u03bf \u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03bf The install method will cause all the _() calls to return the Greek translated strings globally into the built-in namespace. This is because we assigned _ to point to the Greek dictionary of translations. To go back to the English just assign _ to be the original gettext object. _ = gettext . gettext", "title": "Switching Locale"}, {"location": "gettext/#finding-message-catalogs", "text": "When there are cases where you need to locate all translation files at runtime, you can use the find function as provided by the class-based API. This function takes a few parameters in order to retrieve from the disk a list of .mo files available. You can pass a localedir , a domain and a list of languages . If you don\u2019t, the library module will use the respective defaults, which is not what you intended to do in most cases. For example, if you don\u2019t specify a localdir parameter, it will fallback to sys.prefix + \u2018/share/locale\u2019 which is a global locale dir that can contain a lot of random files. The language portion of the path is taken from one of several environment variables that can be used to configure localization features (LANGUAGE, LC_ALL, LC_MESSAGES, and LANG). The first variable found to be set is used. Multiple languages can be selected by separating the values with a colon :. >>> os . environ [ 'LANGUAGE' ] = 'el:en' >>> gettext . find ( 'base' , 'locales' ) 'locales/el/LC_MESSAGES/base.mo' >>> gettext . find ( 'base' , 'locales' , all = True ) [ 'locales/el/LC_MESSAGES/base.mo' , 'locales/en/LC_MESSAGES/base.mo' ]", "title": "Finding Message Catalogs"}, {"location": "gettext/#using-f-strings", "text": "You can't use f-strings inside gettext , you'll get an Seen unexpected token \"f\" error, you need to use the old format method: _ ( 'Hey {} ,' ) . format ( username )", "title": "Using f-strings"}, {"location": "gettext/#integrations", "text": "You can use it with weblate .", "title": "Integrations"}, {"location": "gettext/#references", "text": "Homepage Reference Phrase blog on Localizing with GNU gettext", "title": "References"}, {"location": "gh/", "text": "gh is GitHub\u2019s official command line tool. Installation \u2691 Get the deb file from the releases page and install it with sudo dpkg -i Authenticate the command following the steps of: gh auth login Usage \u2691 Create a pull request \u2691 Whenever you do a git push it will show you the link to create the pull request. To avoid going into the browser, you can use gh pr create . You can also merge a ready pull request with gh pr merge Workflow runs \u2691 With gh run list you can get the list of the last workflow runs. If you want to see the logs of one of the runs, get the id and run gh run view {{ run_id }} . To see what failed, run gh run view {{ run_id }} --log-failed . Pull request checks \u2691 gh allows you to check the status of the checks of a pull requests, this is useful to get an alert once the checks are done. For example you can use the next bash/zsh function: function checks (){ while true ; do gh pr checks if [[ -z \" $( gh pr status --json statusCheckRollup | grep IN_PROGRESS ) \" ]] ; then break fi sleep 1 echo done gh pr checks echo -e '\\a' } Trigger a workflow run \u2691 To manually trigger a workflow you need to first configure it to allow workflow_dispatch events . on : workflow_dispatch : Then you can trigger the workflow with gh workflow run {{ workflow_name }} , where you can get the workflow_name with gh workflow list References \u2691 Git", "title": "Github cli"}, {"location": "gh/#installation", "text": "Get the deb file from the releases page and install it with sudo dpkg -i Authenticate the command following the steps of: gh auth login", "title": "Installation"}, {"location": "gh/#usage", "text": "", "title": "Usage"}, {"location": "gh/#create-a-pull-request", "text": "Whenever you do a git push it will show you the link to create the pull request. To avoid going into the browser, you can use gh pr create . You can also merge a ready pull request with gh pr merge", "title": "Create a pull request"}, {"location": "gh/#workflow-runs", "text": "With gh run list you can get the list of the last workflow runs. If you want to see the logs of one of the runs, get the id and run gh run view {{ run_id }} . To see what failed, run gh run view {{ run_id }} --log-failed .", "title": "Workflow runs"}, {"location": "gh/#pull-request-checks", "text": "gh allows you to check the status of the checks of a pull requests, this is useful to get an alert once the checks are done. For example you can use the next bash/zsh function: function checks (){ while true ; do gh pr checks if [[ -z \" $( gh pr status --json statusCheckRollup | grep IN_PROGRESS ) \" ]] ; then break fi sleep 1 echo done gh pr checks echo -e '\\a' }", "title": "Pull request checks"}, {"location": "gh/#trigger-a-workflow-run", "text": "To manually trigger a workflow you need to first configure it to allow workflow_dispatch events . on : workflow_dispatch : Then you can trigger the workflow with gh workflow run {{ workflow_name }} , where you can get the workflow_name with gh workflow list", "title": "Trigger a workflow run"}, {"location": "gh/#references", "text": "Git", "title": "References"}, {"location": "git/", "text": "Git is a software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development. Its goals include speed, data integrity, and support for distributed, non-linear workflows (thousands of parallel branches running on different systems). Learning git \u2691 Git is a tough nut to crack, no matter how experience you are you'll frequently get surprised. Sadly it's one of the main tools to develop your code, so you must master it as soon as possible. Depending on how you like to learn I've found these options: Written courses: W3 git course Interactive tutorials: Learngitbranching interactive tutorial Written article: Freecode camp article Video courses: Code academy and Udemy Pull Request Process \u2691 This part of the doc is shamefully edited from the source. It was for the k8s project but they are good practices that work for all the projects. It explains the process and best practices for submitting a PR It should serve as a reference for all contributors, and be useful especially to new and infrequent submitters. Before You Submit a PR \u2691 This guide is for contributors who already have a PR to submit. If you're looking for information on setting up your developer environment and creating code to contribute to the project, search the development guide. Make sure your PR adheres to the projects best practices. These include following project conventions, making small PRs, and commenting thoroughly. Run Local Verifications \u2691 You can run the tests in local before you submit your PR to predict the pass or fail of continuous integration. Why is my PR not getting reviewed? \u2691 A few factors affect how long your PR might wait for review. If it's the last few weeks of a milestone, we need to reduce churn and stabilize. Or, it could be related to best practices. One common issue is that the PR is too big to review. Let's say you've touched 39 files and have 8657 insertions. When your would-be reviewers pull up the diffs, they run away - this PR is going to take 4 hours to review and they don't have 4 hours right now. They'll get to it later, just as soon as they have more free time (ha!). There is a detailed rundown of best practices, including how to avoid too-lengthy PRs, in the next section. But, if you've already followed the best practices and you still aren't getting any PR love, here are some things you can do to move the process along: Make sure that your PR has an assigned reviewer (assignee in GitHub). If not, reply to the PR comment stream asking for a reviewer to be assigned. Ping the assignee (@username) on the PR comment stream, and ask for an estimate of when they can get to the review. Ping the assignee by email (many of us have publicly available email addresses). If you're a member of the organization ping the team (via @team-name) that works in the area you're submitting code. If you have fixed all the issues from a review, and you haven't heard back, you should ping the assignee on the comment stream with a \"please take another look\" ( PTAL ) or similar comment indicating that you are ready for another review. Read on to learn more about how to get faster reviews by following best practices. Best Practices for Faster Reviews \u2691 You've just had a brilliant idea on how to make a project better. Let's call that idea Feature-X. Feature-X is not even that complicated. You have a pretty good idea of how to implement it. You jump in and implement it, fixing a bunch of stuff along the way. You send your PR - this is awesome! And it sits. And sits. A week goes by and nobody reviews it. Finally, someone offers a few comments, which you fix up and wait for more review. And you wait. Another week or two go by. This is horrible. Let's talk about best practices so your PR gets reviewed quickly. Familiarize yourself with project conventions \u2691 Search for the Development guide Search for the Coding conventions Search for the API conventions Is the feature wanted? Make a Design Doc or Sketch PR \u2691 Are you sure Feature-X is something the project team wants or will accept? Is it implemented to fit with other changes in flight? Are you willing to bet a few days or weeks of work on it? It's better to get confirmation beforehand. There are two ways to do this: Make a proposal doc (in docs/proposals; for example the QoS proposal ), or reach out to the affected special interest group (SIG). Some projects have that Coordinate your effort with SIG Docs ahead of time Make a sketch PR (e.g., just the API or Go interface). Write or code up just enough to express the idea and the design and why you made those choices Or, do all of the above. Be clear about what type of feedback you are asking for when you submit a proposal doc or sketch PR. Now, if we ask you to change the design, you won't have to re-write it all. Smaller Is Better: Small Commits, Small PRs \u2691 Small commits and small PRs get reviewed faster and are more likely to be correct than big ones. Attention is a scarce resource. If your PR takes 60 minutes to review, the reviewer's eye for detail is not as keen in the last 30 minutes as it was in the first. It might not get reviewed at all if it requires a large continuous block of time from the reviewer. Breaking up commits Break up your PR into multiple commits, at logical break points. Making a series of discrete commits is a powerful way to express the evolution of an idea or the different ideas that make up a single feature. Strive to group logically distinct ideas into separate commits. For example, if you found that Feature-X needed some prefactoring to fit in, make a commit that JUST does that prefactoring. Then make a new commit for Feature-X. Strike a balance with the number of commits. A PR with 25 commits is still very cumbersome to review, so use judgment. Breaking up PRs Or, going back to our prefactoring example, you could also fork a new branch, do the prefactoring there and send a PR for that. If you can extract whole ideas from your PR and send those as PRs of their own, you can avoid the painful problem of continually rebasing. Multiple small PRs are often better than multiple commits. Don't worry about flooding us with PRs. We'd rather have 100 small, obvious PRs than 10 unreviewable monoliths. We want every PR to be useful on its own, so use your best judgment on what should be a PR vs. a commit. As a rule of thumb, if your PR is directly related to Feature-X and nothing else, it should probably be part of the Feature-X PR. If you can explain why you are doing seemingly no-op work (\"it makes the Feature-X change easier, I promise\") we'll probably be OK with it. If you can imagine someone finding value independently of Feature-X, try it as a PR. (Do not link pull requests by # in a commit description, because GitHub creates lots of spam. Instead, reference other PRs via the PR your commit is in.) Open a Different PR for Fixes and Generic Features \u2691 Put changes that are unrelated to your feature into a different PR. Often, as you are implementing Feature-X, you will find bad comments, poorly named functions, bad structure, weak type-safety, etc. You absolutely should fix those things (or at least file issues, please) - but not in the same PR as your feature. Otherwise, your diff will have way too many changes, and your reviewer won't see the forest for the trees. Look for opportunities to pull out generic features. For example, if you find yourself touching a lot of modules, think about the dependencies you are introducing between packages. Can some of what you're doing be made more generic and moved up and out of the Feature-X package? Do you need to use a function or type from an otherwise unrelated package? If so, promote! We have places for hosting more generic code. Likewise, if Feature-X is similar in form to Feature-W which was checked in last month, and you're duplicating some tricky stuff from Feature-W, consider prefactoring the core logic out and using it in both Feature-W and Feature-X. (Do that in its own commit or PR, please.) Comments Matter \u2691 In your code, if someone might not understand why you did something (or you won't remember why later), comment it. Many code-review comments are about this exact issue. If you think there's something pretty obvious that we could follow up on, add a TODO. Test \u2691 Nothing is more frustrating than starting a review, only to find that the tests are inadequate or absent. Very few PRs can touch code and NOT touch tests. If you don't know how to test Feature-X, please ask! We'll be happy to help you design things for easy testing or to suggest appropriate test cases. Squashing and Commit Titles \u2691 Your reviewer has finally sent you feedback on Feature-X. Make the fixups, and don't squash yet. Put them in a new commit, and re-push. That way your reviewer can look at the new commit on its own, which is much faster than starting over. We might still ask you to clean up your commits at the very end for the sake of a more readable history, but don't do this until asked: typically at the point where the PR would otherwise be tagged LGTM . Each commit should have a good title line ( <70 characters) and include an additional description paragraph describing in more detail the change intended. General squashing guidelines: Sausage => squash Do squash when there are several commits to fix bugs in the original commit(s), address reviewer feedback, etc. Really we only want to see the end state and commit message for the whole PR. Layers => don't squash Don't squash when there are independent changes layered to achieve a single goal. For instance, writing a code munger could be one commit, applying it could be another, and adding a precommit check could be a third. One could argue they should be separate PRs, but there's really no way to test/review the munger without seeing it applied, and there needs to be a precommit check to ensure the munged output doesn't immediately get out of date. A commit, as much as possible, should be a single logical change. KISS, YAGNI, MVP, etc. \u2691 Sometimes we need to remind each other of core tenets of software design - Keep It Simple, You Aren't Gonna Need It, Minimum Viable Product, and so on. Adding a feature \"because we might need it later\" is antithetical to software that ships. Add the things you need NOW and (ideally) leave room for things you might need later - but don't implement them now. It's OK to Push Back \u2691 Sometimes reviewers make mistakes. It's OK to push back on changes your reviewer requested. If you have a good reason for doing something a certain way, you are absolutely allowed to debate the merits of a requested change. Both the reviewer and reviewee should strive to discuss these issues in a polite and respectful manner. You might be overruled, but you might also prevail. We're pretty reasonable people. Mostly. Another phenomenon of open-source projects (where anyone can comment on any issue) is the dog-pile - your PR gets so many comments from so many people it becomes hard to follow. In this situation, you can ask the primary reviewer (assignee) whether they want you to fork a new PR to clear out all the comments. You don't HAVE to fix every issue raised by every person who feels like commenting, but you should answer reasonable comments with an explanation. Common Sense and Courtesy \u2691 No document can take the place of common sense and good taste. Use your best judgment, while you put a bit of thought into how your work can be made easier to review. If you do these things your PRs will get merged with less friction. Split long PR into smaller ones \u2691 Start a new branch from where you want to merge. Start an interactive rebase on HEAD: git rebase -i HEAD Get the commits you want: Now comes the clever part, we are going to pick out all the commits we care about from 112-new-feature-branch using the following command: git log --oneline --reverse HEAD..112-new-feature-branch -- app/models/ spec/models Woah thats quite the line! Let\u2019s dissect it first: git log shows a log of what you have done in your project. --online formats the output from a few lines (including author and time of commit), to just \u201c[sha-hash-of-commit] [description-of-commit]\u201d --reverse reverses the log output chronologically (so oldest commit first, newest last). 112-new-feature-branch..HEAD shows the difference in commits from your current branch (HEAD) and the branch you are interested in 112-new-feature-branch. -- app/models/ spec/models Only show commits that changed files in app/models/ or spec/models So that we confine the changes to our model and its tests. Now if you are using vim (or vi or neovim) you can put the results of this command directly into your rebase-todo (which was opened when starting the rebase) using the :r command like so: : r ! git log -- oneline -- reverse HEAD.. 112 - new - feature - branch -- app /models/ Review the commits you want: Now you have a chance to go though your todo once again. First you should remove the noop from above, since you actually do something now. Second you should check the diffs of the sha-hashes. Note: If you are using vim, you might already have the fugitive plug-in. If you haven\u2019t changed the standard configuration, you can just move your cursor over the sha-hashes and press K (note that its capitalized) to see the diff of that commit. If you don\u2019t have fugitive or don\u2019t use vim, you can check the diff using git show SHA-HASH (for example git show c4f74d0 ), which shows the commits data. Now you can prepend and even rearrange the commits (Be careful rearranging or leaving out commits, you might have to fix conflicts later). Execute the rebase: Now you can save and exit the editor and git will try to execute the rebase. If you have conflicts you can fix them just like you do with merges and then continue using git rebase --continue. If you feel like something is going terribly wrong (for example you have a bunch of conflicts in just a few commits), you can abort the rebase using git rebase --abort and it will be like nothing ever happened. Git workflow \u2691 There are many ways of using git, one of the most popular is git flow , please read this article to understand it before going on. Unless you are part of a big team that delivers software that needs to maintain many versions, it's not worth using git flow as it's too complex and cumbersome. Instead I suggest a variation of the Github workflow. To carry out a reliable continuous delivery we must work to comply with the following list of best practices: Everything must be in the git server: source code, tests, pipelines, scripts, templates and documentation. There is only a main branch (main) whose key is that everything is in this branch must be always stable and deployable into production at any time. New branches are created from main in order to develop new features that should be merged into main branch in short development cycles. It is highly recommended to do small commits to have more control over what is being done and to avoid discarding many lines of code if a rollback has to be done. A commit message policy should be set so that they are clear and conform the same pattern, for example semantic versioning . main is blocked to reject direct pushes as well as to protect it of catastrophic deletion. Only pre-validated merge requests are accepted. When a feature is ready, we will open a merge request to merge changes into main branch. Use webhooks to automate the execution of tests and validation tasks in the CI server before/after adding changes in main. It is not needed to discard a merge request if any of the validation tasks failed. We check the code and when the changes are pushed, the CI server will relaunch the validation tasks. If all validation tasks pass, we will assign the merge request to two team developers to review the feature code. After both reviewers validate the code, the merge request can be accepted and the feature branch may be deleted. A clear versioning policy must be adopted for all generated artifacts. Each artifact must be generated once and be promoted to the different environments in different stages. When a developer wants to add code to main should proceed as follows: Wait until the pipeline execution ends if it exists. If that process fails, then the developer must help to other team members to fix the issue before requesting a new merge request. Pull the changes from main and resolve the conflicts locally before pushing the code to the new feature branch. Run a local script that compiles and executes the tests before committing changes. This task can be done executing it manually by the developer or using a git precommit. Open a new merge request setting the feature branch as source branch and main as target branch. The CI server is notified of the new merge request and executes the pipeline which compiles the source code, executes the tests, deploys the artifact, etc. If there are errors in the previous step, the developer must fix the code and push it to the git server as soon as possible so that the CI server validate once again the merge request. If no errors, the CI server will mark the merge request as OK and the developer can assign it to two other team members to review the feature code. At this point, the developer can start with other task. Considerations The build process and the execution of the tests have to be pretty fast. It should not exceed about 10 minutes. Unit tests must be guarantee that they are completely unitary; they must be executed without starting the context of the application, they must not access to the DDBB, external systems, file system, etc. Naming conventions \u2691 The best idea is to use Semantic Versioning to define the names of the branches, for example: feat/add-command-line-support or fix/correct-security-issue , and also for the commit messages . Tag versioning policy \u2691 We will also adopt semantic versioning policy on version management. Versioning control \u2691 When a branch is merged into main, the CI server launches a job which generates a new artifact release as follow: The new version number is calculated taken into account the above considerations. Generates a new artifact named as appname-major.minor.patch.build Upload the previous artifact to the artifact repository manager. Create a git tag on the repository with the same version identifier, major.minor.patch.build Automatically deploy the artifact on the desired environment (dev, pre, etc) Hotfixing \u2691 Hotfix should be developed and fixed using one of the next cases, which has been defined by preference order: Case 1 \u2691 In this case, we have pushed new code to \"main\" branch since the last deploy on production and we want to deploy the new code with the fix code. We have to follow the next steps: Create a branch \"Hotfix\" from commit/tag of the last deploy Fix the bug in \"hotfix\" branch Merge the new branch to \"main\" Deploy main branch Case 2 \u2691 In this case, we have pushed new code to \"main\" branch since the last deploy on production but we don't want to deploy the new code with the fix code. We have to follow the next steps: Create a branch \"Hotfix\" from commit/tag of the last deploy Fix the bug in \"hotfix\" branch Deploy main branch Merge the new branch to \"main. Case 3 \u2691 In this case, we have pushed new code to \"main\" branch since the last deploy on production but we don't want to deploy the new code with the fix code. We have to follow the next steps: Create a branch \"Hotfix\" from commit/tag of the last deploy Fix the bug in \"hotfix\" branch Deploy main branch Merge the new branch to \"main. Git housekeeping \u2691 The best option is to: git fetch --prune git-sweep cleanup To remove the local branches you can: cd myrepo git remote add local $( pwd ) git-sweep cleanup --origin = local git-sweep : For local branches archaeologit : Tool to search strings in the history of a github user jessfraz made a tool ghb0t : For github Submodules \u2691 Shamefully edited from the docs It often happens that while working on one project, you need to use another project from within it. Perhaps it\u2019s a library that a third party developed or that you\u2019re developing separately and using in multiple parent projects. A common issue arises in these scenarios: you want to be able to treat the two projects as separate yet still be able to use one from within the other. Here\u2019s an example. Suppose you\u2019re developing a website and creating Atom feeds. Instead of writing your own Atom-generating code, you decide to use a library. You\u2019re likely to have to either include this code from a shared library like a CPAN install or Ruby gem, or copy the source code into your own project tree. The issue with including the library is that it\u2019s difficult to customize the library in any way and often more difficult to deploy it, because you need to make sure every client has that library available. The issue with copying the code into your own project is that any custom changes you make are difficult to merge when upstream changes become available. Git addresses this issue using submodules. Submodules allow you to keep a Git repository as a subdirectory of another Git repository. This lets you clone another repository into your project and keep your commits separate. It often happens that while working on one project, you need to use another project from within it. Perhaps it\u2019s a library that a third party developed or that you\u2019re developing separately and using in multiple parent projects. A common issue arises in these scenarios: you want to be able to treat the two projects as separate yet still be able to use one from within the other. Here\u2019s an example. Suppose you\u2019re developing a website and creating Atom feeds. Instead of writing your own Atom-generating code, you decide to use a library. You\u2019re likely to have to either include this code from a shared library like a CPAN install or Ruby gem, or copy the source code into your own project tree. The issue with including the library is that it\u2019s difficult to customize the library in any way and often more difficult to deploy it, because you need to make sure every client has that library available. The issue with copying the code into your own project is that any custom changes you make are difficult to merge when upstream changes become available. Git addresses this issue using submodules. Submodules allow you to keep a Git repository as a subdirectory of another Git repository. This lets you clone another repository into your project and keep your commits separate. Submodule tips \u2691 Submodule Foreach \u2691 There is a foreach submodule command to run some arbitrary command in each submodule. This can be really helpful if you have a number of submodules in the same project. For example, let\u2019s say we want to start a new feature or do a bugfix and we have work going on in several submodules. We can easily stash all the work in all our submodules. git submodule foreach 'git stash' Then we can create a new branch and switch to it in all our submodules. git submodule foreach 'git checkout -b featureA' You get the idea. One really useful thing you can do is produce a nice unified diff of what is changed in your main project and all your subprojects as well. git diff ; git submodule foreach 'git diff' Useful Aliases \u2691 You may want to set up some aliases for some of these commands as they can be quite long and you can\u2019t set configuration options for most of them to make them defaults. We covered setting up Git aliases in Git Aliases, but here is an example of what you may want to set up if you plan on working with submodules in Git a lot. git config alias.sdiff '!' \"git diff && git submodule foreach 'git diff'\" git config alias.spush 'push --recurse-submodules=on-demand' git config alias.supdate 'submodule update --remote --merge' This way you can simply run git supdate when you want to update your submodules, or git spush to push with submodule dependency checking. Encrypt sensitive information \u2691 Use git-crypt . Use different git configs \u2691 Include in your ~/.gitconfig [includeIf \"gitdir:~/company_A/\"] path = ~/.config/git/company_A.config Every repository you create under that directory it will append the other configuration Renaming from master to main \u2691 There's been a movement to migrate from master to main , the reason behind it is that the initial branch name, master , is offensive to some people and we empathize with those hurt by the use of that term. Existing versions of Git are capable of working with any branch name; there's nothing special about master except that it has historically been the name used for the first branch when creating a new repository from scratch (with the git init command). Thus many projects use it to represent the primary line of development. We support and encourage projects to switch to branch names that are meaningful and inclusive. To configure git to use main by default run: git config --global init.defaultBranch main It only works on since git version 2.28.0, so you're stuck with manually changing it if you have an earlier version. Change's Controversy \u2691 The change is not free of controversy, for example in the PDM project some people are not sure that it's needed for many reasons. Let's see each of them: The reason people are implementing the change is because other people are doing it : After a quick search I found that the first one to do the change was the software freedom conservancy with the Git project . You can also see Python , Django , Redis , Drupal , CouchDB and Github 's statements. As we're not part of the deciding organisms of the collectives doing the changes, all we can use are their statements and discussions to guess what are the reasons behind their support of the change. Despite that some of them do use the argument that other communities do support the change to emphasize the need of the change, all of them mention that the main reason is that the term is offensive to some people. I don't see an issue using the term master : If you relate to this statement it can be because you're not part of the communities that suffer the oppression tied to the term, and that makes you blind to the issue. It's a lesson I learned on my own skin throughout the years. There are thousand of situations, gestures, double meaning words and sentences that went unnoticed by me until I started discussing it with the people that are suffering them (women, racialized people, LGTBQI+, ...). Throughout my experience I've seen that the more privileged you are, the blinder you become. You can read more on privileged blindness here , here or here (I've skimmed through the articles, and are the first articles I've found, there are probably better references). I'm not saying that privileged people are not aware of the issues or that they can even raise them. We can do so and more we read, discuss and train ourselves, the better we'll detect them. All I'm saying is that a non privileged person will always detect more because they suffer them daily. I understand that for you there is no issue using the word master , there wasn't an issue for me either until I saw these projects doing the change, again I was blinded to the issue as I'm not suffering it. That's because change is not meant for us, as we're not triggered by it. The change is targeted to the people that do perceive that master is an offensive term. What we can do is empathize with them and follow this tiny tiny tiny gesture. It's the least we can do. Think of a term that triggers you, such as heil hitler , imagine that those words were being used to define the main branch of your code, and that everyday you sit in front of your computer you see them. You'll probably be reminded of the historic events, concepts, feelings that are tied to that term each time you see it, and being them quite negative it can slowly mine you. Therefore it's legit that you wouldn't want to be exposed to that negative effects. I don't see who will benefit from this change : Probably the people that belongs to communities that are and have been under constant oppression for a very long time, in this case, specially the racialized ones which have suffered slavery. Sadly you will probably won't see many the affected people speak in these discussions, first because there are not that many, sadly the IT world is dominated by middle aged, economically comfortable, white, cis, hetero, males. Small changes like this are meant to foster diversity in the community by allowing them being more comfortable. Secondly because when they see these debates they move on as they are so fed up on teaching privileged people of their privileges. They not only have to suffer the oppression, we also put the burden on their shoulders to teach us. As and ending thought, if you see yourself being specially troubled by the change, having a discomfort feeling and strong reactions. In my experience these signs are characteristic of privileged people that feel that their privileges are being threatened, I've felt them myself countless times. When I feel it, I usually do two things, fight them as strong as I can, or embrace them, analyze them, and go to the root of them. Depending on how much energy I have I go with the easy or the hard one. I'm not saying that it's you're case, but it could be. References \u2691 FAQ Funny FAQ Nvie post on branching model Courses \u2691 W3 git course Learngitbranching interactive tutorial katakoda Code academy Udemy Freecode camp article Tools \u2691 git-extras", "title": "Git"}, {"location": "git/#learning-git", "text": "Git is a tough nut to crack, no matter how experience you are you'll frequently get surprised. Sadly it's one of the main tools to develop your code, so you must master it as soon as possible. Depending on how you like to learn I've found these options: Written courses: W3 git course Interactive tutorials: Learngitbranching interactive tutorial Written article: Freecode camp article Video courses: Code academy and Udemy", "title": "Learning git"}, {"location": "git/#pull-request-process", "text": "This part of the doc is shamefully edited from the source. It was for the k8s project but they are good practices that work for all the projects. It explains the process and best practices for submitting a PR It should serve as a reference for all contributors, and be useful especially to new and infrequent submitters.", "title": "Pull Request Process"}, {"location": "git/#before-you-submit-a-pr", "text": "This guide is for contributors who already have a PR to submit. If you're looking for information on setting up your developer environment and creating code to contribute to the project, search the development guide. Make sure your PR adheres to the projects best practices. These include following project conventions, making small PRs, and commenting thoroughly.", "title": "Before You Submit a PR"}, {"location": "git/#run-local-verifications", "text": "You can run the tests in local before you submit your PR to predict the pass or fail of continuous integration.", "title": "Run Local Verifications"}, {"location": "git/#why-is-my-pr-not-getting-reviewed", "text": "A few factors affect how long your PR might wait for review. If it's the last few weeks of a milestone, we need to reduce churn and stabilize. Or, it could be related to best practices. One common issue is that the PR is too big to review. Let's say you've touched 39 files and have 8657 insertions. When your would-be reviewers pull up the diffs, they run away - this PR is going to take 4 hours to review and they don't have 4 hours right now. They'll get to it later, just as soon as they have more free time (ha!). There is a detailed rundown of best practices, including how to avoid too-lengthy PRs, in the next section. But, if you've already followed the best practices and you still aren't getting any PR love, here are some things you can do to move the process along: Make sure that your PR has an assigned reviewer (assignee in GitHub). If not, reply to the PR comment stream asking for a reviewer to be assigned. Ping the assignee (@username) on the PR comment stream, and ask for an estimate of when they can get to the review. Ping the assignee by email (many of us have publicly available email addresses). If you're a member of the organization ping the team (via @team-name) that works in the area you're submitting code. If you have fixed all the issues from a review, and you haven't heard back, you should ping the assignee on the comment stream with a \"please take another look\" ( PTAL ) or similar comment indicating that you are ready for another review. Read on to learn more about how to get faster reviews by following best practices.", "title": "Why is my PR not getting reviewed?"}, {"location": "git/#best-practices-for-faster-reviews", "text": "You've just had a brilliant idea on how to make a project better. Let's call that idea Feature-X. Feature-X is not even that complicated. You have a pretty good idea of how to implement it. You jump in and implement it, fixing a bunch of stuff along the way. You send your PR - this is awesome! And it sits. And sits. A week goes by and nobody reviews it. Finally, someone offers a few comments, which you fix up and wait for more review. And you wait. Another week or two go by. This is horrible. Let's talk about best practices so your PR gets reviewed quickly.", "title": "Best Practices for Faster Reviews"}, {"location": "git/#familiarize-yourself-with-project-conventions", "text": "Search for the Development guide Search for the Coding conventions Search for the API conventions", "title": "Familiarize yourself with project conventions"}, {"location": "git/#is-the-feature-wanted-make-a-design-doc-or-sketch-pr", "text": "Are you sure Feature-X is something the project team wants or will accept? Is it implemented to fit with other changes in flight? Are you willing to bet a few days or weeks of work on it? It's better to get confirmation beforehand. There are two ways to do this: Make a proposal doc (in docs/proposals; for example the QoS proposal ), or reach out to the affected special interest group (SIG). Some projects have that Coordinate your effort with SIG Docs ahead of time Make a sketch PR (e.g., just the API or Go interface). Write or code up just enough to express the idea and the design and why you made those choices Or, do all of the above. Be clear about what type of feedback you are asking for when you submit a proposal doc or sketch PR. Now, if we ask you to change the design, you won't have to re-write it all.", "title": "Is the feature wanted? Make a Design Doc or Sketch PR"}, {"location": "git/#smaller-is-better-small-commits-small-prs", "text": "Small commits and small PRs get reviewed faster and are more likely to be correct than big ones. Attention is a scarce resource. If your PR takes 60 minutes to review, the reviewer's eye for detail is not as keen in the last 30 minutes as it was in the first. It might not get reviewed at all if it requires a large continuous block of time from the reviewer. Breaking up commits Break up your PR into multiple commits, at logical break points. Making a series of discrete commits is a powerful way to express the evolution of an idea or the different ideas that make up a single feature. Strive to group logically distinct ideas into separate commits. For example, if you found that Feature-X needed some prefactoring to fit in, make a commit that JUST does that prefactoring. Then make a new commit for Feature-X. Strike a balance with the number of commits. A PR with 25 commits is still very cumbersome to review, so use judgment. Breaking up PRs Or, going back to our prefactoring example, you could also fork a new branch, do the prefactoring there and send a PR for that. If you can extract whole ideas from your PR and send those as PRs of their own, you can avoid the painful problem of continually rebasing. Multiple small PRs are often better than multiple commits. Don't worry about flooding us with PRs. We'd rather have 100 small, obvious PRs than 10 unreviewable monoliths. We want every PR to be useful on its own, so use your best judgment on what should be a PR vs. a commit. As a rule of thumb, if your PR is directly related to Feature-X and nothing else, it should probably be part of the Feature-X PR. If you can explain why you are doing seemingly no-op work (\"it makes the Feature-X change easier, I promise\") we'll probably be OK with it. If you can imagine someone finding value independently of Feature-X, try it as a PR. (Do not link pull requests by # in a commit description, because GitHub creates lots of spam. Instead, reference other PRs via the PR your commit is in.)", "title": "Smaller Is Better: Small Commits, Small PRs"}, {"location": "git/#open-a-different-pr-for-fixes-and-generic-features", "text": "Put changes that are unrelated to your feature into a different PR. Often, as you are implementing Feature-X, you will find bad comments, poorly named functions, bad structure, weak type-safety, etc. You absolutely should fix those things (or at least file issues, please) - but not in the same PR as your feature. Otherwise, your diff will have way too many changes, and your reviewer won't see the forest for the trees. Look for opportunities to pull out generic features. For example, if you find yourself touching a lot of modules, think about the dependencies you are introducing between packages. Can some of what you're doing be made more generic and moved up and out of the Feature-X package? Do you need to use a function or type from an otherwise unrelated package? If so, promote! We have places for hosting more generic code. Likewise, if Feature-X is similar in form to Feature-W which was checked in last month, and you're duplicating some tricky stuff from Feature-W, consider prefactoring the core logic out and using it in both Feature-W and Feature-X. (Do that in its own commit or PR, please.)", "title": "Open a Different PR for Fixes and Generic Features"}, {"location": "git/#comments-matter", "text": "In your code, if someone might not understand why you did something (or you won't remember why later), comment it. Many code-review comments are about this exact issue. If you think there's something pretty obvious that we could follow up on, add a TODO.", "title": "Comments Matter"}, {"location": "git/#test", "text": "Nothing is more frustrating than starting a review, only to find that the tests are inadequate or absent. Very few PRs can touch code and NOT touch tests. If you don't know how to test Feature-X, please ask! We'll be happy to help you design things for easy testing or to suggest appropriate test cases.", "title": "Test"}, {"location": "git/#squashing-and-commit-titles", "text": "Your reviewer has finally sent you feedback on Feature-X. Make the fixups, and don't squash yet. Put them in a new commit, and re-push. That way your reviewer can look at the new commit on its own, which is much faster than starting over. We might still ask you to clean up your commits at the very end for the sake of a more readable history, but don't do this until asked: typically at the point where the PR would otherwise be tagged LGTM . Each commit should have a good title line ( <70 characters) and include an additional description paragraph describing in more detail the change intended. General squashing guidelines: Sausage => squash Do squash when there are several commits to fix bugs in the original commit(s), address reviewer feedback, etc. Really we only want to see the end state and commit message for the whole PR. Layers => don't squash Don't squash when there are independent changes layered to achieve a single goal. For instance, writing a code munger could be one commit, applying it could be another, and adding a precommit check could be a third. One could argue they should be separate PRs, but there's really no way to test/review the munger without seeing it applied, and there needs to be a precommit check to ensure the munged output doesn't immediately get out of date. A commit, as much as possible, should be a single logical change.", "title": "Squashing and Commit Titles"}, {"location": "git/#kiss-yagni-mvp-etc", "text": "Sometimes we need to remind each other of core tenets of software design - Keep It Simple, You Aren't Gonna Need It, Minimum Viable Product, and so on. Adding a feature \"because we might need it later\" is antithetical to software that ships. Add the things you need NOW and (ideally) leave room for things you might need later - but don't implement them now.", "title": "KISS, YAGNI, MVP, etc."}, {"location": "git/#its-ok-to-push-back", "text": "Sometimes reviewers make mistakes. It's OK to push back on changes your reviewer requested. If you have a good reason for doing something a certain way, you are absolutely allowed to debate the merits of a requested change. Both the reviewer and reviewee should strive to discuss these issues in a polite and respectful manner. You might be overruled, but you might also prevail. We're pretty reasonable people. Mostly. Another phenomenon of open-source projects (where anyone can comment on any issue) is the dog-pile - your PR gets so many comments from so many people it becomes hard to follow. In this situation, you can ask the primary reviewer (assignee) whether they want you to fork a new PR to clear out all the comments. You don't HAVE to fix every issue raised by every person who feels like commenting, but you should answer reasonable comments with an explanation.", "title": "It's OK to Push Back"}, {"location": "git/#common-sense-and-courtesy", "text": "No document can take the place of common sense and good taste. Use your best judgment, while you put a bit of thought into how your work can be made easier to review. If you do these things your PRs will get merged with less friction.", "title": "Common Sense and Courtesy"}, {"location": "git/#split-long-pr-into-smaller-ones", "text": "Start a new branch from where you want to merge. Start an interactive rebase on HEAD: git rebase -i HEAD Get the commits you want: Now comes the clever part, we are going to pick out all the commits we care about from 112-new-feature-branch using the following command: git log --oneline --reverse HEAD..112-new-feature-branch -- app/models/ spec/models Woah thats quite the line! Let\u2019s dissect it first: git log shows a log of what you have done in your project. --online formats the output from a few lines (including author and time of commit), to just \u201c[sha-hash-of-commit] [description-of-commit]\u201d --reverse reverses the log output chronologically (so oldest commit first, newest last). 112-new-feature-branch..HEAD shows the difference in commits from your current branch (HEAD) and the branch you are interested in 112-new-feature-branch. -- app/models/ spec/models Only show commits that changed files in app/models/ or spec/models So that we confine the changes to our model and its tests. Now if you are using vim (or vi or neovim) you can put the results of this command directly into your rebase-todo (which was opened when starting the rebase) using the :r command like so: : r ! git log -- oneline -- reverse HEAD.. 112 - new - feature - branch -- app /models/ Review the commits you want: Now you have a chance to go though your todo once again. First you should remove the noop from above, since you actually do something now. Second you should check the diffs of the sha-hashes. Note: If you are using vim, you might already have the fugitive plug-in. If you haven\u2019t changed the standard configuration, you can just move your cursor over the sha-hashes and press K (note that its capitalized) to see the diff of that commit. If you don\u2019t have fugitive or don\u2019t use vim, you can check the diff using git show SHA-HASH (for example git show c4f74d0 ), which shows the commits data. Now you can prepend and even rearrange the commits (Be careful rearranging or leaving out commits, you might have to fix conflicts later). Execute the rebase: Now you can save and exit the editor and git will try to execute the rebase. If you have conflicts you can fix them just like you do with merges and then continue using git rebase --continue. If you feel like something is going terribly wrong (for example you have a bunch of conflicts in just a few commits), you can abort the rebase using git rebase --abort and it will be like nothing ever happened.", "title": "Split long PR into smaller ones"}, {"location": "git/#git-workflow", "text": "There are many ways of using git, one of the most popular is git flow , please read this article to understand it before going on. Unless you are part of a big team that delivers software that needs to maintain many versions, it's not worth using git flow as it's too complex and cumbersome. Instead I suggest a variation of the Github workflow. To carry out a reliable continuous delivery we must work to comply with the following list of best practices: Everything must be in the git server: source code, tests, pipelines, scripts, templates and documentation. There is only a main branch (main) whose key is that everything is in this branch must be always stable and deployable into production at any time. New branches are created from main in order to develop new features that should be merged into main branch in short development cycles. It is highly recommended to do small commits to have more control over what is being done and to avoid discarding many lines of code if a rollback has to be done. A commit message policy should be set so that they are clear and conform the same pattern, for example semantic versioning . main is blocked to reject direct pushes as well as to protect it of catastrophic deletion. Only pre-validated merge requests are accepted. When a feature is ready, we will open a merge request to merge changes into main branch. Use webhooks to automate the execution of tests and validation tasks in the CI server before/after adding changes in main. It is not needed to discard a merge request if any of the validation tasks failed. We check the code and when the changes are pushed, the CI server will relaunch the validation tasks. If all validation tasks pass, we will assign the merge request to two team developers to review the feature code. After both reviewers validate the code, the merge request can be accepted and the feature branch may be deleted. A clear versioning policy must be adopted for all generated artifacts. Each artifact must be generated once and be promoted to the different environments in different stages. When a developer wants to add code to main should proceed as follows: Wait until the pipeline execution ends if it exists. If that process fails, then the developer must help to other team members to fix the issue before requesting a new merge request. Pull the changes from main and resolve the conflicts locally before pushing the code to the new feature branch. Run a local script that compiles and executes the tests before committing changes. This task can be done executing it manually by the developer or using a git precommit. Open a new merge request setting the feature branch as source branch and main as target branch. The CI server is notified of the new merge request and executes the pipeline which compiles the source code, executes the tests, deploys the artifact, etc. If there are errors in the previous step, the developer must fix the code and push it to the git server as soon as possible so that the CI server validate once again the merge request. If no errors, the CI server will mark the merge request as OK and the developer can assign it to two other team members to review the feature code. At this point, the developer can start with other task. Considerations The build process and the execution of the tests have to be pretty fast. It should not exceed about 10 minutes. Unit tests must be guarantee that they are completely unitary; they must be executed without starting the context of the application, they must not access to the DDBB, external systems, file system, etc.", "title": "Git workflow"}, {"location": "git/#naming-conventions", "text": "The best idea is to use Semantic Versioning to define the names of the branches, for example: feat/add-command-line-support or fix/correct-security-issue , and also for the commit messages .", "title": "Naming conventions"}, {"location": "git/#tag-versioning-policy", "text": "We will also adopt semantic versioning policy on version management.", "title": "Tag versioning policy"}, {"location": "git/#versioning-control", "text": "When a branch is merged into main, the CI server launches a job which generates a new artifact release as follow: The new version number is calculated taken into account the above considerations. Generates a new artifact named as appname-major.minor.patch.build Upload the previous artifact to the artifact repository manager. Create a git tag on the repository with the same version identifier, major.minor.patch.build Automatically deploy the artifact on the desired environment (dev, pre, etc)", "title": "Versioning control"}, {"location": "git/#hotfixing", "text": "Hotfix should be developed and fixed using one of the next cases, which has been defined by preference order:", "title": "Hotfixing"}, {"location": "git/#case-1", "text": "In this case, we have pushed new code to \"main\" branch since the last deploy on production and we want to deploy the new code with the fix code. We have to follow the next steps: Create a branch \"Hotfix\" from commit/tag of the last deploy Fix the bug in \"hotfix\" branch Merge the new branch to \"main\" Deploy main branch", "title": "Case 1"}, {"location": "git/#case-2", "text": "In this case, we have pushed new code to \"main\" branch since the last deploy on production but we don't want to deploy the new code with the fix code. We have to follow the next steps: Create a branch \"Hotfix\" from commit/tag of the last deploy Fix the bug in \"hotfix\" branch Deploy main branch Merge the new branch to \"main.", "title": "Case 2"}, {"location": "git/#case-3", "text": "In this case, we have pushed new code to \"main\" branch since the last deploy on production but we don't want to deploy the new code with the fix code. We have to follow the next steps: Create a branch \"Hotfix\" from commit/tag of the last deploy Fix the bug in \"hotfix\" branch Deploy main branch Merge the new branch to \"main.", "title": "Case 3"}, {"location": "git/#git-housekeeping", "text": "The best option is to: git fetch --prune git-sweep cleanup To remove the local branches you can: cd myrepo git remote add local $( pwd ) git-sweep cleanup --origin = local git-sweep : For local branches archaeologit : Tool to search strings in the history of a github user jessfraz made a tool ghb0t : For github", "title": "Git housekeeping"}, {"location": "git/#submodules", "text": "Shamefully edited from the docs It often happens that while working on one project, you need to use another project from within it. Perhaps it\u2019s a library that a third party developed or that you\u2019re developing separately and using in multiple parent projects. A common issue arises in these scenarios: you want to be able to treat the two projects as separate yet still be able to use one from within the other. Here\u2019s an example. Suppose you\u2019re developing a website and creating Atom feeds. Instead of writing your own Atom-generating code, you decide to use a library. You\u2019re likely to have to either include this code from a shared library like a CPAN install or Ruby gem, or copy the source code into your own project tree. The issue with including the library is that it\u2019s difficult to customize the library in any way and often more difficult to deploy it, because you need to make sure every client has that library available. The issue with copying the code into your own project is that any custom changes you make are difficult to merge when upstream changes become available. Git addresses this issue using submodules. Submodules allow you to keep a Git repository as a subdirectory of another Git repository. This lets you clone another repository into your project and keep your commits separate. It often happens that while working on one project, you need to use another project from within it. Perhaps it\u2019s a library that a third party developed or that you\u2019re developing separately and using in multiple parent projects. A common issue arises in these scenarios: you want to be able to treat the two projects as separate yet still be able to use one from within the other. Here\u2019s an example. Suppose you\u2019re developing a website and creating Atom feeds. Instead of writing your own Atom-generating code, you decide to use a library. You\u2019re likely to have to either include this code from a shared library like a CPAN install or Ruby gem, or copy the source code into your own project tree. The issue with including the library is that it\u2019s difficult to customize the library in any way and often more difficult to deploy it, because you need to make sure every client has that library available. The issue with copying the code into your own project is that any custom changes you make are difficult to merge when upstream changes become available. Git addresses this issue using submodules. Submodules allow you to keep a Git repository as a subdirectory of another Git repository. This lets you clone another repository into your project and keep your commits separate.", "title": "Submodules"}, {"location": "git/#submodule-tips", "text": "", "title": "Submodule tips"}, {"location": "git/#submodule-foreach", "text": "There is a foreach submodule command to run some arbitrary command in each submodule. This can be really helpful if you have a number of submodules in the same project. For example, let\u2019s say we want to start a new feature or do a bugfix and we have work going on in several submodules. We can easily stash all the work in all our submodules. git submodule foreach 'git stash' Then we can create a new branch and switch to it in all our submodules. git submodule foreach 'git checkout -b featureA' You get the idea. One really useful thing you can do is produce a nice unified diff of what is changed in your main project and all your subprojects as well. git diff ; git submodule foreach 'git diff'", "title": "Submodule Foreach"}, {"location": "git/#useful-aliases", "text": "You may want to set up some aliases for some of these commands as they can be quite long and you can\u2019t set configuration options for most of them to make them defaults. We covered setting up Git aliases in Git Aliases, but here is an example of what you may want to set up if you plan on working with submodules in Git a lot. git config alias.sdiff '!' \"git diff && git submodule foreach 'git diff'\" git config alias.spush 'push --recurse-submodules=on-demand' git config alias.supdate 'submodule update --remote --merge' This way you can simply run git supdate when you want to update your submodules, or git spush to push with submodule dependency checking.", "title": "Useful Aliases"}, {"location": "git/#encrypt-sensitive-information", "text": "Use git-crypt .", "title": "Encrypt sensitive information"}, {"location": "git/#use-different-git-configs", "text": "Include in your ~/.gitconfig [includeIf \"gitdir:~/company_A/\"] path = ~/.config/git/company_A.config Every repository you create under that directory it will append the other configuration", "title": "Use different git configs"}, {"location": "git/#renaming-from-master-to-main", "text": "There's been a movement to migrate from master to main , the reason behind it is that the initial branch name, master , is offensive to some people and we empathize with those hurt by the use of that term. Existing versions of Git are capable of working with any branch name; there's nothing special about master except that it has historically been the name used for the first branch when creating a new repository from scratch (with the git init command). Thus many projects use it to represent the primary line of development. We support and encourage projects to switch to branch names that are meaningful and inclusive. To configure git to use main by default run: git config --global init.defaultBranch main It only works on since git version 2.28.0, so you're stuck with manually changing it if you have an earlier version.", "title": "Renaming from master to main"}, {"location": "git/#changes-controversy", "text": "The change is not free of controversy, for example in the PDM project some people are not sure that it's needed for many reasons. Let's see each of them: The reason people are implementing the change is because other people are doing it : After a quick search I found that the first one to do the change was the software freedom conservancy with the Git project . You can also see Python , Django , Redis , Drupal , CouchDB and Github 's statements. As we're not part of the deciding organisms of the collectives doing the changes, all we can use are their statements and discussions to guess what are the reasons behind their support of the change. Despite that some of them do use the argument that other communities do support the change to emphasize the need of the change, all of them mention that the main reason is that the term is offensive to some people. I don't see an issue using the term master : If you relate to this statement it can be because you're not part of the communities that suffer the oppression tied to the term, and that makes you blind to the issue. It's a lesson I learned on my own skin throughout the years. There are thousand of situations, gestures, double meaning words and sentences that went unnoticed by me until I started discussing it with the people that are suffering them (women, racialized people, LGTBQI+, ...). Throughout my experience I've seen that the more privileged you are, the blinder you become. You can read more on privileged blindness here , here or here (I've skimmed through the articles, and are the first articles I've found, there are probably better references). I'm not saying that privileged people are not aware of the issues or that they can even raise them. We can do so and more we read, discuss and train ourselves, the better we'll detect them. All I'm saying is that a non privileged person will always detect more because they suffer them daily. I understand that for you there is no issue using the word master , there wasn't an issue for me either until I saw these projects doing the change, again I was blinded to the issue as I'm not suffering it. That's because change is not meant for us, as we're not triggered by it. The change is targeted to the people that do perceive that master is an offensive term. What we can do is empathize with them and follow this tiny tiny tiny gesture. It's the least we can do. Think of a term that triggers you, such as heil hitler , imagine that those words were being used to define the main branch of your code, and that everyday you sit in front of your computer you see them. You'll probably be reminded of the historic events, concepts, feelings that are tied to that term each time you see it, and being them quite negative it can slowly mine you. Therefore it's legit that you wouldn't want to be exposed to that negative effects. I don't see who will benefit from this change : Probably the people that belongs to communities that are and have been under constant oppression for a very long time, in this case, specially the racialized ones which have suffered slavery. Sadly you will probably won't see many the affected people speak in these discussions, first because there are not that many, sadly the IT world is dominated by middle aged, economically comfortable, white, cis, hetero, males. Small changes like this are meant to foster diversity in the community by allowing them being more comfortable. Secondly because when they see these debates they move on as they are so fed up on teaching privileged people of their privileges. They not only have to suffer the oppression, we also put the burden on their shoulders to teach us. As and ending thought, if you see yourself being specially troubled by the change, having a discomfort feeling and strong reactions. In my experience these signs are characteristic of privileged people that feel that their privileges are being threatened, I've felt them myself countless times. When I feel it, I usually do two things, fight them as strong as I can, or embrace them, analyze them, and go to the root of them. Depending on how much energy I have I go with the easy or the hard one. I'm not saying that it's you're case, but it could be.", "title": "Change's Controversy"}, {"location": "git/#references", "text": "FAQ Funny FAQ Nvie post on branching model", "title": "References"}, {"location": "git/#courses", "text": "W3 git course Learngitbranching interactive tutorial katakoda Code academy Udemy Freecode camp article", "title": "Courses"}, {"location": "git/#tools", "text": "git-extras", "title": "Tools"}, {"location": "gitea/", "text": "Gitea is a community managed lightweight code hosting solution written in Go. It's the best self hosted Github alternative in my opinion. Installation \u2691 Gitea provides automatically updated Docker images within its Docker Hub organisation. Using Docker Compose \u2691 Connect with OICD \u2691 Gitea doesn't yet support the mapping of OICD groups to organizations . References \u2691 Home Docs", "title": "Gitea"}, {"location": "gitea/#installation", "text": "Gitea provides automatically updated Docker images within its Docker Hub organisation.", "title": "Installation"}, {"location": "gitea/#using-docker-compose", "text": "", "title": "Using Docker Compose"}, {"location": "gitea/#connect-with-oicd", "text": "Gitea doesn't yet support the mapping of OICD groups to organizations .", "title": "Connect with OICD"}, {"location": "gitea/#references", "text": "Home Docs", "title": "References"}, {"location": "goaccess/", "text": "goaccess is a fast terminal-based log analyzer. Its core idea is to quickly analyze and view web server statistics in real time without needing to use your browser (great if you want to do a quick analysis of your access log via SSH, or if you simply love working in the terminal). While the terminal output is the default output, it has the capability to generate a complete, self-contained real-time HTML report (great for analytics, monitoring and data visualization), as well as a JSON, and CSV report. Installation \u2691 apt-get install goaccess Usage \u2691 Custom log format \u2691 Sometimes the log format isn't supported, then you'll have to specify the log format. For example: goaccess \\ --log-format = '%^,%^,%^: %h:%^ %^ [%d:%t.%^] %^ %^/%^ %^/%^/%^/%^/%^ %s %b - - ---- %^/%^/%^/%^/%^ %^/%^ {%v|} %^ %m \"\"%U\"\" \"%q\"' \\ --date-format '%d/%b/%Y' \\ --time-format '%H:%M:%S' \\ file.log References \u2691 Home Git Docs Tweaking goaccess for analytics post", "title": "goaccess"}, {"location": "goaccess/#installation", "text": "apt-get install goaccess", "title": "Installation"}, {"location": "goaccess/#usage", "text": "", "title": "Usage"}, {"location": "goaccess/#custom-log-format", "text": "Sometimes the log format isn't supported, then you'll have to specify the log format. For example: goaccess \\ --log-format = '%^,%^,%^: %h:%^ %^ [%d:%t.%^] %^ %^/%^ %^/%^/%^/%^/%^ %s %b - - ---- %^/%^/%^/%^/%^ %^/%^ {%v|} %^ %m \"\"%U\"\" \"%q\"' \\ --date-format '%d/%b/%Y' \\ --time-format '%H:%M:%S' \\ file.log", "title": "Custom log format"}, {"location": "goaccess/#references", "text": "Home Git Docs Tweaking goaccess for analytics post", "title": "References"}, {"location": "goodconf/", "text": "goodconf is a thin wrapper over Pydantic's settings management. Allows you to define configuration variables and load them from environment or JSON/YAML file. Also generates initial configuration files and documentation for your defined configuration. Installation \u2691 pip install goodconf or pip install goodconf[yaml] if parsing/generating YAML files is required. Basic Usage \u2691 Define the configuration object in config.py : import base64 import os from goodconf import GoodConf , Field from pydantic import PostgresDsn class AppConfig ( GoodConf ): # type: ignore \"\"\"Configure my application.\"\"\" debug : bool database_url : PostgresDsn = \"postgres://localhost:5432/mydb\" secret_key : str = Field ( initial = lambda : base64 . b64encode ( os . urandom ( 60 )) . decode (), description = \"Used for cryptographic signing. \" \"https://docs.djangoproject.com/en/2.0/ref/settings/#secret-key\" , ) class Config : \"\"\"Define the default files to check.\"\"\" default_files = [ os . path . expanduser ( \"~/.local/share/your_program/config.yaml\" ), \"config.yaml\" , ] config = AppConfig () To load the configuration use config.load() . If you don't pass any file to load() , then the default_files will be read in order. Remember that environment variables always take precedence over variables in the configuration files. For more details see Pydantic's docs for examples of loading: Dotenv (.env) files . Docker secrets . References \u2691 Git", "title": "Goodconf"}, {"location": "goodconf/#installation", "text": "pip install goodconf or pip install goodconf[yaml] if parsing/generating YAML files is required.", "title": "Installation"}, {"location": "goodconf/#basic-usage", "text": "Define the configuration object in config.py : import base64 import os from goodconf import GoodConf , Field from pydantic import PostgresDsn class AppConfig ( GoodConf ): # type: ignore \"\"\"Configure my application.\"\"\" debug : bool database_url : PostgresDsn = \"postgres://localhost:5432/mydb\" secret_key : str = Field ( initial = lambda : base64 . b64encode ( os . urandom ( 60 )) . decode (), description = \"Used for cryptographic signing. \" \"https://docs.djangoproject.com/en/2.0/ref/settings/#secret-key\" , ) class Config : \"\"\"Define the default files to check.\"\"\" default_files = [ os . path . expanduser ( \"~/.local/share/your_program/config.yaml\" ), \"config.yaml\" , ] config = AppConfig () To load the configuration use config.load() . If you don't pass any file to load() , then the default_files will be read in order. Remember that environment variables always take precedence over variables in the configuration files. For more details see Pydantic's docs for examples of loading: Dotenv (.env) files . Docker secrets .", "title": "Basic Usage"}, {"location": "goodconf/#references", "text": "Git", "title": "References"}, {"location": "grapheneos/", "text": "GrapheneOS is a private and secure mobile operating system with Android app compatibility. Developed as a non-profit open source project. GrapheneOS is a private and secure mobile operating system with great functionality and usability. It starts from the strong baseline of the Android Open Source Project (AOSP) and takes great care to avoid increasing attack surface or hurting the strong security model. GrapheneOS makes substantial improvements to both privacy and security through many carefully designed features built to function against real adversaries. The project cares a lot about usability and app compatibility so those are taken into account for all of our features. GrapheneOS is also hard at work on filling in gaps from not bundling Google apps and services into the OS. We aren't against users using Google services but it doesn't belong integrated into the OS in an invasive way. GrapheneOS won't take the shortcut of simply bundling a very incomplete and poorly secured third party reimplementation of Google services into the OS. That wouldn't ever be something users could rely upon. It will also always be chasing a moving target while offering poorer security than the real thing if the focus is on simply getting things working without great care for doing it robustly and securely. Features \u2691 These are a subset some of the features of GrapheneOS beyond what's provided by version 13 of the Android Open Source Project. It only covers our improvements to AOSP and not baseline features. This section doesn't list features like the standard app sandbox, verified boot, exploit mitigations (ASLR, SSP, Shadow Call Stack, Control Flow Integrity, etc.), permission system (foreground-only and one-time permission grants, scoped file access control, etc.) and so on but rather only our improvements to modern Android. Defending against exploitation of unknown vulnerabilities \u2691 The first line of defense is attack surface reduction. Removing unnecessary code or exposed attack surface eliminates many vulnerabilities completely. GrapheneOS avoids removing any useful functionality for end users, but we can still disable lots of functionality by default and require that users opt-in to using it to eliminate it for most of them. An example we landed upstream in Android is disallowing using the kernel's profiling support by default, since it was and still is a major source of Linux kernel vulnerabilities. The next line of defense is preventing an attacker from exploiting a vulnerability, either by making it impossible, unreliable or at least meaningfully harder to develop. The vast majority of vulnerabilities are well understood classes of bugs and exploitation can be prevented by avoiding the bugs via languages/tooling or preventing exploitation with strong exploit mitigations. In many cases, vulnerability classes can be completely wiped out while in many others they can at least be made meaningfully harder to exploit. Android does a lot of work in this area and GrapheneOS has helped to advance this in Android and the Linux kernel. The final line of defense is containment through sandboxing at various levels: fine-grained sandboxes around a specific context like per site browser renderers, sandboxes around a specific component like Android's media codec sandbox and app / workspace sandboxes like the Android app sandbox used to sandbox each app which is also the basis for user/work profiles. GrapheneOS improves all of these sandboxes through fortifying the kernel and other base OS components along with improving the sandboxing policies. Preventing an attacker from persisting their control of a component or the OS / firmware through verified boot and avoiding trust in persistent state also helps to mitigate the damage after a compromise has occurred. Attack surface reduction \u2691 Greatly reduced remote, local and proximity-based attack surface by stripping out unnecessary code, making more features optional and disabling optional features by default (NFC, Bluetooth, etc.), when the screen is locked (connecting new USB peripherals, camera access) and optionally after a timeout (Bluetooth, Wi-Fi) Option to disable native debugging (ptrace) to reduce local attack surface (still enabled by default for compatibility) Downsides \u2691 It looks that the community behind GrapheneOS is not the kindest one, they are sometimes harsh and when they are questioned they enter a defensive position. This can be seen in the discussions regarding whether or not to use the screen pattern lock ( 1 , 2 ). Recommended devices \u2691 They strongly recommend only purchasing one of the following devices for GrapheneOS due to better security and a minimum 5 year guarantee from launch for full security updates and other improvements: Pixel 7 Pro Pixel 7 Pixel 6a Pixel 6 Pro Pixel 6 !!! note \"Check the source as this section is probably outdated\" Newer devices have more of their 5 year minimum guarantee remaining but the actual support time may be longer than the minimum guarantee. The Pixel 7 and Pixel 7 Pro are all around improvements over the Pixel 6 and Pixel 6 Pro with a significantly better GPU and cellular radio along with an incremental CPU upgrade. The 7 th generation Pixels are far more similar to the previous generation than any prior Pixels. The Pixel 6 and Pixel 6 Pro are flagship phones with much nicer hardware than previous generation devices (cameras, CPU, GPU, display, battery). The cheaper Pixel 6 has extremely competitive pricing for the flagship level hardware especially with the guaranteed long term support. Pixel 6 Pro has 50% more memory (12GB instead of 8GB), a higher end screen, a 3 rd rear camera with 4x optical zoom and a higher end front camera. Both devices have the same SoC (CPU, GPU, etc.) and the same main + ultrawide rear cameras. The Pixel 6 is quite large and the Pixel 6 Pro is larger. The Pixel 6a is a budget device with the same 5 years of guaranteed full security support from launch as the flagship 6 th generation Pixels. It also has the same flagship SoC as the higher end devices, the same main rear and front cameras as the Pixel 5 and a rear wide angle lens matching the flagship 6 th generation Pixels. Compared to the 5 th generation Pixels, it has 5 years of full security support remaining instead of less than 2 years and the CPU is 2x faster. We strongly recommend buying the Pixel 6a rather than trying to get a deal with older generation devices. You'll be able to use the Pixel 6a much longer before it needs to be replaced due to lack of support. It's funny though that in the search for security and privacy you end up buying a Google device. If you also reached this thought, you're not alone . Summing up, the Pixel's are in fact the devices that are more secure and that potentially respect your privacy. Installation \u2691 I was not able to follow the web instructions so I had to follow the cli ones. Whenever I run a fastboot command it got stuck in < waiting for devices > , so I added the next rules on the udev configuration at /etc/udev/rules.d/51-android.rules SUBSYSTEM==\"usb\", ATTR{idVendor}==\"18d1\", ATTR{idProduct}==\"4ee7\", MODE=\"0600\", OWNER=\"myuser\" The idProduct and idVendor were deduced from lsusb . Then after a restart everything worked fine. Setup Auditor \u2691 Auditor provides attestation for GrapheneOS phones and the stock operating systems on a number of devices. It uses hardware security features to make sure that the firmware and operating system have not been downgraded or tampered with. Attestation can be done locally by pairing with another Android 8+ device or remotely using the remote attestation service. To make sure that your hardware and operating system is genuine, perform local attestation immediately after the device has been setup and prior to any internet connection. References \u2691 Home Articles Features", "title": "GrapheneOS"}, {"location": "grapheneos/#features", "text": "These are a subset some of the features of GrapheneOS beyond what's provided by version 13 of the Android Open Source Project. It only covers our improvements to AOSP and not baseline features. This section doesn't list features like the standard app sandbox, verified boot, exploit mitigations (ASLR, SSP, Shadow Call Stack, Control Flow Integrity, etc.), permission system (foreground-only and one-time permission grants, scoped file access control, etc.) and so on but rather only our improvements to modern Android.", "title": "Features"}, {"location": "grapheneos/#defending-against-exploitation-of-unknown-vulnerabilities", "text": "The first line of defense is attack surface reduction. Removing unnecessary code or exposed attack surface eliminates many vulnerabilities completely. GrapheneOS avoids removing any useful functionality for end users, but we can still disable lots of functionality by default and require that users opt-in to using it to eliminate it for most of them. An example we landed upstream in Android is disallowing using the kernel's profiling support by default, since it was and still is a major source of Linux kernel vulnerabilities. The next line of defense is preventing an attacker from exploiting a vulnerability, either by making it impossible, unreliable or at least meaningfully harder to develop. The vast majority of vulnerabilities are well understood classes of bugs and exploitation can be prevented by avoiding the bugs via languages/tooling or preventing exploitation with strong exploit mitigations. In many cases, vulnerability classes can be completely wiped out while in many others they can at least be made meaningfully harder to exploit. Android does a lot of work in this area and GrapheneOS has helped to advance this in Android and the Linux kernel. The final line of defense is containment through sandboxing at various levels: fine-grained sandboxes around a specific context like per site browser renderers, sandboxes around a specific component like Android's media codec sandbox and app / workspace sandboxes like the Android app sandbox used to sandbox each app which is also the basis for user/work profiles. GrapheneOS improves all of these sandboxes through fortifying the kernel and other base OS components along with improving the sandboxing policies. Preventing an attacker from persisting their control of a component or the OS / firmware through verified boot and avoiding trust in persistent state also helps to mitigate the damage after a compromise has occurred.", "title": "Defending against exploitation of unknown vulnerabilities"}, {"location": "grapheneos/#attack-surface-reduction", "text": "Greatly reduced remote, local and proximity-based attack surface by stripping out unnecessary code, making more features optional and disabling optional features by default (NFC, Bluetooth, etc.), when the screen is locked (connecting new USB peripherals, camera access) and optionally after a timeout (Bluetooth, Wi-Fi) Option to disable native debugging (ptrace) to reduce local attack surface (still enabled by default for compatibility)", "title": "Attack surface reduction"}, {"location": "grapheneos/#downsides", "text": "It looks that the community behind GrapheneOS is not the kindest one, they are sometimes harsh and when they are questioned they enter a defensive position. This can be seen in the discussions regarding whether or not to use the screen pattern lock ( 1 , 2 ).", "title": "Downsides"}, {"location": "grapheneos/#recommended-devices", "text": "They strongly recommend only purchasing one of the following devices for GrapheneOS due to better security and a minimum 5 year guarantee from launch for full security updates and other improvements: Pixel 7 Pro Pixel 7 Pixel 6a Pixel 6 Pro Pixel 6 !!! note \"Check the source as this section is probably outdated\" Newer devices have more of their 5 year minimum guarantee remaining but the actual support time may be longer than the minimum guarantee. The Pixel 7 and Pixel 7 Pro are all around improvements over the Pixel 6 and Pixel 6 Pro with a significantly better GPU and cellular radio along with an incremental CPU upgrade. The 7 th generation Pixels are far more similar to the previous generation than any prior Pixels. The Pixel 6 and Pixel 6 Pro are flagship phones with much nicer hardware than previous generation devices (cameras, CPU, GPU, display, battery). The cheaper Pixel 6 has extremely competitive pricing for the flagship level hardware especially with the guaranteed long term support. Pixel 6 Pro has 50% more memory (12GB instead of 8GB), a higher end screen, a 3 rd rear camera with 4x optical zoom and a higher end front camera. Both devices have the same SoC (CPU, GPU, etc.) and the same main + ultrawide rear cameras. The Pixel 6 is quite large and the Pixel 6 Pro is larger. The Pixel 6a is a budget device with the same 5 years of guaranteed full security support from launch as the flagship 6 th generation Pixels. It also has the same flagship SoC as the higher end devices, the same main rear and front cameras as the Pixel 5 and a rear wide angle lens matching the flagship 6 th generation Pixels. Compared to the 5 th generation Pixels, it has 5 years of full security support remaining instead of less than 2 years and the CPU is 2x faster. We strongly recommend buying the Pixel 6a rather than trying to get a deal with older generation devices. You'll be able to use the Pixel 6a much longer before it needs to be replaced due to lack of support. It's funny though that in the search for security and privacy you end up buying a Google device. If you also reached this thought, you're not alone . Summing up, the Pixel's are in fact the devices that are more secure and that potentially respect your privacy.", "title": "Recommended devices"}, {"location": "grapheneos/#installation", "text": "I was not able to follow the web instructions so I had to follow the cli ones. Whenever I run a fastboot command it got stuck in < waiting for devices > , so I added the next rules on the udev configuration at /etc/udev/rules.d/51-android.rules SUBSYSTEM==\"usb\", ATTR{idVendor}==\"18d1\", ATTR{idProduct}==\"4ee7\", MODE=\"0600\", OWNER=\"myuser\" The idProduct and idVendor were deduced from lsusb . Then after a restart everything worked fine.", "title": "Installation"}, {"location": "grapheneos/#setup-auditor", "text": "Auditor provides attestation for GrapheneOS phones and the stock operating systems on a number of devices. It uses hardware security features to make sure that the firmware and operating system have not been downgraded or tampered with. Attestation can be done locally by pairing with another Android 8+ device or remotely using the remote attestation service. To make sure that your hardware and operating system is genuine, perform local attestation immediately after the device has been setup and prior to any internet connection.", "title": "Setup Auditor"}, {"location": "grapheneos/#references", "text": "Home Articles Features", "title": "References"}, {"location": "graylog/", "text": "Graylog is a log management tool Tips \u2691 Send a test message to check an input \u2691 The next line will send a test message to the TCP 12201 port of the graylog server, if you use UDP, add the -u flag to the nc command. echo -e '{\"version\": \"1.1\",\"host\":\"example.org\",\"short_message\":\"Short message\",\"full_message\":\"Backtrace here\\n\\nmore stuff\",\"level\":1,\"_user_id\":9001,\"_some_info\":\"foo\",\"_some_env_var\":\"bar\"}\\0' | nc -w 1 my.graylog.server 12201 To see if it arrives, you can check the Input you're trying to access, or at a lower level, you can ngrep with: ngrep -d any port 12201 Or if you're using UDP: ngrep -d any '' udp port 12201 References \u2691 Homepage", "title": "Graylog"}, {"location": "graylog/#tips", "text": "", "title": "Tips"}, {"location": "graylog/#send-a-test-message-to-check-an-input", "text": "The next line will send a test message to the TCP 12201 port of the graylog server, if you use UDP, add the -u flag to the nc command. echo -e '{\"version\": \"1.1\",\"host\":\"example.org\",\"short_message\":\"Short message\",\"full_message\":\"Backtrace here\\n\\nmore stuff\",\"level\":1,\"_user_id\":9001,\"_some_info\":\"foo\",\"_some_env_var\":\"bar\"}\\0' | nc -w 1 my.graylog.server 12201 To see if it arrives, you can check the Input you're trying to access, or at a lower level, you can ngrep with: ngrep -d any port 12201 Or if you're using UDP: ngrep -d any '' udp port 12201", "title": "Send a test message to check an input"}, {"location": "graylog/#references", "text": "Homepage", "title": "References"}, {"location": "grocy_management/", "text": "Buying stuff is an unpleasant activity that drains your energy and time, it's the main perpetrator of the broken capitalist system, but sadly we have to yield to survive. This article explores my thoughts and findings on how to optimize the use of time, money and mental load in grocy management to have enough stuff stored to live, while following the principles of ecology and sustainability. I'm no expert at all on either of these topics. I'm learning and making my mind while writing these lines. grocy is a web-based self-hosted groceries & household management solution for your home. My chosen way to deploy grocy has been using Docker . The hard part comes when you do the initial load, as you have to add all the: User attributes. Product locations. Product groups. Quantity conversions. Products. Tips \u2691 Note Very recommended to use the android app Add first the products with less letters, so add first Toothpaste and then Toothpaste roommate . Do the filling in iterations: Add the common products: this can be done with the ticket of the last groceries, or manually inspecting all the elements in your home. Incrementally add the recipes that you use Add the barcodes in the products that make sense. Add the score and shop userfields for the products, so you can evaluate how much you like the product and where to buy it. If you show them in the columns, you can also filter the shopping list by shop. Minimum quantities \u2691 The minimum quantity defines when does the product is going to be added to the shopping list, it must be enough so we have time to go to the shop to buy more, so it has to follow: minimum quantity = max_shop_frequency * average_consumption_rate * security_factor Where: * max_shop_frequency : is the maximum number of days between I visit the shop where I can obtain that product. If the product can be obtained in several shops we'll take the smallest number of days. * average_consumption_rate : is the average number of units consumed per day. It can be calculated by the following equation: average_consumption_rate = total_units_consumed / days_since_first_unit_bought The calculation could be improved giving more weight to the recent consumption against the overall trend. security_factor : Is an amount to add to take into account the imprecisions on the measures. A starting security_factor could be 1.2. But we won't have most of the required data when we start from scratch, therefore I've followed the next criteria: If the product is critical, I want to always have at least a spare one, so the minimum quantity will be 2. I roughly evaluate the relationship between the average_consumption_rate and the max_shop_frequency . Also, I usually have a recipient for the critical products, so I mark the product as consumed once I transfer it from the original recipient to my recipient. Therefore I always have a security factor. This also helps to reduce the management time. For example, for the fruit, instead of marking as consumed each time I eat a piece, I mark them as consumed when I move them from the fridge to a recipient I've got in the living room. Parent products \u2691 Parent products let you group different kind of products under the same roof. The idea is to set the minimum quantity in the parent product and it will inherit all the quantities of it's children. I've used parent products for example to set a minimum amount of red tea, while storing the different red teas in different products. The advantage of this approach is that you have a detailed product page for each kind of product. This allows you to have different purchase - storage ratio, price evolution, set score and description for the different kinds, set different store... The disadvantage is that you have to add and maintain additional products. So if you expect that the difference between products is relevant split them, if you don't start with one product that aggregates all, like chocolate bar for all kinds of chocolate bars, and maybe in the future refactor it to a parent and child products. Another good use case is if the different brands of a product sell different sizes, so the conversion from buy unit to storage unit is different. Then I'll use a parent product that specifies the minimum and the different sub products with the different conversion rate. On the units \u2691 I've been uncertain on what units use on some products. Imagine you buy a jar of peas, should you use jar or grams? or a bottle of wine should be in bottles or milliliters? The rule of thumb I've been using is: If the product is going to be used in a recipe, use whatever measure the recipe is going to use. For example, grams for the peas. If not, use whatever will cost you less management time. For example, milliliters for the wine (so I only have to update the inventory when the bottle is gone). Future ideas \u2691 I could monitor the ratio of rotting and when a product gets below the minimum stock to optimize the units to buy above the minimum quantity so as to minimize the shopping frequency. It can be saved in the max_amount user field. To calculate it's use I can use the average shelf life, last purchased and last used specified in the product information TODO \u2691 Define the userfields I've used Define the workflow for : initial upload purchase consumption cooking How to interact with humans that don't use the system but live in the same space Unclassified \u2691 When creating a child product, copy the parent buy and stock units and conversion, also the expiration till it's solved the child creation or duplication (search issue) Use of pieza de fruta to monitor the amount instead of per product Caja de pa\u00f1uelos solo se cuentan los que est\u00e1n encima de la nevera La avena he apuntado lo que implica el rellenar el bote para consumir solo cuando lo rellene. Locations are going to be used when you review the inventory so make sure you don't have to walk far Tare weight not supported with transfer it makes no sense to ask for the Location sheet to be editable, you've got the stock overview for that. If you want to consume do so, if you want to add you need to enter information one by one so you can't do it in a batch. If you want only to check if an ingredient exist but don't want to consume it select Only check if a single unit is in stock (a different quantity can then be used above) . Marcar bayetas como abiertas para recordarte que tienes que cambiarla Common userfields should go together Acelga o lechuga dificil de medir por raciones o piezas, tirar de gramos Use stock units accordingly on how you consume them. 1 ration = \u00bd lemon, and adjust the recipes accordingly. For example the acelgas are saved as pieces, lettuce, as two rations per piece, spinach bought as kg and saved as rations Important to specify the location, as you'll use it later for the inventory review IF you don't know the rations per kilogram, use kilograms till you know it. Buy unit the one you are going to encounter in the supermarket, both to input in purchase and to see the evolution of price. In the shops only put the ones you want to buy to, even if in others the product is available Things like the spices add them to recipes without consuming stock, and once you see you are low on the spice consume the rations In the things that are so light that 0.01 means a lot, change the buying unit to the equivalent x1000, even if you have to use other unit that is not the buying unit (species case) When you don't still have the complete inventory and you are cooking with someone, annotate in a paper the recipe or at least the elements it needs and afterwards transfer them to grocy. Evaluate the use of sublocations in grocy, like Freezer:Drawer 1. For products that are in two places, (fregadero and stock), consume the stock one instead of consuming the other and transfering the product. Adapt the due days of the fresh products that don't have it. If you hit enter in any field it commits the product (product description, purchase) Issues \u2691 Standard consumption location : Change it in the products that get consumed elsewhere. Allow stock modifications from the location content sheet page : Nothing to do, start using it. He closed them as duplicate of 1 , 2 and 3 . Resources \u2691 Homepage", "title": "Grocy Management"}, {"location": "grocy_management/#tips", "text": "Note Very recommended to use the android app Add first the products with less letters, so add first Toothpaste and then Toothpaste roommate . Do the filling in iterations: Add the common products: this can be done with the ticket of the last groceries, or manually inspecting all the elements in your home. Incrementally add the recipes that you use Add the barcodes in the products that make sense. Add the score and shop userfields for the products, so you can evaluate how much you like the product and where to buy it. If you show them in the columns, you can also filter the shopping list by shop.", "title": "Tips"}, {"location": "grocy_management/#minimum-quantities", "text": "The minimum quantity defines when does the product is going to be added to the shopping list, it must be enough so we have time to go to the shop to buy more, so it has to follow: minimum quantity = max_shop_frequency * average_consumption_rate * security_factor Where: * max_shop_frequency : is the maximum number of days between I visit the shop where I can obtain that product. If the product can be obtained in several shops we'll take the smallest number of days. * average_consumption_rate : is the average number of units consumed per day. It can be calculated by the following equation: average_consumption_rate = total_units_consumed / days_since_first_unit_bought The calculation could be improved giving more weight to the recent consumption against the overall trend. security_factor : Is an amount to add to take into account the imprecisions on the measures. A starting security_factor could be 1.2. But we won't have most of the required data when we start from scratch, therefore I've followed the next criteria: If the product is critical, I want to always have at least a spare one, so the minimum quantity will be 2. I roughly evaluate the relationship between the average_consumption_rate and the max_shop_frequency . Also, I usually have a recipient for the critical products, so I mark the product as consumed once I transfer it from the original recipient to my recipient. Therefore I always have a security factor. This also helps to reduce the management time. For example, for the fruit, instead of marking as consumed each time I eat a piece, I mark them as consumed when I move them from the fridge to a recipient I've got in the living room.", "title": "Minimum quantities"}, {"location": "grocy_management/#parent-products", "text": "Parent products let you group different kind of products under the same roof. The idea is to set the minimum quantity in the parent product and it will inherit all the quantities of it's children. I've used parent products for example to set a minimum amount of red tea, while storing the different red teas in different products. The advantage of this approach is that you have a detailed product page for each kind of product. This allows you to have different purchase - storage ratio, price evolution, set score and description for the different kinds, set different store... The disadvantage is that you have to add and maintain additional products. So if you expect that the difference between products is relevant split them, if you don't start with one product that aggregates all, like chocolate bar for all kinds of chocolate bars, and maybe in the future refactor it to a parent and child products. Another good use case is if the different brands of a product sell different sizes, so the conversion from buy unit to storage unit is different. Then I'll use a parent product that specifies the minimum and the different sub products with the different conversion rate.", "title": "Parent products"}, {"location": "grocy_management/#on-the-units", "text": "I've been uncertain on what units use on some products. Imagine you buy a jar of peas, should you use jar or grams? or a bottle of wine should be in bottles or milliliters? The rule of thumb I've been using is: If the product is going to be used in a recipe, use whatever measure the recipe is going to use. For example, grams for the peas. If not, use whatever will cost you less management time. For example, milliliters for the wine (so I only have to update the inventory when the bottle is gone).", "title": "On the units"}, {"location": "grocy_management/#future-ideas", "text": "I could monitor the ratio of rotting and when a product gets below the minimum stock to optimize the units to buy above the minimum quantity so as to minimize the shopping frequency. It can be saved in the max_amount user field. To calculate it's use I can use the average shelf life, last purchased and last used specified in the product information", "title": "Future ideas"}, {"location": "grocy_management/#todo", "text": "Define the userfields I've used Define the workflow for : initial upload purchase consumption cooking How to interact with humans that don't use the system but live in the same space", "title": "TODO"}, {"location": "grocy_management/#unclassified", "text": "When creating a child product, copy the parent buy and stock units and conversion, also the expiration till it's solved the child creation or duplication (search issue) Use of pieza de fruta to monitor the amount instead of per product Caja de pa\u00f1uelos solo se cuentan los que est\u00e1n encima de la nevera La avena he apuntado lo que implica el rellenar el bote para consumir solo cuando lo rellene. Locations are going to be used when you review the inventory so make sure you don't have to walk far Tare weight not supported with transfer it makes no sense to ask for the Location sheet to be editable, you've got the stock overview for that. If you want to consume do so, if you want to add you need to enter information one by one so you can't do it in a batch. If you want only to check if an ingredient exist but don't want to consume it select Only check if a single unit is in stock (a different quantity can then be used above) . Marcar bayetas como abiertas para recordarte que tienes que cambiarla Common userfields should go together Acelga o lechuga dificil de medir por raciones o piezas, tirar de gramos Use stock units accordingly on how you consume them. 1 ration = \u00bd lemon, and adjust the recipes accordingly. For example the acelgas are saved as pieces, lettuce, as two rations per piece, spinach bought as kg and saved as rations Important to specify the location, as you'll use it later for the inventory review IF you don't know the rations per kilogram, use kilograms till you know it. Buy unit the one you are going to encounter in the supermarket, both to input in purchase and to see the evolution of price. In the shops only put the ones you want to buy to, even if in others the product is available Things like the spices add them to recipes without consuming stock, and once you see you are low on the spice consume the rations In the things that are so light that 0.01 means a lot, change the buying unit to the equivalent x1000, even if you have to use other unit that is not the buying unit (species case) When you don't still have the complete inventory and you are cooking with someone, annotate in a paper the recipe or at least the elements it needs and afterwards transfer them to grocy. Evaluate the use of sublocations in grocy, like Freezer:Drawer 1. For products that are in two places, (fregadero and stock), consume the stock one instead of consuming the other and transfering the product. Adapt the due days of the fresh products that don't have it. If you hit enter in any field it commits the product (product description, purchase)", "title": "Unclassified"}, {"location": "grocy_management/#issues", "text": "Standard consumption location : Change it in the products that get consumed elsewhere. Allow stock modifications from the location content sheet page : Nothing to do, start using it. He closed them as duplicate of 1 , 2 and 3 .", "title": "Issues"}, {"location": "grocy_management/#resources", "text": "Homepage", "title": "Resources"}, {"location": "hard_drive_health/", "text": "Hard drives die, so we must be ready for that to happen. There are several solutions, such as using RAID to minimize the impact of a disk loss, but even then, we should monitor the bad sectors to see when are our disks dying. S.M.A.R.T (Self-Monitoring, Analysis and Reporting Technology; often written as SMART) is a monitoring system included in computer hard disk drives (HDDs), solid-state drives (SSDs), and eMMC drives. Its primary function is to detect and report various indicators of drive reliability with the intent of anticipating imminent hardware failures. Between all the SMART attributes, some that define define the health status of the hard drive, such as: Reallocated Sectors Count : Count of reallocated sectors. The raw value represents a count of the bad sectors that have been found and remapped. Thus, the higher the attribute value, the more sectors the drive has had to reallocate. This value is primarily used as a metric of the life expectancy of the drive; a drive which has had any reallocations at all is significantly more likely to fail in the immediate months. Spin Retry Count : Count of retry of spin start attempts. This attribute stores a total count of the spin start attempts to reach the fully operational speed (under the condition that the first attempt was unsuccessful). An increase of this attribute value is a sign of problems in the hard disk mechanical subsystem. Reallocate Event Count : Count of remap operations. The raw value of this attribute shows the total count of attempts to transfer data from reallocated sectors to a spare area. Both successful and unsuccessful attempts are counted. Current Pending Sector Count : Count of \"unstable\" sectors (waiting to be remapped, because of unrecoverable read errors). If an unstable sector is subsequently read successfully, the sector is remapped and this value is decreased. Read errors on a sector will not remap the sector immediately (since the correct value cannot be read and so the value to remap is not known, and also it might become readable later); instead, the drive firmware remembers that the sector needs to be remapped, and will remap it the next time it's written. However, some drives will not immediately remap such sectors when written; instead the drive will first attempt to write to the problem sector and if the write operation is successful then the sector will be marked good (in this case, the \"Reallocation Event Count\" (0xC4) will not be increased). This is a serious shortcoming, for if such a drive contains marginal sectors that consistently fail only after some time has passed following a successful write operation, then the drive will never remap these problem sectors. * Offline Uncorrectable Sector Count : The total count of uncorrectable errors when reading/writing a sector. A rise in the value of this attribute indicates defects of the disk surface and/or problems in the mechanical subsystem. Check the warranty status \u2691 If your drive is still under warranty from the manufacturer you may consider RMA\u2019ing the drive (initiating a warranty return process). Seagate Warranty Check Western Digital (WD) Warranty Check HGST Warranty Check Toshiba Warranty Check Wipe all the disk \u2691 Sometimes the CurrentPendingSector doesn't get reallocated, if you don't mind about the data in the disk, you can wipe it all with: dd if = /dev/zero of = /dev/ {{ disk_id }} bs = 4096 status = progress Troubleshooting \u2691 SMART error (CurrentPendingSector) detected on host \u2691 As stated above, this means that at some point, the drive was unable to successfully read the data from X different sectors, and hence have flagged them for possible reallocation. The sector will be marked as reallocated if a subsequent write fails. If the write succeeds, it is removed from current pending sectors and assumed to be OK. Start with a long self test with smartctl . Assuming the disk to test is /dev/sdd : smartctl -t long /dev/sdd The command will respond with an estimate of how long it thinks the test will take to complete. (But this assumes that no errors will be found!) To check progress use: smartctl -A /dev/sdd | grep remaining # or smartctl -c /dev/sdd | grep remaining Don't check too often because it can abort the test with some drives. If you receive an empty output, examine the reported status with: smartctl -l selftest /dev/sdd You will see something like this: === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime ( hours ) LBA_of_first_error # 1 Extended offline Completed: read failure 20% 1596 44724966 So take that 'LBA' of 44724966 and multiply by (512/4096) which is the equivalent of 'divide by 8' 44724966 / 8 = 5590620 .75 The sector to test then is 5590620 . If it is in the middle of a file, overwritting it will corrupt the file. If you are not cool with that, check the following posts to check if that sector belongs to a file: Smartmontools and fixing Unreadable Disk Sectors . Smartmontools Bad Block how to Archlinux Identify damaged files page Archlinux badblocks page If you don't care to corrupt the file, use the following command to 'zero-out' the sector: dd if = /dev/zero of = /dev/sda conv = sync bs = 4096 count = 1 seek = 5590620 1 +0 records in 1 +0 records out sync Now retry the smartctl -t short (or smartctl -t long if short fails) and see if the test is able to finish the test without errors: === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime(hours) LBA_of_first_error # 1 Short offline Completed without error 00% 11699 - # 2 Extended offline Completed: read failure 90% 11680 65344288 # 3 Extended offline Completed: read failure 90% 11675 65344288 If reading errors remain, repeat the steps above until they don't or skip to the bad block analysis step . Current_Pending_Sector should be 0 now and the drive will probably be fine. As long as Reallocated_Sector_Ct is zero, you should be fine. Even a few reallocated sectors seems OK, but if that count starts to increment frequently, then that is a danger sign. To regularly keep a close eye on the counters use smartd to schedule daily tests. If Current_Pending_Sector is still not 0 , we need to do a deeper analysis on the bad blocks . Bad block analysis \u2691 The SMART long test gives no guarantee to find every error . To find them, we're going to use the badblocks tool instead. There is read-only mode (default) which is the least accurate. There is the destructive write-mode (-w option) which is the most accurate but takes longer and will (obviously) destroy all data on the drive, thus making it quite useless for matching sectors up to files. There is finally the non-destructive read-write mode which is probably as accurate as the destructive mode, with the only real downside that it is probably the slowest. However, if a drive is known to be failing then read-only mode is probably still the safest. Links \u2691 S.M.A.R.T Wikipedia article . linux-hardware SMART disk probes . Bad blocks \u2691 Smartmontools and fixing Unreadable Disk Sectors . Smartmontools Bad Block how to Archlinux Identify damaged files page Archlinux badblocks page Hard drive geek guide on reducing the current pending sector count . Hiddencode guide on how to check bad sectors Hiddencode guide on how to fix bad sectors", "title": "Hard drive health"}, {"location": "hard_drive_health/#check-the-warranty-status", "text": "If your drive is still under warranty from the manufacturer you may consider RMA\u2019ing the drive (initiating a warranty return process). Seagate Warranty Check Western Digital (WD) Warranty Check HGST Warranty Check Toshiba Warranty Check", "title": "Check the warranty status"}, {"location": "hard_drive_health/#wipe-all-the-disk", "text": "Sometimes the CurrentPendingSector doesn't get reallocated, if you don't mind about the data in the disk, you can wipe it all with: dd if = /dev/zero of = /dev/ {{ disk_id }} bs = 4096 status = progress", "title": "Wipe all the disk"}, {"location": "hard_drive_health/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "hard_drive_health/#smart-error-currentpendingsector-detected-on-host", "text": "As stated above, this means that at some point, the drive was unable to successfully read the data from X different sectors, and hence have flagged them for possible reallocation. The sector will be marked as reallocated if a subsequent write fails. If the write succeeds, it is removed from current pending sectors and assumed to be OK. Start with a long self test with smartctl . Assuming the disk to test is /dev/sdd : smartctl -t long /dev/sdd The command will respond with an estimate of how long it thinks the test will take to complete. (But this assumes that no errors will be found!) To check progress use: smartctl -A /dev/sdd | grep remaining # or smartctl -c /dev/sdd | grep remaining Don't check too often because it can abort the test with some drives. If you receive an empty output, examine the reported status with: smartctl -l selftest /dev/sdd You will see something like this: === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime ( hours ) LBA_of_first_error # 1 Extended offline Completed: read failure 20% 1596 44724966 So take that 'LBA' of 44724966 and multiply by (512/4096) which is the equivalent of 'divide by 8' 44724966 / 8 = 5590620 .75 The sector to test then is 5590620 . If it is in the middle of a file, overwritting it will corrupt the file. If you are not cool with that, check the following posts to check if that sector belongs to a file: Smartmontools and fixing Unreadable Disk Sectors . Smartmontools Bad Block how to Archlinux Identify damaged files page Archlinux badblocks page If you don't care to corrupt the file, use the following command to 'zero-out' the sector: dd if = /dev/zero of = /dev/sda conv = sync bs = 4096 count = 1 seek = 5590620 1 +0 records in 1 +0 records out sync Now retry the smartctl -t short (or smartctl -t long if short fails) and see if the test is able to finish the test without errors: === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime(hours) LBA_of_first_error # 1 Short offline Completed without error 00% 11699 - # 2 Extended offline Completed: read failure 90% 11680 65344288 # 3 Extended offline Completed: read failure 90% 11675 65344288 If reading errors remain, repeat the steps above until they don't or skip to the bad block analysis step . Current_Pending_Sector should be 0 now and the drive will probably be fine. As long as Reallocated_Sector_Ct is zero, you should be fine. Even a few reallocated sectors seems OK, but if that count starts to increment frequently, then that is a danger sign. To regularly keep a close eye on the counters use smartd to schedule daily tests. If Current_Pending_Sector is still not 0 , we need to do a deeper analysis on the bad blocks .", "title": "SMART error (CurrentPendingSector) detected on host"}, {"location": "hard_drive_health/#bad-block-analysis", "text": "The SMART long test gives no guarantee to find every error . To find them, we're going to use the badblocks tool instead. There is read-only mode (default) which is the least accurate. There is the destructive write-mode (-w option) which is the most accurate but takes longer and will (obviously) destroy all data on the drive, thus making it quite useless for matching sectors up to files. There is finally the non-destructive read-write mode which is probably as accurate as the destructive mode, with the only real downside that it is probably the slowest. However, if a drive is known to be failing then read-only mode is probably still the safest.", "title": "Bad block analysis"}, {"location": "hard_drive_health/#links", "text": "S.M.A.R.T Wikipedia article . linux-hardware SMART disk probes .", "title": "Links"}, {"location": "hard_drive_health/#bad-blocks", "text": "Smartmontools and fixing Unreadable Disk Sectors . Smartmontools Bad Block how to Archlinux Identify damaged files page Archlinux badblocks page Hard drive geek guide on reducing the current pending sector count . Hiddencode guide on how to check bad sectors Hiddencode guide on how to fix bad sectors", "title": "Bad blocks"}, {"location": "helm_git/", "text": "helm-git is a helm downloader plugin that provides GIT protocol support. This fits the following use cases: Need to keep charts private. Doesn't want to package charts before installing. Charts in a sub-path, or with another ref than master. Pull values files directly from (private) Git repository. Installation \u2691 helm plugin install https://github.com/aslafy-z/helm-git --version 0 .11.1 Usage \u2691 helm-git will package any chart that is not so you can directly reference paths to original charts. Here's the Git urls format, followed by examples: git+https://[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+ssh://git@[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+file://[path/to/repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+https://github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&sparse=0 git+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&sparse=1 git+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2 git+https://github.com/istio/istio@install/kubernetes/helm?ref=1.5.4&sparse=0&depupdate=0 Add your repository: helm repo add cert-manager git+https://github.com/jetstack/cert-manager@deploy/charts?ref = v0.6.2 You can use it as any other Helm chart repository. Try: $ helm search cert-manager NAME CHART VERSION APP VERSION DESCRIPTION cert-manager/cert-manager v0.6.6 v0.6.2 A Helm chart for cert-manager $ helm install cert-manager/cert-manager --version \"0.6.6\" Fetching also works: helm fetch cert-manager/cert-manager --version \"0.6.6\" helm fetch git+https://github.com/jetstack/cert-manager@deploy/charts/cert-manager-v0.6.2.tgz?ref = v0.6.2 References \u2691 Git", "title": "Helm Git"}, {"location": "helm_git/#installation", "text": "helm plugin install https://github.com/aslafy-z/helm-git --version 0 .11.1", "title": "Installation"}, {"location": "helm_git/#usage", "text": "helm-git will package any chart that is not so you can directly reference paths to original charts. Here's the Git urls format, followed by examples: git+https://[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+ssh://git@[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+file://[path/to/repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+https://github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&sparse=0 git+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&sparse=1 git+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2 git+https://github.com/istio/istio@install/kubernetes/helm?ref=1.5.4&sparse=0&depupdate=0 Add your repository: helm repo add cert-manager git+https://github.com/jetstack/cert-manager@deploy/charts?ref = v0.6.2 You can use it as any other Helm chart repository. Try: $ helm search cert-manager NAME CHART VERSION APP VERSION DESCRIPTION cert-manager/cert-manager v0.6.6 v0.6.2 A Helm chart for cert-manager $ helm install cert-manager/cert-manager --version \"0.6.6\" Fetching also works: helm fetch cert-manager/cert-manager --version \"0.6.6\" helm fetch git+https://github.com/jetstack/cert-manager@deploy/charts/cert-manager-v0.6.2.tgz?ref = v0.6.2", "title": "Usage"}, {"location": "helm_git/#references", "text": "Git", "title": "References"}, {"location": "html/", "text": "HTML is the standard markup language for Web pages. With HTML you can create your own Website. Document structure \u2691 All HTML documents must start with a document type declaration: <!DOCTYPE html> . The HTML document itself begins with <html> and ends with </html> . The visible part of the HTML document is between <body> and </body> . <!DOCTYPE html> < html > < body > < h1 > My First Heading </ h1 > < p > My first paragraph. </ p > </ body > </ html > HTML elements \u2691 Headings: <h1> to <h6> Paragraphs: <p>This is a paragraph.</p> . Links : <a href=\"https://www.w3schools.com\">This is a link</a> Images : <img src=\"w3schools.jpg\" alt=\"W3Schools.com\" width=\"104\" height=\"142\"> Line breaks: <br> , <hr> Comments: <!-- Write your comments here --> Code: <code> x = 5</code> Links \u2691 HTML links are hyperlinks. You can click on a link and jump to another document. The HTML <a> tag defines a hyperlink. It has the following syntax: < a href = \"url\" > link text </ a > The link text is the part that will be visible to the reader. Link attributes: href : indicates the link's destination. target : specifies where to open the linked document. It can have one of the following values: _self : (Default) Opens the document in the same window/tab as it was clicked. _blank : Opens the document in a new window or tab. _parent : Opens the document in the parent frame. _top : Opens the document in the full body of the window. Images \u2691 The HTML <img> tag is used to embed an image in a web page. Images are not technically inserted into a web page; images are linked to web pages. The <img> tag creates a holding space for the referenced image. The <img> tag is empty, it contains attributes only, and does not have a closing tag. The <img> tag has two required attributes: src : Specifies the path to the image. alt : Specifies an alternate text for the image shown if the user for some reason cannot view it. < img src = \"url\" alt = \"alternatetext\" > Other <img> attributes are: <style> : specify the width and height of an image. < img src = \"img_1.jpg\" alt = \"img_1\" style = \"width:500px;height:600px;\" > Even though you could use width and height , if you use the style attribute you prevent style sheets to change the size of images. <float> : let the image float to the right or to the left of a text. < p >< img src = \"smiley.gif\" alt = \"Smiley face\" style = \"float:right;width:42px;height:42px;\" > The image will float to the right of the text. </ p > < p >< img src = \"smiley.gif\" alt = \"Smiley face\" style = \"float:left;width:42px;height:42px;\" > The image will float to the left of the text. </ p > If you want to use an image as a link use: < a href = \"default.asp\" > < img src = \"smiley.gif\" alt = \"HTML tutorial\" style = \"width:42px;height:42px;\" > </ a > Lists \u2691 HTML lists allow web developers to group a set of related items in lists. Unordered lists: starts with the <ul> tag. Each list item starts with the <li> tag. The list items will be marked with bullets (small black circles) by default: < ul > < li > Coffee </ li > < li > Tea </ li > < li > Milk </ li > </ ul > * Ordered list: Starts with the <ol> tag. Each list item starts with the <li> tag. The list items will be marked with numbers by default: < ol > < li > Coffee </ li > < li > Tea </ li > < li > Milk </ li > </ ol > Tables \u2691 HTML tables allow web developers to arrange data into rows and columns. < table > < tr > < th > Company </ th > < th > Contact </ th > < th > Country </ th > </ tr > < tr > < td > Alfreds Futterkiste </ td > < td > Maria Anders </ td > < td > Germany </ td > </ tr > < tr > < td > Centro comercial Moctezuma </ td > < td > Francisco Chang </ td > < td > Mexico </ td > </ tr > </ table > Where: <th> : Defines the table headers <tr> : Defines the table rows <td> : Defines the table cells Blocks \u2691 A block-level element always starts on a new line, and the browsers automatically add some space (a margin) before and after the element. A block-level element always takes up the full width available (stretches out to the left and right as far as it can). An inline element does not start on a new line and only takes up as much width as necessary. <p> : defines a paragraph in an HTML document. <div> : defines a division or a section in an HTML document. It has no required attributes, but style, class and id are common. When used together with CSS, the <div> element can be used to style blocks of content: < div style = \"background-color:black;color:white;padding:20px;\" > < h2 > London </ h2 > < p > London is the capital city of England. It is the most populous city in the United Kingdom, with a metropolitan area of over 13 million inhabitants. </ p > </ div > * <span> : Is an inline container used to mark up a part of a text, or a part of a document. The <span> element has no required attributes, but style, class and id are common. When used together with CSS, the <span> element can be used to style parts of the text: < p > My mother has < span style = \"color:blue;font-weight:bold\" > blue </ span > eyes and my father has < span style = \"color:darkolivegreen;font-weight:bold\" > dark green </ span > eyes. </ p > Classes \u2691 The class attribute is often used to point to a class name in a style sheet. It can also be used by a JavaScript to access and manipulate elements with the specific class name. In the following example we have three <div> elements with a class attribute with the value of \"city\". All of the three <div> elements will be styled equally according to the .city style definition in the head section: <!DOCTYPE html> < html > < head > < style > . city { background-color : tomato ; color : white ; border : 2 px solid black ; margin : 20 px ; padding : 20 px ; } </ style > </ head > < body > < div class = \"city\" > < h2 > London </ h2 > < p > London is the capital of England. </ p > </ div > < div class = \"city\" > < h2 > Paris </ h2 > < p > Paris is the capital of France. </ p > </ div > < div class = \"city\" > < h2 > Tokyo </ h2 > < p > Tokyo is the capital of Japan. </ p > </ div > </ body > </ html > HTML elements can belong to more than one class. To define multiple classes, separate the class names with a space, e.g. <div class=\"city main\"> . The element will be styled according to all the classes specified. Javascript \u2691 The HTML <script> tag is used to define a client-side script (JavaScript). The <script> element either contains script statements, or it points to an external script file through the src attribute. Common uses for JavaScript are image manipulation, form validation, and dynamic changes of content. This JavaScript example writes \"Hello JavaScript!\" into an HTML element with id=\"demo\" : < script > document . getElementById ( \"demo\" ). innerHTML = \"Hello JavaScript!\" ; < /script> The HTML <noscript> tag defines an alternate content to be displayed to users that have disabled scripts in their browser or have a browser that doesn't support scripts: < script > document . getElementById ( \"demo\" ). innerHTML = \"Hello JavaScript!\" ; </ script > < noscript > Sorry, your browser does not support JavaScript! </ noscript > Head \u2691 The <head> element is a container for metadata (data about data) and is placed between the <html> tag and the <body> tag. HTML metadata is data about the HTML document. Metadata is not displayed. Metadata typically define the document title, character set, styles, scripts, and other meta information. It contains the next sections: <title> : defines the title of the document. The title must be text-only, and it is used to: define the title in the browser toolbar provide a title for the page when it is added to favorites display a title for the page in search engine-results < title > A Meaningful Page Title </ title > <style> : define style information for a single HTML page. < style > body { background-color : powderblue ;} h1 { color : red ;} p { color : blue ;} </ style > <link> : defines the relationship between the current document and an external resource. < link rel = \"stylesheet\" href = \"mystyle.css\" > <meta> : specify the character set, page description, keywords, author of the document, and viewport settings. It won't be displayed on the page, but are used by browsers (how to display content or reload page), by search engines (keywords), and other web services. For example: Define the character set used: <meta charset=\"UTF-8\"> . Define keywords for search engines: <meta name=\"keywords\" content=\"HTML, CSS, JavaScript\"> . Define a description of your web page: <meta name=\"description\" content=\"Free Web tutorials\"> . Define the author of a page: <meta name=\"author\" content=\"John Doe\"> . Refresh document every 30 seconds: <meta http-equiv=\"refresh\" content=\"30\"> . Setting the viewport . <script> : define client-side JavaScripts. < script > function myFunction () { document . getElementById ( \"demo\" ). innerHTML = \"Hello JavaScript!\" ; } </ script > <base> : specifies the base URL and/or target for all relative URLs in a page. The <base> tag must have either an href or a target attribute present, or both. < base href = \"https://www.w3schools.com/\" target = \"_blank\" > Favicon \u2691 A favicon image is displayed to the left of the page title in the browser tab. To add a favicon to your website, either save your favicon image to the root directory of your webserver, or create a folder in the root directory called images, and save your favicon image in this folder. A common name for a favicon image is \"favicon.ico\". Next, add a <link> element to your \"index.html\" file, after the <title> element, like this: <!DOCTYPE html> < html > < head > < title > My Page Title </ title > < link rel = \"icon\" type = \"image/x-icon\" href = \"/images/favicon.ico\" > </ head > < body > Styles \u2691 The HTML style attribute is used to add styles to an element, such as color, font, size, and more. < tagname style = \"property:value;\" > The property is a CSS property. The value is a CSS value. Formatting \u2691 Formatting elements were designed to display special types of text: <b> : Bold text. <strong> : Important text. <i> : Italic text. <em> : Emphasized text. <mark> : Marked text. <small> : Smaller text. <del> : Deleted text. <ins> : Inserted text. <sub> : Subscript text. <sup> : Superscript text. Layout \u2691 Websites often display content in multiple columns (like a magazine or a newspaper). HTML has several semantic elements that define the different parts of a web page: HTML5 Semantic Elements <header> : Defines a header for a document or a section. <nav> : Defines a set of navigation links. <section> : Defines a section in a document. <article> : Defines an independent, self-contained content. <aside> : Defines content aside from the content (like a sidebar). <footer> : Defines a footer for a document or a section. <details> : Defines additional details that the user can open and close on demand. <summary> : Defines a heading for the element. Layout elements \u2691 Section \u2691 A section is a thematic grouping of content, typically with a heading. Examples of where a <section> element can be used: Chapters Introduction News items Contact information < section > < h1 > WWF </ h1 > < p > The World Wide Fund for Nature (WWF) is an international organization working on issues regarding the conservation, research and restoration of the environment, formerly named the World Wildlife Fund. WWF was founded in 1961. </ p > </ section > < section > < h1 > WWF's Panda symbol </ h1 > < p > The Panda has become the symbol of WWF. The well-known panda logo of WWF originated from a panda named Chi Chi that was transferred from the Beijing Zoo to the London Zoo in the same year of the establishment of WWF. </ p > </ section > article \u2691 The <article> element specifies independent, self-contained content. An article should make sense on its own, and it should be possible to distribute it independently from the rest of the web site. Examples of where the <article> element can be used: Forum posts Blog posts User comments Product cards Newspaper articles < article > < h2 > Google Chrome </ h2 > < p > Google Chrome is a web browser developed by Google, released in 2008. Chrome is the world's most popular web browser today! </ p > </ article > < article > < h2 > Mozilla Firefox </ h2 > < p > Mozilla Firefox is an open-source web browser developed by Mozilla. Firefox has been the second most popular web browser since January, 2018. </ p > </ article > < article > < h2 > Microsoft Edge </ h2 > < p > Microsoft Edge is a web browser developed by Microsoft, released in 2015. Microsoft Edge replaced Internet Explorer. </ p > </ article > header \u2691 The <header> element represents a container for introductory content or a set of navigational links. A <header> element typically contains: one or more heading elements ( <h1> - <h6> ) logo or icon authorship information < article > < header > < h1 > What Does WWF Do? </ h1 > < p > WWF's mission: </ p > </ header > < p > WWF's mission is to stop the degradation of our planet's natural environment, and build a future in which humans live in harmony with nature. </ p > </ article > footer \u2691 The <footer> element defines a footer for a document or section. A <footer> element typically contains: authorship information copyright information contact information sitemap back to top links related documents < footer > < p > Author: Hege Refsnes </ p > < p >< a href = \"mailto:hege@example.com\" > hege@example.com </ a ></ p > </ footer > Layout Techniques \u2691 There are four different techniques to create multicolumn layouts. Each technique has its pros and cons: CSS framework CSS float property CSS flexbox CSS grid Frameworks \u2691 If you want to create your layout fast, you can use a CSS framework, like W3.CSS or Bootstrap . Float layout \u2691 It is common to do entire web layouts using the CSS float property. Float is easy to learn - you just need to remember how the float and clear properties work. Disadvantages: Floating elements are tied to the document flow, which may harm the flexibility. Flexbox layout \u2691 Use of flexbox ensures that elements behave predictably when the page layout must accommodate different screen sizes and different display devices. Grid layout \u2691 The CSS Grid Layout Module offers a grid-based layout system, with rows and columns, making it easier to design web pages without having to use floats and positioning. Responsive \u2691 Responsive web design is about creating web pages that look good on all devices. A responsive web design will automatically adjust for different screen sizes and viewports. Setting the viewport \u2691 To create a responsive website, add the following <meta> tag to all your web pages: < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" > This gives the browser instructions on how to control the page's dimensions and scaling. The width=device-width part sets the width of the page to follow the screen-width of the device (which will vary depending on the device). The initial-scale=1.0 part sets the initial zoom level when the page is first loaded by the browser. Responsive images \u2691 Using the max-width property: If the CSS max-width property is set to 100% , the image will be responsive and scale up and down, but never scale up to be larger than its original size: < img src = \"img_girl.jpg\" style = \"max-width:100%;height:auto;\" > Responsive text size \u2691 The text size can be set with a \"vw\" unit, which means the \"viewport width\". That way the text size will follow the size of the browser window: < h1 style = \"font-size:10vw\" > Hello World </ h1 > Viewport is the browser window size. 1vw = 1% of viewport width. If the viewport is 50cm wide, 1vw is 0.5cm. Media queries \u2691 In addition to resize text and images, it is also common to use media queries in responsive web pages. With media queries you can define completely different styles for different browser sizes. The next example will make the three div elements display horizontally on large screens and stacked vertically on small screens: < style > . left , . right { float : left ; width : 20 % ; /* The width is 20%, by default */ } . main { float : left ; width : 60 % ; /* The width is 60%, by default */ } /* Use a media query to add a breakpoint at 800px: */ @ media screen and ( max-width : 800px ) { . left , . main , . right { width : 100 % ; /* The width is 100%, when the viewport is 800px or smaller */ } } </ style > Code Style \u2691 Always declare the document type as the first line in your document. <!DOCTYPE html> Use lowercase element names: < body > < p > This is a paragraph. </ p > </ body > Close all HTML elements. Use lowercase attribute names Always quote attribute values Always Specify alt, width, and height for Images. Don't add spaces between equal signs: <link rel=\"stylesheet\" href=\"styles.css\"> Avoid Long Code Lines Do not add blank lines, spaces, or indentations without a reason. Use two spaces for indentation instead of tab Never Skip the <title> Element Always add the <html> , <head> and <body> tags. Always include the lang attribute inside the <html> tag <!DOCTYPE html> < html lang = \"en-us\" > </ html > * Set the character encoding: <meta charset=\"UTF-8\"> * Set the viewport . Tips \u2691 HTML beautifier \u2691 If you encounter html code that it's not well indented you can use html beautify . References \u2691 W3 tutorial", "title": "HTML"}, {"location": "html/#document-structure", "text": "All HTML documents must start with a document type declaration: <!DOCTYPE html> . The HTML document itself begins with <html> and ends with </html> . The visible part of the HTML document is between <body> and </body> . <!DOCTYPE html> < html > < body > < h1 > My First Heading </ h1 > < p > My first paragraph. </ p > </ body > </ html >", "title": "Document structure"}, {"location": "html/#html-elements", "text": "Headings: <h1> to <h6> Paragraphs: <p>This is a paragraph.</p> . Links : <a href=\"https://www.w3schools.com\">This is a link</a> Images : <img src=\"w3schools.jpg\" alt=\"W3Schools.com\" width=\"104\" height=\"142\"> Line breaks: <br> , <hr> Comments: <!-- Write your comments here --> Code: <code> x = 5</code>", "title": "HTML elements"}, {"location": "html/#links", "text": "HTML links are hyperlinks. You can click on a link and jump to another document. The HTML <a> tag defines a hyperlink. It has the following syntax: < a href = \"url\" > link text </ a > The link text is the part that will be visible to the reader. Link attributes: href : indicates the link's destination. target : specifies where to open the linked document. It can have one of the following values: _self : (Default) Opens the document in the same window/tab as it was clicked. _blank : Opens the document in a new window or tab. _parent : Opens the document in the parent frame. _top : Opens the document in the full body of the window.", "title": "Links"}, {"location": "html/#images", "text": "The HTML <img> tag is used to embed an image in a web page. Images are not technically inserted into a web page; images are linked to web pages. The <img> tag creates a holding space for the referenced image. The <img> tag is empty, it contains attributes only, and does not have a closing tag. The <img> tag has two required attributes: src : Specifies the path to the image. alt : Specifies an alternate text for the image shown if the user for some reason cannot view it. < img src = \"url\" alt = \"alternatetext\" > Other <img> attributes are: <style> : specify the width and height of an image. < img src = \"img_1.jpg\" alt = \"img_1\" style = \"width:500px;height:600px;\" > Even though you could use width and height , if you use the style attribute you prevent style sheets to change the size of images. <float> : let the image float to the right or to the left of a text. < p >< img src = \"smiley.gif\" alt = \"Smiley face\" style = \"float:right;width:42px;height:42px;\" > The image will float to the right of the text. </ p > < p >< img src = \"smiley.gif\" alt = \"Smiley face\" style = \"float:left;width:42px;height:42px;\" > The image will float to the left of the text. </ p > If you want to use an image as a link use: < a href = \"default.asp\" > < img src = \"smiley.gif\" alt = \"HTML tutorial\" style = \"width:42px;height:42px;\" > </ a >", "title": "Images"}, {"location": "html/#lists", "text": "HTML lists allow web developers to group a set of related items in lists. Unordered lists: starts with the <ul> tag. Each list item starts with the <li> tag. The list items will be marked with bullets (small black circles) by default: < ul > < li > Coffee </ li > < li > Tea </ li > < li > Milk </ li > </ ul > * Ordered list: Starts with the <ol> tag. Each list item starts with the <li> tag. The list items will be marked with numbers by default: < ol > < li > Coffee </ li > < li > Tea </ li > < li > Milk </ li > </ ol >", "title": "Lists"}, {"location": "html/#tables", "text": "HTML tables allow web developers to arrange data into rows and columns. < table > < tr > < th > Company </ th > < th > Contact </ th > < th > Country </ th > </ tr > < tr > < td > Alfreds Futterkiste </ td > < td > Maria Anders </ td > < td > Germany </ td > </ tr > < tr > < td > Centro comercial Moctezuma </ td > < td > Francisco Chang </ td > < td > Mexico </ td > </ tr > </ table > Where: <th> : Defines the table headers <tr> : Defines the table rows <td> : Defines the table cells", "title": "Tables"}, {"location": "html/#blocks", "text": "A block-level element always starts on a new line, and the browsers automatically add some space (a margin) before and after the element. A block-level element always takes up the full width available (stretches out to the left and right as far as it can). An inline element does not start on a new line and only takes up as much width as necessary. <p> : defines a paragraph in an HTML document. <div> : defines a division or a section in an HTML document. It has no required attributes, but style, class and id are common. When used together with CSS, the <div> element can be used to style blocks of content: < div style = \"background-color:black;color:white;padding:20px;\" > < h2 > London </ h2 > < p > London is the capital city of England. It is the most populous city in the United Kingdom, with a metropolitan area of over 13 million inhabitants. </ p > </ div > * <span> : Is an inline container used to mark up a part of a text, or a part of a document. The <span> element has no required attributes, but style, class and id are common. When used together with CSS, the <span> element can be used to style parts of the text: < p > My mother has < span style = \"color:blue;font-weight:bold\" > blue </ span > eyes and my father has < span style = \"color:darkolivegreen;font-weight:bold\" > dark green </ span > eyes. </ p >", "title": "Blocks"}, {"location": "html/#classes", "text": "The class attribute is often used to point to a class name in a style sheet. It can also be used by a JavaScript to access and manipulate elements with the specific class name. In the following example we have three <div> elements with a class attribute with the value of \"city\". All of the three <div> elements will be styled equally according to the .city style definition in the head section: <!DOCTYPE html> < html > < head > < style > . city { background-color : tomato ; color : white ; border : 2 px solid black ; margin : 20 px ; padding : 20 px ; } </ style > </ head > < body > < div class = \"city\" > < h2 > London </ h2 > < p > London is the capital of England. </ p > </ div > < div class = \"city\" > < h2 > Paris </ h2 > < p > Paris is the capital of France. </ p > </ div > < div class = \"city\" > < h2 > Tokyo </ h2 > < p > Tokyo is the capital of Japan. </ p > </ div > </ body > </ html > HTML elements can belong to more than one class. To define multiple classes, separate the class names with a space, e.g. <div class=\"city main\"> . The element will be styled according to all the classes specified.", "title": "Classes"}, {"location": "html/#javascript", "text": "The HTML <script> tag is used to define a client-side script (JavaScript). The <script> element either contains script statements, or it points to an external script file through the src attribute. Common uses for JavaScript are image manipulation, form validation, and dynamic changes of content. This JavaScript example writes \"Hello JavaScript!\" into an HTML element with id=\"demo\" : < script > document . getElementById ( \"demo\" ). innerHTML = \"Hello JavaScript!\" ; < /script> The HTML <noscript> tag defines an alternate content to be displayed to users that have disabled scripts in their browser or have a browser that doesn't support scripts: < script > document . getElementById ( \"demo\" ). innerHTML = \"Hello JavaScript!\" ; </ script > < noscript > Sorry, your browser does not support JavaScript! </ noscript >", "title": "Javascript"}, {"location": "html/#head", "text": "The <head> element is a container for metadata (data about data) and is placed between the <html> tag and the <body> tag. HTML metadata is data about the HTML document. Metadata is not displayed. Metadata typically define the document title, character set, styles, scripts, and other meta information. It contains the next sections: <title> : defines the title of the document. The title must be text-only, and it is used to: define the title in the browser toolbar provide a title for the page when it is added to favorites display a title for the page in search engine-results < title > A Meaningful Page Title </ title > <style> : define style information for a single HTML page. < style > body { background-color : powderblue ;} h1 { color : red ;} p { color : blue ;} </ style > <link> : defines the relationship between the current document and an external resource. < link rel = \"stylesheet\" href = \"mystyle.css\" > <meta> : specify the character set, page description, keywords, author of the document, and viewport settings. It won't be displayed on the page, but are used by browsers (how to display content or reload page), by search engines (keywords), and other web services. For example: Define the character set used: <meta charset=\"UTF-8\"> . Define keywords for search engines: <meta name=\"keywords\" content=\"HTML, CSS, JavaScript\"> . Define a description of your web page: <meta name=\"description\" content=\"Free Web tutorials\"> . Define the author of a page: <meta name=\"author\" content=\"John Doe\"> . Refresh document every 30 seconds: <meta http-equiv=\"refresh\" content=\"30\"> . Setting the viewport . <script> : define client-side JavaScripts. < script > function myFunction () { document . getElementById ( \"demo\" ). innerHTML = \"Hello JavaScript!\" ; } </ script > <base> : specifies the base URL and/or target for all relative URLs in a page. The <base> tag must have either an href or a target attribute present, or both. < base href = \"https://www.w3schools.com/\" target = \"_blank\" >", "title": "Head"}, {"location": "html/#favicon", "text": "A favicon image is displayed to the left of the page title in the browser tab. To add a favicon to your website, either save your favicon image to the root directory of your webserver, or create a folder in the root directory called images, and save your favicon image in this folder. A common name for a favicon image is \"favicon.ico\". Next, add a <link> element to your \"index.html\" file, after the <title> element, like this: <!DOCTYPE html> < html > < head > < title > My Page Title </ title > < link rel = \"icon\" type = \"image/x-icon\" href = \"/images/favicon.ico\" > </ head > < body >", "title": "Favicon"}, {"location": "html/#styles", "text": "The HTML style attribute is used to add styles to an element, such as color, font, size, and more. < tagname style = \"property:value;\" > The property is a CSS property. The value is a CSS value.", "title": "Styles"}, {"location": "html/#formatting", "text": "Formatting elements were designed to display special types of text: <b> : Bold text. <strong> : Important text. <i> : Italic text. <em> : Emphasized text. <mark> : Marked text. <small> : Smaller text. <del> : Deleted text. <ins> : Inserted text. <sub> : Subscript text. <sup> : Superscript text.", "title": "Formatting"}, {"location": "html/#layout", "text": "Websites often display content in multiple columns (like a magazine or a newspaper). HTML has several semantic elements that define the different parts of a web page: HTML5 Semantic Elements <header> : Defines a header for a document or a section. <nav> : Defines a set of navigation links. <section> : Defines a section in a document. <article> : Defines an independent, self-contained content. <aside> : Defines content aside from the content (like a sidebar). <footer> : Defines a footer for a document or a section. <details> : Defines additional details that the user can open and close on demand. <summary> : Defines a heading for the element.", "title": "Layout"}, {"location": "html/#layout-elements", "text": "", "title": "Layout elements"}, {"location": "html/#section", "text": "A section is a thematic grouping of content, typically with a heading. Examples of where a <section> element can be used: Chapters Introduction News items Contact information < section > < h1 > WWF </ h1 > < p > The World Wide Fund for Nature (WWF) is an international organization working on issues regarding the conservation, research and restoration of the environment, formerly named the World Wildlife Fund. WWF was founded in 1961. </ p > </ section > < section > < h1 > WWF's Panda symbol </ h1 > < p > The Panda has become the symbol of WWF. The well-known panda logo of WWF originated from a panda named Chi Chi that was transferred from the Beijing Zoo to the London Zoo in the same year of the establishment of WWF. </ p > </ section >", "title": "Section"}, {"location": "html/#article", "text": "The <article> element specifies independent, self-contained content. An article should make sense on its own, and it should be possible to distribute it independently from the rest of the web site. Examples of where the <article> element can be used: Forum posts Blog posts User comments Product cards Newspaper articles < article > < h2 > Google Chrome </ h2 > < p > Google Chrome is a web browser developed by Google, released in 2008. Chrome is the world's most popular web browser today! </ p > </ article > < article > < h2 > Mozilla Firefox </ h2 > < p > Mozilla Firefox is an open-source web browser developed by Mozilla. Firefox has been the second most popular web browser since January, 2018. </ p > </ article > < article > < h2 > Microsoft Edge </ h2 > < p > Microsoft Edge is a web browser developed by Microsoft, released in 2015. Microsoft Edge replaced Internet Explorer. </ p > </ article >", "title": "article"}, {"location": "html/#header", "text": "The <header> element represents a container for introductory content or a set of navigational links. A <header> element typically contains: one or more heading elements ( <h1> - <h6> ) logo or icon authorship information < article > < header > < h1 > What Does WWF Do? </ h1 > < p > WWF's mission: </ p > </ header > < p > WWF's mission is to stop the degradation of our planet's natural environment, and build a future in which humans live in harmony with nature. </ p > </ article >", "title": "header"}, {"location": "html/#footer", "text": "The <footer> element defines a footer for a document or section. A <footer> element typically contains: authorship information copyright information contact information sitemap back to top links related documents < footer > < p > Author: Hege Refsnes </ p > < p >< a href = \"mailto:hege@example.com\" > hege@example.com </ a ></ p > </ footer >", "title": "footer"}, {"location": "html/#layout-techniques", "text": "There are four different techniques to create multicolumn layouts. Each technique has its pros and cons: CSS framework CSS float property CSS flexbox CSS grid", "title": "Layout Techniques"}, {"location": "html/#frameworks", "text": "If you want to create your layout fast, you can use a CSS framework, like W3.CSS or Bootstrap .", "title": "Frameworks"}, {"location": "html/#float-layout", "text": "It is common to do entire web layouts using the CSS float property. Float is easy to learn - you just need to remember how the float and clear properties work. Disadvantages: Floating elements are tied to the document flow, which may harm the flexibility.", "title": "Float layout"}, {"location": "html/#flexbox-layout", "text": "Use of flexbox ensures that elements behave predictably when the page layout must accommodate different screen sizes and different display devices.", "title": "Flexbox layout"}, {"location": "html/#grid-layout", "text": "The CSS Grid Layout Module offers a grid-based layout system, with rows and columns, making it easier to design web pages without having to use floats and positioning.", "title": "Grid layout"}, {"location": "html/#responsive", "text": "Responsive web design is about creating web pages that look good on all devices. A responsive web design will automatically adjust for different screen sizes and viewports.", "title": "Responsive"}, {"location": "html/#setting-the-viewport", "text": "To create a responsive website, add the following <meta> tag to all your web pages: < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" > This gives the browser instructions on how to control the page's dimensions and scaling. The width=device-width part sets the width of the page to follow the screen-width of the device (which will vary depending on the device). The initial-scale=1.0 part sets the initial zoom level when the page is first loaded by the browser.", "title": "Setting the viewport"}, {"location": "html/#responsive-images", "text": "Using the max-width property: If the CSS max-width property is set to 100% , the image will be responsive and scale up and down, but never scale up to be larger than its original size: < img src = \"img_girl.jpg\" style = \"max-width:100%;height:auto;\" >", "title": "Responsive images"}, {"location": "html/#responsive-text-size", "text": "The text size can be set with a \"vw\" unit, which means the \"viewport width\". That way the text size will follow the size of the browser window: < h1 style = \"font-size:10vw\" > Hello World </ h1 > Viewport is the browser window size. 1vw = 1% of viewport width. If the viewport is 50cm wide, 1vw is 0.5cm.", "title": "Responsive text size"}, {"location": "html/#media-queries", "text": "In addition to resize text and images, it is also common to use media queries in responsive web pages. With media queries you can define completely different styles for different browser sizes. The next example will make the three div elements display horizontally on large screens and stacked vertically on small screens: < style > . left , . right { float : left ; width : 20 % ; /* The width is 20%, by default */ } . main { float : left ; width : 60 % ; /* The width is 60%, by default */ } /* Use a media query to add a breakpoint at 800px: */ @ media screen and ( max-width : 800px ) { . left , . main , . right { width : 100 % ; /* The width is 100%, when the viewport is 800px or smaller */ } } </ style >", "title": "Media queries"}, {"location": "html/#code-style", "text": "Always declare the document type as the first line in your document. <!DOCTYPE html> Use lowercase element names: < body > < p > This is a paragraph. </ p > </ body > Close all HTML elements. Use lowercase attribute names Always quote attribute values Always Specify alt, width, and height for Images. Don't add spaces between equal signs: <link rel=\"stylesheet\" href=\"styles.css\"> Avoid Long Code Lines Do not add blank lines, spaces, or indentations without a reason. Use two spaces for indentation instead of tab Never Skip the <title> Element Always add the <html> , <head> and <body> tags. Always include the lang attribute inside the <html> tag <!DOCTYPE html> < html lang = \"en-us\" > </ html > * Set the character encoding: <meta charset=\"UTF-8\"> * Set the viewport .", "title": "Code Style"}, {"location": "html/#tips", "text": "", "title": "Tips"}, {"location": "html/#html-beautifier", "text": "If you encounter html code that it's not well indented you can use html beautify .", "title": "HTML beautifier"}, {"location": "html/#references", "text": "W3 tutorial", "title": "References"}, {"location": "husboard/", "text": "Hushboard is an utility that mutes your microphone while you\u2019re typing. Installation \u2691 They recommend using the Snap Store package but you can also install it manually as follows: sudo apt install libgirepository1.0-dev libcairo2-dev mkvirtualenv hushboard git clone https://github.com/stuartlangridge/hushboard cd hushboard pip install pycairo PyGObject six xlib pip install . deactivate Running the application \u2691 You can run it manually as follows workon hushboard python -m hushboard deactivate Or if you use i3wm, create the following script. #!/usr/bin/env bash source { WORKON_PATH } /hushboard/bin/activate python -m hushboard deactivate You should replace {WORKON_PATH} with your virtual environments path. Then add this line to your i3wm configuration file to start it automatically. exec --no-startup-id ~/scripts/hushboard.sh Reference \u2691 M0wer Husboard article", "title": "Hushboard"}, {"location": "husboard/#installation", "text": "They recommend using the Snap Store package but you can also install it manually as follows: sudo apt install libgirepository1.0-dev libcairo2-dev mkvirtualenv hushboard git clone https://github.com/stuartlangridge/hushboard cd hushboard pip install pycairo PyGObject six xlib pip install . deactivate", "title": "Installation"}, {"location": "husboard/#running-the-application", "text": "You can run it manually as follows workon hushboard python -m hushboard deactivate Or if you use i3wm, create the following script. #!/usr/bin/env bash source { WORKON_PATH } /hushboard/bin/activate python -m hushboard deactivate You should replace {WORKON_PATH} with your virtual environments path. Then add this line to your i3wm configuration file to start it automatically. exec --no-startup-id ~/scripts/hushboard.sh", "title": "Running the application"}, {"location": "husboard/#reference", "text": "M0wer Husboard article", "title": "Reference"}, {"location": "i3wm/", "text": "i3 is a tiling window manager. Layout saving \u2691 Layout saving/restoring allows you to load a JSON layout file so that you can have a base layout to start working with after powering on your computer. First of all arrange the windows in the workspace, then you can save the layout of either a single workspace or an entire output: i3-save-tree --workspace \"1: terminal\" > ~/.i3/workspace-1.json You need to open the created file and remove the comments that match the desired windows under the swallows keys, so transform the next snippet: ... \"swallows\" : [ { // \"class\": \"^URxvt$\", // \"instance\": \"^irssi$\" } ] ... Into: ... \"swallows\" : [ { \"class\" : \"^URxvt$\" , \"instance\" : \"^irssi$\" } ] ... Once is ready close all the windows of the workspace you want to restore (moving them away is not enough!). Then on a terminal you can restore the layout with: i3-msg 'workspace \"1: terminal\"; append_layout ~/.i3/workspace-1.json' It's important that you don't use a relative path Even if you're in ~/.i3/ you have to use i3-msg append_layout ~/.i3/workspace-1.json . This command will create some fake windows (called placeholders) with the layout you had before, i3 will then wait for you to create the windows that match the selection criteria. Once they are, it will put them in their respective placeholders. If you wish to create the layouts at startup you can add the next snippet to your i3 config. exec --no-startup-id \"i3-msg 'workspace \\\"1: terminal\\\"; append_layout ~/.i3/workspace-1.json'\" References \u2691 Home", "title": "i3wm"}, {"location": "i3wm/#layout-saving", "text": "Layout saving/restoring allows you to load a JSON layout file so that you can have a base layout to start working with after powering on your computer. First of all arrange the windows in the workspace, then you can save the layout of either a single workspace or an entire output: i3-save-tree --workspace \"1: terminal\" > ~/.i3/workspace-1.json You need to open the created file and remove the comments that match the desired windows under the swallows keys, so transform the next snippet: ... \"swallows\" : [ { // \"class\": \"^URxvt$\", // \"instance\": \"^irssi$\" } ] ... Into: ... \"swallows\" : [ { \"class\" : \"^URxvt$\" , \"instance\" : \"^irssi$\" } ] ... Once is ready close all the windows of the workspace you want to restore (moving them away is not enough!). Then on a terminal you can restore the layout with: i3-msg 'workspace \"1: terminal\"; append_layout ~/.i3/workspace-1.json' It's important that you don't use a relative path Even if you're in ~/.i3/ you have to use i3-msg append_layout ~/.i3/workspace-1.json . This command will create some fake windows (called placeholders) with the layout you had before, i3 will then wait for you to create the windows that match the selection criteria. Once they are, it will put them in their respective placeholders. If you wish to create the layouts at startup you can add the next snippet to your i3 config. exec --no-startup-id \"i3-msg 'workspace \\\"1: terminal\\\"; append_layout ~/.i3/workspace-1.json'\"", "title": "Layout saving"}, {"location": "i3wm/#references", "text": "Home", "title": "References"}, {"location": "ics/", "text": "ics is a pythonic iCalendar library. Its goals are to read and write ics data in a developer-friendly way. Installation \u2691 Install using pip: pip install ics Usage \u2691 ics will delete all data that it doesn't understand. Maybe it's better for your case to build a parse for ics. Import a calendar from a file \u2691 file = '/tmp/event.ics' from ics import Calendar with open ( file , 'r' ) as fd : calendar = Calendar ( fd . read ()) # <Calendar with 118 events and 0 todo> calendar . events # {<Event 'Visite de \"Fab Bike\"' begin:2016-06-21T15:00:00+00:00 end:2016-06-21T17:00:00+00:00>, # <Event 'Le lundi de l'embarqu\u00e9: Adventure in Espressif Non OS SDK edition' begin:2018-02-19T17:00:00+00:00 end:2018-02-19T22:00:00+00:00>, # ...} event = list ( calendar . timeline )[ 0 ] Export a Calendar to a file \u2691 with open ( 'my.ics' , 'w' ) as f : f . writelines ( calendar . serialize_iter ()) # And it's done ! # iCalendar-formatted data is also available in a string calendar . serialize () # 'BEGIN:VCALENDAR\\nPRODID:... References \u2691 Docs", "title": "ICS"}, {"location": "ics/#installation", "text": "Install using pip: pip install ics", "title": "Installation"}, {"location": "ics/#usage", "text": "ics will delete all data that it doesn't understand. Maybe it's better for your case to build a parse for ics.", "title": "Usage"}, {"location": "ics/#import-a-calendar-from-a-file", "text": "file = '/tmp/event.ics' from ics import Calendar with open ( file , 'r' ) as fd : calendar = Calendar ( fd . read ()) # <Calendar with 118 events and 0 todo> calendar . events # {<Event 'Visite de \"Fab Bike\"' begin:2016-06-21T15:00:00+00:00 end:2016-06-21T17:00:00+00:00>, # <Event 'Le lundi de l'embarqu\u00e9: Adventure in Espressif Non OS SDK edition' begin:2018-02-19T17:00:00+00:00 end:2018-02-19T22:00:00+00:00>, # ...} event = list ( calendar . timeline )[ 0 ]", "title": "Import a calendar from a file"}, {"location": "ics/#export-a-calendar-to-a-file", "text": "with open ( 'my.ics' , 'w' ) as f : f . writelines ( calendar . serialize_iter ()) # And it's done ! # iCalendar-formatted data is also available in a string calendar . serialize () # 'BEGIN:VCALENDAR\\nPRODID:...", "title": "Export a Calendar to a file"}, {"location": "ics/#references", "text": "Docs", "title": "References"}, {"location": "instant_messages_management/", "text": "Instant messaging in all it's forms is becoming the main communication channel. As any other input system, if not used wisely, it can be a sink of productivity. Analyze how often you need to check it \u2691 Follow the interruption analysis to discover how often you need to check it and if you need the notifications or fine grain them to the sources that have higher priority. Once you've decided the frequency, try to respect it!. If you want an example, check my work or personal analysis. Workflow \u2691 I interact with messaging applications in two ways: To read the new items and answer questions. To start a conversation. The passively reading for new items works perfectly with the interruption management processes. Each time you decide to check for new messages, follow the inbox processing guidelines to extract the information to the appropriate system (task manager, calendar or knowledge manager). If you answer someone or if you start a new conversation, assume that any work done in the next 5 to 10 minutes will probably be interrupted, so choose small or mindless tasks. If the person doesn't answer in that time, start a new pomodoro and go back when the next interruption event comes. Use calls for non short conversations \u2691 Chats are good for short conversations that don't require long or quick responses. Even though people may have forgotten it, they are an asynchronous communication channel. They're not suited for long conversations though as: Typing on a keyboard (or a mobile \u1559(\u21c0\u2038\u21bc\u2036)\u1557 ) is slower than talking directly. It's difficult to transmit the conversation tone by message, and each reader can interpret it differently, leading to misunderstandings. If the conversation topic is complex, graphical aids such as screen sharing or doodling can make the conversation more efficient. Unless everyone involved is fully focused on the conversation, the delays between messages can be high, and all that time, the attendees need to manage the interruptions. If you fully focus on the conversation, you're loosing your time while you wait for the other to answer. For all these reasons, whenever a conversation looks not to be short or trivial, arrange a quick call or video call. At work or collectives, use group rooms over direct messages \u2691 Asking for help through direct messages should be avoided whenever possible, instead of interrupting one person, it's better to ask in the group rooms because: More people are reading, so you'll probably get answered sooner. Knowledge is spread throughout the group instead of isolated on specific people. Even if I don't answer a question, I read what others have said thus learning in the process. The responsibility of answering is shared between the group members, making it easier to define the interruptions role . Use threads or replies if the client allows it \u2691 Threads are a feature that allows people to have parallel conversations in the same room in a way that the messages aren't mixed. This makes it easier to maintain the focus and follow past messages. It also allows users that are not interested, to silence the thread, so they won't get application or/and desktop notifications on that particular topic. Replies can be used when the conversation is not lengthy enough to open a thread. They give the benefit of giving context to the user you're replying to. Use chats to transport information, not to store it \u2691 Chat applications were envisioned as a protocol for person A to send information to person B. The fact that the message providers allow users to have almost no limit on their message history has driven people to use them as a knowledge repository. This approach has many problems: As most people don't use end to end encryption (OMEMO/OTR/Signal), the data of their messages is available for the service provider to read. This is a privacy violation that should be avoided. Most providers don't allow you to set a message limit, so you'd have to delete them manually. Searching information in the chats is a nightmare. There are more efficient knowledge repositories to store your information. Use key bindings \u2691 Using the mouse to interact with the chat client graphical interfaces is not efficient, try to learn the key bindings and use them as much as possible. Environment setup \u2691 Account management \u2691 It's common to have more than one account or application to check. There are many instant messaging solutions, such as XMPP, Signal, IRC, Telegram, Slack, Whatssap or Facebook. It would be ideal to have a client that could act as a bridge to all the solutions, but at least I don't know it, so you're forced to install the different applications to interact with them. The obvious suggestion would be to reduce the number of platforms in use, but we all know that it's asking too much as it will probably isolate you from specific people. Once you have the minimum clients chosen, put them all on the same workspace, for example an i3 window manager workspace, and only check them following the workflow rules. Isolate your work and personal environments \u2691 Make sure that you set your environment so that you can't check your personal chats when you're working and the other way around. For example, you could configure different instances of the chat clients and only open the ones that you need to. Or you could avoid configuring the work clients in your personal phone. For example, at work, I have my own account and another for each team I'm part of, the last ones are managed by all the team members. On the personal level, I've got many accounts for the different OpSec profiles or identities. For efficiency reasons, you need to be able to check all of them on one place. You can use an email manager such as Thunderbird . Once you choose one, try to master it. Fine grain configure the notifications \u2691 Modern client applications allow you to define the notifications at room or people level. I usually: Use notifications on all messages on high priority channels. For example the infrastructure monitorization one. Agree with your team to write as less as possible. Use notifications when mentioned on group rooms: Don't get notified on any message unless they add your name on it. Use notifications on direct messages: Decide which people are important enough to activate the notifications. Sometimes the client applications don't give enough granularity, or you would like to show notifications based on more complex conditions, that's why I created the seed project to improve the notification management in Linux .", "title": "Instant messages management"}, {"location": "instant_messages_management/#analyze-how-often-you-need-to-check-it", "text": "Follow the interruption analysis to discover how often you need to check it and if you need the notifications or fine grain them to the sources that have higher priority. Once you've decided the frequency, try to respect it!. If you want an example, check my work or personal analysis.", "title": "Analyze how often you need to check it"}, {"location": "instant_messages_management/#workflow", "text": "I interact with messaging applications in two ways: To read the new items and answer questions. To start a conversation. The passively reading for new items works perfectly with the interruption management processes. Each time you decide to check for new messages, follow the inbox processing guidelines to extract the information to the appropriate system (task manager, calendar or knowledge manager). If you answer someone or if you start a new conversation, assume that any work done in the next 5 to 10 minutes will probably be interrupted, so choose small or mindless tasks. If the person doesn't answer in that time, start a new pomodoro and go back when the next interruption event comes.", "title": "Workflow"}, {"location": "instant_messages_management/#use-calls-for-non-short-conversations", "text": "Chats are good for short conversations that don't require long or quick responses. Even though people may have forgotten it, they are an asynchronous communication channel. They're not suited for long conversations though as: Typing on a keyboard (or a mobile \u1559(\u21c0\u2038\u21bc\u2036)\u1557 ) is slower than talking directly. It's difficult to transmit the conversation tone by message, and each reader can interpret it differently, leading to misunderstandings. If the conversation topic is complex, graphical aids such as screen sharing or doodling can make the conversation more efficient. Unless everyone involved is fully focused on the conversation, the delays between messages can be high, and all that time, the attendees need to manage the interruptions. If you fully focus on the conversation, you're loosing your time while you wait for the other to answer. For all these reasons, whenever a conversation looks not to be short or trivial, arrange a quick call or video call.", "title": "Use calls for non short conversations"}, {"location": "instant_messages_management/#at-work-or-collectives-use-group-rooms-over-direct-messages", "text": "Asking for help through direct messages should be avoided whenever possible, instead of interrupting one person, it's better to ask in the group rooms because: More people are reading, so you'll probably get answered sooner. Knowledge is spread throughout the group instead of isolated on specific people. Even if I don't answer a question, I read what others have said thus learning in the process. The responsibility of answering is shared between the group members, making it easier to define the interruptions role .", "title": "At work or collectives, use group rooms over direct messages"}, {"location": "instant_messages_management/#use-threads-or-replies-if-the-client-allows-it", "text": "Threads are a feature that allows people to have parallel conversations in the same room in a way that the messages aren't mixed. This makes it easier to maintain the focus and follow past messages. It also allows users that are not interested, to silence the thread, so they won't get application or/and desktop notifications on that particular topic. Replies can be used when the conversation is not lengthy enough to open a thread. They give the benefit of giving context to the user you're replying to.", "title": "Use threads or replies if the client allows it"}, {"location": "instant_messages_management/#use-chats-to-transport-information-not-to-store-it", "text": "Chat applications were envisioned as a protocol for person A to send information to person B. The fact that the message providers allow users to have almost no limit on their message history has driven people to use them as a knowledge repository. This approach has many problems: As most people don't use end to end encryption (OMEMO/OTR/Signal), the data of their messages is available for the service provider to read. This is a privacy violation that should be avoided. Most providers don't allow you to set a message limit, so you'd have to delete them manually. Searching information in the chats is a nightmare. There are more efficient knowledge repositories to store your information.", "title": "Use chats to transport information, not to store it"}, {"location": "instant_messages_management/#use-key-bindings", "text": "Using the mouse to interact with the chat client graphical interfaces is not efficient, try to learn the key bindings and use them as much as possible.", "title": "Use key bindings"}, {"location": "instant_messages_management/#environment-setup", "text": "", "title": "Environment setup"}, {"location": "instant_messages_management/#account-management", "text": "It's common to have more than one account or application to check. There are many instant messaging solutions, such as XMPP, Signal, IRC, Telegram, Slack, Whatssap or Facebook. It would be ideal to have a client that could act as a bridge to all the solutions, but at least I don't know it, so you're forced to install the different applications to interact with them. The obvious suggestion would be to reduce the number of platforms in use, but we all know that it's asking too much as it will probably isolate you from specific people. Once you have the minimum clients chosen, put them all on the same workspace, for example an i3 window manager workspace, and only check them following the workflow rules.", "title": "Account management"}, {"location": "instant_messages_management/#isolate-your-work-and-personal-environments", "text": "Make sure that you set your environment so that you can't check your personal chats when you're working and the other way around. For example, you could configure different instances of the chat clients and only open the ones that you need to. Or you could avoid configuring the work clients in your personal phone. For example, at work, I have my own account and another for each team I'm part of, the last ones are managed by all the team members. On the personal level, I've got many accounts for the different OpSec profiles or identities. For efficiency reasons, you need to be able to check all of them on one place. You can use an email manager such as Thunderbird . Once you choose one, try to master it.", "title": "Isolate your work and personal environments"}, {"location": "instant_messages_management/#fine-grain-configure-the-notifications", "text": "Modern client applications allow you to define the notifications at room or people level. I usually: Use notifications on all messages on high priority channels. For example the infrastructure monitorization one. Agree with your team to write as less as possible. Use notifications when mentioned on group rooms: Don't get notified on any message unless they add your name on it. Use notifications on direct messages: Decide which people are important enough to activate the notifications. Sometimes the client applications don't give enough granularity, or you would like to show notifications based on more complex conditions, that's why I created the seed project to improve the notification management in Linux .", "title": "Fine grain configure the notifications"}, {"location": "interruption_management/", "text": "Interruption management is the life management area that gathers the processes to minimize the time and willpower toll consumed by interruptions. We've come to accept that we need to be available 24/7 and answer immediately, that makes us slaves of the interruptions, it drives our work and personal relations. I feel that out of respect of ourselves and the other's time, we need to change that perspective. Most of the times interruptions can wait 20 or 60 minutes, and many of them can be avoided with better task and time planning. Interruptions are one of the main productivity killers. Not only they unexpectedly break your workflow, they also add undesired mental load as you are waiting for them to happen, and need to check them often. As we've seen previously, to be productive you need to be able to work on a task for 20 minutes without checking the interruption channels. Interruption analysis \u2691 The interruption analysis is the main input to do interruption management. With it you consider what are the sources of the interruptions, and for each of them you classify the different source events in categories evaluating: How many interruption events does the source or category create. How many of the events require an action, and if it can be automated. How many hold information that don't need any action, and what do you want to do with that information. How many could be automatically filtered out. What priority do the events have, and if it's the same for all events. How long can the associated action be delayed. Once you have that list, think if you can reduce it. Can you merge or directly remove one of the sources? The less channels to check, the better. Then think which of them you have no control over and think of ways to regain it. If you decide when to address the interruptions, your mind will have less load and will perform better when you're actually working. The ultimate goal of the analysis is to safely define the maximum amount of time you can spend without looking at the channels. Checking them continuously makes no sense, you're breaking your workflow for no good reason, as most times there is nothing new, and if there is, you feel the urge to act upon them, even though they could wait. In some teams, the situation doesn't allow you not to check them frequently. In those cases you can define the interruption manager role. A figure that is rotated by the team's members so that only one human needs to be monitoring the interruption channels, while the rest of them are able to work continuously on their tasks. If you want to see the analysis in action, check my work analysis or my personal one . Workflow \u2691 Once you have all the interruption sources identified, classified, and defined the checking periodicity, you need to decide how to handle them. Define your interruption events \u2691 To minimize the times you interrupt your workflow, aggregate the different sources and schedule when are you want to check them. For example, if the analysis gave the next sources: Source A: check each 4 hours. Source B: check each 5 hours. Source C: check each 20 minutes. You can schedule the next interruption events: Check sources A, B and C: when you start working, before lunch and before the end of the day. Check C: after each Pomodoro iteration . Process the interruption event information \u2691 When an interruption event arrives, process sequentially each source following the inbox emptying guidelines .", "title": "Interruption Management"}, {"location": "interruption_management/#interruption-analysis", "text": "The interruption analysis is the main input to do interruption management. With it you consider what are the sources of the interruptions, and for each of them you classify the different source events in categories evaluating: How many interruption events does the source or category create. How many of the events require an action, and if it can be automated. How many hold information that don't need any action, and what do you want to do with that information. How many could be automatically filtered out. What priority do the events have, and if it's the same for all events. How long can the associated action be delayed. Once you have that list, think if you can reduce it. Can you merge or directly remove one of the sources? The less channels to check, the better. Then think which of them you have no control over and think of ways to regain it. If you decide when to address the interruptions, your mind will have less load and will perform better when you're actually working. The ultimate goal of the analysis is to safely define the maximum amount of time you can spend without looking at the channels. Checking them continuously makes no sense, you're breaking your workflow for no good reason, as most times there is nothing new, and if there is, you feel the urge to act upon them, even though they could wait. In some teams, the situation doesn't allow you not to check them frequently. In those cases you can define the interruption manager role. A figure that is rotated by the team's members so that only one human needs to be monitoring the interruption channels, while the rest of them are able to work continuously on their tasks. If you want to see the analysis in action, check my work analysis or my personal one .", "title": "Interruption analysis"}, {"location": "interruption_management/#workflow", "text": "Once you have all the interruption sources identified, classified, and defined the checking periodicity, you need to decide how to handle them.", "title": "Workflow"}, {"location": "interruption_management/#define-your-interruption-events", "text": "To minimize the times you interrupt your workflow, aggregate the different sources and schedule when are you want to check them. For example, if the analysis gave the next sources: Source A: check each 4 hours. Source B: check each 5 hours. Source C: check each 20 minutes. You can schedule the next interruption events: Check sources A, B and C: when you start working, before lunch and before the end of the day. Check C: after each Pomodoro iteration .", "title": "Define your interruption events"}, {"location": "interruption_management/#process-the-interruption-event-information", "text": "When an interruption event arrives, process sequentially each source following the inbox emptying guidelines .", "title": "Process the interruption event information"}, {"location": "issues/", "text": "I haven't found a tool to monitor the context it made me track certain software issues, so I get lost when updates come. Until a tool shows up, I'll use the good old markdown to keep track. Pydantic errors \u2691 No name 'BaseModel' in module 'pydantic' (no-name-in-module) , you can find a patch in the pydantic article , the pydantic developers took that as a solution as it lays in pylint's roof , once that last issue is solved try to find a better way to improve the patch solution. Vim workflow improvements \u2691 Manually formatting paragraphs is an unproductive pain in the ass, Vim-pencil looks promising but there are still some usability issues that need to be fixed first: Wrong list management: #93 linked to #31 and #95 . Disable wrap of document headers (less important). Gitea improvements \u2691 Replying discussion comments redirects to mail pull request page : Notify the people that it's fixed. Gitea Kanban board improvements \u2691 Remove the Default issue template: #14383 . When it's solved apply it in the work's issue tracker. Docker monitorization \u2691 Integrate diun in the CI pipelines when they support prometheus metrics . Update the docker article too. Gadgetbridge improvements \u2691 Smart alarm support : Use it whenever it's available. GET Sp02 real time data, or at least export it : See how to use this data once it's available. export heart rate for activities without a GPX track : See if I can export the heart rate for post processing. Maybe it's covered here . Add UI and logic for more complex database import, export and merging : Monitor to see if there are new ways or improvements of exporting data. Blog's RSS is not working : Add it to the feed reader once it does, and remove the warning from the gadgetbridge article Integrate with home assistant : Check if the integration with kalliope is easy. Issues with zoom, swipe, interact with graphs : enable back disable swipe between tabs in the chart settings. PAI implementation : Check it once it's ready. Calendar synchronization issue , could be related with notifications work after restart : try it when it's solved Change snooze time span : Change the timespan from 10 to 5 minutes. Ombi improvements \u2691 Ebook requests : Configure it in the service, notify the people and start using it. Add working links to the details pages : nothing to do, just start using it. Allow search by genre : Notify the people and start using it.", "title": "Issues"}, {"location": "issues/#pydantic-errors", "text": "No name 'BaseModel' in module 'pydantic' (no-name-in-module) , you can find a patch in the pydantic article , the pydantic developers took that as a solution as it lays in pylint's roof , once that last issue is solved try to find a better way to improve the patch solution.", "title": "Pydantic errors"}, {"location": "issues/#vim-workflow-improvements", "text": "Manually formatting paragraphs is an unproductive pain in the ass, Vim-pencil looks promising but there are still some usability issues that need to be fixed first: Wrong list management: #93 linked to #31 and #95 . Disable wrap of document headers (less important).", "title": "Vim workflow improvements"}, {"location": "issues/#gitea-improvements", "text": "Replying discussion comments redirects to mail pull request page : Notify the people that it's fixed.", "title": "Gitea improvements"}, {"location": "issues/#gitea-kanban-board-improvements", "text": "Remove the Default issue template: #14383 . When it's solved apply it in the work's issue tracker.", "title": "Gitea Kanban board improvements"}, {"location": "issues/#docker-monitorization", "text": "Integrate diun in the CI pipelines when they support prometheus metrics . Update the docker article too.", "title": "Docker monitorization"}, {"location": "issues/#gadgetbridge-improvements", "text": "Smart alarm support : Use it whenever it's available. GET Sp02 real time data, or at least export it : See how to use this data once it's available. export heart rate for activities without a GPX track : See if I can export the heart rate for post processing. Maybe it's covered here . Add UI and logic for more complex database import, export and merging : Monitor to see if there are new ways or improvements of exporting data. Blog's RSS is not working : Add it to the feed reader once it does, and remove the warning from the gadgetbridge article Integrate with home assistant : Check if the integration with kalliope is easy. Issues with zoom, swipe, interact with graphs : enable back disable swipe between tabs in the chart settings. PAI implementation : Check it once it's ready. Calendar synchronization issue , could be related with notifications work after restart : try it when it's solved Change snooze time span : Change the timespan from 10 to 5 minutes.", "title": "Gadgetbridge improvements"}, {"location": "issues/#ombi-improvements", "text": "Ebook requests : Configure it in the service, notify the people and start using it. Add working links to the details pages : nothing to do, just start using it. Allow search by genre : Notify the people and start using it.", "title": "Ombi improvements"}, {"location": "javascript_snippets/", "text": "Set variable if it's undefined \u2691 var x = ( x === undefined ) ? your_default_value : x ; Concatenate two arrays \u2691 const arr1 = [ \"Cecilie\" , \"Lone\" ]; const arr2 = [ \"Emil\" , \"Tobias\" , \"Linus\" ]; const children = arr1 . concat ( arr2 ); To join more arrays you can use: const arr1 = [ \"Cecilie\" , \"Lone\" ]; const arr2 = [ \"Emil\" , \"Tobias\" , \"Linus\" ]; const arr3 = [ \"Robin\" ]; const children = arr1 . concat ( arr2 , arr3 ); Check if a variable is not undefined \u2691 if ( typeof lastname !== \"undefined\" ) { alert ( \"Hi. Variable is defined.\" ); } Select a substring \u2691 'long string' . substring ( startIndex , endIndex ) Round a number \u2691 Math . round ( 2.5 ) Remove focus from element \u2691 document . activeElement . blur ();", "title": "Javascript snippets"}, {"location": "javascript_snippets/#set-variable-if-its-undefined", "text": "var x = ( x === undefined ) ? your_default_value : x ;", "title": "Set variable if it's undefined"}, {"location": "javascript_snippets/#concatenate-two-arrays", "text": "const arr1 = [ \"Cecilie\" , \"Lone\" ]; const arr2 = [ \"Emil\" , \"Tobias\" , \"Linus\" ]; const children = arr1 . concat ( arr2 ); To join more arrays you can use: const arr1 = [ \"Cecilie\" , \"Lone\" ]; const arr2 = [ \"Emil\" , \"Tobias\" , \"Linus\" ]; const arr3 = [ \"Robin\" ]; const children = arr1 . concat ( arr2 , arr3 );", "title": "Concatenate two arrays"}, {"location": "javascript_snippets/#check-if-a-variable-is-not-undefined", "text": "if ( typeof lastname !== \"undefined\" ) { alert ( \"Hi. Variable is defined.\" ); }", "title": "Check if a variable is not undefined"}, {"location": "javascript_snippets/#select-a-substring", "text": "'long string' . substring ( startIndex , endIndex )", "title": "Select a substring"}, {"location": "javascript_snippets/#round-a-number", "text": "Math . round ( 2.5 )", "title": "Round a number"}, {"location": "javascript_snippets/#remove-focus-from-element", "text": "document . activeElement . blur ();", "title": "Remove focus from element"}, {"location": "jellyfin/", "text": "Jellyfin is a Free Software Media System that puts you in control of managing and streaming your media. It is an alternative to the proprietary Emby and Plex, to provide media from a dedicated server to end-user devices via multiple apps. Jellyfin is descended from Emby's 3.5.2 release and ported to the .NET Core framework to enable full cross-platform support. There are no strings attached, no premium licenses or features, and no hidden agendas: just a team who want to build something better and work together to achieve it. Troubleshooting \u2691 Wrong image covers \u2691 Remove all the jpg files of the directory and then fetch again the data from your favourite media management software. Green bars in the reproduction \u2691 It's related to some hardware transcoding issue related to some video codecs, the solution is to either get a file with other codec, or convert it yourself without the hardware transcoding with: ffmpeg -i input.avi -c:v libx264 out.mp4 Stuck at login page \u2691 Sometimes Jellyfin gets stuck at the login screen when trying to log in with an endlessly spinning loading wheel. It looks like it's already fixed, so first try to update to the latest version. If the error remains, follow the next steps: To fix it run the next snippet: systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service # Go to JF URL, get asked to log in even though # there are no Users in the JF DB now systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service If you use jfa-go for the invites, you may need to regenerate all the user profiles , so that the problem is not introduced again. Issues \u2691 Subtitles get delayed from the video on some devices: 1 , 2 , 3 . There is a feature request for a fix. Once it's solved notify the users once it's solved. Trailers not working : No solution until it's fixed Unnecessary transcoding : nothing to do Local social features : test it and see how to share rating between users. Skip intro/outro/credits : try it. Music star rating : try it and plan to migrate everything to Jellyfin. Remove pagination/use lazy loading : try it. Support 2FA : try it. Mysql server backend : implement it to add robustness. Watched history : try it. A richer ePub reader : migrate from Polar and add jellyfin to the awesome selfhosted list. Prometheus exporter : monitor it. Easy Import/Export Jellyfin settings : add to the backup process. Temporary direct file sharing links : try it. Remember subtitle and audio track choice between episodes : try it. IMBD Rating and Rotten Tomatoes Audiance Rating and Fresh rating on Movies and TV Shows : try the new ratings. Trailers Plugin : Once it's merged to the core, remove the plugin. Jellyfin for apple tv : tell the people that use the shitty device. References \u2691 Home Git Blog ( RSS )", "title": "Jellyfin"}, {"location": "jellyfin/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "jellyfin/#wrong-image-covers", "text": "Remove all the jpg files of the directory and then fetch again the data from your favourite media management software.", "title": "Wrong image covers"}, {"location": "jellyfin/#green-bars-in-the-reproduction", "text": "It's related to some hardware transcoding issue related to some video codecs, the solution is to either get a file with other codec, or convert it yourself without the hardware transcoding with: ffmpeg -i input.avi -c:v libx264 out.mp4", "title": "Green bars in the reproduction"}, {"location": "jellyfin/#stuck-at-login-page", "text": "Sometimes Jellyfin gets stuck at the login screen when trying to log in with an endlessly spinning loading wheel. It looks like it's already fixed, so first try to update to the latest version. If the error remains, follow the next steps: To fix it run the next snippet: systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service # Go to JF URL, get asked to log in even though # there are no Users in the JF DB now systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service If you use jfa-go for the invites, you may need to regenerate all the user profiles , so that the problem is not introduced again.", "title": "Stuck at login page"}, {"location": "jellyfin/#issues", "text": "Subtitles get delayed from the video on some devices: 1 , 2 , 3 . There is a feature request for a fix. Once it's solved notify the users once it's solved. Trailers not working : No solution until it's fixed Unnecessary transcoding : nothing to do Local social features : test it and see how to share rating between users. Skip intro/outro/credits : try it. Music star rating : try it and plan to migrate everything to Jellyfin. Remove pagination/use lazy loading : try it. Support 2FA : try it. Mysql server backend : implement it to add robustness. Watched history : try it. A richer ePub reader : migrate from Polar and add jellyfin to the awesome selfhosted list. Prometheus exporter : monitor it. Easy Import/Export Jellyfin settings : add to the backup process. Temporary direct file sharing links : try it. Remember subtitle and audio track choice between episodes : try it. IMBD Rating and Rotten Tomatoes Audiance Rating and Fresh rating on Movies and TV Shows : try the new ratings. Trailers Plugin : Once it's merged to the core, remove the plugin. Jellyfin for apple tv : tell the people that use the shitty device.", "title": "Issues"}, {"location": "jellyfin/#references", "text": "Home Git Blog ( RSS )", "title": "References"}, {"location": "kag/", "text": "King Arthur Gold , also known as KAG, is a free Medieval Build n'Kill Multiplayer Game with Destructible Environments. Construct freeform forts as a medieval Builder, fight in sword duels as a Knight or snipe with your bow as an Archer. KAG blends the cooperative aspects of Lost Vikings, mashes them with the full destructibility of Worms and the visual style and action of Metal Slug, brought to you by the creators of Soldat. Guides \u2691 Archer guides \u2691 Turtlebutt and Bunnie Coroz and RedOTheWisp Builder guides \u2691 Turtlebutt and Bunnie", "title": "Kag"}, {"location": "kag/#guides", "text": "", "title": "Guides"}, {"location": "kag/#archer-guides", "text": "Turtlebutt and Bunnie Coroz and RedOTheWisp", "title": "Archer guides"}, {"location": "kag/#builder-guides", "text": "Turtlebutt and Bunnie", "title": "Builder guides"}, {"location": "khal/", "text": "khal is a standards based Python CLI (console) calendar program, able to synchronize with CalDAV servers through vdirsyncer . Features: Can read and write events/icalendars to vdir, so vdirsyncer can be used to synchronize calendars with a variety of other programs, for example CalDAV servers. Fast and easy way to add new events ikhal (interactive khal ) lets you browse and edit calendars and events. Limitations: It's not easy to get an idea of what you need to do in the week. At least not as comfortable as a graphical interface. Editing events with ikhal is a little bit cumbersome. Only rudimentary support for creating and editing recursion rules. You cannot edit the timezones of events. Installation \u2691 Although it's available in the major package managers, you can get a more bleeding edge version with pip . pipx install khal If you don't have pipx you can use pip . Configuration \u2691 khal reads configuration files in the ini syntax. If you do not have a configuration file yet, running khal configure will launch a small, interactive tool that should help you with initial configuration of khal. khal is looking for configuration files in the following places and order: $XDG_CONFIG_HOME/khal/config : (on most systems this is ~/.config/khal/config ), ~/.khal/khal.conf (deprecated) A khal.conf file in the current directory (deprecated). Alternatively you can specify which configuration file to use with -c path/to/config at runtime. The calendars section \u2691 The [calendars] section is mandatory and must contain at least one subsection. Every subsection must have a unique name (enclosed by two square brackets). Each subsection needs exactly one path setting, everything else is optional. Here is a small example: [calendars] [[home]] path = ~/.calendars/home/ color = dark green priority = 20 [[work]] path = ~/.calendars/work/ readonly = True Some properties are: path : The path to an existing directory where this calendar is saved as a vdir. color : khal will use this color for coloring this calendar\u2019s event. The following color names are supported: black , white , brown , yellow , dark gray , dark green , dark blue , light gray , light green , light blue , dark magenta , dark cyan , dark red , light magenta , light cyan , light red . priority : When coloring days, the color will be determined based on the calendar with the highest priority. If the priorities are equal, then the \u201cmultiple\u201d color will be used. readonly : Setting this to True, will keep khal from making any changes to this calendar. The default section \u2691 Some of this configurations do not affect ikhal . default_calendar : The calendar to use if none is specified for some operation (e.g. if adding a new event). If this is not set, such operations require an explicit value. default_dayevent_duration : Define the default duration for an event ( khal new only). 1h by default. default_event_duration : Define the default duration for a day-long event ( khal new only). 1d by default. highlight_event_days : If true, khal will highlight days with events. Options for highlighting are in highlight_days section. The key bindings section \u2691 Key bindings for ikhal are set here. You can bind more than one key (combination) to a command by supplying a comma-separated list of keys. For binding key combinations concatenate them keys (with a space in between), for example ctrl n . Action Default Description down down, j Move the cursor down (in the calendar browser). up up, k Move the cursor up (in the calendar browser). left left, h, backspace Move the cursor left (in the calendar browser). right right, l, space Move the cursor right (in the calendar browser). view enter Show details or edit (if details are already shown) the currently selected event. save meta enter Save the currently edited event and leave the event editor. quit q, Q Quit. new n Create a new event on the selected date. delete d Delete the currently selected event. search / Open a text field to start a search for events. mark v Go into highlight (visual) mode to choose a date range. other o In highlight mode go to the other end of the highlighted date range. today t Focus the calendar browser on today. duplicate p Duplicate the currently selected event. export e Export event as a .ics file. log L Show logged messages. external_edit meta E Edit the currently selected events\u2019 raw .ics file with $EDITOR Use the external_edit with caution, the icalendar library we use doesn't do a lot of validation, it silently disregards most invalid data. Syncing \u2691 To get khal working with CalDAV you will first need to setup vdirsyncer . After each start khal will automatically check if anything has changed and automatically update its caching db (this may take some time after the initial sync, especially for large calendar collections). Therefore, you might want to execute khal automatically after syncing with vdirsyncer (for example via cron ). Usage \u2691 khal offers a set of commands, most importantly: list : Shows all events scheduled for a given date (or datetime) range, with custom formatting. calendar : Shows a calendar (similar to cal(1)) and list. new : Allows for adding new events. search : Search for events matching a search string and print them. at : shows all events scheduled for a given datetime. edit : An interactive command for editing and deleting events using a search string. interactive : Invokes the interactive version of khal , can also be invoked by calling ikhal . printcalendars : printformats new \u2691 khal new [-a CALENDAR] [OPTIONS] [START [END | DELTA] [TIMEZONE] SUMMARY [:: DESCRIPTION]] Where start and end are either datetimes, times, or keywords and times in the formats defined in the config file. If no calendar is given via -a , the default calendar is used. For example: khal new 18 :00 Awesome Event Adds a new event starting today at 18:00 with summary Awesome event (lasting for the default time of one hour) to the default calendar. khal new tomorrow 16 :30 Coffee Break Adds a new event tomorrow at 16:30. khal new 25 .10. 18 :00 24 :00 Another Event :: with Alice and Bob Adds a new event on 25 th of October lasting from 18:00 to 24:00 with an additional description. khal new -a work 26 .07. Great Event -g meeting -r weekly Adds a new all day event on 26 th of July to the calendar work in the meeting category, which recurs every week. Interactive \u2691 When the calendar on the left is in focus, you can: Move through the calendar (default keybindings are the arrow keys, space and backspace, those keybindings are configurable in the config file). Focus on the right column by pressing tab or enter . Focus on the current date, default keybinding t as in today. Marking a date range, default keybinding v , as in visual, think visual mode in Vim, pressing esc escapes this visual mode. If in visual mode, you can select the other end of the currently marked range, default keybinding o as in other (again as in Vim). Create a new event on the currently focused day (or date range if a range is selected), default keybinding n . Search for events, default keybinding / , a pop-up will ask for your search term. When an event list is in focus, you can: View an event\u2019s details with pressing enter (or tab) and edit it with pressing enter (or tab) again (if [view] event_view_always_visible is set to True , the event in focus will always be shown in detail). Toggle an event\u2019s deletion status, default keybinding d , events marked for deletion will appear with a D in front and will be deleted when khal exits. Duplicate the selected event, default keybinding p Export the selected event, default keybinding e . In the event editor, you can: Jump to the next (previous) selectable element with pressing tab (shift+tab) Quick save, default keybinding meta+enter (meta will probably be alt). Use some common editing short cuts in most text fields (ctrl+w deletes word before cursor, ctrl+u (ctrl+k) deletes till the beginning (end) of the line, ctrl+a (ctrl+e) will jump to the beginning (end) of the line. In the date and time fields you can increment and decrement the number under the cursor with ctrl+a and ctrl+x (time in 15 minute steps) In the date fields you can access a miniature calendar by pressing enter. Activate actions by pressing enter on text enclosed by angled brackets, e.g. \\< Save > (sometimes this might open a pop up). Pressing esc will cancel the current action and/or take you back to the previously shown pane (i.e. what you see when you open ikhal), if you are at the start pane, ikhal will quit on pressing esc again. Tricks \u2691 Edit the events in a more pleasant way \u2691 The ikhal event editor is not comfortable for me. I usually only change the title or the start date and in the default interface you need to press many keystrokes to make it happen. A patch solution is to pass a custom script on the EDITOR environmental variable. Assuming you have questionary and ics installed you can save the next snippet into an edit_event file in your PATH : #!/usr/bin/python3 \"\"\"Edit an ics calendar event.\"\"\" import sys import questionary from ics import Calendar # Load the event file = sys . argv [ 1 ] with open ( file , \"r\" ) as fd : calendar = Calendar ( fd . read ()) event = list ( calendar . timeline )[ 0 ] # Modify the event event . name = questionary . text ( \"Title: \" , default = event . name ) . ask () start = questionary . text ( \"Start: \" , default = f \" { str ( event . begin . hour ) . zfill ( 2 ) } : { str ( event . begin . minute ) . zfill ( 2 ) } \" , ) . ask () event . begin = event . begin . replace ( hour = int ( start . split ( \":\" )[ 0 ]), minute = int ( start . split ( \":\" )[ 1 ]) ) # Save the event with open ( file , \"w\" ) as fd : fd . writelines ( calendar . serialize_iter ()) Now if you open ikhal as EDITOR=edit_event ikhal , whenever you edit one event you'll get a better interface. Add to your .zshrc or .bashrc : alias ikhal = 'EDITOR=edit_event ikhal' The default keybinding for the edition is not very comfortable either, add the next snippet on your config: [keybindings] external_edit = e export = meta e References \u2691 Docs Git", "title": "Khal"}, {"location": "khal/#installation", "text": "Although it's available in the major package managers, you can get a more bleeding edge version with pip . pipx install khal If you don't have pipx you can use pip .", "title": "Installation"}, {"location": "khal/#configuration", "text": "khal reads configuration files in the ini syntax. If you do not have a configuration file yet, running khal configure will launch a small, interactive tool that should help you with initial configuration of khal. khal is looking for configuration files in the following places and order: $XDG_CONFIG_HOME/khal/config : (on most systems this is ~/.config/khal/config ), ~/.khal/khal.conf (deprecated) A khal.conf file in the current directory (deprecated). Alternatively you can specify which configuration file to use with -c path/to/config at runtime.", "title": "Configuration"}, {"location": "khal/#the-calendars-section", "text": "The [calendars] section is mandatory and must contain at least one subsection. Every subsection must have a unique name (enclosed by two square brackets). Each subsection needs exactly one path setting, everything else is optional. Here is a small example: [calendars] [[home]] path = ~/.calendars/home/ color = dark green priority = 20 [[work]] path = ~/.calendars/work/ readonly = True Some properties are: path : The path to an existing directory where this calendar is saved as a vdir. color : khal will use this color for coloring this calendar\u2019s event. The following color names are supported: black , white , brown , yellow , dark gray , dark green , dark blue , light gray , light green , light blue , dark magenta , dark cyan , dark red , light magenta , light cyan , light red . priority : When coloring days, the color will be determined based on the calendar with the highest priority. If the priorities are equal, then the \u201cmultiple\u201d color will be used. readonly : Setting this to True, will keep khal from making any changes to this calendar.", "title": "The calendars section"}, {"location": "khal/#the-default-section", "text": "Some of this configurations do not affect ikhal . default_calendar : The calendar to use if none is specified for some operation (e.g. if adding a new event). If this is not set, such operations require an explicit value. default_dayevent_duration : Define the default duration for an event ( khal new only). 1h by default. default_event_duration : Define the default duration for a day-long event ( khal new only). 1d by default. highlight_event_days : If true, khal will highlight days with events. Options for highlighting are in highlight_days section.", "title": "The default section"}, {"location": "khal/#the-key-bindings-section", "text": "Key bindings for ikhal are set here. You can bind more than one key (combination) to a command by supplying a comma-separated list of keys. For binding key combinations concatenate them keys (with a space in between), for example ctrl n . Action Default Description down down, j Move the cursor down (in the calendar browser). up up, k Move the cursor up (in the calendar browser). left left, h, backspace Move the cursor left (in the calendar browser). right right, l, space Move the cursor right (in the calendar browser). view enter Show details or edit (if details are already shown) the currently selected event. save meta enter Save the currently edited event and leave the event editor. quit q, Q Quit. new n Create a new event on the selected date. delete d Delete the currently selected event. search / Open a text field to start a search for events. mark v Go into highlight (visual) mode to choose a date range. other o In highlight mode go to the other end of the highlighted date range. today t Focus the calendar browser on today. duplicate p Duplicate the currently selected event. export e Export event as a .ics file. log L Show logged messages. external_edit meta E Edit the currently selected events\u2019 raw .ics file with $EDITOR Use the external_edit with caution, the icalendar library we use doesn't do a lot of validation, it silently disregards most invalid data.", "title": "The key bindings section"}, {"location": "khal/#syncing", "text": "To get khal working with CalDAV you will first need to setup vdirsyncer . After each start khal will automatically check if anything has changed and automatically update its caching db (this may take some time after the initial sync, especially for large calendar collections). Therefore, you might want to execute khal automatically after syncing with vdirsyncer (for example via cron ).", "title": "Syncing"}, {"location": "khal/#usage", "text": "khal offers a set of commands, most importantly: list : Shows all events scheduled for a given date (or datetime) range, with custom formatting. calendar : Shows a calendar (similar to cal(1)) and list. new : Allows for adding new events. search : Search for events matching a search string and print them. at : shows all events scheduled for a given datetime. edit : An interactive command for editing and deleting events using a search string. interactive : Invokes the interactive version of khal , can also be invoked by calling ikhal . printcalendars : printformats", "title": "Usage"}, {"location": "khal/#new", "text": "khal new [-a CALENDAR] [OPTIONS] [START [END | DELTA] [TIMEZONE] SUMMARY [:: DESCRIPTION]] Where start and end are either datetimes, times, or keywords and times in the formats defined in the config file. If no calendar is given via -a , the default calendar is used. For example: khal new 18 :00 Awesome Event Adds a new event starting today at 18:00 with summary Awesome event (lasting for the default time of one hour) to the default calendar. khal new tomorrow 16 :30 Coffee Break Adds a new event tomorrow at 16:30. khal new 25 .10. 18 :00 24 :00 Another Event :: with Alice and Bob Adds a new event on 25 th of October lasting from 18:00 to 24:00 with an additional description. khal new -a work 26 .07. Great Event -g meeting -r weekly Adds a new all day event on 26 th of July to the calendar work in the meeting category, which recurs every week.", "title": "new"}, {"location": "khal/#interactive", "text": "When the calendar on the left is in focus, you can: Move through the calendar (default keybindings are the arrow keys, space and backspace, those keybindings are configurable in the config file). Focus on the right column by pressing tab or enter . Focus on the current date, default keybinding t as in today. Marking a date range, default keybinding v , as in visual, think visual mode in Vim, pressing esc escapes this visual mode. If in visual mode, you can select the other end of the currently marked range, default keybinding o as in other (again as in Vim). Create a new event on the currently focused day (or date range if a range is selected), default keybinding n . Search for events, default keybinding / , a pop-up will ask for your search term. When an event list is in focus, you can: View an event\u2019s details with pressing enter (or tab) and edit it with pressing enter (or tab) again (if [view] event_view_always_visible is set to True , the event in focus will always be shown in detail). Toggle an event\u2019s deletion status, default keybinding d , events marked for deletion will appear with a D in front and will be deleted when khal exits. Duplicate the selected event, default keybinding p Export the selected event, default keybinding e . In the event editor, you can: Jump to the next (previous) selectable element with pressing tab (shift+tab) Quick save, default keybinding meta+enter (meta will probably be alt). Use some common editing short cuts in most text fields (ctrl+w deletes word before cursor, ctrl+u (ctrl+k) deletes till the beginning (end) of the line, ctrl+a (ctrl+e) will jump to the beginning (end) of the line. In the date and time fields you can increment and decrement the number under the cursor with ctrl+a and ctrl+x (time in 15 minute steps) In the date fields you can access a miniature calendar by pressing enter. Activate actions by pressing enter on text enclosed by angled brackets, e.g. \\< Save > (sometimes this might open a pop up). Pressing esc will cancel the current action and/or take you back to the previously shown pane (i.e. what you see when you open ikhal), if you are at the start pane, ikhal will quit on pressing esc again.", "title": "Interactive"}, {"location": "khal/#tricks", "text": "", "title": "Tricks"}, {"location": "khal/#edit-the-events-in-a-more-pleasant-way", "text": "The ikhal event editor is not comfortable for me. I usually only change the title or the start date and in the default interface you need to press many keystrokes to make it happen. A patch solution is to pass a custom script on the EDITOR environmental variable. Assuming you have questionary and ics installed you can save the next snippet into an edit_event file in your PATH : #!/usr/bin/python3 \"\"\"Edit an ics calendar event.\"\"\" import sys import questionary from ics import Calendar # Load the event file = sys . argv [ 1 ] with open ( file , \"r\" ) as fd : calendar = Calendar ( fd . read ()) event = list ( calendar . timeline )[ 0 ] # Modify the event event . name = questionary . text ( \"Title: \" , default = event . name ) . ask () start = questionary . text ( \"Start: \" , default = f \" { str ( event . begin . hour ) . zfill ( 2 ) } : { str ( event . begin . minute ) . zfill ( 2 ) } \" , ) . ask () event . begin = event . begin . replace ( hour = int ( start . split ( \":\" )[ 0 ]), minute = int ( start . split ( \":\" )[ 1 ]) ) # Save the event with open ( file , \"w\" ) as fd : fd . writelines ( calendar . serialize_iter ()) Now if you open ikhal as EDITOR=edit_event ikhal , whenever you edit one event you'll get a better interface. Add to your .zshrc or .bashrc : alias ikhal = 'EDITOR=edit_event ikhal' The default keybinding for the edition is not very comfortable either, add the next snippet on your config: [keybindings] external_edit = e export = meta e", "title": "Edit the events in a more pleasant way"}, {"location": "khal/#references", "text": "Docs Git", "title": "References"}, {"location": "kitty/", "text": "kitty is a fast, feature-rich, GPU based terminal emulator written in C and Python with nice features for the keyboard driven humans like me. Installation \u2691 Although it's in the official repos, the version of Debian is quite old, instead you can install it for the current user with: curl -L https://sw.kovidgoyal.net/kitty/installer.sh | sh /dev/stdin You'll need to add the next alias too to your .zshrc or .bashrc alias kitty = \"~/.local/kitty.app/bin/kitty\" Configuration \u2691 It's configuration is a simple, human editable, single file for easy reproducibility stored at ~/.config/kitty/kitty.conf Print images in the terminal \u2691 Create an alias in your .zshrc : alias icat = \"kitty +kitten icat\" Colors \u2691 The themes kitten allows you to easily change color themes, from a collection of almost two hundred pre-built themes available at kitty-themes. To use it run: kitty +kitten themes The kitten allows you to pick a theme, with live previews of the colors. You can choose between light and dark themes and search by theme name by just typing a few characters from the name. If you want to tweak some colors once you select a theme, you can use terminal sexy . Make the background transparent \u2691 File: ~/.config/kitty/kitty.conf background_opacity 0.85 A number between 0 and 1, where 1 is opaque and 0 is fully transparent. This will only work if supported by the OS (for instance, when using a compositor under X11). If you're using i3wm you need to configure compton Install it with sudo apt-get install compton , and configure i3 to start it in the background adding exec --no-startup-id compton to your i3 config. Terminal bell \u2691 I hate the auditive terminal bell, disable it with: enable_audio_bell no Movement \u2691 By default the movement is not vim friendly because if you use the same keystrokes, they will be captured by kitty and not forwarded to the application. The closest I got is: # Movement map ctrl+shift+k scroll_line_up map ctrl+shift+j scroll_line_down map ctrl+shift+u scroll_page_up map ctrl+shift+d scroll_page_down If you need more fine grained movement, use the scrollback buffer . The scrollback buffer \u2691 kitty supports scrolling back to view history, just like most terminals. You can use either keyboard shortcuts or the mouse scroll wheel to do so. However, kitty has an extra, neat feature. Sometimes you need to explore the scrollback buffer in more detail, maybe search for some text or refer to it side-by-side while typing in a follow-up command. kitty allows you to do this by pressing the ctrl+shift+h key-combination, which will open the scrollback buffer in your favorite pager program (which is less by default). Colors and text formatting are preserved. You can explore the scrollback buffer comfortably within the pager. To use nvim as the pager follow this discussion , the latest working snippet was: # Scrollback buffer # https://sw.kovidgoyal.net/kitty/overview/#the-scrollback-buffer # `bash -c '...'` Run everything in a shell taking the scrollback content on stdin # `-u NORC` Load plugins but not initialization files # `-c \"map q :qa!<CR>\"` Close with `q` key # `-c \"autocmd TermOpen * normal G\"` On opening of the embedded terminal go to last line # `-c \"terminal cat /proc/$$/fd/0 -\"` Open the embedded terminal and read stdin of the shell # `-c \"set clipboard+=unnamedplus\"` Always use clipboard to yank/put instead of having to specify + scrollback_pager bash -c 'nvim </dev/null -u NORC -c \"map q :qa!<CR>\" -c \"autocmd TermOpen * normal G\" -c \"terminal cat /proc/$$/fd/0 -\" -c \"set clipboard+=unnamedplus\" -c \"call cursor(CURSOR_LINE, CURSOR_COLUMN)\"' To make the history scrollback infinite add the next lines: scrollback_lines -1 scrollback_pager_history_size 0 Clipboard management \u2691 # Clipboard map ctrl+v paste_from_clipboard Troubleshooting \u2691 Scrollback when ssh into a machine doesn't work \u2691 This happens because the kitty terminfo files are not available on the server. You can ssh in using the following command which will automatically copy the terminfo files to the server: kitty +kitten ssh myserver This ssh kitten takes all the same command line arguments as ssh, you can alias it to ssh in your shell\u2019s rc files to avoid having to type it each time: alias ssh = \"kitty +kitten ssh\" Reasons to migrate from urxvt to kitty \u2691 It doesn't fuck up your terminal colors. You can use peek to record your screen. Easier to extend. References \u2691 Homepage", "title": "Kitty"}, {"location": "kitty/#installation", "text": "Although it's in the official repos, the version of Debian is quite old, instead you can install it for the current user with: curl -L https://sw.kovidgoyal.net/kitty/installer.sh | sh /dev/stdin You'll need to add the next alias too to your .zshrc or .bashrc alias kitty = \"~/.local/kitty.app/bin/kitty\"", "title": "Installation"}, {"location": "kitty/#configuration", "text": "It's configuration is a simple, human editable, single file for easy reproducibility stored at ~/.config/kitty/kitty.conf", "title": "Configuration"}, {"location": "kitty/#print-images-in-the-terminal", "text": "Create an alias in your .zshrc : alias icat = \"kitty +kitten icat\"", "title": "Print images in the terminal"}, {"location": "kitty/#colors", "text": "The themes kitten allows you to easily change color themes, from a collection of almost two hundred pre-built themes available at kitty-themes. To use it run: kitty +kitten themes The kitten allows you to pick a theme, with live previews of the colors. You can choose between light and dark themes and search by theme name by just typing a few characters from the name. If you want to tweak some colors once you select a theme, you can use terminal sexy .", "title": "Colors"}, {"location": "kitty/#make-the-background-transparent", "text": "File: ~/.config/kitty/kitty.conf background_opacity 0.85 A number between 0 and 1, where 1 is opaque and 0 is fully transparent. This will only work if supported by the OS (for instance, when using a compositor under X11). If you're using i3wm you need to configure compton Install it with sudo apt-get install compton , and configure i3 to start it in the background adding exec --no-startup-id compton to your i3 config.", "title": "Make the background transparent"}, {"location": "kitty/#terminal-bell", "text": "I hate the auditive terminal bell, disable it with: enable_audio_bell no", "title": "Terminal bell"}, {"location": "kitty/#movement", "text": "By default the movement is not vim friendly because if you use the same keystrokes, they will be captured by kitty and not forwarded to the application. The closest I got is: # Movement map ctrl+shift+k scroll_line_up map ctrl+shift+j scroll_line_down map ctrl+shift+u scroll_page_up map ctrl+shift+d scroll_page_down If you need more fine grained movement, use the scrollback buffer .", "title": "Movement"}, {"location": "kitty/#the-scrollback-buffer", "text": "kitty supports scrolling back to view history, just like most terminals. You can use either keyboard shortcuts or the mouse scroll wheel to do so. However, kitty has an extra, neat feature. Sometimes you need to explore the scrollback buffer in more detail, maybe search for some text or refer to it side-by-side while typing in a follow-up command. kitty allows you to do this by pressing the ctrl+shift+h key-combination, which will open the scrollback buffer in your favorite pager program (which is less by default). Colors and text formatting are preserved. You can explore the scrollback buffer comfortably within the pager. To use nvim as the pager follow this discussion , the latest working snippet was: # Scrollback buffer # https://sw.kovidgoyal.net/kitty/overview/#the-scrollback-buffer # `bash -c '...'` Run everything in a shell taking the scrollback content on stdin # `-u NORC` Load plugins but not initialization files # `-c \"map q :qa!<CR>\"` Close with `q` key # `-c \"autocmd TermOpen * normal G\"` On opening of the embedded terminal go to last line # `-c \"terminal cat /proc/$$/fd/0 -\"` Open the embedded terminal and read stdin of the shell # `-c \"set clipboard+=unnamedplus\"` Always use clipboard to yank/put instead of having to specify + scrollback_pager bash -c 'nvim </dev/null -u NORC -c \"map q :qa!<CR>\" -c \"autocmd TermOpen * normal G\" -c \"terminal cat /proc/$$/fd/0 -\" -c \"set clipboard+=unnamedplus\" -c \"call cursor(CURSOR_LINE, CURSOR_COLUMN)\"' To make the history scrollback infinite add the next lines: scrollback_lines -1 scrollback_pager_history_size 0", "title": "The scrollback buffer"}, {"location": "kitty/#clipboard-management", "text": "# Clipboard map ctrl+v paste_from_clipboard", "title": "Clipboard management"}, {"location": "kitty/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "kitty/#scrollback-when-ssh-into-a-machine-doesnt-work", "text": "This happens because the kitty terminfo files are not available on the server. You can ssh in using the following command which will automatically copy the terminfo files to the server: kitty +kitten ssh myserver This ssh kitten takes all the same command line arguments as ssh, you can alias it to ssh in your shell\u2019s rc files to avoid having to type it each time: alias ssh = \"kitty +kitten ssh\"", "title": "Scrollback when ssh into a machine doesn't work"}, {"location": "kitty/#reasons-to-migrate-from-urxvt-to-kitty", "text": "It doesn't fuck up your terminal colors. You can use peek to record your screen. Easier to extend.", "title": "Reasons to migrate from urxvt to kitty"}, {"location": "kitty/#references", "text": "Homepage", "title": "References"}, {"location": "krew/", "text": "Krew is a tool that makes it easy to use kubectl plugins. Krew helps you discover plugins, install and manage them on your machine. It is similar to tools like apt, dnf or brew. Installation \u2691 Run this command to download and install krew: ( set -x ; cd \" $( mktemp -d ) \" && OS = \" $( uname | tr '[:upper:]' '[:lower:]' ) \" && ARCH = \" $( uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/' ) \" && KREW = \"krew- ${ OS } _ ${ ARCH } \" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/ ${ KREW } .tar.gz\" && tar zxvf \" ${ KREW } .tar.gz\" && ./ \" ${ KREW } \" install krew ) Add the $HOME/.krew/bin directory to your PATH environment variable. To do this, update your .bashrc or .zshrc file and append the following line: export PATH = \" ${ KREW_ROOT :- $HOME /.krew } /bin: $PATH \" Restart your shell. Run kubectl krew to check the installation. References \u2691 Git Docs", "title": "Krew"}, {"location": "krew/#installation", "text": "Run this command to download and install krew: ( set -x ; cd \" $( mktemp -d ) \" && OS = \" $( uname | tr '[:upper:]' '[:lower:]' ) \" && ARCH = \" $( uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/' ) \" && KREW = \"krew- ${ OS } _ ${ ARCH } \" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/ ${ KREW } .tar.gz\" && tar zxvf \" ${ KREW } .tar.gz\" && ./ \" ${ KREW } \" install krew ) Add the $HOME/.krew/bin directory to your PATH environment variable. To do this, update your .bashrc or .zshrc file and append the following line: export PATH = \" ${ KREW_ROOT :- $HOME /.krew } /bin: $PATH \" Restart your shell. Run kubectl krew to check the installation.", "title": "Installation"}, {"location": "krew/#references", "text": "Git Docs", "title": "References"}, {"location": "ksniff/", "text": "Ksniff is a Kubectl plugin to ease sniffing on kubernetes pods using tcpdump and wireshark. Installation \u2691 Recommended installation is done via krew kubectl krew install sniff For manual installation, download the latest release package, unzip it and use the attached makefile: unzip ksniff.zip make install (I tried doing it manually and it failed for me). Usage \u2691 kubectl sniff <POD_NAME> [ -n <NAMESPACE_NAME> ] [ -c <CONTAINER_NAME> ] [ -i <INTERFACE_NAME> ] [ -f <CAPTURE_FILTER> ] [ -o OUTPUT_FILE ] [ -l LOCAL_TCPDUMP_FILE ] [ -r REMOTE_TCPDUMP_FILE ] POD_NAME: Required. the name of the kubernetes pod to start capture it 's traffic. NAMESPACE_NAME: Optional. Namespace name. used to specify the target namespace to operate on. CONTAINER_NAME: Optional. If omitted, the first container in the pod will be chosen. INTERFACE_NAME: Optional. Pod Interface to capture from. If omitted, all Pod interfaces will be captured. CAPTURE_FILTER: Optional. specify a specific tcpdump capture filter. If omitted no filter will be used. OUTPUT_FILE: Optional. if specified, ksniff will redirect tcpdump output to local file instead of wireshark. Use ' - ' for stdout. LOCAL_TCPDUMP_FILE: Optional. if specified, ksniff will use this path as the local path of the static tcpdump binary. REMOTE_TCPDUMP_FILE: Optional. if specified, ksniff will use the specified path as the remote path to upload static tcpdump to. You'll need to remove the pods manually once you've finished analyzing the traffic. Issues \u2691 WTAP_ENCAP = 0 \u2691 Upgrade your wireshark to a version greater or equal to 3.3.0 . References \u2691 Git", "title": "Ksniff"}, {"location": "ksniff/#installation", "text": "Recommended installation is done via krew kubectl krew install sniff For manual installation, download the latest release package, unzip it and use the attached makefile: unzip ksniff.zip make install (I tried doing it manually and it failed for me).", "title": "Installation"}, {"location": "ksniff/#usage", "text": "kubectl sniff <POD_NAME> [ -n <NAMESPACE_NAME> ] [ -c <CONTAINER_NAME> ] [ -i <INTERFACE_NAME> ] [ -f <CAPTURE_FILTER> ] [ -o OUTPUT_FILE ] [ -l LOCAL_TCPDUMP_FILE ] [ -r REMOTE_TCPDUMP_FILE ] POD_NAME: Required. the name of the kubernetes pod to start capture it 's traffic. NAMESPACE_NAME: Optional. Namespace name. used to specify the target namespace to operate on. CONTAINER_NAME: Optional. If omitted, the first container in the pod will be chosen. INTERFACE_NAME: Optional. Pod Interface to capture from. If omitted, all Pod interfaces will be captured. CAPTURE_FILTER: Optional. specify a specific tcpdump capture filter. If omitted no filter will be used. OUTPUT_FILE: Optional. if specified, ksniff will redirect tcpdump output to local file instead of wireshark. Use ' - ' for stdout. LOCAL_TCPDUMP_FILE: Optional. if specified, ksniff will use this path as the local path of the static tcpdump binary. REMOTE_TCPDUMP_FILE: Optional. if specified, ksniff will use the specified path as the remote path to upload static tcpdump to. You'll need to remove the pods manually once you've finished analyzing the traffic.", "title": "Usage"}, {"location": "ksniff/#issues", "text": "", "title": "Issues"}, {"location": "ksniff/#wtap_encap-0", "text": "Upgrade your wireshark to a version greater or equal to 3.3.0 .", "title": "WTAP_ENCAP = 0"}, {"location": "ksniff/#references", "text": "Git", "title": "References"}, {"location": "kubernetes_debugging/", "text": "Network debugging \u2691 Sometimes you need to monitor the network traffic that goes between pods to solve an issue. There are different ways to see it: Using Mizu Running tcpdump against a running container Using ksniff Using ephemeral debug containers Of all the solutions, the cleaner and easier is to use Mizu . Running tcpdump against a running container \u2691 If the pod you want to analyze has root permissions (bad idea) you'll be able to install tcpdump ( apt-get install tcpdump ) and pipe it into wireshark on your local machine. kubectl exec my-app-pod -- tcpdump -i eth0 -w - | wireshark -k -i - There's some issues with this, though: You have to kubectl exec and install arbitrary software from the internet on a running Pod. This is fine for internet-connected dev environments, but probably not something you'd want to do (or be able to do) in production. If this app had been using a minimal distroless base image or was built with a buildpack you won't be able to install tcpdump . Using ephemeral debug containers \u2691 Kubernetes 1.16 has a new Ephemeral Containers feature that is perfect for our use case. With Ephemeral Containers, we can ask for a new temporary container with the image of our choosing to run inside an existing Pod. This means we can keep the main images for our applications lightweight and then bolt on a heavy image with all of our favorite debug tools when necessary. kubectl debug -it pod-to-debug-id --image = nicolaka/netshoot --target = pod-to-debug -- tcpdump -i eth0 -w - | wireshark -k -i Where nicolaka/netshoot is an optimized network troubleshooting docker. There's some issues with this too, for example, as of Kubernetes 1.21 Ephemeral containers are not enabled by default, so chances are you won't have access to them yet in your environment.", "title": "Debugging"}, {"location": "kubernetes_debugging/#network-debugging", "text": "Sometimes you need to monitor the network traffic that goes between pods to solve an issue. There are different ways to see it: Using Mizu Running tcpdump against a running container Using ksniff Using ephemeral debug containers Of all the solutions, the cleaner and easier is to use Mizu .", "title": "Network debugging"}, {"location": "kubernetes_debugging/#running-tcpdump-against-a-running-container", "text": "If the pod you want to analyze has root permissions (bad idea) you'll be able to install tcpdump ( apt-get install tcpdump ) and pipe it into wireshark on your local machine. kubectl exec my-app-pod -- tcpdump -i eth0 -w - | wireshark -k -i - There's some issues with this, though: You have to kubectl exec and install arbitrary software from the internet on a running Pod. This is fine for internet-connected dev environments, but probably not something you'd want to do (or be able to do) in production. If this app had been using a minimal distroless base image or was built with a buildpack you won't be able to install tcpdump .", "title": "Running tcpdump against a running container"}, {"location": "kubernetes_debugging/#using-ephemeral-debug-containers", "text": "Kubernetes 1.16 has a new Ephemeral Containers feature that is perfect for our use case. With Ephemeral Containers, we can ask for a new temporary container with the image of our choosing to run inside an existing Pod. This means we can keep the main images for our applications lightweight and then bolt on a heavy image with all of our favorite debug tools when necessary. kubectl debug -it pod-to-debug-id --image = nicolaka/netshoot --target = pod-to-debug -- tcpdump -i eth0 -w - | wireshark -k -i Where nicolaka/netshoot is an optimized network troubleshooting docker. There's some issues with this too, for example, as of Kubernetes 1.21 Ephemeral containers are not enabled by default, so chances are you won't have access to them yet in your environment.", "title": "Using ephemeral debug containers"}, {"location": "lazy_loading/", "text": "Lazy loading is an programming implementation paradigm which delays the evaluation of an expression until its value is needed and which also avoids repeated evaluations. Lazy evaluation is the preferred implementation when the operation is expensive, requiring either extensive processing time or memory. For example, in Python, one of the best-known techniques involving lazy evaluation is generators. Instead of creating whole sequences for the iteration, which can consume lots of memory, generators lazily evaluate the current need and yield one element at a time when requested. Other example are attributes that take long to compute: class Person : def __init__ ( self , name , occupation ): self . name = name self . occupation = occupation self . relatives = self . _get_all_relatives () def _get_all_relatives (): ... # This is an expensive operation This approach may cause initialization to take unnecessarily long, especially when you don't always need to access Person.relatives . A better strategy would be to get relatives when it's needed. class Person : def __init__ ( self , name , occupation ): self . name = name self . occupation = occupation self . _relatives = None @property def relatives ( self ): if self . _relatives is None : self . _relatives = ... # Get all relatives return self . _relatives In this case, the list of relatives is computed the first time Person.relatives is accessed. After that, it's stored in Person._relatives to prevent repeated evaluations. A perhaps more Pythonic approach would be to use a decorator that makes a property lazy-evaluated. def lazy_property ( fn ): '''Decorator that makes a property lazy-evaluated. ''' attr_name = '_lazy_' + fn . __name__ @property def _lazy_property ( self ): if not hasattr ( self , attr_name ): setattr ( self , attr_name , fn ( self )) return getattr ( self , attr_name ) return _lazy_property class Person : def __init__ ( self , name , occupation ): self . name = name self . occupation = occupation @lazy_property def relatives ( self ): # Get all relatives relatives = ... return relatives This removes a lot of boilerplate, especially when an object has many lazily-evaluated properties. Another approach is to use the getattr special method . References \u2691 Steven Loria article on Lazy Properties Yong Cui article on Lazy attributes", "title": "Lazy loading"}, {"location": "lazy_loading/#references", "text": "Steven Loria article on Lazy Properties Yong Cui article on Lazy attributes", "title": "References"}, {"location": "libreelec/", "text": "LibreElec is the lightweight distribution to run Kodi The root filesystem is mounted as readonly. Mount directories with sshfs \u2691 Install the network-tool LibreElec addon. Configure the ssh credentials Add the following service file: /storage/.config/system.d/storage-media.mount [Unit] Description=remote external drive share Requires=multi-user.target network-online.service After=multi-user.target network-online.service Before=kodi.service [Mount] What=/storage/.kodi/addons/virtual.network-tools/bin/sshfs#{{ user }}@{{ host }}:{{ source_path }} Where=/storage/media [Install] WantedBy=multi-user.target", "title": "Libreelec"}, {"location": "libreelec/#mount-directories-with-sshfs", "text": "Install the network-tool LibreElec addon. Configure the ssh credentials Add the following service file: /storage/.config/system.d/storage-media.mount [Unit] Description=remote external drive share Requires=multi-user.target network-online.service After=multi-user.target network-online.service Before=kodi.service [Mount] What=/storage/.kodi/addons/virtual.network-tools/bin/sshfs#{{ user }}@{{ host }}:{{ source_path }} Where=/storage/media [Install] WantedBy=multi-user.target", "title": "Mount directories with sshfs"}, {"location": "life_management/", "text": "I understand life management as the act of analyzing yourself and your interactions with the world to define processes and automations that shape the way to efficiently achieve your goals.", "title": "Life Management"}, {"location": "linux_snippets/", "text": "df and du showing different results \u2691 Sometimes on a linux machine you will notice that both df command (display free disk space) and du command (display disk usage statistics) report different output. Usually, df will output a bigger disk usage than du . The du command estimates file space usage, and the df command shows file system disk space usage. There are many reasons why this could be happening: Disk mounted over data \u2691 If you mount a disk on a directory that already holds data, then when you run du that data won't show, but df knows it's there. To troubleshoot this, umount one by one of your disks, and do an ls to see if there's any remaining data in the mount point. Used deleted files \u2691 When a file is deleted under Unix/Linux, the disk space occupied by the file will not be released immediately in some cases. The result of the command du doesn\u2019t include the size of the deleting file. But the impact of the command df for the deleting file\u2019s size due to its disk space is not released immediately. Hence, after deleting the file, the results of df and du are different until the disk space is freed. Open file descriptor is main causes of such wrong information. For example, if a file called /tmp/application.log is open by a third-party application OR by a user and the same file is deleted, both df and du report different outputs. You can use the lsof command to verify this: lsof | grep tmp To fix it: Use the lsof command as discussed above to find a deleted file opened by other users and apps. See how to list all users in the system for more info. Then, close those apps and log out of those Linux and Unix users. As a sysadmin you restart any process or kill the process under Linux and Unix that did not release the deleted file. Flush the filesystem using the sync command that synchronizes cached writes to persistent disk storage. If everything else fails, try restarting the system using the reboot command or shutdown command. Scan a physical page in Linux \u2691 Install xsane and run it. Git checkout to main with master as a fallback \u2691 I usually use the alias gcm to change to the main branch of the repository, given the change from main to master now I have some repos that use one or the other, but I still want gcm to go to the correct one. The solution is to use: alias gcm = 'git checkout \"$(git symbolic-ref refs/remotes/origin/HEAD | cut -d' / ' -f4)\"' Create QR code \u2691 qrencode -o qrcode.png 'Hello World!' Trim silences of sound files \u2691 To trim all silence longer than 2 seconds down to only 2 seconds long. sox in .wav out6.wav silence -l 1 0 .1 1 % -1 2 .0 1 % Note that SoX does nothing to bits of silence shorter than 2 seconds. If you encounter the sox FAIL formats: no handler for file extension 'mp3' error you'll need to install the libsox-fmt-all package. Adjust the replay gain of many sound files \u2691 sudo apt-get install python-rgain replaygain -f *.mp3 Check vulnerabilities in Node.js applications \u2691 With yarn audit you'll see the vulnerabilities, with yarn outdated you can see the packages that you need to update. Check vulnerabilities in rails dependencies \u2691 gem install bundler-audit cd project_with_gem_lock bundler-audit Create Basic Auth header \u2691 $ echo -n user:password | base64 dXNlcjpwYXNzd29yZA == Without the -n it won't work well. Install one package from Debian unstable \u2691 Add the unstable repository to your /etc/apt/sources.list # Unstable deb http://deb.debian.org/debian/ unstable main contrib non-free deb-src http://deb.debian.org/debian/ unstable main contrib non-free Configure apt to only use unstable when specified !!! note \"File: /etc/apt/preferences \" ``` Package: * Pin: release a=stable Pin-Priority: 700 Package: * Pin: release a=testing Pin-Priority: 600 Package: * Pin: release a=unstable Pin-Priority: 100 ``` Update the package data with apt-get update . See that the new versions are available with apt-cache policy <package_name> To install a package from unstable you can run apt-get install -t unstable <package_name> . Fix the following packages have been kept back \u2691 sudo apt-get --with-new-pkgs upgrade Monitor outgoing traffic \u2691 Easy and quick way watch & lsof \u2691 You can simply use a combination of watch & lsof command in Linux to get an idea of outgoing traffic on specific ports. Here is an example of outgoing traffic on ports 80 and 443 . $ watch -n1 lsof -i TCP:80,443 Here is a sample output. dropbox 2280 saml 23u IPv4 56015285 0t0 TCP www.example.local:56003->snt-re3-6c.sjc.dropbox.com:http ( ESTABLISHED ) thunderbi 2306 saml 60u IPv4 56093767 0t0 TCP www.example.local:34788->ord08s09-in-f20.1e100.net:https ( ESTABLISHED ) mono 2322 saml 15u IPv4 56012349 0t0 TCP www.example.local:54018->204-62-14-135.static.6sync.net:https ( ESTABLISHED ) chrome 4068 saml 175u IPv4 56021419 0t0 TCP www.example.local:42182->stackoverflow.com:http ( ESTABLISHED ) You'll miss the short lived connections though. Fine grained with tcpdump \u2691 You can also use tcpdump command to capture all raw packets, on all interfaces, on all ports, and write them to file. sudo tcpdump -tttt -i any -w /tmp/http.log Or you can limit it to a specific port adding the arguments port 443 or 80 . The -tttt flag is used to capture the packets with a human readable timestamp. To read the recorded information, run the tcpdump command with -A option. It will print ASCII text in recorded packets, that you can browse using page up/down keys. tcpdump -A -r /tmp/http.log | less However, tcpdump cannot decrypt information, so you cannot view information about HTTPS requests in it. Clean up system space \u2691 Clean package data \u2691 There is a couple of things to do when we want to free space in a no-brainer way. First, we want to remove those deb packages that get cached every time we do apt-get install . apt-get clean Also, the system might keep packages that were downloaded as dependencies but are not needed anymore. We can get rid of them with apt-get autoremove Remove data of unpurged packages . sudo apt-get purge $( dpkg -l | grep '^rc' | awk '{print $2}' ) If we want things tidy, we must know that whenever we apt-get remove a package, the configuration will be kept in case we want to install it again. In most cases we want to use apt-get purge . To clean those configurations from removed packages, we can use dpkg --list | grep \"^rc\" | cut -d \" \" -f 3 | xargs --no-run-if-empty sudo dpkg --purge So far we have not uninstalled anything. If now we want to inspect what packages are consuming the most space, we can type dpkg-query -Wf '${Installed-Size}\\t${Package}\\n' | sort -n Clean snap data \u2691 If you're using snap you can clean space by: Reduce the number of versions kept of a package with snap set system refresh.retain=2 Remove the old versions with clean_snap.sh #!/bin/bash #Removes old revisions of snaps #CLOSE ALL SNAPS BEFORE RUNNING THIS set -eu LANG = en_US.UTF-8 snap list --all | awk '/disabled/{print $1, $3}' | while read snapname revision ; do snap remove \" $snapname \" --revision = \" $revision \" done Clean journalctl data \u2691 Check how much space it's using: journalctl --disk-usage Rotate the logs: journalctl --rotate Then you have three ways to reduce the data: Clear journal log older than X days: journalctl --vacuum-time=2d Restrict logs to a certain size: journalctl --vacuum-size=100M Restrict number of log files: journactl --vacuum-files=5 . The operations above will affect the logs you have right now, but it won't solve the problem in the future. To let journalctl know the space you want to use open the /etc/systemd/journald.conf file and set the SystemMaxUse to the amount you want (for example 1000M for a gigabyte). Once edited restart the service with sudo systemctl restart systemd-journald . Clean up docker data \u2691 To remove unused docker data you can run docker system prune -a . This will remove: All stopped containers All networks not used by at least one container All images without at least one container associated to them All build cache Sometimes that's not enough, and your /var/lib/docker directory still weights more than it should. In those cases: Stop the docker service. Remove or move the data to another directory Start the docker service. In order not to loose your persisted data, you need to configure your dockers to mount the data from a directory that's not within /var/lib/docker . Set up docker logs rotation \u2691 By default, the stdout and stderr of the container are written in a JSON file located in /var/lib/docker/containers/[container-id]/[container-id]-json.log . If you leave it unattended, it can take up a large amount of disk space. If this JSON log file takes up a significant amount of the disk, we can purge it using the next command. truncate -s 0 <logfile> We could setup a cronjob to purge these JSON log files regularly. But for the long term, it would be better to setup log rotation. This can be done by adding the following values in /etc/docker/daemon.json . { \"log-driver\" : \"json-file\" , \"log-opts\" : { \"max-size\" : \"10m\" , \"max-file\" : \"10\" } } Clean old kernels \u2691 !!! warning \"I don't recommend using this step, rely on apt-get autoremove , it' safer\" The full command is dpkg -l linux-* | awk '/^ii/{ print $2}' | grep -v -e ` uname -r | cut -f1,2 -d \"-\" ` | grep -e [ 0 -9 ] | grep -E \"(image|headers)\" | xargs sudo apt-get -y purge To test what packages will it remove use: dpkg -l linux-* | awk '/^ii/{ print $2}' | grep -v -e ` uname -r | cut -f1,2 -d \"-\" ` | grep -e [ 0 -9 ] | grep -e \"(image|headers)\" | xargs sudo apt-get --dry-run remove Remember that your running kernel can be obtained by uname -r . Replace a string with sed recursively \u2691 find . -type f -exec sed -i 's/foo/bar/g' {} + Bypass client SSL certificate with cli tool \u2691 Websites that require clients to authorize with an TLS certificate are difficult to interact with through command line tools that don't support this feature. To solve it, we can use a transparent proxy that does the exchange for us. Export your certificate: If you have a p12 certificate, you first need to extract the key, crt and the ca from the certificate into the site.pem . openssl pkcs12 -in certificate.p12 -out site.key.pem -nocerts -nodes # It asks for the p12 password openssl pkcs12 -in certificate.p12 -out site.crt.pem -clcerts -nokeys openssl pkcs12 -cacerts -nokeys -in certificate.p12 -out site-ca-cert.ca cat site.key.pem site.crt.pem site-ca-cert.ca > site.pem Build the proxy ca: Then we merge the site and the client ca's into the site-ca-file.cert file: openssl s_client -connect www.site.org:443 2 >/dev/null | openssl x509 -text > site-ca-file.cert cat site-ca-cert.ca >> web-ca-file.cert Change your hosts file to redirect all requests to the proxy. # vim /etc/ hosts [...] 0 . 0 . 0 . 0 www.site.org Run the proxy docker run --rm \\ -v $( pwd ) :/certs/ \\ -p 3001 :3001 \\ -it ghostunnel/ghostunnel \\ client \\ --listen 0 .0.0.0:3001 \\ --target www.site.org:443 \\ --keystore /certs/site.pem \\ --cacert /certs/site-ca-file.cert \\ --unsafe-listen Run the command line tool using the http protocol on the port 3001: wpscan --url http://www.site.org:3001/ --disable-tls-checks Remember to clean up your env afterwards. Allocate space for a virtual filesystem \u2691 Also useful to simulate big files fallocate -l 20G /path/to/file Identify what a string or file contains \u2691 Identify anything. pyWhat easily lets you identify emails, IP addresses, and more. Feed it a .pcap file or some text and it'll tell you what it is. Split a file into many with equal number of lines \u2691 You could do something like this: split -l 200000 filename Which will create files each with 200000 lines named xaa , xab , xac , ... Check if an rsync command has gone well \u2691 Sometimes after you do an rsync between two directories of different devices (an usb and your hard drive for example), the sizes of the directories don't match. I've seen a difference of a 30% less on the destination. du , ncdu and and have a long story of reporting wrong sizes with advanced filesystems (ZFS, VxFS or compressing filesystems), these do a lot of things to reduce the disk usage (deduplication, compression, extents, files with holes...) which may lead to the difference in space. To check if everything went alright run diff -r --brief source/ dest/ , and check that there is no output. List all process swap usage \u2691 for file in /proc/*/status ; do awk '/VmSwap|Name/{printf $2 \" \" $3}END{ print \"\"}' $file ; done | sort -k 2 -n -r | less", "title": "Linux Snippets"}, {"location": "linux_snippets/#df-and-du-showing-different-results", "text": "Sometimes on a linux machine you will notice that both df command (display free disk space) and du command (display disk usage statistics) report different output. Usually, df will output a bigger disk usage than du . The du command estimates file space usage, and the df command shows file system disk space usage. There are many reasons why this could be happening:", "title": "df and du showing different results"}, {"location": "linux_snippets/#disk-mounted-over-data", "text": "If you mount a disk on a directory that already holds data, then when you run du that data won't show, but df knows it's there. To troubleshoot this, umount one by one of your disks, and do an ls to see if there's any remaining data in the mount point.", "title": "Disk mounted over data"}, {"location": "linux_snippets/#used-deleted-files", "text": "When a file is deleted under Unix/Linux, the disk space occupied by the file will not be released immediately in some cases. The result of the command du doesn\u2019t include the size of the deleting file. But the impact of the command df for the deleting file\u2019s size due to its disk space is not released immediately. Hence, after deleting the file, the results of df and du are different until the disk space is freed. Open file descriptor is main causes of such wrong information. For example, if a file called /tmp/application.log is open by a third-party application OR by a user and the same file is deleted, both df and du report different outputs. You can use the lsof command to verify this: lsof | grep tmp To fix it: Use the lsof command as discussed above to find a deleted file opened by other users and apps. See how to list all users in the system for more info. Then, close those apps and log out of those Linux and Unix users. As a sysadmin you restart any process or kill the process under Linux and Unix that did not release the deleted file. Flush the filesystem using the sync command that synchronizes cached writes to persistent disk storage. If everything else fails, try restarting the system using the reboot command or shutdown command.", "title": "Used deleted files"}, {"location": "linux_snippets/#scan-a-physical-page-in-linux", "text": "Install xsane and run it.", "title": "Scan a physical page in Linux"}, {"location": "linux_snippets/#git-checkout-to-main-with-master-as-a-fallback", "text": "I usually use the alias gcm to change to the main branch of the repository, given the change from main to master now I have some repos that use one or the other, but I still want gcm to go to the correct one. The solution is to use: alias gcm = 'git checkout \"$(git symbolic-ref refs/remotes/origin/HEAD | cut -d' / ' -f4)\"'", "title": "Git checkout to main with master as a fallback"}, {"location": "linux_snippets/#create-qr-code", "text": "qrencode -o qrcode.png 'Hello World!'", "title": "Create QR code"}, {"location": "linux_snippets/#trim-silences-of-sound-files", "text": "To trim all silence longer than 2 seconds down to only 2 seconds long. sox in .wav out6.wav silence -l 1 0 .1 1 % -1 2 .0 1 % Note that SoX does nothing to bits of silence shorter than 2 seconds. If you encounter the sox FAIL formats: no handler for file extension 'mp3' error you'll need to install the libsox-fmt-all package.", "title": "Trim silences of sound files"}, {"location": "linux_snippets/#adjust-the-replay-gain-of-many-sound-files", "text": "sudo apt-get install python-rgain replaygain -f *.mp3", "title": "Adjust the replay gain of many sound files"}, {"location": "linux_snippets/#check-vulnerabilities-in-nodejs-applications", "text": "With yarn audit you'll see the vulnerabilities, with yarn outdated you can see the packages that you need to update.", "title": "Check vulnerabilities in Node.js applications"}, {"location": "linux_snippets/#check-vulnerabilities-in-rails-dependencies", "text": "gem install bundler-audit cd project_with_gem_lock bundler-audit", "title": "Check vulnerabilities in rails dependencies"}, {"location": "linux_snippets/#create-basic-auth-header", "text": "$ echo -n user:password | base64 dXNlcjpwYXNzd29yZA == Without the -n it won't work well.", "title": "Create Basic Auth header"}, {"location": "linux_snippets/#install-one-package-from-debian-unstable", "text": "Add the unstable repository to your /etc/apt/sources.list # Unstable deb http://deb.debian.org/debian/ unstable main contrib non-free deb-src http://deb.debian.org/debian/ unstable main contrib non-free Configure apt to only use unstable when specified !!! note \"File: /etc/apt/preferences \" ``` Package: * Pin: release a=stable Pin-Priority: 700 Package: * Pin: release a=testing Pin-Priority: 600 Package: * Pin: release a=unstable Pin-Priority: 100 ``` Update the package data with apt-get update . See that the new versions are available with apt-cache policy <package_name> To install a package from unstable you can run apt-get install -t unstable <package_name> .", "title": "Install one package from Debian unstable"}, {"location": "linux_snippets/#fix-the-following-packages-have-been-kept-back", "text": "sudo apt-get --with-new-pkgs upgrade", "title": "Fix the following packages have been kept back"}, {"location": "linux_snippets/#monitor-outgoing-traffic", "text": "", "title": "Monitor outgoing traffic"}, {"location": "linux_snippets/#easy-and-quick-way-watch-lsof", "text": "You can simply use a combination of watch & lsof command in Linux to get an idea of outgoing traffic on specific ports. Here is an example of outgoing traffic on ports 80 and 443 . $ watch -n1 lsof -i TCP:80,443 Here is a sample output. dropbox 2280 saml 23u IPv4 56015285 0t0 TCP www.example.local:56003->snt-re3-6c.sjc.dropbox.com:http ( ESTABLISHED ) thunderbi 2306 saml 60u IPv4 56093767 0t0 TCP www.example.local:34788->ord08s09-in-f20.1e100.net:https ( ESTABLISHED ) mono 2322 saml 15u IPv4 56012349 0t0 TCP www.example.local:54018->204-62-14-135.static.6sync.net:https ( ESTABLISHED ) chrome 4068 saml 175u IPv4 56021419 0t0 TCP www.example.local:42182->stackoverflow.com:http ( ESTABLISHED ) You'll miss the short lived connections though.", "title": "Easy and quick way watch &amp; lsof"}, {"location": "linux_snippets/#fine-grained-with-tcpdump", "text": "You can also use tcpdump command to capture all raw packets, on all interfaces, on all ports, and write them to file. sudo tcpdump -tttt -i any -w /tmp/http.log Or you can limit it to a specific port adding the arguments port 443 or 80 . The -tttt flag is used to capture the packets with a human readable timestamp. To read the recorded information, run the tcpdump command with -A option. It will print ASCII text in recorded packets, that you can browse using page up/down keys. tcpdump -A -r /tmp/http.log | less However, tcpdump cannot decrypt information, so you cannot view information about HTTPS requests in it.", "title": "Fine grained with tcpdump"}, {"location": "linux_snippets/#clean-up-system-space", "text": "", "title": "Clean up system space"}, {"location": "linux_snippets/#clean-package-data", "text": "There is a couple of things to do when we want to free space in a no-brainer way. First, we want to remove those deb packages that get cached every time we do apt-get install . apt-get clean Also, the system might keep packages that were downloaded as dependencies but are not needed anymore. We can get rid of them with apt-get autoremove Remove data of unpurged packages . sudo apt-get purge $( dpkg -l | grep '^rc' | awk '{print $2}' ) If we want things tidy, we must know that whenever we apt-get remove a package, the configuration will be kept in case we want to install it again. In most cases we want to use apt-get purge . To clean those configurations from removed packages, we can use dpkg --list | grep \"^rc\" | cut -d \" \" -f 3 | xargs --no-run-if-empty sudo dpkg --purge So far we have not uninstalled anything. If now we want to inspect what packages are consuming the most space, we can type dpkg-query -Wf '${Installed-Size}\\t${Package}\\n' | sort -n", "title": "Clean package data"}, {"location": "linux_snippets/#clean-snap-data", "text": "If you're using snap you can clean space by: Reduce the number of versions kept of a package with snap set system refresh.retain=2 Remove the old versions with clean_snap.sh #!/bin/bash #Removes old revisions of snaps #CLOSE ALL SNAPS BEFORE RUNNING THIS set -eu LANG = en_US.UTF-8 snap list --all | awk '/disabled/{print $1, $3}' | while read snapname revision ; do snap remove \" $snapname \" --revision = \" $revision \" done", "title": "Clean snap data"}, {"location": "linux_snippets/#clean-journalctl-data", "text": "Check how much space it's using: journalctl --disk-usage Rotate the logs: journalctl --rotate Then you have three ways to reduce the data: Clear journal log older than X days: journalctl --vacuum-time=2d Restrict logs to a certain size: journalctl --vacuum-size=100M Restrict number of log files: journactl --vacuum-files=5 . The operations above will affect the logs you have right now, but it won't solve the problem in the future. To let journalctl know the space you want to use open the /etc/systemd/journald.conf file and set the SystemMaxUse to the amount you want (for example 1000M for a gigabyte). Once edited restart the service with sudo systemctl restart systemd-journald .", "title": "Clean journalctl data"}, {"location": "linux_snippets/#clean-up-docker-data", "text": "To remove unused docker data you can run docker system prune -a . This will remove: All stopped containers All networks not used by at least one container All images without at least one container associated to them All build cache Sometimes that's not enough, and your /var/lib/docker directory still weights more than it should. In those cases: Stop the docker service. Remove or move the data to another directory Start the docker service. In order not to loose your persisted data, you need to configure your dockers to mount the data from a directory that's not within /var/lib/docker .", "title": "Clean up docker data"}, {"location": "linux_snippets/#set-up-docker-logs-rotation", "text": "By default, the stdout and stderr of the container are written in a JSON file located in /var/lib/docker/containers/[container-id]/[container-id]-json.log . If you leave it unattended, it can take up a large amount of disk space. If this JSON log file takes up a significant amount of the disk, we can purge it using the next command. truncate -s 0 <logfile> We could setup a cronjob to purge these JSON log files regularly. But for the long term, it would be better to setup log rotation. This can be done by adding the following values in /etc/docker/daemon.json . { \"log-driver\" : \"json-file\" , \"log-opts\" : { \"max-size\" : \"10m\" , \"max-file\" : \"10\" } }", "title": "Set up docker logs rotation"}, {"location": "linux_snippets/#clean-old-kernels", "text": "!!! warning \"I don't recommend using this step, rely on apt-get autoremove , it' safer\" The full command is dpkg -l linux-* | awk '/^ii/{ print $2}' | grep -v -e ` uname -r | cut -f1,2 -d \"-\" ` | grep -e [ 0 -9 ] | grep -E \"(image|headers)\" | xargs sudo apt-get -y purge To test what packages will it remove use: dpkg -l linux-* | awk '/^ii/{ print $2}' | grep -v -e ` uname -r | cut -f1,2 -d \"-\" ` | grep -e [ 0 -9 ] | grep -e \"(image|headers)\" | xargs sudo apt-get --dry-run remove Remember that your running kernel can be obtained by uname -r .", "title": "Clean old kernels"}, {"location": "linux_snippets/#replace-a-string-with-sed-recursively", "text": "find . -type f -exec sed -i 's/foo/bar/g' {} +", "title": "Replace a string with sed recursively"}, {"location": "linux_snippets/#bypass-client-ssl-certificate-with-cli-tool", "text": "Websites that require clients to authorize with an TLS certificate are difficult to interact with through command line tools that don't support this feature. To solve it, we can use a transparent proxy that does the exchange for us. Export your certificate: If you have a p12 certificate, you first need to extract the key, crt and the ca from the certificate into the site.pem . openssl pkcs12 -in certificate.p12 -out site.key.pem -nocerts -nodes # It asks for the p12 password openssl pkcs12 -in certificate.p12 -out site.crt.pem -clcerts -nokeys openssl pkcs12 -cacerts -nokeys -in certificate.p12 -out site-ca-cert.ca cat site.key.pem site.crt.pem site-ca-cert.ca > site.pem Build the proxy ca: Then we merge the site and the client ca's into the site-ca-file.cert file: openssl s_client -connect www.site.org:443 2 >/dev/null | openssl x509 -text > site-ca-file.cert cat site-ca-cert.ca >> web-ca-file.cert Change your hosts file to redirect all requests to the proxy. # vim /etc/ hosts [...] 0 . 0 . 0 . 0 www.site.org Run the proxy docker run --rm \\ -v $( pwd ) :/certs/ \\ -p 3001 :3001 \\ -it ghostunnel/ghostunnel \\ client \\ --listen 0 .0.0.0:3001 \\ --target www.site.org:443 \\ --keystore /certs/site.pem \\ --cacert /certs/site-ca-file.cert \\ --unsafe-listen Run the command line tool using the http protocol on the port 3001: wpscan --url http://www.site.org:3001/ --disable-tls-checks Remember to clean up your env afterwards.", "title": "Bypass client SSL certificate with cli tool"}, {"location": "linux_snippets/#allocate-space-for-a-virtual-filesystem", "text": "Also useful to simulate big files fallocate -l 20G /path/to/file", "title": "Allocate space for a virtual filesystem"}, {"location": "linux_snippets/#identify-what-a-string-or-file-contains", "text": "Identify anything. pyWhat easily lets you identify emails, IP addresses, and more. Feed it a .pcap file or some text and it'll tell you what it is.", "title": "Identify what a string or file contains"}, {"location": "linux_snippets/#split-a-file-into-many-with-equal-number-of-lines", "text": "You could do something like this: split -l 200000 filename Which will create files each with 200000 lines named xaa , xab , xac , ...", "title": "Split a file into many with equal number of lines"}, {"location": "linux_snippets/#check-if-an-rsync-command-has-gone-well", "text": "Sometimes after you do an rsync between two directories of different devices (an usb and your hard drive for example), the sizes of the directories don't match. I've seen a difference of a 30% less on the destination. du , ncdu and and have a long story of reporting wrong sizes with advanced filesystems (ZFS, VxFS or compressing filesystems), these do a lot of things to reduce the disk usage (deduplication, compression, extents, files with holes...) which may lead to the difference in space. To check if everything went alright run diff -r --brief source/ dest/ , and check that there is no output.", "title": "Check if an rsync command has gone well"}, {"location": "linux_snippets/#list-all-process-swap-usage", "text": "for file in /proc/*/status ; do awk '/VmSwap|Name/{printf $2 \" \" $3}END{ print \"\"}' $file ; done | sort -k 2 -n -r | less", "title": "List all process swap usage"}, {"location": "maison/", "text": "Maison is a Python library to read configuration settings from configuration files using pydantic behind the scenes. It's useful to parse TOML config files. Note: \"If you want to use YAML for your config files use goodconf instead.\" Installation \u2691 pip install maison Usage \u2691 from maison import ProjectConfig config = ProjectConfig ( project_name = \"acme\" ) foo_option = config . get_option ( \"foo\" ) print ( foo_option ) Read from file \u2691 By default, maison will look for a pyproject.toml file. If you prefer to look elsewhere, provide a source_files list to ProjectConfig and maison will select the first source file it finds from the list. from maison import ProjectConfig config = ProjectConfig ( project_name = \"acme\" , source_files = [ \"acme.ini\" , \"pyproject.toml\" ]) References \u2691 Git Docs", "title": "Maison"}, {"location": "maison/#installation", "text": "pip install maison", "title": "Installation"}, {"location": "maison/#usage", "text": "from maison import ProjectConfig config = ProjectConfig ( project_name = \"acme\" ) foo_option = config . get_option ( \"foo\" ) print ( foo_option )", "title": "Usage"}, {"location": "maison/#read-from-file", "text": "By default, maison will look for a pyproject.toml file. If you prefer to look elsewhere, provide a source_files list to ProjectConfig and maison will select the first source file it finds from the list. from maison import ProjectConfig config = ProjectConfig ( project_name = \"acme\" , source_files = [ \"acme.ini\" , \"pyproject.toml\" ])", "title": "Read from file"}, {"location": "maison/#references", "text": "Git Docs", "title": "References"}, {"location": "map_management/", "text": "As a privacy minded human, I try to avoid using proprietary software and services as much as possible. Map management is not an exception. Google maps is monopolizing mapping and routing, but there are better alternatives out there. For navigating on the go, I strongly recommend OSMand+ , for browsing maps in the browser, use OpenStreetMaps or CyclOSM if you want to move by bike. To plan routes, you can use brouter.de , it works perfectly for bikes. For hiking is awesome too, it shows you a lot of data needed to plan your tracks (check the settings on the right). If you want to invest a little more time, you can even set your personalize profiles , so that the routing algorithm prioritizes the routes to your desires. It's based on brouter and both can be self-hosted , although brouter does not yet use Docker .", "title": "Map management"}, {"location": "mbsync/", "text": "mbsync is a command line application which synchronizes mailboxes; currently Maildir and IMAP4 mailboxes are supported. New messages, message deletions and flag changes can be propagated both ways; the operation set can be selected in a fine-grained manner. Installation \u2691 apt-get install isync Configuration \u2691 Assuming that you want to sync the mails of example@examplehost.com and that you have your password stored in pass under mail/example . File: ~/.mbsyncrc IMAPAccount example Host examplehost.com User \"example@examplehost.com\" PassCmd \"/usr/bin/pass mail/example\" IMAPStore example-remote Account example UseNamespace no MaildirStore example-local Path ~/mail/example/ Inbox ~/mail/example/Inbox Channel example Master :example-remote: Slave :example-local: Create Both Patterns * SyncState * CopyArrivalDate yes Sync Pull You need to manually create the directories where you store the emails. mkdir -p ~/mail/example References \u2691 Homepage", "title": "mbsync"}, {"location": "mbsync/#installation", "text": "apt-get install isync", "title": "Installation"}, {"location": "mbsync/#configuration", "text": "Assuming that you want to sync the mails of example@examplehost.com and that you have your password stored in pass under mail/example . File: ~/.mbsyncrc IMAPAccount example Host examplehost.com User \"example@examplehost.com\" PassCmd \"/usr/bin/pass mail/example\" IMAPStore example-remote Account example UseNamespace no MaildirStore example-local Path ~/mail/example/ Inbox ~/mail/example/Inbox Channel example Master :example-remote: Slave :example-local: Create Both Patterns * SyncState * CopyArrivalDate yes Sync Pull You need to manually create the directories where you store the emails. mkdir -p ~/mail/example", "title": "Configuration"}, {"location": "mbsync/#references", "text": "Homepage", "title": "References"}, {"location": "mdformat/", "text": "MDFormat is an opinionated Markdown formatter that can be used to enforce a consistent style in Markdown files. Mdformat is a Unix-style command-line tool as well as a Python library. The features/opinions of the formatter include: Consistent indentation and whitespace across the board Always use ATX style headings Move all link references to the bottom of the document (sorted by label) Reformat indented code blocks as fenced code blocks Use 1. as the ordered list marker if possible, also for noninitial list items. It's based on the markdown-it-py Markdown parser, which is a Python implementation of markdown-it . Installation \u2691 By default it uses CommonMark support: pip install mdformat This won't support task lists , if you want them use the github flavoured parser instead: pip install mdformat-gfm You may want to also install some interesting plugins: mdformat-beautysh : format bash and sh code blocks. mdformat-black : format python code blocks. mdformat-config : format json , toml and yaml code blocks. mdformat-web : format javascript , css , html and xml code blocks. mdformat-tables : Adds support for Github Flavored Markdown style tables. mdformat-frontmatter : Adds support for the yaml header with metadata of the file. To install them with pipx you can run: pipx install --include-deps mdformat-gfm pipx inject mdformat-gfm mdformat-beautysh mdformat-black mdformat-config \\ mdformat-web mdformat-tables mdformat-frontmatter Desires \u2691 These are the functionalities I miss when writing markdown that can be currently fixed with mdformat : Long lines are wrapped. Long lines in lists are wrapped and the indentation is respected. Add correct blank lines between sections. I haven't found yet a way to achieve: Links are sent to the bottom of the document. Do typographic replacements End paragraphs with a dot. Developing mdformat plugins \u2691 There are two kinds of plugins: Formatters: They change the output of the text. For example mdformatormat-black . Parsers: They are extensions to the base CommonMark parser. You can see some plugin examples here . Issues \u2691 It doesn't yet support admonitions You can't ignore some files , nor some part of the file References \u2691 Docs Source", "title": "mdformat"}, {"location": "mdformat/#installation", "text": "By default it uses CommonMark support: pip install mdformat This won't support task lists , if you want them use the github flavoured parser instead: pip install mdformat-gfm You may want to also install some interesting plugins: mdformat-beautysh : format bash and sh code blocks. mdformat-black : format python code blocks. mdformat-config : format json , toml and yaml code blocks. mdformat-web : format javascript , css , html and xml code blocks. mdformat-tables : Adds support for Github Flavored Markdown style tables. mdformat-frontmatter : Adds support for the yaml header with metadata of the file. To install them with pipx you can run: pipx install --include-deps mdformat-gfm pipx inject mdformat-gfm mdformat-beautysh mdformat-black mdformat-config \\ mdformat-web mdformat-tables mdformat-frontmatter", "title": "Installation"}, {"location": "mdformat/#desires", "text": "These are the functionalities I miss when writing markdown that can be currently fixed with mdformat : Long lines are wrapped. Long lines in lists are wrapped and the indentation is respected. Add correct blank lines between sections. I haven't found yet a way to achieve: Links are sent to the bottom of the document. Do typographic replacements End paragraphs with a dot.", "title": "Desires"}, {"location": "mdformat/#developing-mdformat-plugins", "text": "There are two kinds of plugins: Formatters: They change the output of the text. For example mdformatormat-black . Parsers: They are extensions to the base CommonMark parser. You can see some plugin examples here .", "title": "Developing mdformat plugins"}, {"location": "mdformat/#issues", "text": "It doesn't yet support admonitions You can't ignore some files , nor some part of the file", "title": "Issues"}, {"location": "mdformat/#references", "text": "Docs Source", "title": "References"}, {"location": "meditation/", "text": "Meditation is a practice where an individual uses a technique,such as mindfulness, or focusing the mind on a particular object, thought, or activity, to train attention and awareness, and achieve a mentally clear and emotionally calm and stable state. Meditation may reduce stress, anxiety, depression, and pain, and enhance peace, perception, self-concept, and well-being. Types of meditation \u2691 Although there isn't a right or wrong way to meditate, it\u2019s important to find a practice that meets your needs and complements your personality. There are nine popular types of meditation practice: Mindfulness meditation : You pay attention to your thoughts as they pass through your mind. You don't judge the thoughts or become involved with them. You simply observe and take note of any patterns. This practice combines concentration with awareness. You may find it helpful to focus on an object or your breath while you observe any bodily sensations, thoughts, or feelings. This type of meditation is good for people who don\u2019t have a teacher to guide them, as it can be easily practiced alone. Focused meditation : Involves concentration using any of the five senses. For example, you can focus on something internal, like your breath, or you can bring in external influences to help focus your attention. Try counting mala beads, listening to a gong, or staring at a candle flame. This practice may be simple in theory, but it can be difficult for beginners to hold their focus for longer than a few minutes at first. If your mind does wander, it\u2019s important to come back to the practice and refocus. As the name suggests, this practice is ideal for anyone who requires additional focus in their life. Movement meditation : It\u2019s an active form of meditation where the movement guides you. It can be achieved through yoga, martial arts or by walking through the woods, gardening, qigong, and other gentle forms of motion. Movement meditation is good for people who find peace in action and prefer to let their minds wander. Mantra meditation : Uses a repetitive sound to clear the mind. It can be a word, phrase, or sound, such as the popular \u201cOm.\u201d It doesn't matter if your mantra is spoken loudly or quietly. After chanting the mantra for some time, you\u2019ll be more alert and in tune with your environment. This allows you to experience deeper levels of awareness. Some people enjoy mantra meditation because they find it easier to focus on a word than on their breath. This is also a good practice for people who don't like silence and enjoy repetition. Transcendental Meditation : It is more customizable than mantra meditation, using a mantra or series of words that are specific to each practitioner. This practice is for those who like structure and are serious about maintaining a meditation practice. Progressive relaxation : Also known as body scan meditation, it's a practice aimed at reducing tension in the body and promoting relaxation. Oftentimes, this form of meditation involves slowly tightening and relaxing one muscle group at a time throughout the body. In some cases, it may also encourage you to imagine a gentle wave flowing through your body to help release any tension. This form of meditation is often used to relieve stress and unwind before bedtime. Loving-kindness meditation : is used to strengthen feelings of compassion, kindness, and acceptance toward oneself and others. It typically involves opening the mind to receive love from others and then sending a series of well wishes to loved ones, friends, acquaintances, and all living beings. Because this type of meditation is intended to promote compassion and kindness, it may be ideal for those holding feelings of anger or resentment. * Visualization meditation : Is a technique focused on enhancing feelings of relaxation, peace, and calmness by visualizing positive scenes or images. With this practice, it\u2019s important to imagine the scene vividly and use all five senses to add as much detail as possible. Another form of visualization meditation involves imagining yourself succeeding at specific goals, which is intended to increase focus and motivation. Many people use visualization meditation to boost their mood, reduce stress levels, and promote inner peace. Spiritual meditation : Spiritual meditation is used in Eastern religions, such as Hinduism and Daoism, and in Christian faith.. It\u2019s similar to prayer in that you reflect on the silence around you and seek a deeper connection with your God or Universe. How to get started \u2691 The easiest way to begin is to sit quietly and focus on your breath for 20 minutes every day. If it's too much for you, start in small moments of time, even 5 or 10 minutes, and grow from there. References \u2691 healthline article on types of meditation NonCompete video on meditation for anti-capitalists To review \u2691 https://wiki.nikitavoloboev.xyz/mindfulness/meditation https://www.healthline.com/health/4-7-8-breathing#Other-techniques-to-help-you-sleep https://threader.app/thread/1261481222359801856 https://quietkit.com/box-breathing/ https://www.healthline.com/health/mental-health/best-mindfulness-blogs#8 https://www.mindful.org/how-to-meditate/ Books \u2691 The Mind Illuminated: A Complete Meditation Guide Integrating Buddhist Wisdom and Brain Science by Culadasa (John Yates)", "title": "Meditation"}, {"location": "meditation/#types-of-meditation", "text": "Although there isn't a right or wrong way to meditate, it\u2019s important to find a practice that meets your needs and complements your personality. There are nine popular types of meditation practice: Mindfulness meditation : You pay attention to your thoughts as they pass through your mind. You don't judge the thoughts or become involved with them. You simply observe and take note of any patterns. This practice combines concentration with awareness. You may find it helpful to focus on an object or your breath while you observe any bodily sensations, thoughts, or feelings. This type of meditation is good for people who don\u2019t have a teacher to guide them, as it can be easily practiced alone. Focused meditation : Involves concentration using any of the five senses. For example, you can focus on something internal, like your breath, or you can bring in external influences to help focus your attention. Try counting mala beads, listening to a gong, or staring at a candle flame. This practice may be simple in theory, but it can be difficult for beginners to hold their focus for longer than a few minutes at first. If your mind does wander, it\u2019s important to come back to the practice and refocus. As the name suggests, this practice is ideal for anyone who requires additional focus in their life. Movement meditation : It\u2019s an active form of meditation where the movement guides you. It can be achieved through yoga, martial arts or by walking through the woods, gardening, qigong, and other gentle forms of motion. Movement meditation is good for people who find peace in action and prefer to let their minds wander. Mantra meditation : Uses a repetitive sound to clear the mind. It can be a word, phrase, or sound, such as the popular \u201cOm.\u201d It doesn't matter if your mantra is spoken loudly or quietly. After chanting the mantra for some time, you\u2019ll be more alert and in tune with your environment. This allows you to experience deeper levels of awareness. Some people enjoy mantra meditation because they find it easier to focus on a word than on their breath. This is also a good practice for people who don't like silence and enjoy repetition. Transcendental Meditation : It is more customizable than mantra meditation, using a mantra or series of words that are specific to each practitioner. This practice is for those who like structure and are serious about maintaining a meditation practice. Progressive relaxation : Also known as body scan meditation, it's a practice aimed at reducing tension in the body and promoting relaxation. Oftentimes, this form of meditation involves slowly tightening and relaxing one muscle group at a time throughout the body. In some cases, it may also encourage you to imagine a gentle wave flowing through your body to help release any tension. This form of meditation is often used to relieve stress and unwind before bedtime. Loving-kindness meditation : is used to strengthen feelings of compassion, kindness, and acceptance toward oneself and others. It typically involves opening the mind to receive love from others and then sending a series of well wishes to loved ones, friends, acquaintances, and all living beings. Because this type of meditation is intended to promote compassion and kindness, it may be ideal for those holding feelings of anger or resentment. * Visualization meditation : Is a technique focused on enhancing feelings of relaxation, peace, and calmness by visualizing positive scenes or images. With this practice, it\u2019s important to imagine the scene vividly and use all five senses to add as much detail as possible. Another form of visualization meditation involves imagining yourself succeeding at specific goals, which is intended to increase focus and motivation. Many people use visualization meditation to boost their mood, reduce stress levels, and promote inner peace. Spiritual meditation : Spiritual meditation is used in Eastern religions, such as Hinduism and Daoism, and in Christian faith.. It\u2019s similar to prayer in that you reflect on the silence around you and seek a deeper connection with your God or Universe.", "title": "Types of meditation"}, {"location": "meditation/#how-to-get-started", "text": "The easiest way to begin is to sit quietly and focus on your breath for 20 minutes every day. If it's too much for you, start in small moments of time, even 5 or 10 minutes, and grow from there.", "title": "How to get started"}, {"location": "meditation/#references", "text": "healthline article on types of meditation NonCompete video on meditation for anti-capitalists", "title": "References"}, {"location": "meditation/#to-review", "text": "https://wiki.nikitavoloboev.xyz/mindfulness/meditation https://www.healthline.com/health/4-7-8-breathing#Other-techniques-to-help-you-sleep https://threader.app/thread/1261481222359801856 https://quietkit.com/box-breathing/ https://www.healthline.com/health/mental-health/best-mindfulness-blogs#8 https://www.mindful.org/how-to-meditate/", "title": "To review"}, {"location": "meditation/#books", "text": "The Mind Illuminated: A Complete Meditation Guide Integrating Buddhist Wisdom and Brain Science by Culadasa (John Yates)", "title": "Books"}, {"location": "mentoring/", "text": "Mentoring is a process for the informal transmission of knowledge, social capital, and the psychosocial support perceived by the recipient as relevant to work, career, or professional development; mentoring entails informal communication, usually face-to-face and during a sustained period of time, between a person who is perceived to have greater relevant knowledge, wisdom, or experience (the mentor) and a person who is perceived to have less (the apprentice). Obstacles \u2691 Apprentice obstacles \u2691 The most common obstacles I've found apprentices have in their early steps of learning are: Not knowing where to start. Not having a clear roadmap. Having wrong expectations. Feeling overwhelmed by big tasks. Not knowing how to break a big task in small actionable steps. Given a requirement, design possible solutions and choose the best one. Feeling insecure about themselves. Suffering from the impostor syndrome A mentor can greatly help the apprentice overcome them. Mentor obstacles \u2691 The most common obstacles I've found as a mentor are: Use concepts that the apprentice doesn't yet understand. Try to impose my way of doing things. Try to impose the best solution or practices even though they are out of reach of the apprentice yet. Mentorship principles \u2691 People involved in a mentorship experience a strong personal relationship, in order to make it pleasant and healthy it must be based on the next principles: Care Equality Transparency Care \u2691 As in any relationship, care must be one of the main focuses of both parties, by care I mean: Actively read the other person mood and state and adjust your behaviours accordingly. Ask for the other person's well being, keep track of the events of their lives, and ask them how they went afterwards. Know your weak spots, have an improvement plan, and make them visible when they arise. Actively search for ways to make their life easier and more pleasant. Respect the other person's time, don't be late. Men must put special interest in this point as we're usually not taught on caring for others. Equality \u2691 There's a high risk of having unhealthy power dynamics where the mentor is taken as in a higher position than the apprentice because he has more knowledge in the specific field of study. The reality is that there is a lot more involved in the experience than the transmission of knowledge of the field from mentor to apprentice. Only if you see yourself as equals you can build the best experience. Transparency \u2691 Shared roles \u2691 Actively defend and follow the mentorship principles . Review and improve the mentoring workflows. Mentor roles \u2691 The mentor can help through the next ways: Roadmap definition and maintenance Task management Overcome the mentor's obstacles Roadmap definition and maintenance \u2691 Get to know each other \u2691 First of all we need to know what are the underlying goals of the apprentice in order to sketch the best roadmap. Define and maintain a roadmap \u2691 It's very important that the apprentice has a clear idea of In order to Beginner Junior Overcome the mentor obstacles \u2691 Be attentive of the apprentice reactions Task management \u2691 Apprentice roles \u2691 Try it's best to follow the agreed roadmap and tasks. Analyze themselves with respect to the mentoring workflows. Overcome the apprentice's obstacles. *", "title": "Mentoring"}, {"location": "mentoring/#obstacles", "text": "", "title": "Obstacles"}, {"location": "mentoring/#apprentice-obstacles", "text": "The most common obstacles I've found apprentices have in their early steps of learning are: Not knowing where to start. Not having a clear roadmap. Having wrong expectations. Feeling overwhelmed by big tasks. Not knowing how to break a big task in small actionable steps. Given a requirement, design possible solutions and choose the best one. Feeling insecure about themselves. Suffering from the impostor syndrome A mentor can greatly help the apprentice overcome them.", "title": "Apprentice obstacles"}, {"location": "mentoring/#mentor-obstacles", "text": "The most common obstacles I've found as a mentor are: Use concepts that the apprentice doesn't yet understand. Try to impose my way of doing things. Try to impose the best solution or practices even though they are out of reach of the apprentice yet.", "title": "Mentor obstacles"}, {"location": "mentoring/#mentorship-principles", "text": "People involved in a mentorship experience a strong personal relationship, in order to make it pleasant and healthy it must be based on the next principles: Care Equality Transparency", "title": "Mentorship principles"}, {"location": "mentoring/#care", "text": "As in any relationship, care must be one of the main focuses of both parties, by care I mean: Actively read the other person mood and state and adjust your behaviours accordingly. Ask for the other person's well being, keep track of the events of their lives, and ask them how they went afterwards. Know your weak spots, have an improvement plan, and make them visible when they arise. Actively search for ways to make their life easier and more pleasant. Respect the other person's time, don't be late. Men must put special interest in this point as we're usually not taught on caring for others.", "title": "Care"}, {"location": "mentoring/#equality", "text": "There's a high risk of having unhealthy power dynamics where the mentor is taken as in a higher position than the apprentice because he has more knowledge in the specific field of study. The reality is that there is a lot more involved in the experience than the transmission of knowledge of the field from mentor to apprentice. Only if you see yourself as equals you can build the best experience.", "title": "Equality"}, {"location": "mentoring/#transparency", "text": "", "title": "Transparency"}, {"location": "mentoring/#shared-roles", "text": "Actively defend and follow the mentorship principles . Review and improve the mentoring workflows.", "title": "Shared roles"}, {"location": "mentoring/#mentor-roles", "text": "The mentor can help through the next ways: Roadmap definition and maintenance Task management Overcome the mentor's obstacles", "title": "Mentor roles"}, {"location": "mentoring/#roadmap-definition-and-maintenance", "text": "", "title": "Roadmap definition and maintenance"}, {"location": "mentoring/#get-to-know-each-other", "text": "First of all we need to know what are the underlying goals of the apprentice in order to sketch the best roadmap.", "title": "Get to know each other"}, {"location": "mentoring/#define-and-maintain-a-roadmap", "text": "It's very important that the apprentice has a clear idea of In order to Beginner Junior", "title": "Define and maintain a roadmap"}, {"location": "mentoring/#overcome-the-mentor-obstacles", "text": "Be attentive of the apprentice reactions", "title": "Overcome the mentor obstacles"}, {"location": "mentoring/#task-management", "text": "", "title": "Task management"}, {"location": "mentoring/#apprentice-roles", "text": "Try it's best to follow the agreed roadmap and tasks. Analyze themselves with respect to the mentoring workflows. Overcome the apprentice's obstacles. *", "title": "Apprentice roles"}, {"location": "mermaidjs/", "text": "MermaidJS is a Javascript library that lets you create diagrams using text and code. It can render the next diagram types : Flowchart Sequence. Gantt Class Git graph Entity Relationship User journey Installation \u2691 Installing it requires node, I've only used it in mkdocs , which is easier to install and use. Usage \u2691 Flowchart \u2691 It can have two orientations top to bottom ( TB ) or left to right ( LR ). graph TD Start --> Stop By default the text shown is the same as the id, if you need a big text it's recommended to use the id1[This is the text in the box] syntax so it's easy to reference the node in the relationships. To link nodes, use --> or --- . If you cant to add text to the link use A-- text -->B Adding links \u2691 You can add click events to the diagrams: graph LR; A-->B; B-->C; C-->D; click A callback \"Tooltip for a callback\" click B \"http://www.github.com\" \"This is a tooltip for a link\" click A call callback() \"Tooltip for a callback\" click B href \"http://www.github.com\" \"This is a tooltip for a link\" By default the links are opened in the same browser tab/window. It is possible to change this by adding a link target to the click definition ( _self , _blank , _parent , or _top ). graph LR; A-->B; B-->C; C-->D; D-->E; click A \"http://www.github.com\" _blank Node styling \u2691 You can define the style for each node with: graph LR id1(Start)-->id2(Stop) style id1 fill:#f9f,stroke:#333,stroke-width:4px style id2 fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 Or if you're going to use the same style for multiple nodes, you can define classes: graph LR A:::someclass --> B classDef someclass fill:#f96; References \u2691 Docs", "title": "MermaidJS"}, {"location": "mermaidjs/#installation", "text": "Installing it requires node, I've only used it in mkdocs , which is easier to install and use.", "title": "Installation"}, {"location": "mermaidjs/#usage", "text": "", "title": "Usage"}, {"location": "mermaidjs/#flowchart", "text": "It can have two orientations top to bottom ( TB ) or left to right ( LR ). graph TD Start --> Stop By default the text shown is the same as the id, if you need a big text it's recommended to use the id1[This is the text in the box] syntax so it's easy to reference the node in the relationships. To link nodes, use --> or --- . If you cant to add text to the link use A-- text -->B", "title": "Flowchart"}, {"location": "mermaidjs/#adding-links", "text": "You can add click events to the diagrams: graph LR; A-->B; B-->C; C-->D; click A callback \"Tooltip for a callback\" click B \"http://www.github.com\" \"This is a tooltip for a link\" click A call callback() \"Tooltip for a callback\" click B href \"http://www.github.com\" \"This is a tooltip for a link\" By default the links are opened in the same browser tab/window. It is possible to change this by adding a link target to the click definition ( _self , _blank , _parent , or _top ). graph LR; A-->B; B-->C; C-->D; D-->E; click A \"http://www.github.com\" _blank", "title": "Adding links"}, {"location": "mermaidjs/#node-styling", "text": "You can define the style for each node with: graph LR id1(Start)-->id2(Stop) style id1 fill:#f9f,stroke:#333,stroke-width:4px style id2 fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 Or if you're going to use the same style for multiple nodes, you can define classes: graph LR A:::someclass --> B classDef someclass fill:#f96;", "title": "Node styling"}, {"location": "mermaidjs/#references", "text": "Docs", "title": "References"}, {"location": "mizu/", "text": "Mizu is an API Traffic Viewer for Kubernetes, think TCPDump and Chrome Dev Tools combined. Installation \u2691 curl -Lo mizu \\ https://github.com/up9inc/mizu/releases/latest/download/mizu_linux_amd64 \\ && chmod 755 mizu Usage \u2691 At the core of Mizu functionality is the pod tap mizu tap <podname> To view traffic of several pods, identified by a regular expression: mizu tap \"(catalo*|front-end*)\" After tapping your pods, Mizu will tell you that \"Web interface is now available at https://localhost:8899/ . Visit the link from Mizu to view traffic in the Mizu UI . References \u2691 Homepage Docs", "title": "Mizu"}, {"location": "mizu/#installation", "text": "curl -Lo mizu \\ https://github.com/up9inc/mizu/releases/latest/download/mizu_linux_amd64 \\ && chmod 755 mizu", "title": "Installation"}, {"location": "mizu/#usage", "text": "At the core of Mizu functionality is the pod tap mizu tap <podname> To view traffic of several pods, identified by a regular expression: mizu tap \"(catalo*|front-end*)\" After tapping your pods, Mizu will tell you that \"Web interface is now available at https://localhost:8899/ . Visit the link from Mizu to view traffic in the Mizu UI .", "title": "Usage"}, {"location": "mizu/#references", "text": "Homepage Docs", "title": "References"}, {"location": "money_management/", "text": "Money management is the act of analyzing where you spend your money on with the least amount of mental load. Some years ago I started using the double entry counting method with beancount . System inputs \u2691 I have two types of financial transactions to track: The credit/debit card movements: Easy to track as usually the banks support exporting them as CSV, and beancount have specific bank importers . The cash movements: Harder to track as you need to keep them manually. This has been my biggest source of errors, once I understood how to correctly use beancount . In the latest iteration, I'm using the cone Android app to keep track of these expenses. Workflow \u2691 Beancount ledger organization \u2691 My beancount project directory tree is: . \u251c\u2500\u2500 .git \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 2011 \u2502 \u251c\u2500\u2500 09.book \u2502 \u251c\u2500\u2500 10.book \u2502 \u251c\u2500\u2500 11.book \u2502 \u251c\u2500\u2500 12.book \u2502 \u2514\u2500\u2500 year.book \u251c\u2500\u2500 ... \u251c\u2500\u2500 2020 \u2502 \u251c\u2500\u2500 01.book \u2502 \u251c\u2500\u2500 02.book \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 11.book \u2502 \u251c\u2500\u2500 12.book \u2502 \u2514\u2500\u2500 year.book \u251c\u2500\u2500 ledger.book \u251c\u2500\u2500 .closed.accounts.book \u251c\u2500\u2500 roadmap.md \u2514\u2500\u2500 to.process \u251c\u2500\u2500 cone.book \u251c\u2500\u2500 bank1.csv \u2514\u2500\u2500 bank2.csv Where: .git : I keep everything in a git repository to have a version controlled ledger. Each year has it's own directory with: A book file per month, check below it's contents. A year.book file with just include statements: include \"01.book\" include \"02.book\" include \"03.book\" include \"04.book\" include \"05.book\" include \"06.book\" include \"07.book\" include \"08.book\" include \"09.book\" # include \"10.book\" # include \"11.book\" # include \"12.book\" * ledger.book : The beancount entry point where the accounts are defined. * .closed.accounts.book : To store the account closing statements. * roadmap.md : To store the financial plan for the semester/year/life. * to.process : To store the raw data from external sources. The main ledger \u2691 The ledger.book file contains the beancount configuration, with the opening of accounts and inclusion of the monthly books. I like to split it in sections. TL;DR: The full ledger.book # Options option \"title\" \"Lyz Lair Ledge\" option \"operating_currency\" \"EUR\" # Events 2016-12-19 event \"employer\" \"XXX\" # Eternal accounts # Assets 2010-05-17 open Assets:Cash EUR 2021-01-25 open Assets:Cash:Coins EUR 2021-01-25 open Assets:Cash:Paper EUR 2018-09-10 open Assets:Cashbox EUR 2019-01-11 open Assets:Cashbox:Coins EUR 2019-01-11 open Assets:Cashbox:Paper EUR 2018-04-01 open Assets:Savings EUR 2019-08-01 open Assets:Savings:CashFlowRefiller EUR 2019-08-01 open Assets:Savings:UnexpectedExpenses EUR 2019-08-01 open Assets:Savings:Home EUR 2018-04-01 open Assets:CashFlowCard EUR 2018-04-01 open Assets:CashDeposit EUR # Debts 2016-01-01 open Assets:Debt:Person1 EUR 2016-01-01 open Assets:Debt:Person2 EUR # Income 2016-12-01 open Income:Employer1 EUR 2019-05-21 open Income:Employer2 EUR 2010-05-17 open Income:State EUR 2019-01-01 open Income:Gifts EUR # Equity 2010-05-17 open Equity:Opening-Balances 2010-05-17 open Equity:Errors 2010-05-17 open Equity:Forgiven # Expenses 2010-01-01 open Expenses:Bills EUR 2013-01-01 open Expenses:Bills:Gas EUR 2010-01-01 open Expenses:Bills:Phone EUR 2019-01-01 open Expenses:Bills:Light EUR 2010-01-01 open Expenses:Bills:Rent EUR 2010-01-01 open Expenses:Bills:PublicTransport EUR 2017-01-01 open Expenses:Bills:Subscriptions EUR 2016-01-01 open Expenses:Bills:Union EUR 2010-01-01 open Expenses:Books EUR 2010-12-01 open Expenses:Car EUR 2010-12-01 open Expenses:Car:Fuel EUR 2010-12-01 open Expenses:Car:Insurance EUR 2010-12-01 open Expenses:Car:Repair EUR 2010-12-01 open Expenses:Car:Taxes EUR 2010-12-01 open Expenses:Car:Tickets EUR 2010-01-01 open Expenses:Clothes EUR 2018-11-01 open Expenses:Donations EUR 2010-05-17 open Expenses:Financial EUR 2010-01-01 open Expenses:Games EUR 2010-01-01 open Expenses:Games:Steam EUR 2010-01-01 open Expenses:Games:HumbleBundle EUR 2019-06-01 open Expenses:Games:GOG EUR 2020-06-01 open Expenses:Games:Itchio EUR 2010-01-01 open Expenses:Gifts EUR 2010-01-01 open Expenses:Gifts:Person1 EUR 2010-01-01 open Expenses:Gifts:Person2 EUR 2010-01-01 open Expenses:Gifts:Mine EUR 2010-01-01 open Expenses:Groceries EUR 2018-11-01 open Expenses:Groceries:Extras EUR 2020-01-01 open Expenses:Groceries:Supermarket EUR 2020-01-01 open Expenses:Groceries:Prepared EUR 2020-01-01 open Expenses:Groceries:GreenGrocery EUR 2010-01-01 open Expenses:Hardware EUR 2010-01-01 open Expenses:Home EUR 2010-01-01 open Expenses:Home:WashingMachine EUR 2010-01-01 open Expenses:Home:DishWasher EUR 2010-01-01 open Expenses:Home:Fridge EUR 2020-06-01 open Expenses:Legal EUR 2010-01-01 open Expenses:Medicines EUR 2010-01-01 open Expenses:Social EUR 2010-01-01 open Expenses:Social:Eat EUR 2010-01-01 open Expenses:Social:Drink EUR 2019-06-01 open Expenses:Taxes:Tax1 EUR 2016-01-01 open Expenses:Taxes:Tax2 EUR 2010-05-17 open Expenses:Trips EUR 2010-05-17 open Expenses:Trips:Accommodation EUR 2010-05-17 open Expenses:Trips:Drink EUR 2010-05-17 open Expenses:Trips:Food EUR 2010-05-17 open Expenses:Trips:Tickets EUR 2010-05-17 open Expenses:Trips:Transport EUR 2019-05-20 open Expenses:Work EUR 2019-05-20 open Expenses:Work:Phone EUR 2019-05-20 open Expenses:Work:Hardware EUR 2019-05-20 open Expenses:Work:Trips EUR 2019-05-20 open Expenses:Work:Trips:Accommodation EUR 2019-05-20 open Expenses:Work:Trips:Drink EUR 2019-05-20 open Expenses:Work:Trips:Food EUR 2019-05-20 open Expenses:Work:Trips:Tickets EUR 2019-05-20 open Expenses:Work:Trips:Transport EUR ## Initialization 2010-05-17 pad Assets:Cash Equity:Opening-Balances 2016-01-01 pad Assets:Debt:Person1 Equity:Opening-Balances # Transfers include \".closed.accounts.book\" include \"2011/year.book\" include \"2012/year.book\" include \"2013/year.book\" include \"2014/year.book\" include \"2015/year.book\" include \"2016/year.book\" include \"2017/year.book\" include \"2018/year.book\" include \"2019/year.book\" include \"2020/year.book\" Assets \u2691 Asset accounts represent something you have. # Assets 2010-05-17 open Assets:Cash EUR 2021-01-25 open Assets:Cash:Coins EUR 2021-01-25 open Assets:Cash:Paper EUR 2018-09-10 open Assets:Cashbox EUR 2019-01-11 open Assets:Cashbox:Coins EUR 2019-01-11 open Assets:Cashbox:Paper EUR 2018-04-01 open Assets:Savings EUR 2019-08-01 open Assets:Savings:CashFlowRefiller EUR 2019-08-01 open Assets:Savings:UnexpectedExpenses EUR 2019-08-01 open Assets:Savings:Home EUR 2018-04-01 open Assets:CashFlowCard EUR 2018-04-01 open Assets:CashDeposit EUR Being a privacy minded person, I try to pay everything by cash. To track it, I've created the following asset accounts: Assets:Cash:Paper : Paper money in my wallet. I like to have a 5, 10 and 20 euro bills as it gives the best flexibility without carrying too much money. Assets:Cash:Coins : Coins in my wallet. Assets:Cashbox:Paper : Paper money stored at home. I fill it up monthly to my average monthly expenses, so I reduce the trips to the ATM to the minimum. Once this account is below 100 EUR, I add the mental task to refill it. Assets:Cashbox:Coins : Coins stored at home. I keep it at 10 EUR in coins of 2 EUR, so it's quick to count at the same time as it's able to cover most of the things you need to buy with coins. Assets:Cashbox:SmallCoins : If my coins wallet is starting to get heavy, I extract the coins smaller than 50 cents into a container with copper coins. Assets:CashDeposit : You never know when the bank system is going to fuck you, so it's always good to have some cash under the mattress. Having this level of granularity and doing weekly balances of each of those accounts has helped me understand the flaws in my processes that lead to the cash accounting errors. As most humans living in the first world, I'm forced to have at least one bank account. For security reasons I have two: Assets:CashFlowCard : The bank account with an associated debit card. Here is from where I make my expenses, such as home rental, supplies payment, ATM money withdrawal. As it is exposed to all the payment platforms, I assume that it will come a time when a vulnerability is found in one of them, so I keep the least amount of money I can. As with the Cashbox I monthly refill it with the expected expenses amount plus a safety amount. Assets:Savings : The bank account where I store my savings. I have it subdivided in three sections: Assets:Savings:CashFlowRefiller : Here I store the average monthly expenses for the following two months. Assets:Savings:UnexpectedExpenses : Deposit for unexpected expenses such as car or domestic appliances repairs. Assets:Savings:Home : Deposit for the initial payment or a house. Debts \u2691 Debts can be tracked either as an asset or as a liability. If you expect them to owe you more often (you lend a friend some money), model it as an asset, if you're going to owe them (you borrow from the bank to buy a house), model it as a liability. # Debts 2016-01-01 open Assets:Debt:Person1 EUR 2016-01-01 open Assets:Debt:Person2 EUR Income \u2691 Income accounts represent where you get the money from. # Income 2016-12-01 open Income:Employer1 EUR 2019-05-21 open Income:Employer2 EUR 2010-05-17 open Income:State EUR 2019-01-01 open Income:Gifts EUR Equity \u2691 I use equity accounts to make adjustments. # Equity 2010-05-17 open Equity:Opening-Balances 2010-05-17 open Equity:Errors 2010-05-17 open Equity:Forgiven Equity:Opening-Balances : Used to set the initial balance of an account. Equity:Errors : Used with the pad statements to track the errors in the accounting. Equity:Forgiven : Used in the transactions to forgive someone's debts. Expenses \u2691 Expense accounts model where you expend the money on. # Expenses 2010-01-01 open Expenses:Bills EUR 2013-01-01 open Expenses:Bills:Gas EUR 2010-01-01 open Expenses:Bills:Phone EUR 2019-01-01 open Expenses:Bills:Light EUR 2010-01-01 open Expenses:Bills:Rent EUR 2010-01-01 open Expenses:Bills:PublicTransport EUR 2017-01-01 open Expenses:Bills:Subscriptions EUR 2016-01-01 open Expenses:Bills:Union EUR 2010-01-01 open Expenses:Books EUR 2010-12-01 open Expenses:Car EUR 2010-12-01 open Expenses:Car:Fuel EUR 2010-12-01 open Expenses:Car:Insurance EUR 2010-12-01 open Expenses:Car:Repair EUR 2010-12-01 open Expenses:Car:Taxes EUR 2010-12-01 open Expenses:Car:Tickets EUR 2010-01-01 open Expenses:Clothes EUR 2018-11-01 open Expenses:Donations EUR 2010-05-17 open Expenses:Financial EUR 2010-01-01 open Expenses:Games EUR 2010-01-01 open Expenses:Games:Steam EUR 2010-01-01 open Expenses:Games:HumbleBundle EUR 2019-06-01 open Expenses:Games:GOG EUR 2020-06-01 open Expenses:Games:Itchio EUR 2010-01-01 open Expenses:Gifts EUR 2010-01-01 open Expenses:Gifts:Person1 EUR 2010-01-01 open Expenses:Gifts:Person2 EUR 2010-01-01 open Expenses:Gifts:Mine EUR 2010-01-01 open Expenses:Groceries EUR 2018-11-01 open Expenses:Groceries:Extras EUR 2020-01-01 open Expenses:Groceries:Supermarket EUR 2020-01-01 open Expenses:Groceries:Prepared EUR 2020-01-01 open Expenses:Groceries:GreenGrocery EUR 2010-01-01 open Expenses:Hardware EUR 2010-01-01 open Expenses:Home EUR 2010-01-01 open Expenses:Home:WashingMachine EUR 2010-01-01 open Expenses:Home:DishWasher EUR 2010-01-01 open Expenses:Home:Fridge EUR 2020-06-01 open Expenses:Legal EUR 2010-01-01 open Expenses:Medicines EUR 2010-01-01 open Expenses:Social EUR 2010-01-01 open Expenses:Social:Eat EUR 2010-01-01 open Expenses:Social:Drink EUR 2019-06-01 open Expenses:Taxes:Tax1 EUR 2016-01-01 open Expenses:Taxes:Tax2 EUR 2010-05-17 open Expenses:Trips EUR 2010-05-17 open Expenses:Trips:Accommodation EUR 2010-05-17 open Expenses:Trips:Drink EUR 2010-05-17 open Expenses:Trips:Food EUR 2010-05-17 open Expenses:Trips:Tickets EUR 2010-05-17 open Expenses:Trips:Transport EUR 2019-05-20 open Expenses:Work EUR 2019-05-20 open Expenses:Work:Phone EUR 2019-05-20 open Expenses:Work:Hardware EUR 2019-05-20 open Expenses:Work:Trips EUR 2019-05-20 open Expenses:Work:Trips:Accommodation EUR 2019-05-20 open Expenses:Work:Trips:Drink EUR 2019-05-20 open Expenses:Work:Trips:Food EUR 2019-05-20 open Expenses:Work:Trips:Tickets EUR 2019-05-20 open Expenses:Work:Trips:Transport EUR I decided to split my expenses in: Expenses:Bills : All the periodic bills I pay Expenses:Bills:Gas : Expenses:Bills:Phone : Expenses:Bills:Light : Expenses:Bills:Rent : Expenses:Bills:PublicTransport : Expenses:Bills:Subscriptions : Newspaper, magazine, web service subscriptions. Expenses:Bills:Union : Expenses:Books : Expenses:Car : Expenses:Car:Fuel : Expenses:Car:Insurance : Expenses:Car:Repair : Expenses:Car:Taxes : Expenses:Car:Tickets : Expenses:Clothes : Expenses:Donations : Expenses:Financial : Expenses related to financial operations or account maintenance. Expenses:Games : Expenses:Games:Steam : Expenses:Games:HumbleBundle : Expenses:Games:GOG : Expenses:Games:Itchio : Expenses:Gifts : Expenses:Gifts:Person1 : Expenses:Gifts:Person2 : Expenses:Gifts:Mine : Expenses:Groceries : Expenses:Groceries:Extras : Expenses:Groceries:Supermarket : Expenses:Groceries:Prepared : Expenses:Groceries:GreenGrocery : Expenses:Hardware : Expenses:Home : Expenses:Home:WashingMachine : Expenses:Home:DishWasher : Expenses:Home:Fridge : Expenses:Legal : Expenses:Medicines : Expenses:Social : Expenses:Social:Eat : Expenses:Social:Drink : Expenses:Taxes : Expenses:Taxes:Tax1 : Expenses:Taxes:Tax2 : Expenses:Trips : Expenses:Trips:Accommodation : Expenses:Trips:Drink : Expenses:Trips:Food : Expenses:Trips:Tickets : Expenses:Trips:Transport : Expenses:Work : Expenses:Work:Phone : Expenses:Work:Hardware : Expenses:Work:Trips : Expenses:Work:Trips:Accommodation : Expenses:Work:Trips:Drink : Expenses:Work:Trips:Food : Expenses:Work:Trips:Tickets : Expenses:Work:Trips:Transport : Initialization of accounts \u2691 # Initialization 2010-05-17 pad Assets:Cash Equity:Opening-Balances 2016-01-01 pad Assets:Debt:Person1 Equity:Opening-Balances Transfer includes \u2691 I reference each year's year.book and the .closed.accounts.book . # Transfers include \".closed.accounts.book\" include \"2011/year.book\" ... include \"2020/year.book\" The monthly book \u2691 Each month has a file with this structure: # Cash transfers # CashFlowCard # Savings # Balances taken at 2020-12-06T19:32 ## Active accounts 2020-12-06 balance Assets:Cashbox:Paper EUR 2020-12-06 balance Assets:Cashbox:Coins EUR 2020-12-06 balance Assets:Cash:Paper EUR 2020-12-06 balance Assets:Cash:Coins EUR 2020-12-06 balance Assets:CashFlowCard EUR 2020-12-06 balance Assets:Savings EUR ## Deposits 2020-12-06 balance Assets:Savings:CashFlowRefiller XXX EUR 2020-12-06 balance Assets:Savings:UnexpectedExpenses XXX EUR 2020-12-06 balance Assets:CashDeposit XXX EUR ## Debts 2020-12-06 balance Assets:Debt:Person1 XXX EUR ## Equity # 2020-12-05 pad Assets:Cash Equity:Errors # 2020-12-05 pad Assets:Cashbox Equity:Errors # Weekly balances ## Measure done on 2020-09-04T17:10 2020-09-04 balance Assets:Cash:Coins XXX EUR 2020-09-04 balance Assets:Cash:Paper XXX EUR 2020-09-04 balance Assets:Cashbox:Coins XXX EUR 2020-09-04 balance Assets:Cashbox:Paper XXX EUR Where each section stores: Cash transfers : The transactions done by cash, extracted from the Android cone application. CashFlowCard : Bank account extracts transformed from the csv to postings with bean-extract . Savings : Bank account extracts transformed from the csv to postings with bean-extract . Monthly balances : I try to review the accounts once each month. This section is subdivided in: Active accounts : The accounts whose value changes monthly. Deposits : The accounts that don't change much each month. Debts : The balance of debt accounts. Equity : The pad statements to track the errors in the monthly account. Weekly balances : As doing the monthly review is long, but it doesn't give me the enough information to not mess up the cash transactions, I do a weekly balance of those accounts.", "title": "Money Management"}, {"location": "money_management/#system-inputs", "text": "I have two types of financial transactions to track: The credit/debit card movements: Easy to track as usually the banks support exporting them as CSV, and beancount have specific bank importers . The cash movements: Harder to track as you need to keep them manually. This has been my biggest source of errors, once I understood how to correctly use beancount . In the latest iteration, I'm using the cone Android app to keep track of these expenses.", "title": "System inputs"}, {"location": "money_management/#workflow", "text": "", "title": "Workflow"}, {"location": "money_management/#beancount-ledger-organization", "text": "My beancount project directory tree is: . \u251c\u2500\u2500 .git \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 2011 \u2502 \u251c\u2500\u2500 09.book \u2502 \u251c\u2500\u2500 10.book \u2502 \u251c\u2500\u2500 11.book \u2502 \u251c\u2500\u2500 12.book \u2502 \u2514\u2500\u2500 year.book \u251c\u2500\u2500 ... \u251c\u2500\u2500 2020 \u2502 \u251c\u2500\u2500 01.book \u2502 \u251c\u2500\u2500 02.book \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 11.book \u2502 \u251c\u2500\u2500 12.book \u2502 \u2514\u2500\u2500 year.book \u251c\u2500\u2500 ledger.book \u251c\u2500\u2500 .closed.accounts.book \u251c\u2500\u2500 roadmap.md \u2514\u2500\u2500 to.process \u251c\u2500\u2500 cone.book \u251c\u2500\u2500 bank1.csv \u2514\u2500\u2500 bank2.csv Where: .git : I keep everything in a git repository to have a version controlled ledger. Each year has it's own directory with: A book file per month, check below it's contents. A year.book file with just include statements: include \"01.book\" include \"02.book\" include \"03.book\" include \"04.book\" include \"05.book\" include \"06.book\" include \"07.book\" include \"08.book\" include \"09.book\" # include \"10.book\" # include \"11.book\" # include \"12.book\" * ledger.book : The beancount entry point where the accounts are defined. * .closed.accounts.book : To store the account closing statements. * roadmap.md : To store the financial plan for the semester/year/life. * to.process : To store the raw data from external sources.", "title": "Beancount ledger organization"}, {"location": "money_management/#the-main-ledger", "text": "The ledger.book file contains the beancount configuration, with the opening of accounts and inclusion of the monthly books. I like to split it in sections. TL;DR: The full ledger.book # Options option \"title\" \"Lyz Lair Ledge\" option \"operating_currency\" \"EUR\" # Events 2016-12-19 event \"employer\" \"XXX\" # Eternal accounts # Assets 2010-05-17 open Assets:Cash EUR 2021-01-25 open Assets:Cash:Coins EUR 2021-01-25 open Assets:Cash:Paper EUR 2018-09-10 open Assets:Cashbox EUR 2019-01-11 open Assets:Cashbox:Coins EUR 2019-01-11 open Assets:Cashbox:Paper EUR 2018-04-01 open Assets:Savings EUR 2019-08-01 open Assets:Savings:CashFlowRefiller EUR 2019-08-01 open Assets:Savings:UnexpectedExpenses EUR 2019-08-01 open Assets:Savings:Home EUR 2018-04-01 open Assets:CashFlowCard EUR 2018-04-01 open Assets:CashDeposit EUR # Debts 2016-01-01 open Assets:Debt:Person1 EUR 2016-01-01 open Assets:Debt:Person2 EUR # Income 2016-12-01 open Income:Employer1 EUR 2019-05-21 open Income:Employer2 EUR 2010-05-17 open Income:State EUR 2019-01-01 open Income:Gifts EUR # Equity 2010-05-17 open Equity:Opening-Balances 2010-05-17 open Equity:Errors 2010-05-17 open Equity:Forgiven # Expenses 2010-01-01 open Expenses:Bills EUR 2013-01-01 open Expenses:Bills:Gas EUR 2010-01-01 open Expenses:Bills:Phone EUR 2019-01-01 open Expenses:Bills:Light EUR 2010-01-01 open Expenses:Bills:Rent EUR 2010-01-01 open Expenses:Bills:PublicTransport EUR 2017-01-01 open Expenses:Bills:Subscriptions EUR 2016-01-01 open Expenses:Bills:Union EUR 2010-01-01 open Expenses:Books EUR 2010-12-01 open Expenses:Car EUR 2010-12-01 open Expenses:Car:Fuel EUR 2010-12-01 open Expenses:Car:Insurance EUR 2010-12-01 open Expenses:Car:Repair EUR 2010-12-01 open Expenses:Car:Taxes EUR 2010-12-01 open Expenses:Car:Tickets EUR 2010-01-01 open Expenses:Clothes EUR 2018-11-01 open Expenses:Donations EUR 2010-05-17 open Expenses:Financial EUR 2010-01-01 open Expenses:Games EUR 2010-01-01 open Expenses:Games:Steam EUR 2010-01-01 open Expenses:Games:HumbleBundle EUR 2019-06-01 open Expenses:Games:GOG EUR 2020-06-01 open Expenses:Games:Itchio EUR 2010-01-01 open Expenses:Gifts EUR 2010-01-01 open Expenses:Gifts:Person1 EUR 2010-01-01 open Expenses:Gifts:Person2 EUR 2010-01-01 open Expenses:Gifts:Mine EUR 2010-01-01 open Expenses:Groceries EUR 2018-11-01 open Expenses:Groceries:Extras EUR 2020-01-01 open Expenses:Groceries:Supermarket EUR 2020-01-01 open Expenses:Groceries:Prepared EUR 2020-01-01 open Expenses:Groceries:GreenGrocery EUR 2010-01-01 open Expenses:Hardware EUR 2010-01-01 open Expenses:Home EUR 2010-01-01 open Expenses:Home:WashingMachine EUR 2010-01-01 open Expenses:Home:DishWasher EUR 2010-01-01 open Expenses:Home:Fridge EUR 2020-06-01 open Expenses:Legal EUR 2010-01-01 open Expenses:Medicines EUR 2010-01-01 open Expenses:Social EUR 2010-01-01 open Expenses:Social:Eat EUR 2010-01-01 open Expenses:Social:Drink EUR 2019-06-01 open Expenses:Taxes:Tax1 EUR 2016-01-01 open Expenses:Taxes:Tax2 EUR 2010-05-17 open Expenses:Trips EUR 2010-05-17 open Expenses:Trips:Accommodation EUR 2010-05-17 open Expenses:Trips:Drink EUR 2010-05-17 open Expenses:Trips:Food EUR 2010-05-17 open Expenses:Trips:Tickets EUR 2010-05-17 open Expenses:Trips:Transport EUR 2019-05-20 open Expenses:Work EUR 2019-05-20 open Expenses:Work:Phone EUR 2019-05-20 open Expenses:Work:Hardware EUR 2019-05-20 open Expenses:Work:Trips EUR 2019-05-20 open Expenses:Work:Trips:Accommodation EUR 2019-05-20 open Expenses:Work:Trips:Drink EUR 2019-05-20 open Expenses:Work:Trips:Food EUR 2019-05-20 open Expenses:Work:Trips:Tickets EUR 2019-05-20 open Expenses:Work:Trips:Transport EUR ## Initialization 2010-05-17 pad Assets:Cash Equity:Opening-Balances 2016-01-01 pad Assets:Debt:Person1 Equity:Opening-Balances # Transfers include \".closed.accounts.book\" include \"2011/year.book\" include \"2012/year.book\" include \"2013/year.book\" include \"2014/year.book\" include \"2015/year.book\" include \"2016/year.book\" include \"2017/year.book\" include \"2018/year.book\" include \"2019/year.book\" include \"2020/year.book\"", "title": "The main ledger"}, {"location": "money_management/#assets", "text": "Asset accounts represent something you have. # Assets 2010-05-17 open Assets:Cash EUR 2021-01-25 open Assets:Cash:Coins EUR 2021-01-25 open Assets:Cash:Paper EUR 2018-09-10 open Assets:Cashbox EUR 2019-01-11 open Assets:Cashbox:Coins EUR 2019-01-11 open Assets:Cashbox:Paper EUR 2018-04-01 open Assets:Savings EUR 2019-08-01 open Assets:Savings:CashFlowRefiller EUR 2019-08-01 open Assets:Savings:UnexpectedExpenses EUR 2019-08-01 open Assets:Savings:Home EUR 2018-04-01 open Assets:CashFlowCard EUR 2018-04-01 open Assets:CashDeposit EUR Being a privacy minded person, I try to pay everything by cash. To track it, I've created the following asset accounts: Assets:Cash:Paper : Paper money in my wallet. I like to have a 5, 10 and 20 euro bills as it gives the best flexibility without carrying too much money. Assets:Cash:Coins : Coins in my wallet. Assets:Cashbox:Paper : Paper money stored at home. I fill it up monthly to my average monthly expenses, so I reduce the trips to the ATM to the minimum. Once this account is below 100 EUR, I add the mental task to refill it. Assets:Cashbox:Coins : Coins stored at home. I keep it at 10 EUR in coins of 2 EUR, so it's quick to count at the same time as it's able to cover most of the things you need to buy with coins. Assets:Cashbox:SmallCoins : If my coins wallet is starting to get heavy, I extract the coins smaller than 50 cents into a container with copper coins. Assets:CashDeposit : You never know when the bank system is going to fuck you, so it's always good to have some cash under the mattress. Having this level of granularity and doing weekly balances of each of those accounts has helped me understand the flaws in my processes that lead to the cash accounting errors. As most humans living in the first world, I'm forced to have at least one bank account. For security reasons I have two: Assets:CashFlowCard : The bank account with an associated debit card. Here is from where I make my expenses, such as home rental, supplies payment, ATM money withdrawal. As it is exposed to all the payment platforms, I assume that it will come a time when a vulnerability is found in one of them, so I keep the least amount of money I can. As with the Cashbox I monthly refill it with the expected expenses amount plus a safety amount. Assets:Savings : The bank account where I store my savings. I have it subdivided in three sections: Assets:Savings:CashFlowRefiller : Here I store the average monthly expenses for the following two months. Assets:Savings:UnexpectedExpenses : Deposit for unexpected expenses such as car or domestic appliances repairs. Assets:Savings:Home : Deposit for the initial payment or a house.", "title": "Assets"}, {"location": "money_management/#debts", "text": "Debts can be tracked either as an asset or as a liability. If you expect them to owe you more often (you lend a friend some money), model it as an asset, if you're going to owe them (you borrow from the bank to buy a house), model it as a liability. # Debts 2016-01-01 open Assets:Debt:Person1 EUR 2016-01-01 open Assets:Debt:Person2 EUR", "title": "Debts"}, {"location": "money_management/#income", "text": "Income accounts represent where you get the money from. # Income 2016-12-01 open Income:Employer1 EUR 2019-05-21 open Income:Employer2 EUR 2010-05-17 open Income:State EUR 2019-01-01 open Income:Gifts EUR", "title": "Income"}, {"location": "money_management/#equity", "text": "I use equity accounts to make adjustments. # Equity 2010-05-17 open Equity:Opening-Balances 2010-05-17 open Equity:Errors 2010-05-17 open Equity:Forgiven Equity:Opening-Balances : Used to set the initial balance of an account. Equity:Errors : Used with the pad statements to track the errors in the accounting. Equity:Forgiven : Used in the transactions to forgive someone's debts.", "title": "Equity"}, {"location": "money_management/#expenses", "text": "Expense accounts model where you expend the money on. # Expenses 2010-01-01 open Expenses:Bills EUR 2013-01-01 open Expenses:Bills:Gas EUR 2010-01-01 open Expenses:Bills:Phone EUR 2019-01-01 open Expenses:Bills:Light EUR 2010-01-01 open Expenses:Bills:Rent EUR 2010-01-01 open Expenses:Bills:PublicTransport EUR 2017-01-01 open Expenses:Bills:Subscriptions EUR 2016-01-01 open Expenses:Bills:Union EUR 2010-01-01 open Expenses:Books EUR 2010-12-01 open Expenses:Car EUR 2010-12-01 open Expenses:Car:Fuel EUR 2010-12-01 open Expenses:Car:Insurance EUR 2010-12-01 open Expenses:Car:Repair EUR 2010-12-01 open Expenses:Car:Taxes EUR 2010-12-01 open Expenses:Car:Tickets EUR 2010-01-01 open Expenses:Clothes EUR 2018-11-01 open Expenses:Donations EUR 2010-05-17 open Expenses:Financial EUR 2010-01-01 open Expenses:Games EUR 2010-01-01 open Expenses:Games:Steam EUR 2010-01-01 open Expenses:Games:HumbleBundle EUR 2019-06-01 open Expenses:Games:GOG EUR 2020-06-01 open Expenses:Games:Itchio EUR 2010-01-01 open Expenses:Gifts EUR 2010-01-01 open Expenses:Gifts:Person1 EUR 2010-01-01 open Expenses:Gifts:Person2 EUR 2010-01-01 open Expenses:Gifts:Mine EUR 2010-01-01 open Expenses:Groceries EUR 2018-11-01 open Expenses:Groceries:Extras EUR 2020-01-01 open Expenses:Groceries:Supermarket EUR 2020-01-01 open Expenses:Groceries:Prepared EUR 2020-01-01 open Expenses:Groceries:GreenGrocery EUR 2010-01-01 open Expenses:Hardware EUR 2010-01-01 open Expenses:Home EUR 2010-01-01 open Expenses:Home:WashingMachine EUR 2010-01-01 open Expenses:Home:DishWasher EUR 2010-01-01 open Expenses:Home:Fridge EUR 2020-06-01 open Expenses:Legal EUR 2010-01-01 open Expenses:Medicines EUR 2010-01-01 open Expenses:Social EUR 2010-01-01 open Expenses:Social:Eat EUR 2010-01-01 open Expenses:Social:Drink EUR 2019-06-01 open Expenses:Taxes:Tax1 EUR 2016-01-01 open Expenses:Taxes:Tax2 EUR 2010-05-17 open Expenses:Trips EUR 2010-05-17 open Expenses:Trips:Accommodation EUR 2010-05-17 open Expenses:Trips:Drink EUR 2010-05-17 open Expenses:Trips:Food EUR 2010-05-17 open Expenses:Trips:Tickets EUR 2010-05-17 open Expenses:Trips:Transport EUR 2019-05-20 open Expenses:Work EUR 2019-05-20 open Expenses:Work:Phone EUR 2019-05-20 open Expenses:Work:Hardware EUR 2019-05-20 open Expenses:Work:Trips EUR 2019-05-20 open Expenses:Work:Trips:Accommodation EUR 2019-05-20 open Expenses:Work:Trips:Drink EUR 2019-05-20 open Expenses:Work:Trips:Food EUR 2019-05-20 open Expenses:Work:Trips:Tickets EUR 2019-05-20 open Expenses:Work:Trips:Transport EUR I decided to split my expenses in: Expenses:Bills : All the periodic bills I pay Expenses:Bills:Gas : Expenses:Bills:Phone : Expenses:Bills:Light : Expenses:Bills:Rent : Expenses:Bills:PublicTransport : Expenses:Bills:Subscriptions : Newspaper, magazine, web service subscriptions. Expenses:Bills:Union : Expenses:Books : Expenses:Car : Expenses:Car:Fuel : Expenses:Car:Insurance : Expenses:Car:Repair : Expenses:Car:Taxes : Expenses:Car:Tickets : Expenses:Clothes : Expenses:Donations : Expenses:Financial : Expenses related to financial operations or account maintenance. Expenses:Games : Expenses:Games:Steam : Expenses:Games:HumbleBundle : Expenses:Games:GOG : Expenses:Games:Itchio : Expenses:Gifts : Expenses:Gifts:Person1 : Expenses:Gifts:Person2 : Expenses:Gifts:Mine : Expenses:Groceries : Expenses:Groceries:Extras : Expenses:Groceries:Supermarket : Expenses:Groceries:Prepared : Expenses:Groceries:GreenGrocery : Expenses:Hardware : Expenses:Home : Expenses:Home:WashingMachine : Expenses:Home:DishWasher : Expenses:Home:Fridge : Expenses:Legal : Expenses:Medicines : Expenses:Social : Expenses:Social:Eat : Expenses:Social:Drink : Expenses:Taxes : Expenses:Taxes:Tax1 : Expenses:Taxes:Tax2 : Expenses:Trips : Expenses:Trips:Accommodation : Expenses:Trips:Drink : Expenses:Trips:Food : Expenses:Trips:Tickets : Expenses:Trips:Transport : Expenses:Work : Expenses:Work:Phone : Expenses:Work:Hardware : Expenses:Work:Trips : Expenses:Work:Trips:Accommodation : Expenses:Work:Trips:Drink : Expenses:Work:Trips:Food : Expenses:Work:Trips:Tickets : Expenses:Work:Trips:Transport :", "title": "Expenses"}, {"location": "money_management/#initialization-of-accounts", "text": "# Initialization 2010-05-17 pad Assets:Cash Equity:Opening-Balances 2016-01-01 pad Assets:Debt:Person1 Equity:Opening-Balances", "title": "Initialization of accounts"}, {"location": "money_management/#transfer-includes", "text": "I reference each year's year.book and the .closed.accounts.book . # Transfers include \".closed.accounts.book\" include \"2011/year.book\" ... include \"2020/year.book\"", "title": "Transfer includes"}, {"location": "money_management/#the-monthly-book", "text": "Each month has a file with this structure: # Cash transfers # CashFlowCard # Savings # Balances taken at 2020-12-06T19:32 ## Active accounts 2020-12-06 balance Assets:Cashbox:Paper EUR 2020-12-06 balance Assets:Cashbox:Coins EUR 2020-12-06 balance Assets:Cash:Paper EUR 2020-12-06 balance Assets:Cash:Coins EUR 2020-12-06 balance Assets:CashFlowCard EUR 2020-12-06 balance Assets:Savings EUR ## Deposits 2020-12-06 balance Assets:Savings:CashFlowRefiller XXX EUR 2020-12-06 balance Assets:Savings:UnexpectedExpenses XXX EUR 2020-12-06 balance Assets:CashDeposit XXX EUR ## Debts 2020-12-06 balance Assets:Debt:Person1 XXX EUR ## Equity # 2020-12-05 pad Assets:Cash Equity:Errors # 2020-12-05 pad Assets:Cashbox Equity:Errors # Weekly balances ## Measure done on 2020-09-04T17:10 2020-09-04 balance Assets:Cash:Coins XXX EUR 2020-09-04 balance Assets:Cash:Paper XXX EUR 2020-09-04 balance Assets:Cashbox:Coins XXX EUR 2020-09-04 balance Assets:Cashbox:Paper XXX EUR Where each section stores: Cash transfers : The transactions done by cash, extracted from the Android cone application. CashFlowCard : Bank account extracts transformed from the csv to postings with bean-extract . Savings : Bank account extracts transformed from the csv to postings with bean-extract . Monthly balances : I try to review the accounts once each month. This section is subdivided in: Active accounts : The accounts whose value changes monthly. Deposits : The accounts that don't change much each month. Debts : The balance of debt accounts. Equity : The pad statements to track the errors in the monthly account. Weekly balances : As doing the monthly review is long, but it doesn't give me the enough information to not mess up the cash transactions, I do a weekly balance of those accounts.", "title": "The monthly book"}, {"location": "monitoring_comparison/", "text": "As with any technology, when you want to adopt it, you first need to analyze your options. In this article we're going to compare the two most popular solutions at the moment, Nagios and Prometheus. Zabbix is similar in architecture and features to Nagios, so for the first iteration we're going to skip it. TL;DR: Prometheus is better, but it needs more effort. Nagios is suitable for basic monitoring of small and/or static systems where blackbox probing is sufficient. If you want to do whitebox monitoring, or have a dynamic or cloud based environment, then Prometheus is a good choice. Nagios \u2691 Nagios is an industry leader in IT infrastructure monitoring. It has four different products to choose from: Nagios XI: Is an enterprise-ready server and network monitoring system that supplies data to track app or network infrastructure health, performance, availability, of the components, protocols, and services. It has a user-friendly interface that allows UI configuration, customized visualizations, and alert preferences. Nagios Log Server: It's used for log management and analysis of user scenarios. It has the ability to correlate logged events across different services and servers in real time, which helps with the investigation of incidents and the performance of root cause analysis. Because Nagios Log Server\u2019s design is specifically for network security and audits, it lets users generate alerts for suspicious operations and commands. Log Server retains historical data from all events, supplying organizations with everything they need to pass a security audit. Nagios Network Analyzer: It's a tool for collecting and displaying either metrics or extra information about an application network. It identifies which IPs are communicating with the application servers and what requests they\u2019re sending. The Network Analyzer maintains a record of all server traffic, including who connected a specific server, to a specific port and the specific request. This helps plan out server and network capacity, plus understand various kinds of security breaches likes unauthorized access, data leaks, DDoS, and viruses or malwares on servers. Nagios Fusion: is a compilation of the three tools Nagios offers. It provides a complete solution that assists businesses in satisfying any and all of their monitoring requirements. Its design is for scalability and for visibility of the application and all of its dependencies. Prometheus \u2691 Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy. At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets. The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications. There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented. Comparison \u2691 For each dimension we'll check how each solution meets the criteria. An aggregation of all the results can be found in the summary . Open source \u2691 Only the Nagios Core is open sourced , it provides basic monitoring but it's enhanced by community contributions . It's also the base of the rest solutions, which are proprietary. Prometheus is completely open source under the Apache 2.0 license. Community \u2691 In Nagios, only the Nagios Core is an open-source tool. The rest are proprietary, so there is no community behind them. Community contributions to Nagios are gathered in the Nagios Exchange , it's hard to get other activity statistics than the overall number of contributions, but there are more than 850 addons, 4.5k plugins and 300 documentation contributions. Overall metrics (2021-02-22): Metric Nagios Core Prometheus Stars 932 35.4k Forks 341 5.7k Watch 121 1.2k Commits 3.4k 8.5k Open Issues 195 290 Closed Issues 455 3.5k Open PR 9 116 Closed PR 155 4.5k Last month metrics (2021-02-22): Metric Nagios Core Prometheus Active PR 1 80 Active Issues 3 64 Commits 0 74 Authors 0 35 We can see that Prometheus in comparison with Nagios Core is: More popular in terms of community contributions. More maintained. Growing more. Development is more distributed. Manages the issues collaboratively. This comparison is biased though, because Nagios comes from a time where GitHub and Git (and Youtube!) did not exist, and the communities formed around different sites. Also, given that Nagios has almost 20 years of existence, and that it forked from a previous monitoring project (NetSaint), the low number contributions indicate a stable and mature product, whereas the high numbers for Prometheus are indicators of a young, still in development product. Keep in mind that this comparison only analyzes the core, it doesn't take into account the metrics of the community contributions, as it is not easy to aggregate their statistics. Which makes Prometheus one of the biggest open-source projects in existence. It actually has hundreds of contributors maintaining it. The tool continues to be up-to-date to contemporary and popular apps, extending its list of exporters and responding to requests. On 16 January 2014 , Nagios Enterprises redirected the nagios-plugins.org domain to a web server controlled by Nagios Enterprises without explicitly notifying the Nagios Plugins community team the consequences of their actions. Nagios Enterprises replaced the nagios-plugins team with a group of new, different members. The community team members who were replaced continued their work under the name Monitoring Plugins along with a new website with the new domain of monitoring-plugins.org. Which is a nasty move against the community. Configuration and usage \u2691 Neither solution is easy to configure, you need to invest time in them. Nagios is easier to use for non technical users though. Visualizations \u2691 The graphs and dashboards Prometheus provides don't meet today's needs. As a result, users resort to other visualization tools to display metrics collected by Prometheus, often Grafana. Nagios comes with a set of dashboards that fit the requirements of monitoring networks and infrastructure components. Yet, it still lacks graphs for more applicative-related issues. Personally I find Grafana dashboards more beautiful and easier to change. It also has a massive community behind providing customizable dashboards for free. Installation \u2691 Nagios comes as a downloadable bundle with dedicated packages for every product with Windows or Linux distributions. After downloading and installing the tool, a set of first-time configurations is required. Once you\u2019ve installed the Nagios agents, data should start streaming into Nagios and its generic dashboards. Prometheus deployment is done through Docker containers that can spin up on every machine type, or through pre-compiled or self-compiled binaries. There are community maintained ansible roles for both solutions, doing a quick search I've found a Prometheus one that it's more maintained. For Kubernetes installation, I've only found helm charts for Prometheus. Kubernetes integration \u2691 Prometheus, as Kubernetes are leading projects of the Cloud Native Computing Foundation , which is a Linux Foundation project that was founded in 2015 to help advance container technology and align the tech industry around its evolution. Prometheus has native support to be run in and to monitor Kubernetes clusters. Although Nagios can monitor Kubernetes, it's not meant to be run inside it. Documentation \u2691 I haven't used much the Nagios documentation , but I can tell you that even though it's improving Prometheus ' is not very complete, and you find yourself often looking at issues and stackoverflow. Integrations \u2691 Official Prometheus\u2019 integrations are practically boundless . The long list of existing exporters combined with the user\u2019s ability to write new exporters allows integration with any tool, and PromQL allows users to query Prometheus data from any visualization tool that supports it. Nagios has a very limited list of official integrations . Most of them are operating systems which use the agents to monitor other network components. Others include MongoDB, Oracle, Selenium, and VMware. Once again, the community comes to rescue us with their contributions , keep in mind that you'll need to dive into the exchange for special monitoring needs. Alerts \u2691 Prometheus offers Alertmanager, a simple service that allows users to set thresholds and push alerts when breaches occur. Nagios uses a variety of media channels for alerts, including email, SMS, and audio alerts. Because its integration with the operating system is swift, Nagios even knows to generate a WinPopup message with the alert details. On a side note, there is an alert Nagios plugin that alerts for Prometheus query results. As Nagios doesn't support labels for the metrics, so there is no grouping, routing or deduplication of alerts as Prometheus do. Also the silence of alerts is done individually on each alert, while in Prometheus it's done using labels, which is more powerful. Advanced monitorization \u2691 Nagios alerting is based on the return codes of scripts, Prometheus on the other hand alerts based on metrics, this fact together with the easy and powerful query language PromQL allows the user to make much more rich alerts that better represent the state of the system to monitor. In Nagios there is no concept of making queries to the gathered data. Data storage \u2691 Nagios has no storage per-se, beyond the current check state. There are plugins which can store data such as for visualisation . Prometheus has a defined amount of data that's available (for example 30 days), to be able to store more you need to use Thanos, the prometheus long term storage solution. High availability \u2691 Nagios servers are standalone, they are not meant to collaborate with other instances, so to achieve high availability you need to do it the old way, with multiple independent instances with a loadbalancer upfront. Prometheus can have different servers running collaboratively, monitoring between themselves. So you get high availability for free without any special configuration. Dynamic infrastructure \u2691 In the past, infrastructure had a low rate of change, it was strange that you needed to add something to the monitorization system. Nowadays, with cloud infrastructures and kubernetes, instances are spawned and killed continuously. In Nagios, you need to manually configure each new service following the push architecture. In prometheus, thanks to the pull architecture and service discovery, new services are added and dead one removed automatically. Custom script execution \u2691 Nagios alerting is based on the return codes of scripts, therefore it's straightforward to create an alert based on a custom script. If you need to monitor something in Prometheus, and nobody has done it before, the development costs of an ad-hoc solutions are incredibly high, compared to Nagios. You'd need either to: Use the script_exporter with your script. I've seen their repo, and the last commit is from March, and they don't have a helm chart to install it . I've searched other alternative exporters, but this one seems to be the best for this approach. The advantages of this approach is that you don't need to create and maintain a new prometheus exporter. The disadvantages though are that you'd have to: Manually install the required exporter resources in the cluster until a helm chart exists. Create the helm charts yourself if they don't develop it. Integrate your tool inside the script_exporter docker through one of these ways: Changing the exporter Docker image to add it. Which would mean a Docker image to maintain. Mounting the binary through a volume inside kubernetes. Which would mean defining a way on how to upload it and assume the high availability penalty that a stateful kubernetes service entail with the cluster configuration right now. If it's not already in your stack, it would mean adding a new exporter to maintain and a new development team to depend on. Alternatively you can use the script exporter binary in a baremetal or virtualized server instead of using a docker, that way you wouldn't need to maintain the different dockers for the different solutions, but you'd need a \"dedicated\" server for this purpose. Create your own exporter. You'd need to create a docker that exposes the command line functionality through a metrics endpoint. You wouldn't depend on a third party development team and would be able to use your script. On the other side it has the following disadvantages: We would need to create and maintain a new prometheus exporter. That would mean creating and maintaining the Docker with the command line tool and a simple http server that exposes the /metrics endpoint, that will run the command whenever the Prometheus server accesses this endpoint. We add a new exporter to maintain but we develop it ourselves, so we don't depend on third party developers. Use other exporters to do the check. For example, if you can deduce the critical API call that will decide if the script fails or succeeds, you could use the blackbox exporter to monitor it instead. The advantages of this solution are: We don't add new infrastructure to develop or maintain. We don't depend on third party development teams. And the disadvantage is that if the logic changes, we would need to update how we do the check. Network monitorization \u2691 Both can use the Simple Network Management Protocol (SNMP) to communicate with network switches or other components by using SNMP protocol to query their status. Not being an expert on the topic, knowing it's been one of the core focus of Nagios in the past years and as I've not been able to find good comparison between both, I'm going to suppose that even though both support network monitoring, Nagios does a better job. Summary \u2691 Metric Nagios Prometheus Open Source \u2713* \u2713\u2713 Community \u2713 \u2713\u2713 Configuration and usage \u2713 x Visualizations \u2713 \u2713\u2713 Ansible Role \u2713 \u2713\u2713 Helm chart x \u2713 Kubernetes x \u2713 Documentation \u2713 x Integrations \u2713 \u2713\u2713 Alerts \u2713 \u2713\u2713 Advanced monitoring x \u2713 Custom script execution \u2713\u2713 \u2713 Data storage x \u2713 Dynamic infrastructure x \u2713 High availability \u2713 \u2713\u2713 Network Monitoring \u2713 \u2713 * Only Nagios Core and the community contributions are open sourced. Where each symbol means: x: Doesn't meet the criteria. \u2713: Meets the criteria. \u2713\u2713: Meets the criteria and it's better than the other solution. ?: I'm not sure. Nagios is the reference of the old-school monitoring solutions, suitable for basic monitoring of small, static and/or old-school systems where blackbox probing is sufficient. Prometheus is the reference of the new-wave monitoring solutions, suitable for more advanced monitoring of dynamic, new-wave systems (web applications, cloud, containers or Kubernetes) where whitebox monitoring is desired. References \u2691 Logz io post on Prometheus vs Nagios", "title": "Monitoring Comparison"}, {"location": "monitoring_comparison/#nagios", "text": "Nagios is an industry leader in IT infrastructure monitoring. It has four different products to choose from: Nagios XI: Is an enterprise-ready server and network monitoring system that supplies data to track app or network infrastructure health, performance, availability, of the components, protocols, and services. It has a user-friendly interface that allows UI configuration, customized visualizations, and alert preferences. Nagios Log Server: It's used for log management and analysis of user scenarios. It has the ability to correlate logged events across different services and servers in real time, which helps with the investigation of incidents and the performance of root cause analysis. Because Nagios Log Server\u2019s design is specifically for network security and audits, it lets users generate alerts for suspicious operations and commands. Log Server retains historical data from all events, supplying organizations with everything they need to pass a security audit. Nagios Network Analyzer: It's a tool for collecting and displaying either metrics or extra information about an application network. It identifies which IPs are communicating with the application servers and what requests they\u2019re sending. The Network Analyzer maintains a record of all server traffic, including who connected a specific server, to a specific port and the specific request. This helps plan out server and network capacity, plus understand various kinds of security breaches likes unauthorized access, data leaks, DDoS, and viruses or malwares on servers. Nagios Fusion: is a compilation of the three tools Nagios offers. It provides a complete solution that assists businesses in satisfying any and all of their monitoring requirements. Its design is for scalability and for visibility of the application and all of its dependencies.", "title": "Nagios"}, {"location": "monitoring_comparison/#prometheus", "text": "Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy. At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets. The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications. There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented.", "title": "Prometheus"}, {"location": "monitoring_comparison/#comparison", "text": "For each dimension we'll check how each solution meets the criteria. An aggregation of all the results can be found in the summary .", "title": "Comparison"}, {"location": "monitoring_comparison/#open-source", "text": "Only the Nagios Core is open sourced , it provides basic monitoring but it's enhanced by community contributions . It's also the base of the rest solutions, which are proprietary. Prometheus is completely open source under the Apache 2.0 license.", "title": "Open source"}, {"location": "monitoring_comparison/#community", "text": "In Nagios, only the Nagios Core is an open-source tool. The rest are proprietary, so there is no community behind them. Community contributions to Nagios are gathered in the Nagios Exchange , it's hard to get other activity statistics than the overall number of contributions, but there are more than 850 addons, 4.5k plugins and 300 documentation contributions. Overall metrics (2021-02-22): Metric Nagios Core Prometheus Stars 932 35.4k Forks 341 5.7k Watch 121 1.2k Commits 3.4k 8.5k Open Issues 195 290 Closed Issues 455 3.5k Open PR 9 116 Closed PR 155 4.5k Last month metrics (2021-02-22): Metric Nagios Core Prometheus Active PR 1 80 Active Issues 3 64 Commits 0 74 Authors 0 35 We can see that Prometheus in comparison with Nagios Core is: More popular in terms of community contributions. More maintained. Growing more. Development is more distributed. Manages the issues collaboratively. This comparison is biased though, because Nagios comes from a time where GitHub and Git (and Youtube!) did not exist, and the communities formed around different sites. Also, given that Nagios has almost 20 years of existence, and that it forked from a previous monitoring project (NetSaint), the low number contributions indicate a stable and mature product, whereas the high numbers for Prometheus are indicators of a young, still in development product. Keep in mind that this comparison only analyzes the core, it doesn't take into account the metrics of the community contributions, as it is not easy to aggregate their statistics. Which makes Prometheus one of the biggest open-source projects in existence. It actually has hundreds of contributors maintaining it. The tool continues to be up-to-date to contemporary and popular apps, extending its list of exporters and responding to requests. On 16 January 2014 , Nagios Enterprises redirected the nagios-plugins.org domain to a web server controlled by Nagios Enterprises without explicitly notifying the Nagios Plugins community team the consequences of their actions. Nagios Enterprises replaced the nagios-plugins team with a group of new, different members. The community team members who were replaced continued their work under the name Monitoring Plugins along with a new website with the new domain of monitoring-plugins.org. Which is a nasty move against the community.", "title": "Community"}, {"location": "monitoring_comparison/#configuration-and-usage", "text": "Neither solution is easy to configure, you need to invest time in them. Nagios is easier to use for non technical users though.", "title": "Configuration and usage"}, {"location": "monitoring_comparison/#visualizations", "text": "The graphs and dashboards Prometheus provides don't meet today's needs. As a result, users resort to other visualization tools to display metrics collected by Prometheus, often Grafana. Nagios comes with a set of dashboards that fit the requirements of monitoring networks and infrastructure components. Yet, it still lacks graphs for more applicative-related issues. Personally I find Grafana dashboards more beautiful and easier to change. It also has a massive community behind providing customizable dashboards for free.", "title": "Visualizations"}, {"location": "monitoring_comparison/#installation", "text": "Nagios comes as a downloadable bundle with dedicated packages for every product with Windows or Linux distributions. After downloading and installing the tool, a set of first-time configurations is required. Once you\u2019ve installed the Nagios agents, data should start streaming into Nagios and its generic dashboards. Prometheus deployment is done through Docker containers that can spin up on every machine type, or through pre-compiled or self-compiled binaries. There are community maintained ansible roles for both solutions, doing a quick search I've found a Prometheus one that it's more maintained. For Kubernetes installation, I've only found helm charts for Prometheus.", "title": "Installation"}, {"location": "monitoring_comparison/#kubernetes-integration", "text": "Prometheus, as Kubernetes are leading projects of the Cloud Native Computing Foundation , which is a Linux Foundation project that was founded in 2015 to help advance container technology and align the tech industry around its evolution. Prometheus has native support to be run in and to monitor Kubernetes clusters. Although Nagios can monitor Kubernetes, it's not meant to be run inside it.", "title": "Kubernetes integration"}, {"location": "monitoring_comparison/#documentation", "text": "I haven't used much the Nagios documentation , but I can tell you that even though it's improving Prometheus ' is not very complete, and you find yourself often looking at issues and stackoverflow.", "title": "Documentation"}, {"location": "monitoring_comparison/#integrations", "text": "Official Prometheus\u2019 integrations are practically boundless . The long list of existing exporters combined with the user\u2019s ability to write new exporters allows integration with any tool, and PromQL allows users to query Prometheus data from any visualization tool that supports it. Nagios has a very limited list of official integrations . Most of them are operating systems which use the agents to monitor other network components. Others include MongoDB, Oracle, Selenium, and VMware. Once again, the community comes to rescue us with their contributions , keep in mind that you'll need to dive into the exchange for special monitoring needs.", "title": "Integrations"}, {"location": "monitoring_comparison/#alerts", "text": "Prometheus offers Alertmanager, a simple service that allows users to set thresholds and push alerts when breaches occur. Nagios uses a variety of media channels for alerts, including email, SMS, and audio alerts. Because its integration with the operating system is swift, Nagios even knows to generate a WinPopup message with the alert details. On a side note, there is an alert Nagios plugin that alerts for Prometheus query results. As Nagios doesn't support labels for the metrics, so there is no grouping, routing or deduplication of alerts as Prometheus do. Also the silence of alerts is done individually on each alert, while in Prometheus it's done using labels, which is more powerful.", "title": "Alerts"}, {"location": "monitoring_comparison/#advanced-monitorization", "text": "Nagios alerting is based on the return codes of scripts, Prometheus on the other hand alerts based on metrics, this fact together with the easy and powerful query language PromQL allows the user to make much more rich alerts that better represent the state of the system to monitor. In Nagios there is no concept of making queries to the gathered data.", "title": "Advanced monitorization"}, {"location": "monitoring_comparison/#data-storage", "text": "Nagios has no storage per-se, beyond the current check state. There are plugins which can store data such as for visualisation . Prometheus has a defined amount of data that's available (for example 30 days), to be able to store more you need to use Thanos, the prometheus long term storage solution.", "title": "Data storage"}, {"location": "monitoring_comparison/#high-availability", "text": "Nagios servers are standalone, they are not meant to collaborate with other instances, so to achieve high availability you need to do it the old way, with multiple independent instances with a loadbalancer upfront. Prometheus can have different servers running collaboratively, monitoring between themselves. So you get high availability for free without any special configuration.", "title": "High availability"}, {"location": "monitoring_comparison/#dynamic-infrastructure", "text": "In the past, infrastructure had a low rate of change, it was strange that you needed to add something to the monitorization system. Nowadays, with cloud infrastructures and kubernetes, instances are spawned and killed continuously. In Nagios, you need to manually configure each new service following the push architecture. In prometheus, thanks to the pull architecture and service discovery, new services are added and dead one removed automatically.", "title": "Dynamic infrastructure"}, {"location": "monitoring_comparison/#custom-script-execution", "text": "Nagios alerting is based on the return codes of scripts, therefore it's straightforward to create an alert based on a custom script. If you need to monitor something in Prometheus, and nobody has done it before, the development costs of an ad-hoc solutions are incredibly high, compared to Nagios. You'd need either to: Use the script_exporter with your script. I've seen their repo, and the last commit is from March, and they don't have a helm chart to install it . I've searched other alternative exporters, but this one seems to be the best for this approach. The advantages of this approach is that you don't need to create and maintain a new prometheus exporter. The disadvantages though are that you'd have to: Manually install the required exporter resources in the cluster until a helm chart exists. Create the helm charts yourself if they don't develop it. Integrate your tool inside the script_exporter docker through one of these ways: Changing the exporter Docker image to add it. Which would mean a Docker image to maintain. Mounting the binary through a volume inside kubernetes. Which would mean defining a way on how to upload it and assume the high availability penalty that a stateful kubernetes service entail with the cluster configuration right now. If it's not already in your stack, it would mean adding a new exporter to maintain and a new development team to depend on. Alternatively you can use the script exporter binary in a baremetal or virtualized server instead of using a docker, that way you wouldn't need to maintain the different dockers for the different solutions, but you'd need a \"dedicated\" server for this purpose. Create your own exporter. You'd need to create a docker that exposes the command line functionality through a metrics endpoint. You wouldn't depend on a third party development team and would be able to use your script. On the other side it has the following disadvantages: We would need to create and maintain a new prometheus exporter. That would mean creating and maintaining the Docker with the command line tool and a simple http server that exposes the /metrics endpoint, that will run the command whenever the Prometheus server accesses this endpoint. We add a new exporter to maintain but we develop it ourselves, so we don't depend on third party developers. Use other exporters to do the check. For example, if you can deduce the critical API call that will decide if the script fails or succeeds, you could use the blackbox exporter to monitor it instead. The advantages of this solution are: We don't add new infrastructure to develop or maintain. We don't depend on third party development teams. And the disadvantage is that if the logic changes, we would need to update how we do the check.", "title": "Custom script execution"}, {"location": "monitoring_comparison/#network-monitorization", "text": "Both can use the Simple Network Management Protocol (SNMP) to communicate with network switches or other components by using SNMP protocol to query their status. Not being an expert on the topic, knowing it's been one of the core focus of Nagios in the past years and as I've not been able to find good comparison between both, I'm going to suppose that even though both support network monitoring, Nagios does a better job.", "title": "Network monitorization"}, {"location": "monitoring_comparison/#summary", "text": "Metric Nagios Prometheus Open Source \u2713* \u2713\u2713 Community \u2713 \u2713\u2713 Configuration and usage \u2713 x Visualizations \u2713 \u2713\u2713 Ansible Role \u2713 \u2713\u2713 Helm chart x \u2713 Kubernetes x \u2713 Documentation \u2713 x Integrations \u2713 \u2713\u2713 Alerts \u2713 \u2713\u2713 Advanced monitoring x \u2713 Custom script execution \u2713\u2713 \u2713 Data storage x \u2713 Dynamic infrastructure x \u2713 High availability \u2713 \u2713\u2713 Network Monitoring \u2713 \u2713 * Only Nagios Core and the community contributions are open sourced. Where each symbol means: x: Doesn't meet the criteria. \u2713: Meets the criteria. \u2713\u2713: Meets the criteria and it's better than the other solution. ?: I'm not sure. Nagios is the reference of the old-school monitoring solutions, suitable for basic monitoring of small, static and/or old-school systems where blackbox probing is sufficient. Prometheus is the reference of the new-wave monitoring solutions, suitable for more advanced monitoring of dynamic, new-wave systems (web applications, cloud, containers or Kubernetes) where whitebox monitoring is desired.", "title": "Summary"}, {"location": "monitoring_comparison/#references", "text": "Logz io post on Prometheus vs Nagios", "title": "References"}, {"location": "mopidy/", "text": "Mopidy is an extensible music server written in Python. The key features are: Plays music from many sources: local disk, Spotify, SoundCloud, Google Play Music, and more. Can be used as a server: Out of the box, Mopidy is an HTTP server. If you install the Mopidy-MPD extension, it becomes an MPD server too. Given that MPD is a popular, old, and robust solution, you can benefit of the many solutions that exist out there for MPD. Edit the playlist from any phone, tablet, or computer using a variety of MPD and web clients. It supports Beets as a library source. Is hackable: The awesome documentation, being Python based, the extension system, JSON-RPC, and JavaScript APIs make Mopidy a perfect base for your projects. References \u2691 Docs Git Home Developer info \u2691 API reference Write your own extension", "title": "Mopidy"}, {"location": "mopidy/#references", "text": "Docs Git Home", "title": "References"}, {"location": "mopidy/#developer-info", "text": "API reference Write your own extension", "title": "Developer info"}, {"location": "music_management/", "text": "Music management is the set of systems and processes to get and categorize songs so it's easy to browse and discover new content. It involves the next actions: Automatically index and download metadata of new songs. Notify the user when a new song is added. Monitor the songs of an artist, and get them once they are released. A nice interface to browse the existent library, with the possibility of filtering by author, genre, years, tags or release types. An interface to listen to the music An interface to rate and review library items. An interface to discover new content based on the ratings and item metadata. Components \u2691 I've got a music collection built from mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I have a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation. playlist_generator helped me with the last point, based on the metadata gathered with mep , but it's still not enough. So I'm in my way of migrate all the library to beets , and then I'll deprecate mep in favor to a mpd client that allows me to keep on saving the same metadata. Once it's implemented, I'll migrate all the metadata to the new system. Lidarr \u2691 I'm also using Lidarr to manage what content is missing. Both Lidarr and beets get their from MusicBrainz . This means that sometimes some artist may lack a release, if they do, please contribute to MusicBrainz and add the information. Be patient, Lidarr may take some time to fetch the information, as it probably is not available straight away from the API. One awesome feature of Lidarr is that you can select the type of releases you want for each artist. They are defined in Settings/Profiles/Metadata Profiles . To be able to fine grain your settings, you first need to understand what do , Primary Types , Secondary Types and Release Status means. If you want to set the missing picture of an artist, you need to add it at fanart.tv . It's a process that needs the moderators approval, so don't expect it to be automatic. They are really kind when you don't do things right, but still, check their upload guidelines before you contribute.", "title": "Music management"}, {"location": "music_management/#components", "text": "I've got a music collection built from mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I have a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation. playlist_generator helped me with the last point, based on the metadata gathered with mep , but it's still not enough. So I'm in my way of migrate all the library to beets , and then I'll deprecate mep in favor to a mpd client that allows me to keep on saving the same metadata. Once it's implemented, I'll migrate all the metadata to the new system.", "title": "Components"}, {"location": "music_management/#lidarr", "text": "I'm also using Lidarr to manage what content is missing. Both Lidarr and beets get their from MusicBrainz . This means that sometimes some artist may lack a release, if they do, please contribute to MusicBrainz and add the information. Be patient, Lidarr may take some time to fetch the information, as it probably is not available straight away from the API. One awesome feature of Lidarr is that you can select the type of releases you want for each artist. They are defined in Settings/Profiles/Metadata Profiles . To be able to fine grain your settings, you first need to understand what do , Primary Types , Secondary Types and Release Status means. If you want to set the missing picture of an artist, you need to add it at fanart.tv . It's a process that needs the moderators approval, so don't expect it to be automatic. They are really kind when you don't do things right, but still, check their upload guidelines before you contribute.", "title": "Lidarr"}, {"location": "musicbrainz/", "text": "MusicBrainz is an open music encyclopedia that collects music metadata and makes it available to the public. MusicBrainz aims to be: The ultimate source of music information by allowing anyone to contribute and releasing the data under open licenses. The universal lingua franca for music by providing a reliable and unambiguous form of music identification, enabling both people and machines to have meaningful conversations about music. Like Wikipedia, MusicBrainz is maintained by a global community of users and we want everyone \u2014 including you \u2014 to participate and contribute . Contributing \u2691 Creating an account is free and easy. To be able to add new releases easier, I've seen that there are some UserScript importers , they suggest to use the ViolentMonkey addon and install the desired plugins. With the Discogs one, if you don't see the Import into MB button is because you can't import a Master release, you have to click on a specific release. If that doesn't work for you, check these issues ( 1 and 2 ). It works without authentication. All the data is fetched for you except for the album cover, which you have to manually add. Make sure that you fill up the Release Status otherwise it won't show up in Lidarr . Then be patient , sometimes you need to wait hours before the changes are propagated to Lidarr. Filling up releases \u2691 Some notes on the release fields Primary types \u2691 Album : Perhaps better defined as a \"Long Play\" (LP) release, generally consists of previously unreleased material (unless this type is combined with secondary types which change that, such as \"Compilation\"). Single : A single typically has one main song and possibly a handful of additional tracks or remixes of the main track; the single is usually named after its main song; the single is primarily released to get radio play and to promote release sales. EP : An EP is a so-called \"Extended Play\" release and often contains the letters EP in the title. Generally an EP will be shorter than a full length release (an LP or \"Long Play\"), usually less than four tracks, and the tracks are usually exclusive to the EP, in other words the tracks don't come from a previously issued release. EP is fairly difficult to define; usually it should only be assumed that a release is an EP if the artist defines it as such. Broadcast : An episodic release that was originally broadcast via radio, television, or the Internet, including podcasts. Other : Any release that does not fit or can't decisively be placed in any of the categories above. Secondary types \u2691 Compilation : A compilation, for the purposes of the MusicBrainz database, covers the following types of releases: A collection of recordings from various old sources (not necessarily released) combined together. For example a \"best of\", retrospective or rarities type release. A various artists song collection, usually based on a general theme (\"Songs for Lovers\"), a particular time period (\"Hits of 1998\"), or some other kind of grouping (\"Songs From the Movies\", the \"Caf\u00e9 del Mar\" series, etc). The MusicBrainz project does not generally consider the following to be compilations: A reissue of an album, even if it includes bonus tracks. A tribute release containing covers of music by another artist. A classical release containing new recordings of works by a classical artist. A split release containing new music by several artists Compilation should be used in addition to, not instead of, other types: for example, a various artists soundtrack using pre-released music should be marked as both a soundtrack and a compilation. As a general rule, always select every secondary type that applies. Soundtrack : A soundtrack is the musical score to a movie, TV series, stage show, video game, or other medium. Video game CDs with audio tracks should be classified as soundtracks because the musical properties of the CDs are more interesting to MusicBrainz than their data properties. Spokenword : Non-music spoken word releases. Interview : An interview release contains an interview, generally with an artist. Audiobook : An audiobook is a book read by a narrator without music. Audio drama : An audio drama is an audio-only performance of a play (often, but not always, meant for radio). Unlike audiobooks, it usually has multiple performers rather than a main narrator. Live : A release that was recorded live. Remix : A release that primarily contains remixed material. DJ-mix : A DJ-mix is a sequence of several recordings played one after the other, each one modified so that they blend together into a continuous flow of music. A DJ mix release requires that the recordings be modified in some manner, and the DJ who does this modification is usually (although not always) credited in a fairly prominent way. Mixtape/Street : Promotional in nature (but not necessarily free), mixtapes and street albums are often released by artists to promote new artists, or upcoming studio albums by prominent artists. They are also sometimes used to keep fans' attention between studio releases and are most common in rap & hip hop genres. They are often not sanctioned by the artist's label, may lack proper sample or song clearances and vary widely in production and recording quality. While mixtapes are generally DJ-mixed, they are distinct from commercial DJ mixes (which are usually deemed compilations) and are defined by having a significant proportion of new material, including original production or original vocals over top of other artists' instrumentals. They are distinct from demos in that they are designed for release directly to the public and fans; not to labels. Release status \u2691 Status describes how \"official\" a release is. Possible values are: Official : Any release officially sanctioned by the artist and/or their record company. Most releases will fit into this category. Promotional : A give-away release or a release intended to promote an upcoming official release (e.g. pre-release versions, releases included with a magazine, versions supplied to radio DJs for air-play). Bootleg : An unofficial/underground release that was not sanctioned by the artist and/or the record company. This includes unofficial live recordings and pirated releases. Pseudo-release : An alternate version of a release where the titles have been changed. These don't correspond to any real release and should be linked to the original release using the transl(iter)ation relationship. References \u2691 Home", "title": "MusicBrainz"}, {"location": "musicbrainz/#contributing", "text": "Creating an account is free and easy. To be able to add new releases easier, I've seen that there are some UserScript importers , they suggest to use the ViolentMonkey addon and install the desired plugins. With the Discogs one, if you don't see the Import into MB button is because you can't import a Master release, you have to click on a specific release. If that doesn't work for you, check these issues ( 1 and 2 ). It works without authentication. All the data is fetched for you except for the album cover, which you have to manually add. Make sure that you fill up the Release Status otherwise it won't show up in Lidarr . Then be patient , sometimes you need to wait hours before the changes are propagated to Lidarr.", "title": "Contributing"}, {"location": "musicbrainz/#filling-up-releases", "text": "Some notes on the release fields", "title": "Filling up releases"}, {"location": "musicbrainz/#primary-types", "text": "Album : Perhaps better defined as a \"Long Play\" (LP) release, generally consists of previously unreleased material (unless this type is combined with secondary types which change that, such as \"Compilation\"). Single : A single typically has one main song and possibly a handful of additional tracks or remixes of the main track; the single is usually named after its main song; the single is primarily released to get radio play and to promote release sales. EP : An EP is a so-called \"Extended Play\" release and often contains the letters EP in the title. Generally an EP will be shorter than a full length release (an LP or \"Long Play\"), usually less than four tracks, and the tracks are usually exclusive to the EP, in other words the tracks don't come from a previously issued release. EP is fairly difficult to define; usually it should only be assumed that a release is an EP if the artist defines it as such. Broadcast : An episodic release that was originally broadcast via radio, television, or the Internet, including podcasts. Other : Any release that does not fit or can't decisively be placed in any of the categories above.", "title": "Primary types"}, {"location": "musicbrainz/#secondary-types", "text": "Compilation : A compilation, for the purposes of the MusicBrainz database, covers the following types of releases: A collection of recordings from various old sources (not necessarily released) combined together. For example a \"best of\", retrospective or rarities type release. A various artists song collection, usually based on a general theme (\"Songs for Lovers\"), a particular time period (\"Hits of 1998\"), or some other kind of grouping (\"Songs From the Movies\", the \"Caf\u00e9 del Mar\" series, etc). The MusicBrainz project does not generally consider the following to be compilations: A reissue of an album, even if it includes bonus tracks. A tribute release containing covers of music by another artist. A classical release containing new recordings of works by a classical artist. A split release containing new music by several artists Compilation should be used in addition to, not instead of, other types: for example, a various artists soundtrack using pre-released music should be marked as both a soundtrack and a compilation. As a general rule, always select every secondary type that applies. Soundtrack : A soundtrack is the musical score to a movie, TV series, stage show, video game, or other medium. Video game CDs with audio tracks should be classified as soundtracks because the musical properties of the CDs are more interesting to MusicBrainz than their data properties. Spokenword : Non-music spoken word releases. Interview : An interview release contains an interview, generally with an artist. Audiobook : An audiobook is a book read by a narrator without music. Audio drama : An audio drama is an audio-only performance of a play (often, but not always, meant for radio). Unlike audiobooks, it usually has multiple performers rather than a main narrator. Live : A release that was recorded live. Remix : A release that primarily contains remixed material. DJ-mix : A DJ-mix is a sequence of several recordings played one after the other, each one modified so that they blend together into a continuous flow of music. A DJ mix release requires that the recordings be modified in some manner, and the DJ who does this modification is usually (although not always) credited in a fairly prominent way. Mixtape/Street : Promotional in nature (but not necessarily free), mixtapes and street albums are often released by artists to promote new artists, or upcoming studio albums by prominent artists. They are also sometimes used to keep fans' attention between studio releases and are most common in rap & hip hop genres. They are often not sanctioned by the artist's label, may lack proper sample or song clearances and vary widely in production and recording quality. While mixtapes are generally DJ-mixed, they are distinct from commercial DJ mixes (which are usually deemed compilations) and are defined by having a significant proportion of new material, including original production or original vocals over top of other artists' instrumentals. They are distinct from demos in that they are designed for release directly to the public and fans; not to labels.", "title": "Secondary types"}, {"location": "musicbrainz/#release-status", "text": "Status describes how \"official\" a release is. Possible values are: Official : Any release officially sanctioned by the artist and/or their record company. Most releases will fit into this category. Promotional : A give-away release or a release intended to promote an upcoming official release (e.g. pre-release versions, releases included with a magazine, versions supplied to radio DJs for air-play). Bootleg : An unofficial/underground release that was not sanctioned by the artist and/or the record company. This includes unofficial live recordings and pirated releases. Pseudo-release : An alternate version of a release where the titles have been changed. These don't correspond to any real release and should be linked to the original release using the transl(iter)ation relationship.", "title": "Release status"}, {"location": "musicbrainz/#references", "text": "Home", "title": "References"}, {"location": "nas/", "text": "Network-attached storage or NAS, is a computer data storage server connected to a computer network providing data access to many other devices. Basically a computer where you can attach many hard drives. A quick search revealed two kind of solutions: Plug and play. Do It Yourself. The first ones are servers that some companies like Synology or Qnap sell with their own software. They're meant for users that want to have something that works and does not give too much work at the expense of being restricted to what the provider gives you. The second ones are similar servers but you need to build it from scratch with existent tools like Linux or ZFS . I personally feel a bit constrained and vendor locked by the first. For example they are not user-repairable, so if a part breaks after warranty, you have to replace the whole server . Or you may use without knowing their proprietary storage format, that may mean that it's difficult to recover the data once you want to move away from their solution It makes sense then to trade time and dedication for freedom. Software \u2691 TrueNAS \u2691 TrueNAS (formerly known as FreeNAS) is one of the most popular solution operating systems for storage servers. It\u2019s open-source, and it\u2019s been around for almost 20 years, so it seemed like a reliable choice. As you can use it on any hardware, therefore removing the vendor locking, and it's open source. They have different solutions, being the Core the most basic. They also have Truenas Scale with which you could even build distributed systems. A trusted friend prevented me from going in this direction as he felt that GlusterFS over ZFS was not a good idea. TrueNAS core looked good but I still felt a bit constrained and out of control, so I discarded it. Unraid \u2691 Unraid is a proprietary Linux-based operating system designed to run on home media server setups that operates as a network-attached storage device, application server, and virtualization host. I've heard that it can do wonders like tweaking the speed of disks based on the need. It's not \"that expensive\" but still it's a proprietary so nope. Debian with ZFS \u2691 This solution gives you the most freedom and if you're used to use Linux like me, is the one where you may feel most at home. The idea is to use a normal Debian and configure it to use ZFS to manage the storage. Hardware \u2691 Depending the amount of data you need to hold and how do you expect it to grow you need to find the solution that suits your needs. After looking to many I've decided to make my own from scratch. Warning: If you pursue the beautiful and hard path of building one yourself, don't just buy the components online, there are thousands of things that can go wrong that will make you loose money. Instead go to your local hardware store and try to build the server with them. Even if it's a little bit more expensive you'll save energy and peace of mind. Disks \u2691 ZFS tutorials suggest buying everything all at once for the final stage of your server. Knowing the disadvantages it entails, I'll start with a solution for 16TB that supports to easily expand in the future. I'll start with 5 disks of 8TB, make sure you read the ZFS section on storage planning to understand why. The analysis of choosing the disks to hold data gave two IronWolf Pro and two Exos 7E8 for a total amount of 1062$. I'll add another IronWolf Pro as a cold spare. RAM \u2691 Most ZFS resources suggest using ECC RAM. The provider gives me two options: Kingston Server Premier DDR4 3200MHz 16GB CL22 Kingston Server Premier DDR4 2666MHz 16GB CL19 I'll go with two modules of 3200MHz CL22 because it has a smaller RAM latency . Which was a bummer, as it turned out that my motherboard doesn't support Registered ECC ram, but only Unregistered ECC ram. Motherboard \u2691 After reading these reviews( 1 , 2 ) I've come to the decision to purchase the ASRock X570M Pro4 because, It supports: 8 x SATA3 disks 2 x M.2 disks 4 x DDR4 RAM slots with speeds up to 4200+ and ECC support 1 x AMD AM4 Socket Ryzen\u2122 2000, 3000, 4000 G-Series, 5000 and 5000 G-Series Desktop Processors Supports NVMe SSD as boot disks Micro ATX Form Factor. And it gives me room enough to grow: It supports PCI 4.0 for the M.2 which is said to be capable of perform twice the speed compared to previous 3 rd generation. the chosen M2 are of 3 rd generation, so if I need more speed I can change them. I'm only going to use 2 slots of RAM giving me 32GB, but I could grow 32 more easily. CPU \u2691 After doing some basic research I've chosen the Ryzen 7 5700x . CPU cooler \u2691 After doing some basic research I've chosen the Dark Rock 4 but just because the Enermax ETS-T50 AXE Silent Edition doesn't fit my case :(. Graphic card \u2691 As it's going to be a server and I don't need it for transcoding or gaming, I'll start without a graphic card. Server Case \u2691 Computer cases are boxes that hold all your computer components together. I'm ruling out the next ones: Fractal Design R6 : More expensive than the Node 804 and it doesn't have hot swappable disks. Silverstone Technology SST-CS381: Even though it's gorgeous it's too expensive. Silverstone DS380: It only supports Mini-ITX which I don't have. The remaining are: Model Fractal Node 804 Silverstone CS380 Form factor Micro - ATX Mid tower Motherboard Micro ATX Micro ATX Drive bays 8 x 3.5\", 2 x 2.5\" 8 x 3.5\", 2 x 5.25\" Hot-swap No yes Expansion Slots 5 7 CPU cooler height 160mm 146 mm PSU compatibility ATX ATX Fans Front: 4, Top: 4, Rear 3 Side: 2, Rear: 1 Price 115 184 Size 34 x 31 x 39 cm 35 x 28 x 21 cm I like the Fractal Node 804 better and it's cheaper. Power supply unit \u2691 Using PCPartPicker I've seen that with 4 disks it consumes approximately 264W, when I have the 8 disks, it will consume up to 344W, if I want to increase the ram then it will reach 373W. So in theory I can go with a 400W power supply unit. You need to make sure that it has enough wires to connect to all the disks. Although that usually is not a problem as there are adapters: Molex to sata Sata to sata After an analysis on the different power supply units , I've decided to go with Be Quiet! Straight Power 11 450W Gold Wires \u2691 Usually disks come without sata wires, so you have to buy them Hardware conclusion \u2691 Piece Purpose Number Total price ($) Seagate IronWolf Pro (8TB) Data disk 3 Seagate Exos 7E8 (8TB) Data disk 2 438 WD Red SN700 (1TB) M.2 disks 2 246 Kingston Server DDR4 (16GB) ECC RAM 2 187 AsRock X570M Pro4 Motherboard 1 225 Ryzen 7 5700x CPU 1 274 Fractal Node 804 Case 1 137 Dark Rock 4 CPU Cooler 1 Be Quiet! Straight Power 11 PSU 1 99 Sata wires Sata 3 6.5 No graphic card Graphic card 0 0 CPU thermal paste thermal paste 0 0 References \u2691 mtlynch NAS building guide NAS master building guide", "title": "NAS"}, {"location": "nas/#software", "text": "", "title": "Software"}, {"location": "nas/#truenas", "text": "TrueNAS (formerly known as FreeNAS) is one of the most popular solution operating systems for storage servers. It\u2019s open-source, and it\u2019s been around for almost 20 years, so it seemed like a reliable choice. As you can use it on any hardware, therefore removing the vendor locking, and it's open source. They have different solutions, being the Core the most basic. They also have Truenas Scale with which you could even build distributed systems. A trusted friend prevented me from going in this direction as he felt that GlusterFS over ZFS was not a good idea. TrueNAS core looked good but I still felt a bit constrained and out of control, so I discarded it.", "title": "TrueNAS"}, {"location": "nas/#unraid", "text": "Unraid is a proprietary Linux-based operating system designed to run on home media server setups that operates as a network-attached storage device, application server, and virtualization host. I've heard that it can do wonders like tweaking the speed of disks based on the need. It's not \"that expensive\" but still it's a proprietary so nope.", "title": "Unraid"}, {"location": "nas/#debian-with-zfs", "text": "This solution gives you the most freedom and if you're used to use Linux like me, is the one where you may feel most at home. The idea is to use a normal Debian and configure it to use ZFS to manage the storage.", "title": "Debian with ZFS"}, {"location": "nas/#hardware", "text": "Depending the amount of data you need to hold and how do you expect it to grow you need to find the solution that suits your needs. After looking to many I've decided to make my own from scratch. Warning: If you pursue the beautiful and hard path of building one yourself, don't just buy the components online, there are thousands of things that can go wrong that will make you loose money. Instead go to your local hardware store and try to build the server with them. Even if it's a little bit more expensive you'll save energy and peace of mind.", "title": "Hardware"}, {"location": "nas/#disks", "text": "ZFS tutorials suggest buying everything all at once for the final stage of your server. Knowing the disadvantages it entails, I'll start with a solution for 16TB that supports to easily expand in the future. I'll start with 5 disks of 8TB, make sure you read the ZFS section on storage planning to understand why. The analysis of choosing the disks to hold data gave two IronWolf Pro and two Exos 7E8 for a total amount of 1062$. I'll add another IronWolf Pro as a cold spare.", "title": "Disks"}, {"location": "nas/#ram", "text": "Most ZFS resources suggest using ECC RAM. The provider gives me two options: Kingston Server Premier DDR4 3200MHz 16GB CL22 Kingston Server Premier DDR4 2666MHz 16GB CL19 I'll go with two modules of 3200MHz CL22 because it has a smaller RAM latency . Which was a bummer, as it turned out that my motherboard doesn't support Registered ECC ram, but only Unregistered ECC ram.", "title": "RAM"}, {"location": "nas/#motherboard", "text": "After reading these reviews( 1 , 2 ) I've come to the decision to purchase the ASRock X570M Pro4 because, It supports: 8 x SATA3 disks 2 x M.2 disks 4 x DDR4 RAM slots with speeds up to 4200+ and ECC support 1 x AMD AM4 Socket Ryzen\u2122 2000, 3000, 4000 G-Series, 5000 and 5000 G-Series Desktop Processors Supports NVMe SSD as boot disks Micro ATX Form Factor. And it gives me room enough to grow: It supports PCI 4.0 for the M.2 which is said to be capable of perform twice the speed compared to previous 3 rd generation. the chosen M2 are of 3 rd generation, so if I need more speed I can change them. I'm only going to use 2 slots of RAM giving me 32GB, but I could grow 32 more easily.", "title": "Motherboard"}, {"location": "nas/#cpu", "text": "After doing some basic research I've chosen the Ryzen 7 5700x .", "title": "CPU"}, {"location": "nas/#cpu-cooler", "text": "After doing some basic research I've chosen the Dark Rock 4 but just because the Enermax ETS-T50 AXE Silent Edition doesn't fit my case :(.", "title": "CPU cooler"}, {"location": "nas/#graphic-card", "text": "As it's going to be a server and I don't need it for transcoding or gaming, I'll start without a graphic card.", "title": "Graphic card"}, {"location": "nas/#server-case", "text": "Computer cases are boxes that hold all your computer components together. I'm ruling out the next ones: Fractal Design R6 : More expensive than the Node 804 and it doesn't have hot swappable disks. Silverstone Technology SST-CS381: Even though it's gorgeous it's too expensive. Silverstone DS380: It only supports Mini-ITX which I don't have. The remaining are: Model Fractal Node 804 Silverstone CS380 Form factor Micro - ATX Mid tower Motherboard Micro ATX Micro ATX Drive bays 8 x 3.5\", 2 x 2.5\" 8 x 3.5\", 2 x 5.25\" Hot-swap No yes Expansion Slots 5 7 CPU cooler height 160mm 146 mm PSU compatibility ATX ATX Fans Front: 4, Top: 4, Rear 3 Side: 2, Rear: 1 Price 115 184 Size 34 x 31 x 39 cm 35 x 28 x 21 cm I like the Fractal Node 804 better and it's cheaper.", "title": "Server Case"}, {"location": "nas/#power-supply-unit", "text": "Using PCPartPicker I've seen that with 4 disks it consumes approximately 264W, when I have the 8 disks, it will consume up to 344W, if I want to increase the ram then it will reach 373W. So in theory I can go with a 400W power supply unit. You need to make sure that it has enough wires to connect to all the disks. Although that usually is not a problem as there are adapters: Molex to sata Sata to sata After an analysis on the different power supply units , I've decided to go with Be Quiet! Straight Power 11 450W Gold", "title": "Power supply unit"}, {"location": "nas/#wires", "text": "Usually disks come without sata wires, so you have to buy them", "title": "Wires"}, {"location": "nas/#hardware-conclusion", "text": "Piece Purpose Number Total price ($) Seagate IronWolf Pro (8TB) Data disk 3 Seagate Exos 7E8 (8TB) Data disk 2 438 WD Red SN700 (1TB) M.2 disks 2 246 Kingston Server DDR4 (16GB) ECC RAM 2 187 AsRock X570M Pro4 Motherboard 1 225 Ryzen 7 5700x CPU 1 274 Fractal Node 804 Case 1 137 Dark Rock 4 CPU Cooler 1 Be Quiet! Straight Power 11 PSU 1 99 Sata wires Sata 3 6.5 No graphic card Graphic card 0 0 CPU thermal paste thermal paste 0 0", "title": "Hardware conclusion"}, {"location": "nas/#references", "text": "mtlynch NAS building guide NAS master building guide", "title": "References"}, {"location": "networkx/", "text": "NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. References \u2691 Docs Git Home", "title": "NetworkX"}, {"location": "networkx/#references", "text": "Docs Git Home", "title": "References"}, {"location": "news_management/", "text": "The information world of today is overwhelming. It can reach a point that you just want to disconnect so as to avoid the continuous bombardment, but that leads to loosing connection with what's happening in the world. Without knowing what's going on it's impossible to act to shape it better. The problem to solve is to: Keep updated on the important articles for you. Don't invest too much time on it. Don't loose time reading articles you're not interested on. Workflow \u2691 I've found three information types to explore: Written content: articles, blogs, newspapers... Listened content: mainly podcasts. Viewed content: youtube, twich channels. Each has it's advantages and disadvantages. I like the written content as it lets me decide the pace of information ingestion, it's compatible with incremental reading, and it's the best medium to learn by making annotations and summaries, it requires your full attention though. Listened content is best to keep updated while you do brainless tasks such as cooking or cleaning, but it makes difficult to save references or ideas. Viewed content is as attention demanding as reading unless you don't care about the visual content and take it as a podcast. Written content \u2691 To process the written content I use an RSS reader ( Feeder ) to gather all written content in one place. I skim through the elements without reading them, and I send the ones that catch my attention to wallabag for later reading. Then I go to wallabag and read the elements that feels more attractive at that moment. Before starting to read, I define the amount of time I want to spend on getting updated. Half of that time goes to skimming through, and the other to deep reading the selected content. You'll probably won't be able to process either the whole content on your RSS reader nor the selected content, that's why a recommender system would be awesome. Finding the reading devices is very important. I prefer to browse it on a tablet as it's much more pleasant than a mobile or a computer, an e-reader would be better, although wallabag is supported on some e-readers, I haven't tried it yet. I can't wait for the PineNote to be released. The moments I've found suitable for reading content are while eating breakfast or dinner when I'm alone. Listened content \u2691 I've selected a small number of podcasts that I listen with AntennaPod while cooking or cleaning, instead of listening directly from the mobile, I use a bluetooth loudspeaker that I carry everywhere I go (at home! use headphones when you are outside. people with loudspeakers on the public transport or streets are hateful), if there is a reference I want to save, I write it down in the mobile inbox and process it later with pynbox . The perfect software solution \u2691 My current workflow could be improved by software, currently the key features I'd want are: One place for all sources: It's useless to go to n different websites to see if there is new information. RSS has been with us for too long to fall on that. The user has control of it's data: The user should be able to decide which information is private and which one is public. Only the people it trusts will have access to it's private data. There must be a filter of the incoming elements: It doesn't matter how well you choose your sources, there's always going to be content that is not interesting for you. So there needs to be a powerful filtering system. Content filtering \u2691 Filtering content is a subsection of the recommender systems , of all the basic models, the ones that apply are: Collaborative filtering : Where the data of many users is used to filter out the relevant items. Content based filtering : Where the data of the user on past items is used to filter new elements. Collaborative filtering \u2691 External users give information on how they see the items, and the algorithm can use that data to decide which ones are relevant for a desired user. It's how social networks operate, and if you use Mastodon, Reddit, HackerNews, Facebook, Twitter or similar, then you may not even be interested in this article at all. All those platforms have one or more of the next flaws for me: There is no one place that aggregates the information of all information sources. You can't mark content as seen. Your data lives in the servers of other people. They are based on closed sourced software. There's a mix of information with conversation between users. You may not be interested in all the elements the source publishes. Some of them support the export to RSS and if they don't, you'll probably can find bridge platforms that do. That'll solve all the problems but the two last ones. I've used the RSS feed of mastodon users, and I finally removed them because there was a lot of content I didn't like. I've also used the reddit and hackernews rss, but again, most of the posts weren't interesting to me. A partial solution I've been using with my friends is to share relevant content through wallabag . It's a read-it-later application that creates RSS feeds for the liked elements. That way you can get filtered content from the people you know. The downsides are: You get one feed for all the content, you don't have the possibility to filter out by categories or tags. It's not very collaborative. You need to ask one by one for their RSS. A prettier (but more difficult) solution would be to create communities that commit to share the articles interesting for a specific topic. Like HackerNews but at smaller level, where you know and trust the members of the community, and where people outside the community can not upload data, just read it. This could be done through one instance of lemmy , with a closed community, but that service is envisioned to be federated and to encourage the interaction of the users on the site. Another possibility would be to create a simple backend that mimics the api interface of wallabag, so that users could use the browser addon and all the interfaces existent, for example the Feeder integration with wallabag instances. You'll be able to create communities, and invite users to them, they will be able to link a \"wallabag\" tag with a specific community channel. That way, an RSS feed will be created per community with all the articles shared by their members. Optionally, users could decide to ignore the entries of another user. But for any of these solutions to work, you'll need to convince the people to tweak how they browse the internet in order to contribute to the system, which it's kind of difficult :(. Content based filtering \u2691 If you don't manage to convince the people to use collaborative filtering, you're on your own. Your best bet then is to deduce which elements are interesting based on how you rated other elements. I haven't found any rss reader that does good filtering of the elements. I've used Newsblur in the past, you're able to assign scores for the element tags and user. But I didn't find it very useful. Also when it comes to self host it, it's a little bit of a nightmare . Intermediate solutions between the sources and the reader aren't a viable option either, as you need to interact with that middleware outside the RSS reader.", "title": "News Management"}, {"location": "news_management/#workflow", "text": "I've found three information types to explore: Written content: articles, blogs, newspapers... Listened content: mainly podcasts. Viewed content: youtube, twich channels. Each has it's advantages and disadvantages. I like the written content as it lets me decide the pace of information ingestion, it's compatible with incremental reading, and it's the best medium to learn by making annotations and summaries, it requires your full attention though. Listened content is best to keep updated while you do brainless tasks such as cooking or cleaning, but it makes difficult to save references or ideas. Viewed content is as attention demanding as reading unless you don't care about the visual content and take it as a podcast.", "title": "Workflow"}, {"location": "news_management/#written-content", "text": "To process the written content I use an RSS reader ( Feeder ) to gather all written content in one place. I skim through the elements without reading them, and I send the ones that catch my attention to wallabag for later reading. Then I go to wallabag and read the elements that feels more attractive at that moment. Before starting to read, I define the amount of time I want to spend on getting updated. Half of that time goes to skimming through, and the other to deep reading the selected content. You'll probably won't be able to process either the whole content on your RSS reader nor the selected content, that's why a recommender system would be awesome. Finding the reading devices is very important. I prefer to browse it on a tablet as it's much more pleasant than a mobile or a computer, an e-reader would be better, although wallabag is supported on some e-readers, I haven't tried it yet. I can't wait for the PineNote to be released. The moments I've found suitable for reading content are while eating breakfast or dinner when I'm alone.", "title": "Written content"}, {"location": "news_management/#listened-content", "text": "I've selected a small number of podcasts that I listen with AntennaPod while cooking or cleaning, instead of listening directly from the mobile, I use a bluetooth loudspeaker that I carry everywhere I go (at home! use headphones when you are outside. people with loudspeakers on the public transport or streets are hateful), if there is a reference I want to save, I write it down in the mobile inbox and process it later with pynbox .", "title": "Listened content"}, {"location": "news_management/#the-perfect-software-solution", "text": "My current workflow could be improved by software, currently the key features I'd want are: One place for all sources: It's useless to go to n different websites to see if there is new information. RSS has been with us for too long to fall on that. The user has control of it's data: The user should be able to decide which information is private and which one is public. Only the people it trusts will have access to it's private data. There must be a filter of the incoming elements: It doesn't matter how well you choose your sources, there's always going to be content that is not interesting for you. So there needs to be a powerful filtering system.", "title": "The perfect software solution"}, {"location": "news_management/#content-filtering", "text": "Filtering content is a subsection of the recommender systems , of all the basic models, the ones that apply are: Collaborative filtering : Where the data of many users is used to filter out the relevant items. Content based filtering : Where the data of the user on past items is used to filter new elements.", "title": "Content filtering"}, {"location": "news_management/#collaborative-filtering", "text": "External users give information on how they see the items, and the algorithm can use that data to decide which ones are relevant for a desired user. It's how social networks operate, and if you use Mastodon, Reddit, HackerNews, Facebook, Twitter or similar, then you may not even be interested in this article at all. All those platforms have one or more of the next flaws for me: There is no one place that aggregates the information of all information sources. You can't mark content as seen. Your data lives in the servers of other people. They are based on closed sourced software. There's a mix of information with conversation between users. You may not be interested in all the elements the source publishes. Some of them support the export to RSS and if they don't, you'll probably can find bridge platforms that do. That'll solve all the problems but the two last ones. I've used the RSS feed of mastodon users, and I finally removed them because there was a lot of content I didn't like. I've also used the reddit and hackernews rss, but again, most of the posts weren't interesting to me. A partial solution I've been using with my friends is to share relevant content through wallabag . It's a read-it-later application that creates RSS feeds for the liked elements. That way you can get filtered content from the people you know. The downsides are: You get one feed for all the content, you don't have the possibility to filter out by categories or tags. It's not very collaborative. You need to ask one by one for their RSS. A prettier (but more difficult) solution would be to create communities that commit to share the articles interesting for a specific topic. Like HackerNews but at smaller level, where you know and trust the members of the community, and where people outside the community can not upload data, just read it. This could be done through one instance of lemmy , with a closed community, but that service is envisioned to be federated and to encourage the interaction of the users on the site. Another possibility would be to create a simple backend that mimics the api interface of wallabag, so that users could use the browser addon and all the interfaces existent, for example the Feeder integration with wallabag instances. You'll be able to create communities, and invite users to them, they will be able to link a \"wallabag\" tag with a specific community channel. That way, an RSS feed will be created per community with all the articles shared by their members. Optionally, users could decide to ignore the entries of another user. But for any of these solutions to work, you'll need to convince the people to tweak how they browse the internet in order to contribute to the system, which it's kind of difficult :(.", "title": "Collaborative filtering"}, {"location": "news_management/#content-based-filtering", "text": "If you don't manage to convince the people to use collaborative filtering, you're on your own. Your best bet then is to deduce which elements are interesting based on how you rated other elements. I haven't found any rss reader that does good filtering of the elements. I've used Newsblur in the past, you're able to assign scores for the element tags and user. But I didn't find it very useful. Also when it comes to self host it, it's a little bit of a nightmare . Intermediate solutions between the sources and the reader aren't a viable option either, as you need to interact with that middleware outside the RSS reader.", "title": "Content based filtering"}, {"location": "notmuch/", "text": "notmuch is a command-line based program for indexing, searching, reading, and tagging large collections of email messages. Installation \u2691 In order to use Notmuch, you will need to have your email messages stored in your local filesystem, one message per file. You can use mbsync to do that. sudo apt-get install notmuch Configuration \u2691 To configure Notmuch, just run notmuch This will interactively guide you through the setup process, and save the configuration to ~/.notmuch-config . If you'd like to change the configuration in the future, you can either edit that file directly, or run notmuch setup . If you plan to use afew set the tags to new . To test everything works as expected, and create a database that indexes all of your mail run: notmuch new References \u2691 Docs", "title": "notmuch"}, {"location": "notmuch/#installation", "text": "In order to use Notmuch, you will need to have your email messages stored in your local filesystem, one message per file. You can use mbsync to do that. sudo apt-get install notmuch", "title": "Installation"}, {"location": "notmuch/#configuration", "text": "To configure Notmuch, just run notmuch This will interactively guide you through the setup process, and save the configuration to ~/.notmuch-config . If you'd like to change the configuration in the future, you can either edit that file directly, or run notmuch setup . If you plan to use afew set the tags to new . To test everything works as expected, and create a database that indexes all of your mail run: notmuch new", "title": "Configuration"}, {"location": "notmuch/#references", "text": "Docs", "title": "References"}, {"location": "ombi/", "text": "Ombi is a self-hosted web application that automatically gives your shared Jellyfin users the ability to request content by themselves! Ombi can be linked to multiple TV Show and Movie DVR tools to create a seamless end-to-end experience for your users. If Ombi is not for you, you may try Overseerr . References \u2691 Homepage Docs", "title": "Ombi"}, {"location": "ombi/#references", "text": "Homepage Docs", "title": "References"}, {"location": "openproject/", "text": "OpenProject is an Open source project management software. The benefits over other similar software are : It's popular: More than 6.2k stars on github, 1.7k forks. It's development is active : in the last week they've merged 44 merged pull requests by 16 people. They use their own software to track their bugs Easy to install Easy to use The community version is flexible enough to adapt to different workflows. Good installation and operation's documentation . Very good API documentation. Supports LDAP The things I don't like are: It's not keyboard driven, you use the mouse a lot . The task editor doesn't support markdown You can't sort the work package views You can't fold the hierarchy trees so it's difficult to manage the tasks once you have many. You can see my struggles with this issue here . You can't order the tasks inside the Relations tab of a task. You can't propagate easily the change of attributes to all it's children. For example if you want to make a parent task and all it's children appear on a report that is searching for an attribute. You need to go to a view where you see all the tasks (an hierarchy view) select them all and do a bulk edit. Versions or sprints can't be used across projects even if they are subprojects of a project. The manual order of the tasks is not saved across views, so you need to have independent views in order not to get confused on which is the prioritized list. Data can be exported as XML or CSV but it doesn't export everything. You have access to the database though, so if you'd like a better extraction of the data you in theory can do a selective dump of whatever you need. It doesn't yet have tag support . You can meanwhile add the strings you would use as tags in the description, and then filter by text in description. There is no demo instance where you can try it. It's easy though to launch a Proof of Concept environment yourself if you already know docker-compose . You can't hide an element from a report for a day. For example if there is a blocked task that you can't work on for today, you can't hide it till tomorrow. Even thought the Community (free) version has many features the next aren't: The status column is not showing the status color . Status boards : you can't have Kanban boards that show the state of the issues as columns. You can make it yourself through a Basic board and with the columns as the name of the state. But when you transition an issue from state, you need to move the issue and change the property yourself. I've thought of creating a script that works with the API to do this automatically, maybe through the webhooks of the openproject, but it would make more sense to spend time on pydo . Version boards : Useful to transition issues between sprints when you didn't finish them in time. Probably this is easily solved through bulk editing the issues. Custom actions looks super cool, but as this gives additional value compared with the competitors, I understand it's a paid feature. Display relations in the work package list : It would be useful to quickly see which tasks are blocked, by whom and why. Nothing critical though. Multiselect custom fields : You can only do single valued fields. Can't understand why this is a paid feature. 2FA authentication is only an Enterprise feature. OpenID and SAML are an enterprise feature. Installation \u2691 Proof of Concept \u2691 It can be installed both on kubernetes and through docker-compose ). I'm going to follow the docker-compose instructions for a Proof of Concept: Clone this repository: git clone https://github.com/opf/openproject-deploy --depth = 1 --branch = stable/12 openproject cd openproject/compose Make sure you are using the latest version of the Docker images: docker-compose pull Launch the containers: OPENPROJECT_HTTPS = false PORT = 127 .0.0.1:8080 docker-compose up Where: OPENPROJECT_HTTPS=false : Is required if you want to try it locally and you haven't yet configured the proxy to do the ssl termination. PORT=127.0.0.1:8080 : Is required so that you only expose the service to your localhost. After a while, OpenProject should be up and running on http://localhost:8080 . Production \u2691 It can be installed both on kubernetes and through docker-compose ). I'm going to follow the docker-compose : Clone this repository: git clone https://github.com/opf/openproject-deploy --depth = 1 --branch = stable/12 /data/config cd /data/config/compose Make sure you are using the latest version of the Docker images: docker-compose pull Tweak the docker-compose.yaml file through the docker-compose.override.yml Add the required environmental variables through a .env file OPENPROJECT_HOST__NAME=openproject.example.com OPENPROJECT_SECRET_KEY_BASE=secret PGDATA=/path/to/postgres/data OPDATA=/path/to/openproject/data Where secret is the value of head /dev/urandom | tr -dc A-Za-z0-9 | head -c 32 ; echo '' Launch the containers: docker-compose up Configure the ssl proxy. Connect with user admin and password admin . Create the systemd service in /etc/systemd/system/openproject.service [Unit] Description=openproject Requires=docker.service After=docker.service [Service] Restart=always User=root Group=docker WorkingDirectory=/data/config/compose # Shutdown container (if running) when unit is started TimeoutStartSec=100 RestartSec=2s # Start container when unit is started ExecStart=/usr/bin/docker-compose up # Stop container when unit is stopped ExecStop=/usr/bin/docker-compose down [Install] WantedBy=multi-user.target Operations \u2691 Doing backups Upgrading Restoring the service Workflows \u2691 The plans \u2691 I usually do a day, week, month, trimestre and year plans. To model this in OpenProjects I've created a version with each of these values. To sort them as I want them to appear I had to append a number so it would be: Day Week Month ... Tips \u2691 Bulk editing \u2691 Select the work packages to edit holding the Ctrl key and then right click over them and select Bulk Edit . Form editing \u2691 Even though it looks that you can't tweak the forms of the issues you can add the sections on the right grey column to the ones on the left blue . You can't however: Remove a section. Rename a section. Tweaking the work package status \u2691 Once you create a new status you need to tweak the workflows to be able to transition the different statuses. In the admin settings select \u201cWorkflow\u201d in the left menu, select the role and Type to which you want to assign the status. Then uncheck \u201cOnly display statuses that are used by this type\u201d and click \u201cEdit\u201d. In the table check the transitions you want to allow for the selected role and type and click \u201cSave\u201d. Deal with big number of tasks \u2691 As the number of tasks increase, the views of your work packages starts becoming more cluttered. As you can't fold the hierarchy trees it's difficult to efficiently manage your backlog. I've tried setting up a work package type that is only used for the subtasks so that they are filtered out of the view, but then you don't know if they are parent tasks unless you use the details window. It's inconvenient but having to collapse the tasks every time it's more cumbersome. You'll also need to reserve the selected subtask type (in my case Task ) for the subtasks. Sorting work package views \u2691 They are sorted alphabetically, so the only way to sort them is by prepending a number. You can do 0. Today instead of Today . It's good to do big increments between numbers, so the next report could be 10. Backlog . That way if you later realize you want another report between Today and Backlog, you can use 5. New Report and not rename all the reports. Pasting text into the descriptions \u2691 When I paste the content of the clipboard in the description, all new lines are removed ( \\n ), the workaround is to paste it inside a code snippet . References \u2691 Docs Bug tracker Git Homepage Upgrading notes", "title": "OpenProject"}, {"location": "openproject/#installation", "text": "", "title": "Installation"}, {"location": "openproject/#proof-of-concept", "text": "It can be installed both on kubernetes and through docker-compose ). I'm going to follow the docker-compose instructions for a Proof of Concept: Clone this repository: git clone https://github.com/opf/openproject-deploy --depth = 1 --branch = stable/12 openproject cd openproject/compose Make sure you are using the latest version of the Docker images: docker-compose pull Launch the containers: OPENPROJECT_HTTPS = false PORT = 127 .0.0.1:8080 docker-compose up Where: OPENPROJECT_HTTPS=false : Is required if you want to try it locally and you haven't yet configured the proxy to do the ssl termination. PORT=127.0.0.1:8080 : Is required so that you only expose the service to your localhost. After a while, OpenProject should be up and running on http://localhost:8080 .", "title": "Proof of Concept"}, {"location": "openproject/#production", "text": "It can be installed both on kubernetes and through docker-compose ). I'm going to follow the docker-compose : Clone this repository: git clone https://github.com/opf/openproject-deploy --depth = 1 --branch = stable/12 /data/config cd /data/config/compose Make sure you are using the latest version of the Docker images: docker-compose pull Tweak the docker-compose.yaml file through the docker-compose.override.yml Add the required environmental variables through a .env file OPENPROJECT_HOST__NAME=openproject.example.com OPENPROJECT_SECRET_KEY_BASE=secret PGDATA=/path/to/postgres/data OPDATA=/path/to/openproject/data Where secret is the value of head /dev/urandom | tr -dc A-Za-z0-9 | head -c 32 ; echo '' Launch the containers: docker-compose up Configure the ssl proxy. Connect with user admin and password admin . Create the systemd service in /etc/systemd/system/openproject.service [Unit] Description=openproject Requires=docker.service After=docker.service [Service] Restart=always User=root Group=docker WorkingDirectory=/data/config/compose # Shutdown container (if running) when unit is started TimeoutStartSec=100 RestartSec=2s # Start container when unit is started ExecStart=/usr/bin/docker-compose up # Stop container when unit is stopped ExecStop=/usr/bin/docker-compose down [Install] WantedBy=multi-user.target", "title": "Production"}, {"location": "openproject/#operations", "text": "Doing backups Upgrading Restoring the service", "title": "Operations"}, {"location": "openproject/#workflows", "text": "", "title": "Workflows"}, {"location": "openproject/#the-plans", "text": "I usually do a day, week, month, trimestre and year plans. To model this in OpenProjects I've created a version with each of these values. To sort them as I want them to appear I had to append a number so it would be: Day Week Month ...", "title": "The plans"}, {"location": "openproject/#tips", "text": "", "title": "Tips"}, {"location": "openproject/#bulk-editing", "text": "Select the work packages to edit holding the Ctrl key and then right click over them and select Bulk Edit .", "title": "Bulk editing"}, {"location": "openproject/#form-editing", "text": "Even though it looks that you can't tweak the forms of the issues you can add the sections on the right grey column to the ones on the left blue . You can't however: Remove a section. Rename a section.", "title": "Form editing"}, {"location": "openproject/#tweaking-the-work-package-status", "text": "Once you create a new status you need to tweak the workflows to be able to transition the different statuses. In the admin settings select \u201cWorkflow\u201d in the left menu, select the role and Type to which you want to assign the status. Then uncheck \u201cOnly display statuses that are used by this type\u201d and click \u201cEdit\u201d. In the table check the transitions you want to allow for the selected role and type and click \u201cSave\u201d.", "title": "Tweaking the work package status"}, {"location": "openproject/#deal-with-big-number-of-tasks", "text": "As the number of tasks increase, the views of your work packages starts becoming more cluttered. As you can't fold the hierarchy trees it's difficult to efficiently manage your backlog. I've tried setting up a work package type that is only used for the subtasks so that they are filtered out of the view, but then you don't know if they are parent tasks unless you use the details window. It's inconvenient but having to collapse the tasks every time it's more cumbersome. You'll also need to reserve the selected subtask type (in my case Task ) for the subtasks.", "title": "Deal with big number of tasks"}, {"location": "openproject/#sorting-work-package-views", "text": "They are sorted alphabetically, so the only way to sort them is by prepending a number. You can do 0. Today instead of Today . It's good to do big increments between numbers, so the next report could be 10. Backlog . That way if you later realize you want another report between Today and Backlog, you can use 5. New Report and not rename all the reports.", "title": "Sorting work package views"}, {"location": "openproject/#pasting-text-into-the-descriptions", "text": "When I paste the content of the clipboard in the description, all new lines are removed ( \\n ), the workaround is to paste it inside a code snippet .", "title": "Pasting text into the descriptions"}, {"location": "openproject/#references", "text": "Docs Bug tracker Git Homepage Upgrading notes", "title": "References"}, {"location": "oracle_database/", "text": "Oracle Database is an awful proprietary database, run away from it! Install \u2691 Download or clone the files of their docker repository . Create an account in their page to be able to download the required binary files . Fake person generator might come handy for this step. Download the files . After downloading the file we need to copy it to the folder referring to the oracle version in the cloned folder. In this case, 19.3.0: mv ~/Download/LINUX.X64_193000_db_home.zip ./docker-images/OracleDatabase/SingleInstance/dockerfiles/19.3.0/ The next step is to build the image. You need at least 20G free in /var/lib/docker . ./docker-images/OracleDatabase/SingleInstance/dockerfiles/19.3.0/buildDockerImage.sh -v 19 .3.0 -e Confirm that the image was created docker images REPOSITORY TAG IMAGE ID CREATED SIZE oracle/database 19 .3.0-ee d8be8934332d 53 minutes ago 6 .54GB Run the database docker. docker run --name myOracle1930 \\ -p 127 .0.0.1:1521:1521 \\ -p 127 .0.0.1:5500:5500 \\ -e ORACLE_SID = ORCLCDB \\ -e ORACLE_PDB = ORCLPDB1 \\ -e ORACLE_PWD = root \\ -e INIT_SGA_SIZE = 1024 \\ -e INIT_PGA_SIZE = 1024 \\ -e ORACLE_CHARACTERSET = AL32UTF8 \\ oracle/database:19.3.0-ee", "title": "Oracle Database"}, {"location": "oracle_database/#install", "text": "Download or clone the files of their docker repository . Create an account in their page to be able to download the required binary files . Fake person generator might come handy for this step. Download the files . After downloading the file we need to copy it to the folder referring to the oracle version in the cloned folder. In this case, 19.3.0: mv ~/Download/LINUX.X64_193000_db_home.zip ./docker-images/OracleDatabase/SingleInstance/dockerfiles/19.3.0/ The next step is to build the image. You need at least 20G free in /var/lib/docker . ./docker-images/OracleDatabase/SingleInstance/dockerfiles/19.3.0/buildDockerImage.sh -v 19 .3.0 -e Confirm that the image was created docker images REPOSITORY TAG IMAGE ID CREATED SIZE oracle/database 19 .3.0-ee d8be8934332d 53 minutes ago 6 .54GB Run the database docker. docker run --name myOracle1930 \\ -p 127 .0.0.1:1521:1521 \\ -p 127 .0.0.1:5500:5500 \\ -e ORACLE_SID = ORCLCDB \\ -e ORACLE_PDB = ORCLPDB1 \\ -e ORACLE_PWD = root \\ -e INIT_SGA_SIZE = 1024 \\ -e INIT_PGA_SIZE = 1024 \\ -e ORACLE_CHARACTERSET = AL32UTF8 \\ oracle/database:19.3.0-ee", "title": "Install"}, {"location": "origami/", "text": "Origami , is the art of paper folding, it comes from ori meaning \"folding\", and kami meaning \"paper\" (kami changes to gami due to rendaku)). In modern usage, the word \"origami\" is used as an inclusive term for all folding practices, regardless of their culture of origin. The goal is to transform a flat square sheet of paper into a finished sculpture through folding and sculpting techniques. Modern origami practitioners generally discourage the use of cuts, glue, or markings on the paper. References \u2691 Mark1626 Digital garden origami section , for example the Clover and Hydrangea Tesslation .", "title": "Origami"}, {"location": "origami/#references", "text": "Mark1626 Digital garden origami section , for example the Clover and Hydrangea Tesslation .", "title": "References"}, {"location": "osmand/", "text": "OsmAnd is a mobile application for global map viewing and navigating based on OpenStreetMaps . Perfect if you're looking for a privacy focused, community maintained open source alternative to google maps. Issues \u2691 Live map update not working : test that it works. Add amenity to favourites : Once it's done, remove the \"Restaurant\", or any other amenity words from the name of the favourite. Search within favourites description : Nothing to do. Favourites inherit the POI data : Nothing to do. References \u2691 Home Git Reddit", "title": "OsmAnd"}, {"location": "osmand/#issues", "text": "Live map update not working : test that it works. Add amenity to favourites : Once it's done, remove the \"Restaurant\", or any other amenity words from the name of the favourite. Search within favourites description : Nothing to do. Favourites inherit the POI data : Nothing to do.", "title": "Issues"}, {"location": "osmand/#references", "text": "Home Git Reddit", "title": "References"}, {"location": "outrun/", "text": "Outrun lets you execute a local command using the processing power of another Linux machine. Installation \u2691 pip install outrun References \u2691 Git", "title": "Outrun"}, {"location": "outrun/#installation", "text": "pip install outrun", "title": "Installation"}, {"location": "outrun/#references", "text": "Git", "title": "References"}, {"location": "park_programming/", "text": "Park programming is as you may guess, programming in parks, something I'm trying these days, let's see how it goes. How to park program \u2691 To get the best experience out of it I've seen that before you get out it's good to: Store music in a directory so you don't have to stream it online. Find the best spots . Find the best times . Get out of your dark themed comfort zone and configure a light theme for the windows you're going to use. Put on sun screen! Bring enough water. Bring a small cushion for the ass, and maybe another for the back. Pee before going out. Go with someone. I haven't yet tried how comfortable is park pair programming though. While you're at it: Teether the internet from the mobile. Use headphones, be respectful (\u0482\u2323\u0300_\u2323\u0301) . Sit well, with your back straight. Find the best spots \u2691 The best spots are the ones that meet the next criteria: Quiet places with few human transit. Green surrounding environment. Pleasant heat, sunlight and humidity. Correct seat height. Has laptop support. Seat with back support. Far from bugs. So far the best spots I've found have been: Benches in parks. Chess desks. Look out for different spots to fulfill the different scenarios in terms of amount of heat and sunlight. Find the best times \u2691 The best times are the ones that maximize the best spots criteria so: Weekdays over weekend Early mornings and late afternoons on summer, middle day on spring and autumn. Benefits \u2691 By doing park programming you: Move your ass out of your lair. Get a sun bath Breath fresh open air. Get to smile at passing by dogs or enjoy seeing them play. See trees and nature. Get higher chances to interact with other humans. Inconveniences \u2691 You'll probably don't be as comfortable as in your common development place.", "title": "Park programming"}, {"location": "park_programming/#how-to-park-program", "text": "To get the best experience out of it I've seen that before you get out it's good to: Store music in a directory so you don't have to stream it online. Find the best spots . Find the best times . Get out of your dark themed comfort zone and configure a light theme for the windows you're going to use. Put on sun screen! Bring enough water. Bring a small cushion for the ass, and maybe another for the back. Pee before going out. Go with someone. I haven't yet tried how comfortable is park pair programming though. While you're at it: Teether the internet from the mobile. Use headphones, be respectful (\u0482\u2323\u0300_\u2323\u0301) . Sit well, with your back straight.", "title": "How to park program"}, {"location": "park_programming/#find-the-best-spots", "text": "The best spots are the ones that meet the next criteria: Quiet places with few human transit. Green surrounding environment. Pleasant heat, sunlight and humidity. Correct seat height. Has laptop support. Seat with back support. Far from bugs. So far the best spots I've found have been: Benches in parks. Chess desks. Look out for different spots to fulfill the different scenarios in terms of amount of heat and sunlight.", "title": "Find the best spots"}, {"location": "park_programming/#find-the-best-times", "text": "The best times are the ones that maximize the best spots criteria so: Weekdays over weekend Early mornings and late afternoons on summer, middle day on spring and autumn.", "title": "Find the best times"}, {"location": "park_programming/#benefits", "text": "By doing park programming you: Move your ass out of your lair. Get a sun bath Breath fresh open air. Get to smile at passing by dogs or enjoy seeing them play. See trees and nature. Get higher chances to interact with other humans.", "title": "Benefits"}, {"location": "park_programming/#inconveniences", "text": "You'll probably don't be as comfortable as in your common development place.", "title": "Inconveniences"}, {"location": "pdm/", "text": "PDM is a modern Python package manager with PEP 582 support. It installs and manages packages in a similar way to npm that doesn't need to create a virtualenv at all! Features \u2691 PEP 582 local package installer and runner, no virtualenv involved at all. Simple and relatively fast dependency resolver, mainly for large binary distributions. A PEP 517 build backend. PEP 621 project metadata. Installation \u2691 Recommended installation method \u2691 curl -sSL https://raw.githubusercontent.com/pdm-project/pdm/main/install-pdm.py | python3 - For security reasons, you should verify the checksum. The sha256 checksum is: 70ac95c53830ff41d700051c9caebd83b2b85b5d6066e8f853006f9f07293ff0 , if it doesn't match check if there is a newer version . Other methods \u2691 pip install --user pdm Enable PEP 582 globally \u2691 To make the Python interpreters aware of PEP 582 packages, one need to add the pdm/pep582/sitecustomize.py to the Python library search path. pdm --pep582 zsh >> ~/.zshrc Use it with the IDE \u2691 Now there are not built-in support or plugins for PEP 582 in most IDEs, you have to configure your tools manually. They say how to configure Pycharm and VSCode, but there's still no instructions for vim . PDM will write and store project-wide configurations in .pdm.toml and you are recommended to add following lines in the .gitignore : .pdm.toml __pypackages__/ Usage \u2691 PDM provides a bunch of handful commands to help manage your project and dependencies. Initialize a project \u2691 pdm init Answer several questions asked by PDM and a pyproject.toml will be created for you in the project root: [project] name = \"pdm-test\" version = \"0.0.0\" description = \"\" requires-python = \">=3.7\" dependencies = [] [[project.authors]] name = \"Frost Ming\" email = \"mianghong@gmail.com\" [project.license] text = \"MIT\" If pyproject.toml is already present, it will be updated with the metadata following the PEP 621 specification. Import project metadata from existing project files \u2691 If you are already other package manager tools like Pipenv or Poetry , it is easy to migrate to PDM. PDM provides import command so that you don't have to initialize the project manually, it now supports: Pipenv's Pipfile Poetry's section in pyproject.toml Flit's section in pyproject.toml requirements.txt format used by Pip Also, when you are executing pdm init or pdm install , PDM can auto-detect possible files to import if your PDM project has not been initialized yet. Adding dependencies \u2691 pdm add can be followed by one or several dependencies, and the dependency specification is described in PEP 508 , you have a summary of the possibilities here . pdm add requests PDM also allows extra dependency groups by providing -G/--group <name> option, and those dependencies will go to [project.optional-dependencies.<name>] table in the project file, respectively. After that, dependencies and sub-dependencies will be resolved properly and installed for you, you can view pdm.lock to see the resolved result of all dependencies. Add local dependencies \u2691 Local packages can be added with their paths: pdm add ./sub-package Local packages can be installed in editable mode (just like pip install -e <local project path> would) using pdm add -e/--editable <local project path> . Add development only dependencies \u2691 PDM also supports defining groups of dependencies that are useful for development, e.g. some for testing and others for linting. We usually don't want these dependencies appear in the distribution's metadata so using optional-dependencies is probably not a good idea. We can define them as development dependencies: pdm add -d pytest This will result in a pyproject.toml as following: [tool.pdm.dev-dependencies] test = [ \"pytest\" ,] Save version specifiers \u2691 If the package is given without a version specifier like pdm add requests . PDM provides three different behaviors of what version specifier is saved for the dependency, which is given by --save-<strategy> (Assume 2.21.0 is the latest version that can be found for the dependency): minimum : Save the minimum version specifier: >=2.21.0 (default). compatible : Save the compatible version specifier: >=2.21.0,<3.0.0 (default). exact : Save the exact version specifier: ==2.21.0 . wildcard : Don't constrain version and leave the specifier to be wildcard: * . Supporting pre-releases \u2691 To help package maintainers, you can allow pre-releases to be validate candidates, that way you'll get the issues sooner. It will mean more time to maintain the broken CIs if you update your packages daily (as you should!), but it's the least you can do to help your downstream library maintainers By default, pdm 's dependency resolver will ignore prereleases unless there are no stable versions for the given version range of a dependency. This behavior can be changed by setting allow_prereleases to true in [tool.pdm] table: [tool.pdm] allow_prereleases = true Update existing dependencies \u2691 To update all dependencies in the lock file use: pdm update To update the specified package(s): pdm update requests To update multiple groups of dependencies: pdm update -G security -G http To update a given package in the specified group: pdm update -G security cryptography If the group is not given, PDM will search for the requirement in the default dependencies set and raises an error if none is found. To update packages in development dependencies: # Update all default + dev-dependencies pdm update -d # Update a package in the specified group of dev-dependencies pdm update -dG test pytest Keep in mind that pdm update doesn't touch the constrains in pyproject.toml , if you want to update them you'd need to use the --unconstrained flag which will ignore all the constrains of downstream packages and update them to the latest version setting the pin accordingly to your update strategy . Updating the pyproject.toml constrains to match the pdm.lock as close as possible makes sense to avoid unexpected errors when users use other version of the libraries, as the tests are run only against the versions specified in pdm.lock . About update strategy \u2691 Similarly, PDM also provides 2 different behaviors of updating dependencies and sub-dependencies\uff0c which is given by --update-<strategy> option: reuse : Keep all locked dependencies except for those given in the command line (default). eager : Try to lock a newer version of the packages in command line and their recursive sub-dependencies and keep other dependencies as they are. Update packages to the versions that break the version specifiers \u2691 One can give -u/--unconstrained to tell PDM to ignore the version specifiers in the pyproject.toml . This works similarly to the yarn upgrade -L/--latest command. Besides, pdm update also supports the --pre/--prerelease option. Remove existing dependencies \u2691 To remove existing dependencies from project file and the library directory: # Remove requests from the default dependencies pdm remove requests # Remove h11 from the 'web' group of optional-dependencies pdm remove -G web h11 # Remove pytest-cov from the ` test ` group of dev-dependencies pdm remove -d pytest-cov Install the packages pinned in lock file \u2691 There are two similar commands to do this job with a slight difference: pdm install will check the lock file and relock if it mismatches with project file, then install. pdm sync installs dependencies in the lock file and will error out if it doesn't exist. Besides, pdm sync can also remove unneeded packages if --clean option is given. All development dependencies are included as long as --prod is not passed and -G doesn't specify any dev groups. Besides, if you don't want the root project to be installed, add --no-self option, and --no-editable can be used when you want all packages to be installed in non-editable versions. With --no-editable turn on, you can safely archive the whole __pypackages__ and copy it to the target environment for deployment. Show what packages are installed \u2691 Similar to pip list , you can list all packages installed in the packages directory: pdm list Or show a dependency graph by: $ pdm list --graph tempenv 0.0.0 \u2514\u2500\u2500 click 7.0 [ required: <7.0.0,>=6.7 ] black 19.10b0 \u251c\u2500\u2500 appdirs 1.4.3 [ required: Any ] \u251c\u2500\u2500 attrs 19.3.0 [ required: >=18.1.0 ] \u251c\u2500\u2500 click 7.0 [ required: >=6.5 ] \u251c\u2500\u2500 pathspec 0.7.0 [ required: <1,>=0.6 ] \u251c\u2500\u2500 regex 2020.2.20 [ required: Any ] \u251c\u2500\u2500 toml 0.10.0 [ required: >=0.9.4 ] \u2514\u2500\u2500 typed-ast 1.4.1 [ required: >=1.4.0 ] bump2version 1.0.0 Solve the locking failure \u2691 If PDM is not able to find a resolution to satisfy the requirements, it will raise an error. For example, pdm django == 3 .1.4 \"asgiref<3\" ... \ud83d\udd12 Lock failed Unable to find a resolution for asgiref because of the following conflicts: asgiref< 3 ( from project ) asgiref< 4 ,> = 3 .2.10 ( from <Candidate django 3 .1.4 from https://pypi.org/simple/django/> ) To fix this, you could loosen the dependency version constraints in pyproject.toml. If that is not possible, you could also override the resolved version in [ tool.pdm.overrides ] table. You can either change to a lower version of django or remove the upper bound of asgiref . But if it is not eligible for your project, you can tell PDM to forcely resolve asgiref to a specific version by adding the following lines to pyproject.toml : [tool.pdm.overrides] asgiref = \">=3.2.10\" Each entry of that table is a package name with the wanted version. The value can also be a URL to a file or a VCS repository like git+https://... . On reading this, PDM will pin asgiref@3.2.10 or the greater version in the lock file no matter whether there is any other resolution available. Note: By using [tool.pdm.overrides] setting, you are at your own risk of any incompatibilities from that resolution. It can only be used if there is no valid resolution for your requirements and you know the specific version works. Most of the time, you can just add any transient constraints to the dependencies array. Solve circular dependencies \u2691 Sometimes pdm is not able to locate the best package combination , or it does too many loops, so to help it you can update your version constrains so that it has the minimum number of candidates. To solve circular dependencies we first need to locate what are the conflicting packages, pdm doesn't make it easy to detect them . To do that first try to update each of your groups independently with pdm update -G group_name . If that doesn't work remove from your pyproject.toml groups of dependencies until the command works and add back one group by group until you detect the ones that fail. Also it's useful to reduce the number of possibilities of versions of each dependency to make things easier to pdm . Locate all the outdated packages by doing pdm show on each package until this issue is solved and run pdm update {package} --unconstrained for each of them. If you're already on the latest version, update your pyproject.toml to match the latest state. Once you have everything to the latest compatible version, you can try to upgrade the rest of the packages one by one to the latest with --unconstrained . In the process of doing these steps you'll see some conflicts in the dependencies that can be manually solved by preventing those versions to be installed or maybe changing the python-requires , although this should be done as the last resource. It also helps to run pdm update with the -v flag, that way you see which are the candidates that are rejected, and you can put the constrain you want. For example, I was seeing the next traceback: pdm.termui: Conflicts detected: pyflakes>=3.0.0 (from <Candidate autoflake 2.0.0 from https://pypi.org/simple/autoflake/>) pyflakes<2.5.0,>=2.4.0 (from <Candidate flake8 4.0.1 from unknown>) So I added a new dependency to pin it: [tool.pdm.dev-dependencies] # The next ones are required to manually solve the dependencies issues dependencies = [ # Until flakeheaven supports flake8 5.x # https://github.com/flakeheaven/flakeheaven/issues/132 \"flake8>=4.0.1,<5.0.0\", \"pyflakes<2.5.0\", ] If none of the above works, you can override them: [tool.pdm.overrides] # To be removed once https://github.com/flakeheaven/flakeheaven/issues/132 is solved \"importlib-metadata\" = \">=3.10\" If you get lost in understanding your dependencies, you can try using pydeps to get your head around it. Building packages \u2691 PDM can act as a PEP 517 build backend, to enable that, write the following lines in your pyproject.toml . [build-system] requires = [ \"pdm-pep517\" ,] build-backend = \"pdm.pep517.api\" pip will read the backend settings to install or build a package. Choose a Python interpreter \u2691 If you have used pdm init , you must have already seen how PDM detects and selects the Python interpreter. After initialized, you can also change the settings by pdm use <python_version_or_path> . The argument can be either a version specifier of any length, or a relative or absolute path to the python interpreter, but remember the Python interpreter must conform with the python_requires constraint in the project file. How requires-python controls the project \u2691 PDM respects the value of requires-python in the way that it tries to pick package candidates that can work on all python versions that requires-python contains. For example, if requires-python is >=2.7 , PDM will try to find the latest version of foo , whose requires-python version range is a superset of >=2.7 . So, make sure you write requires-python properly if you don't want any outdated packages to be locked. Build distribution artifacts \u2691 $ pdm build - Building sdist... - Built pdm-test-0.0.0.tar.gz - Building wheel... - Built pdm_test-0.0.0-py3-none-any.whl Publishing artifacts \u2691 The artifacts can then be uploaded to PyPI by twine or through the pdm-publish plugin. The main developer didn't thought it was worth it, so branchvincent made the plugin (I love this possibility). Install it with pdm plugin add pdm-publish . Then you can upload them with; # Using token auth pdm publish --password token # To test PyPI using basic auth pdm publish -r testpypi -u username -P password # To custom index pdm publish -r https://custom.index.com/ If you don't want to use your credentials in plaintext on the command, you can use the environmental variables PDM_PUBLISH_PASSWORD and PDM_PUBLISH_USER . Show the current Python environment \u2691 $ pdm info PDM version: 1.11.3 Python Interpreter: /usr/local/bin/python3.9 (3.9) Project Root: /tmp/tmp.dBlK2rAn2x Project Packages: /tmp/tmp.dBlK2rAn2x/__pypackages__/3.9 $ pdm info --env { \"implementation_name\": \"cpython\", \"implementation_version\": \"3.9.8\", \"os_name\": \"posix\", \"platform_machine\": \"x86_64\", \"platform_release\": \"4.19.0-5-amd64\", \"platform_system\": \"Linux\", \"platform_version\": \"#1 SMP Debian 4.19.37-5+deb10u1 (2019-07-19)\", \"python_full_version\": \"3.9.8\", \"platform_python_implementation\": \"CPython\", \"python_version\": \"3.9\", \"sys_platform\": \"linux\" } Manage project configuration \u2691 Show the current configurations: pdm config Get one single configuration: pdm config pypi.url Change a configuration value and store in home configuration: pdm config pypi.url \"https://test.pypi.org/simple\" By default, the configuration are changed globally, if you want to make the config seen by this project only, add a --local flag: pdm config --local pypi.url \"https://test.pypi.org/simple\" Any local configurations will be stored in .pdm.toml under the project root directory. The configuration files are searched in the following order: <PROJECT_ROOT>/.pdm.toml - The project configuration. ~/.pdm/config.toml - The home configuration. If -g/--global option is used, the first item will be replaced by ~/.pdm/global-project/.pdm.toml . You can find all available configuration items in Configuration Page . Run Scripts in Isolated Environment \u2691 With PDM, you can run arbitrary scripts or commands with local packages loaded: pdm run flask run -p 54321 PDM also supports custom script shortcuts in the optional [tool.pdm.scripts] section of pyproject.toml . You can then run pdm run <shortcut_name> to invoke the script in the context of your PDM project. For example: [tool.pdm.scripts] start_server = \"flask run -p 54321\" And then in your terminal: $ pdm run start_server Flask server started at http://127.0.0.1:54321 Any extra arguments will be appended to the command: $ pdm run start_server -h 0 .0.0.0 Flask server started at http://0.0.0.0:54321 PDM supports 3 types of scripts: Normal command \u2691 Plain text scripts are regarded as normal command, or you can explicitly specify it: [tool.pdm.scripts.start_server] cmd = \"flask run -p 54321\" In some cases, such as when wanting to add comments between parameters, it might be more convenient to specify the command as an array instead of a string: [tool.pdm.scripts.start_server] cmd = [ \"flask\" , \"run\" , \"-p\" , \"54321\" ,] Shell script \u2691 Shell scripts can be used to run more shell-specific tasks, such as pipeline and output redirecting. This is basically run via subprocess.Popen() with shell=True : [tool.pdm.scripts.filter_error] shell = \"cat error.log|grep CRITICAL > critical.log\" Call a Python function \u2691 The script can be also defined as calling a python function in the form <module_name>:<func_name> : [tool.pdm.scripts.foobar] call = \"foo_package.bar_module:main\" The function can be supplied with literal arguments: [tool.pdm.scripts.foobar] call = \"foo_package.bar_module:main('dev')\" Environment variables support \u2691 All environment variables set in the current shell can be seen by pdm run and will be expanded when executed. Besides, you can also define some fixed environment variables in your pyproject.toml : [tool.pdm.scripts.start_server] cmd = \"flask run -p 54321\" [tool.pdm.scripts.start_server.env] FOO = \"bar\" FLASK_ENV = \"development\" Note how we use TOML's syntax to define a compound dictionary. A dotenv file is also supported via env_file = \"<file_path>\" setting. For environment variables and/or dotenv file shared by all scripts, you can define env and env_file settings under a special key named _ of tool.pdm.scripts table: [tool.pdm.scripts] start_server = \"flask run -p 54321\" migrate_db = \"flask db upgrade\" [tool.pdm.scripts._] env_file = \".env\" Besides, PDM also injects the root path of the project via PDM_PROJECT_ROOT environment variable. Load site-packages in the running environment \u2691 To make sure the running environment is properly isolated from the outer Python interpreter, site-packages from the selected interpreter WON'T be loaded into sys.path , unless any of the following conditions holds: The executable is from PATH but not inside the __pypackages__ folder. -s/--site-packages flag is following pdm run . site_packages = true is in either the script table or the global setting key _ . Note that site-packages will always be loaded if running with PEP 582 enabled(without the pdm run prefix). Show the list of scripts shortcuts \u2691 Use pdm run --list/-l to show the list of available script shortcuts: $ pdm run --list Name Type Script Description ----------- ----- ---------------- ---------------------- test_cmd cmd flask db upgrade test_script call test_script:main call a python function test_shell shell echo $FOO shell command You can add an help option with the description of the script, and it will be displayed in the Description column in the above output. Manage caches \u2691 PDM provides a convenient command group to manage the cache, there are four kinds of caches: wheels/ stores the built results of non-wheel distributions and files. http/ stores the HTTP response content. metadata/ stores package metadata retrieved by the resolver. hashes/ stores the file hashes fetched from the package index or calculated locally. packages/ The centrialized repository for installed wheels. See the current cache usage by typing pdm cache info . Besides, you can use add , remove and list subcommands to manage the cache content. Manage global dependencies \u2691 Sometimes users may want to keep track of the dependencies of global Python interpreter as well. It is easy to do so with PDM, via -g/--global option which is supported by most subcommands. If the option is passed, ~/.pdm/global-project will be used as the project directory, which is almost the same as normal project except that pyproject.toml will be created automatically for you and it doesn't support build features. The idea is taken from Haskell's stack . However, unlike stack , by default, PDM won't use global project automatically if a local project is not found. Users should pass -g/--global explicitly to activate it, since it is not very pleasing if packages go to a wrong place. But PDM also leave the decision to users, just set the config auto_global to true . If you want global project to track another project file other than ~/.pdm/global-project , you can provide the project path via -p/--project <path> option. Warning: Be careful with remove and sync --clean commands when global project is used, because it may remove packages installed in your system Python. Configuration \u2691 All available configurations can be seen here . Dependency specification \u2691 The project.dependencies is an array of dependency specification strings following the PEP 440 and PEP 508 . Examples: dependencies = [ \"requests\" , \"flask >= 1.1.0\" , \"pywin32; sys_platform == 'win32'\" , \"pip @ https://github.com/pypa/pip.git@20.3.1\" ,] Editable requirement \u2691 Beside of the normal dependency specifications, one can also have some packages installed in editable mode. The editable specification string format is the same as Pip's editable install mode . Examples: dependencies = [ ..., # Local dependency \"-e path/to/SomeProject\", # Dependency cloned \"-e git+http://repo/my_project.git#egg=SomeProject\" ] Note: About editable installation. One can have editable installation and normal installation for the same package. The one that comes at last wins. However, editable dependencies WON'T be included in the metadata of the built artifacts since they are not valid PEP 508 strings. They only exist for development purpose. Optional dependencies \u2691 You can have some requirements optional, which is similar to setuptools ' extras_require parameter. [project.optional-dependencies] socks = [ \"PySocks >= 1.5.6, != 1.5.7, < 2\" ,] tests = [ \"ddt >= 1.2.2, < 2\" , \"pytest < 6\" , \"mock >= 1.0.1, < 4; python_version < \\\"3.4\\\"\" ,] To install a group of optional dependencies: pdm install -G socks -G option can be given multiple times to include more than one group. Development dependencies groups \u2691 You can have several groups of development only dependencies. Unlike optional-dependencies , they won't appear in the package distribution metadata such as PKG-INFO or METADATA . And the package index won't be aware of these dependencies. The schema is similar to that of optional-dependencies , except that it is in tool.pdm table. [tool.pdm.dev-dependencies] lint = [ \"flake8\" , \"black\" ,] test = [ \"pytest\" , \"pytest-cov\" ,] doc = [ \"mkdocs\" ,] To install all of them: pdm install For more CLI usage, please refer to Manage Dependencies Show outdated packages \u2691 pdm update --dry-run --unconstrained Console scripts \u2691 The following content: [project.scripts] mycli = \"mycli.__main__:main\" will be translated to setuptools style: entry_points = { \"console_scripts\" : [ \"mycli=mycli.__main__:main\" ]} Also, [project.gui-scripts] will be translated to gui_scripts entry points group in setuptools style. Entry points \u2691 Other types of entry points are given by [project.entry-points.<type>] section, with the same format of [project.scripts] : [project.entry-points.pytest11] myplugin = \"mypackage.plugin:pytest_plugin\" Include and exclude package files \u2691 The way of specifying include and exclude files are simple, they are given as a list of glob patterns: includes = [ \"**/*.json\" , \"mypackage/\" ,] excludes = [ \"mypackage/_temp/*\" ,] In case you want some files to be included in sdist only, you use the source-includes field: includes = [...] excludes = [...] source-includes = [ \"tests/\" ] Note that the files defined in source-includes will be excluded automatically from non-sdist builds. Default values for includes and excludes \u2691 If you don't specify any of these fields, PDM also provides smart default values to fit the most common workflows. Top-level packages will be included. tests package will be excluded from non-sdist builds. src directory will be detected as the package-dir if it exists. If your project follows the above conventions you don't need to config any of these fields and it just works. Be aware PDM won't add PEP 420 implicit namespace packages automatically and they should always be specified in includes explicitly. Determine the package version dynamically \u2691 The package version can be retrieved from the __version__ variable of a given file. To do this, put the following under the [tool.pdm] table: [tool.pdm.version] from = \"mypackage/__init__.py\" Remember set dynamic = [\"version\"] in [project] metadata. PDM can also read version from SCM tags. If you are using git or hg as the version control system, define the version as follows: [tool.pdm.version] use_scm = true In either case, you MUST delete the version field from the [project] table, and include version in the dynamic field, or the backend will raise an error: dynamic = [ \"version\" ,] Cache the installation of wheels \u2691 If a package is required by many projects on the system, each project has to keep its own copy. This may become a waste of disk space especially for data science and machine learning libraries. PDM supports caching the installations of the same wheel by installing it into a centralized package repository and linking to that installation in different projects. To enabled it, run: pdm config feature.install_cache on It can be enabled on a project basis, by adding --local option to the command. The caches are located under $(pdm config cache_dir)/packages . One can view the cache usage by pdm cache info . But be noted the cached installations are managed automatically. They get deleted when not linked from any projects. Manually deleting the caches from the disk may break some projects on the system. Note: Only the installation of named requirements resolved from PyPI can be cached. Working with a virtualenv \u2691 Although PDM enforces PEP 582 by default, it also allows users to install packages into the virtualenv. It is controlled by the configuration item use_venv . When it is set to True (default), PDM will use the virtualenv if: A virtualenv is already activated. Any of venv , .venv , env is a valid virtualenv folder. Besides, when use-venv is on and the interpreter path given is a venv-like path, PDM will reuse that venv directory as well. For enhanced virtualenv support such as virtualenv management and auto-creation, please go for pdm-venv , which can be installed as a plugin. Use PDM in Continuous Integration \u2691 Fortunately, if you are using GitHub Action, there is pdm-project/setup-pdm to make this process easier. Here is an example workflow of GitHub Actions, while you can adapt it for other CI platforms. Testing : runs-on : ${{ matrix.os }} strategy : matrix : python-version : [ 3.7 , 3.8 , 3.9 , 3.10 ] os : [ ubuntu-latest , macOS-latest , windows-latest ] steps : - uses : actions/checkout@v1 - name : Set up PDM uses : pdm-project/setup-pdm@main with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | pdm sync -d -G testing - name : Run Tests run : | pdm run -v pytest tests Note: Tips for GitHub Action users, there is a known compatibility issue on Ubuntu virtual environment. If PDM parallel install is failed on that machine you should either set parallel_install to false or set env LD_PRELOAD=/lib/x86_64-linux-gnu/libgcc_s.so.1 . It is already handled by the pdm-project/setup-pdm action. Note: If your CI scripts run without a proper user set, you might get permission errors when PDM tries to create its cache directory. To work around this, you can set the HOME environment variable yourself, to a writable directory, for example: ```bash export HOME=/tmp/home ``` How does it work \u2691 Why you don't need to use virtualenvs \u2691 When you develop a Python project, you need to install the project's dependencies. For a long time, tutorials and articles have told you to use a virtual environment to isolate the project's dependencies. This way you don't contaminate the working set of other projects, or the global interpreter, to avoid possible version conflicts. Problems of the virtualenvs \u2691 Virtualenvs are confusing for people that are starting with python. They also use a lot of space, as many virtualenvs have their own copy of the same libraries. They help us isolate project dependencies though, but things get tricky when it comes to nested venvs. One installs the virtualenv manager(like Pipenv or Poetry) using a venv encapsulated Python, and creates more venvs using the tool which is based on an encapsulated Python. One day a minor release of Python is out and one has to check all those venvs and upgrade them if required before they can safely delete the out-dated Python version. Another scenario is global tools. There are many tools that are not tied to any specific virtualenv and are supposed to work with each of them. Examples are profiling tools and third-party REPLs. We also wish them to be installed in their own isolated environments. It's impossible to make them work with virtualenv, even if you have activated the virtualenv of the target project you want to work on because the tool is lying in its own virtualenv and it can only see the libraries installed in it. So we have to install the tool for each project. The solution has been existing for a long time. PEP 582 was originated in 2018 and is still a draft proposal till the time I copied this article. Say you have a project with the following structure: . \u251c\u2500\u2500 __pypackages__ \u2502 \u2514\u2500\u2500 3.8 \u2502 \u2514\u2500\u2500 lib \u2514\u2500\u2500 my_script.py As specified in the PEP 582, if you run python3.8 /path/to/my_script.py , __pypackages__/3.8/lib will be added to sys.path , and the libraries inside will become import-able in my_script.py . Now let's review the two problems mentioned above under PEP 582. For the first problem, the main cause is that the virtual environment is bound to a cloned Python interpreter on which the subsequent library searching based. It takes advantage of Python's existing mechanisms without any other complex changes but makes the entire virtual environment to become unavailable when the Python interpreter is stale. With the local packages directory, you don't have a Python interpreter any more, the library path is directly appended to sys.path , so you can freely move and copy it. For the second, once again, you just call the tool against the project you want to analyze, and the __pypackages__ sitting inside the project will be loaded automatically. This way you only need to keep one copy of the global tool and make it work with multiple projects. pdm installs dependencies into the local package directory __package__ and makes Python interpreters aware of it with a very simple setup . How we make PEP 582 packages available to the Python interpreter \u2691 Thanks to the site packages loading on Python startup. It is possible to patch the sys.path by executing the sitecustomize.py shipped with PDM. The interpreter can search the directories for the nearest __pypackage__ folder and append it to the sys.path variable. Plugins \u2691 PDM is aiming at being a community driven package manager. It is shipped with a full-featured plug-in system, with which you can: Develop a new command for PDM. Add additional options to existing PDM commands. Change PDM's behavior by reading dditional config items. Control the process of dependency resolution or installation. If you want to write a plugin, start here . Issues \u2691 You can't still run mypy with pdm without virtualenvs. pawamoy created a patch that is supposed to work, but I'd rather use virtualenvs until it's supported. Once it's supported check the vim-test issue to see how to integrate it. It's not yet supported by dependabot . Once supported add it back to the cookiecutter template and spread it. References \u2691 Git Docs", "title": "PDM"}, {"location": "pdm/#features", "text": "PEP 582 local package installer and runner, no virtualenv involved at all. Simple and relatively fast dependency resolver, mainly for large binary distributions. A PEP 517 build backend. PEP 621 project metadata.", "title": "Features"}, {"location": "pdm/#installation", "text": "", "title": "Installation"}, {"location": "pdm/#recommended-installation-method", "text": "curl -sSL https://raw.githubusercontent.com/pdm-project/pdm/main/install-pdm.py | python3 - For security reasons, you should verify the checksum. The sha256 checksum is: 70ac95c53830ff41d700051c9caebd83b2b85b5d6066e8f853006f9f07293ff0 , if it doesn't match check if there is a newer version .", "title": "Recommended installation method"}, {"location": "pdm/#other-methods", "text": "pip install --user pdm", "title": "Other methods"}, {"location": "pdm/#enable-pep-582-globally", "text": "To make the Python interpreters aware of PEP 582 packages, one need to add the pdm/pep582/sitecustomize.py to the Python library search path. pdm --pep582 zsh >> ~/.zshrc", "title": "Enable PEP 582 globally"}, {"location": "pdm/#use-it-with-the-ide", "text": "Now there are not built-in support or plugins for PEP 582 in most IDEs, you have to configure your tools manually. They say how to configure Pycharm and VSCode, but there's still no instructions for vim . PDM will write and store project-wide configurations in .pdm.toml and you are recommended to add following lines in the .gitignore : .pdm.toml __pypackages__/", "title": "Use it with the IDE"}, {"location": "pdm/#usage", "text": "PDM provides a bunch of handful commands to help manage your project and dependencies.", "title": "Usage"}, {"location": "pdm/#initialize-a-project", "text": "pdm init Answer several questions asked by PDM and a pyproject.toml will be created for you in the project root: [project] name = \"pdm-test\" version = \"0.0.0\" description = \"\" requires-python = \">=3.7\" dependencies = [] [[project.authors]] name = \"Frost Ming\" email = \"mianghong@gmail.com\" [project.license] text = \"MIT\" If pyproject.toml is already present, it will be updated with the metadata following the PEP 621 specification.", "title": "Initialize a project"}, {"location": "pdm/#import-project-metadata-from-existing-project-files", "text": "If you are already other package manager tools like Pipenv or Poetry , it is easy to migrate to PDM. PDM provides import command so that you don't have to initialize the project manually, it now supports: Pipenv's Pipfile Poetry's section in pyproject.toml Flit's section in pyproject.toml requirements.txt format used by Pip Also, when you are executing pdm init or pdm install , PDM can auto-detect possible files to import if your PDM project has not been initialized yet.", "title": "Import project metadata from existing project files"}, {"location": "pdm/#adding-dependencies", "text": "pdm add can be followed by one or several dependencies, and the dependency specification is described in PEP 508 , you have a summary of the possibilities here . pdm add requests PDM also allows extra dependency groups by providing -G/--group <name> option, and those dependencies will go to [project.optional-dependencies.<name>] table in the project file, respectively. After that, dependencies and sub-dependencies will be resolved properly and installed for you, you can view pdm.lock to see the resolved result of all dependencies.", "title": "Adding dependencies"}, {"location": "pdm/#add-local-dependencies", "text": "Local packages can be added with their paths: pdm add ./sub-package Local packages can be installed in editable mode (just like pip install -e <local project path> would) using pdm add -e/--editable <local project path> .", "title": "Add local dependencies"}, {"location": "pdm/#add-development-only-dependencies", "text": "PDM also supports defining groups of dependencies that are useful for development, e.g. some for testing and others for linting. We usually don't want these dependencies appear in the distribution's metadata so using optional-dependencies is probably not a good idea. We can define them as development dependencies: pdm add -d pytest This will result in a pyproject.toml as following: [tool.pdm.dev-dependencies] test = [ \"pytest\" ,]", "title": "Add development only dependencies"}, {"location": "pdm/#save-version-specifiers", "text": "If the package is given without a version specifier like pdm add requests . PDM provides three different behaviors of what version specifier is saved for the dependency, which is given by --save-<strategy> (Assume 2.21.0 is the latest version that can be found for the dependency): minimum : Save the minimum version specifier: >=2.21.0 (default). compatible : Save the compatible version specifier: >=2.21.0,<3.0.0 (default). exact : Save the exact version specifier: ==2.21.0 . wildcard : Don't constrain version and leave the specifier to be wildcard: * .", "title": "Save version specifiers"}, {"location": "pdm/#supporting-pre-releases", "text": "To help package maintainers, you can allow pre-releases to be validate candidates, that way you'll get the issues sooner. It will mean more time to maintain the broken CIs if you update your packages daily (as you should!), but it's the least you can do to help your downstream library maintainers By default, pdm 's dependency resolver will ignore prereleases unless there are no stable versions for the given version range of a dependency. This behavior can be changed by setting allow_prereleases to true in [tool.pdm] table: [tool.pdm] allow_prereleases = true", "title": "Supporting pre-releases"}, {"location": "pdm/#update-existing-dependencies", "text": "To update all dependencies in the lock file use: pdm update To update the specified package(s): pdm update requests To update multiple groups of dependencies: pdm update -G security -G http To update a given package in the specified group: pdm update -G security cryptography If the group is not given, PDM will search for the requirement in the default dependencies set and raises an error if none is found. To update packages in development dependencies: # Update all default + dev-dependencies pdm update -d # Update a package in the specified group of dev-dependencies pdm update -dG test pytest Keep in mind that pdm update doesn't touch the constrains in pyproject.toml , if you want to update them you'd need to use the --unconstrained flag which will ignore all the constrains of downstream packages and update them to the latest version setting the pin accordingly to your update strategy . Updating the pyproject.toml constrains to match the pdm.lock as close as possible makes sense to avoid unexpected errors when users use other version of the libraries, as the tests are run only against the versions specified in pdm.lock .", "title": "Update existing dependencies"}, {"location": "pdm/#about-update-strategy", "text": "Similarly, PDM also provides 2 different behaviors of updating dependencies and sub-dependencies\uff0c which is given by --update-<strategy> option: reuse : Keep all locked dependencies except for those given in the command line (default). eager : Try to lock a newer version of the packages in command line and their recursive sub-dependencies and keep other dependencies as they are.", "title": "About update strategy"}, {"location": "pdm/#update-packages-to-the-versions-that-break-the-version-specifiers", "text": "One can give -u/--unconstrained to tell PDM to ignore the version specifiers in the pyproject.toml . This works similarly to the yarn upgrade -L/--latest command. Besides, pdm update also supports the --pre/--prerelease option.", "title": "Update packages to the versions that break the version specifiers"}, {"location": "pdm/#remove-existing-dependencies", "text": "To remove existing dependencies from project file and the library directory: # Remove requests from the default dependencies pdm remove requests # Remove h11 from the 'web' group of optional-dependencies pdm remove -G web h11 # Remove pytest-cov from the ` test ` group of dev-dependencies pdm remove -d pytest-cov", "title": "Remove existing dependencies"}, {"location": "pdm/#install-the-packages-pinned-in-lock-file", "text": "There are two similar commands to do this job with a slight difference: pdm install will check the lock file and relock if it mismatches with project file, then install. pdm sync installs dependencies in the lock file and will error out if it doesn't exist. Besides, pdm sync can also remove unneeded packages if --clean option is given. All development dependencies are included as long as --prod is not passed and -G doesn't specify any dev groups. Besides, if you don't want the root project to be installed, add --no-self option, and --no-editable can be used when you want all packages to be installed in non-editable versions. With --no-editable turn on, you can safely archive the whole __pypackages__ and copy it to the target environment for deployment.", "title": "Install the packages pinned in lock file"}, {"location": "pdm/#show-what-packages-are-installed", "text": "Similar to pip list , you can list all packages installed in the packages directory: pdm list Or show a dependency graph by: $ pdm list --graph tempenv 0.0.0 \u2514\u2500\u2500 click 7.0 [ required: <7.0.0,>=6.7 ] black 19.10b0 \u251c\u2500\u2500 appdirs 1.4.3 [ required: Any ] \u251c\u2500\u2500 attrs 19.3.0 [ required: >=18.1.0 ] \u251c\u2500\u2500 click 7.0 [ required: >=6.5 ] \u251c\u2500\u2500 pathspec 0.7.0 [ required: <1,>=0.6 ] \u251c\u2500\u2500 regex 2020.2.20 [ required: Any ] \u251c\u2500\u2500 toml 0.10.0 [ required: >=0.9.4 ] \u2514\u2500\u2500 typed-ast 1.4.1 [ required: >=1.4.0 ] bump2version 1.0.0", "title": "Show what packages are installed"}, {"location": "pdm/#solve-the-locking-failure", "text": "If PDM is not able to find a resolution to satisfy the requirements, it will raise an error. For example, pdm django == 3 .1.4 \"asgiref<3\" ... \ud83d\udd12 Lock failed Unable to find a resolution for asgiref because of the following conflicts: asgiref< 3 ( from project ) asgiref< 4 ,> = 3 .2.10 ( from <Candidate django 3 .1.4 from https://pypi.org/simple/django/> ) To fix this, you could loosen the dependency version constraints in pyproject.toml. If that is not possible, you could also override the resolved version in [ tool.pdm.overrides ] table. You can either change to a lower version of django or remove the upper bound of asgiref . But if it is not eligible for your project, you can tell PDM to forcely resolve asgiref to a specific version by adding the following lines to pyproject.toml : [tool.pdm.overrides] asgiref = \">=3.2.10\" Each entry of that table is a package name with the wanted version. The value can also be a URL to a file or a VCS repository like git+https://... . On reading this, PDM will pin asgiref@3.2.10 or the greater version in the lock file no matter whether there is any other resolution available. Note: By using [tool.pdm.overrides] setting, you are at your own risk of any incompatibilities from that resolution. It can only be used if there is no valid resolution for your requirements and you know the specific version works. Most of the time, you can just add any transient constraints to the dependencies array.", "title": "Solve the locking failure"}, {"location": "pdm/#solve-circular-dependencies", "text": "Sometimes pdm is not able to locate the best package combination , or it does too many loops, so to help it you can update your version constrains so that it has the minimum number of candidates. To solve circular dependencies we first need to locate what are the conflicting packages, pdm doesn't make it easy to detect them . To do that first try to update each of your groups independently with pdm update -G group_name . If that doesn't work remove from your pyproject.toml groups of dependencies until the command works and add back one group by group until you detect the ones that fail. Also it's useful to reduce the number of possibilities of versions of each dependency to make things easier to pdm . Locate all the outdated packages by doing pdm show on each package until this issue is solved and run pdm update {package} --unconstrained for each of them. If you're already on the latest version, update your pyproject.toml to match the latest state. Once you have everything to the latest compatible version, you can try to upgrade the rest of the packages one by one to the latest with --unconstrained . In the process of doing these steps you'll see some conflicts in the dependencies that can be manually solved by preventing those versions to be installed or maybe changing the python-requires , although this should be done as the last resource. It also helps to run pdm update with the -v flag, that way you see which are the candidates that are rejected, and you can put the constrain you want. For example, I was seeing the next traceback: pdm.termui: Conflicts detected: pyflakes>=3.0.0 (from <Candidate autoflake 2.0.0 from https://pypi.org/simple/autoflake/>) pyflakes<2.5.0,>=2.4.0 (from <Candidate flake8 4.0.1 from unknown>) So I added a new dependency to pin it: [tool.pdm.dev-dependencies] # The next ones are required to manually solve the dependencies issues dependencies = [ # Until flakeheaven supports flake8 5.x # https://github.com/flakeheaven/flakeheaven/issues/132 \"flake8>=4.0.1,<5.0.0\", \"pyflakes<2.5.0\", ] If none of the above works, you can override them: [tool.pdm.overrides] # To be removed once https://github.com/flakeheaven/flakeheaven/issues/132 is solved \"importlib-metadata\" = \">=3.10\" If you get lost in understanding your dependencies, you can try using pydeps to get your head around it.", "title": "Solve circular dependencies"}, {"location": "pdm/#building-packages", "text": "PDM can act as a PEP 517 build backend, to enable that, write the following lines in your pyproject.toml . [build-system] requires = [ \"pdm-pep517\" ,] build-backend = \"pdm.pep517.api\" pip will read the backend settings to install or build a package.", "title": "Building packages"}, {"location": "pdm/#choose-a-python-interpreter", "text": "If you have used pdm init , you must have already seen how PDM detects and selects the Python interpreter. After initialized, you can also change the settings by pdm use <python_version_or_path> . The argument can be either a version specifier of any length, or a relative or absolute path to the python interpreter, but remember the Python interpreter must conform with the python_requires constraint in the project file.", "title": "Choose a Python interpreter"}, {"location": "pdm/#how-requires-python-controls-the-project", "text": "PDM respects the value of requires-python in the way that it tries to pick package candidates that can work on all python versions that requires-python contains. For example, if requires-python is >=2.7 , PDM will try to find the latest version of foo , whose requires-python version range is a superset of >=2.7 . So, make sure you write requires-python properly if you don't want any outdated packages to be locked.", "title": "How requires-python controls the project"}, {"location": "pdm/#build-distribution-artifacts", "text": "$ pdm build - Building sdist... - Built pdm-test-0.0.0.tar.gz - Building wheel... - Built pdm_test-0.0.0-py3-none-any.whl", "title": "Build distribution artifacts"}, {"location": "pdm/#publishing-artifacts", "text": "The artifacts can then be uploaded to PyPI by twine or through the pdm-publish plugin. The main developer didn't thought it was worth it, so branchvincent made the plugin (I love this possibility). Install it with pdm plugin add pdm-publish . Then you can upload them with; # Using token auth pdm publish --password token # To test PyPI using basic auth pdm publish -r testpypi -u username -P password # To custom index pdm publish -r https://custom.index.com/ If you don't want to use your credentials in plaintext on the command, you can use the environmental variables PDM_PUBLISH_PASSWORD and PDM_PUBLISH_USER .", "title": "Publishing artifacts"}, {"location": "pdm/#show-the-current-python-environment", "text": "$ pdm info PDM version: 1.11.3 Python Interpreter: /usr/local/bin/python3.9 (3.9) Project Root: /tmp/tmp.dBlK2rAn2x Project Packages: /tmp/tmp.dBlK2rAn2x/__pypackages__/3.9 $ pdm info --env { \"implementation_name\": \"cpython\", \"implementation_version\": \"3.9.8\", \"os_name\": \"posix\", \"platform_machine\": \"x86_64\", \"platform_release\": \"4.19.0-5-amd64\", \"platform_system\": \"Linux\", \"platform_version\": \"#1 SMP Debian 4.19.37-5+deb10u1 (2019-07-19)\", \"python_full_version\": \"3.9.8\", \"platform_python_implementation\": \"CPython\", \"python_version\": \"3.9\", \"sys_platform\": \"linux\" }", "title": "Show the current Python environment"}, {"location": "pdm/#manage-project-configuration", "text": "Show the current configurations: pdm config Get one single configuration: pdm config pypi.url Change a configuration value and store in home configuration: pdm config pypi.url \"https://test.pypi.org/simple\" By default, the configuration are changed globally, if you want to make the config seen by this project only, add a --local flag: pdm config --local pypi.url \"https://test.pypi.org/simple\" Any local configurations will be stored in .pdm.toml under the project root directory. The configuration files are searched in the following order: <PROJECT_ROOT>/.pdm.toml - The project configuration. ~/.pdm/config.toml - The home configuration. If -g/--global option is used, the first item will be replaced by ~/.pdm/global-project/.pdm.toml . You can find all available configuration items in Configuration Page .", "title": "Manage project configuration"}, {"location": "pdm/#run-scripts-in-isolated-environment", "text": "With PDM, you can run arbitrary scripts or commands with local packages loaded: pdm run flask run -p 54321 PDM also supports custom script shortcuts in the optional [tool.pdm.scripts] section of pyproject.toml . You can then run pdm run <shortcut_name> to invoke the script in the context of your PDM project. For example: [tool.pdm.scripts] start_server = \"flask run -p 54321\" And then in your terminal: $ pdm run start_server Flask server started at http://127.0.0.1:54321 Any extra arguments will be appended to the command: $ pdm run start_server -h 0 .0.0.0 Flask server started at http://0.0.0.0:54321 PDM supports 3 types of scripts:", "title": "Run Scripts in Isolated Environment"}, {"location": "pdm/#normal-command", "text": "Plain text scripts are regarded as normal command, or you can explicitly specify it: [tool.pdm.scripts.start_server] cmd = \"flask run -p 54321\" In some cases, such as when wanting to add comments between parameters, it might be more convenient to specify the command as an array instead of a string: [tool.pdm.scripts.start_server] cmd = [ \"flask\" , \"run\" , \"-p\" , \"54321\" ,]", "title": "Normal command"}, {"location": "pdm/#shell-script", "text": "Shell scripts can be used to run more shell-specific tasks, such as pipeline and output redirecting. This is basically run via subprocess.Popen() with shell=True : [tool.pdm.scripts.filter_error] shell = \"cat error.log|grep CRITICAL > critical.log\"", "title": "Shell script"}, {"location": "pdm/#call-a-python-function", "text": "The script can be also defined as calling a python function in the form <module_name>:<func_name> : [tool.pdm.scripts.foobar] call = \"foo_package.bar_module:main\" The function can be supplied with literal arguments: [tool.pdm.scripts.foobar] call = \"foo_package.bar_module:main('dev')\"", "title": "Call a Python function"}, {"location": "pdm/#environment-variables-support", "text": "All environment variables set in the current shell can be seen by pdm run and will be expanded when executed. Besides, you can also define some fixed environment variables in your pyproject.toml : [tool.pdm.scripts.start_server] cmd = \"flask run -p 54321\" [tool.pdm.scripts.start_server.env] FOO = \"bar\" FLASK_ENV = \"development\" Note how we use TOML's syntax to define a compound dictionary. A dotenv file is also supported via env_file = \"<file_path>\" setting. For environment variables and/or dotenv file shared by all scripts, you can define env and env_file settings under a special key named _ of tool.pdm.scripts table: [tool.pdm.scripts] start_server = \"flask run -p 54321\" migrate_db = \"flask db upgrade\" [tool.pdm.scripts._] env_file = \".env\" Besides, PDM also injects the root path of the project via PDM_PROJECT_ROOT environment variable.", "title": "Environment variables support"}, {"location": "pdm/#load-site-packages-in-the-running-environment", "text": "To make sure the running environment is properly isolated from the outer Python interpreter, site-packages from the selected interpreter WON'T be loaded into sys.path , unless any of the following conditions holds: The executable is from PATH but not inside the __pypackages__ folder. -s/--site-packages flag is following pdm run . site_packages = true is in either the script table or the global setting key _ . Note that site-packages will always be loaded if running with PEP 582 enabled(without the pdm run prefix).", "title": "Load site-packages in the running environment"}, {"location": "pdm/#show-the-list-of-scripts-shortcuts", "text": "Use pdm run --list/-l to show the list of available script shortcuts: $ pdm run --list Name Type Script Description ----------- ----- ---------------- ---------------------- test_cmd cmd flask db upgrade test_script call test_script:main call a python function test_shell shell echo $FOO shell command You can add an help option with the description of the script, and it will be displayed in the Description column in the above output.", "title": "Show the list of scripts shortcuts"}, {"location": "pdm/#manage-caches", "text": "PDM provides a convenient command group to manage the cache, there are four kinds of caches: wheels/ stores the built results of non-wheel distributions and files. http/ stores the HTTP response content. metadata/ stores package metadata retrieved by the resolver. hashes/ stores the file hashes fetched from the package index or calculated locally. packages/ The centrialized repository for installed wheels. See the current cache usage by typing pdm cache info . Besides, you can use add , remove and list subcommands to manage the cache content.", "title": "Manage caches"}, {"location": "pdm/#manage-global-dependencies", "text": "Sometimes users may want to keep track of the dependencies of global Python interpreter as well. It is easy to do so with PDM, via -g/--global option which is supported by most subcommands. If the option is passed, ~/.pdm/global-project will be used as the project directory, which is almost the same as normal project except that pyproject.toml will be created automatically for you and it doesn't support build features. The idea is taken from Haskell's stack . However, unlike stack , by default, PDM won't use global project automatically if a local project is not found. Users should pass -g/--global explicitly to activate it, since it is not very pleasing if packages go to a wrong place. But PDM also leave the decision to users, just set the config auto_global to true . If you want global project to track another project file other than ~/.pdm/global-project , you can provide the project path via -p/--project <path> option. Warning: Be careful with remove and sync --clean commands when global project is used, because it may remove packages installed in your system Python.", "title": "Manage global dependencies"}, {"location": "pdm/#configuration", "text": "All available configurations can be seen here .", "title": "Configuration"}, {"location": "pdm/#dependency-specification", "text": "The project.dependencies is an array of dependency specification strings following the PEP 440 and PEP 508 . Examples: dependencies = [ \"requests\" , \"flask >= 1.1.0\" , \"pywin32; sys_platform == 'win32'\" , \"pip @ https://github.com/pypa/pip.git@20.3.1\" ,]", "title": "Dependency specification"}, {"location": "pdm/#editable-requirement", "text": "Beside of the normal dependency specifications, one can also have some packages installed in editable mode. The editable specification string format is the same as Pip's editable install mode . Examples: dependencies = [ ..., # Local dependency \"-e path/to/SomeProject\", # Dependency cloned \"-e git+http://repo/my_project.git#egg=SomeProject\" ] Note: About editable installation. One can have editable installation and normal installation for the same package. The one that comes at last wins. However, editable dependencies WON'T be included in the metadata of the built artifacts since they are not valid PEP 508 strings. They only exist for development purpose.", "title": "Editable requirement"}, {"location": "pdm/#optional-dependencies", "text": "You can have some requirements optional, which is similar to setuptools ' extras_require parameter. [project.optional-dependencies] socks = [ \"PySocks >= 1.5.6, != 1.5.7, < 2\" ,] tests = [ \"ddt >= 1.2.2, < 2\" , \"pytest < 6\" , \"mock >= 1.0.1, < 4; python_version < \\\"3.4\\\"\" ,] To install a group of optional dependencies: pdm install -G socks -G option can be given multiple times to include more than one group.", "title": "Optional dependencies"}, {"location": "pdm/#development-dependencies-groups", "text": "You can have several groups of development only dependencies. Unlike optional-dependencies , they won't appear in the package distribution metadata such as PKG-INFO or METADATA . And the package index won't be aware of these dependencies. The schema is similar to that of optional-dependencies , except that it is in tool.pdm table. [tool.pdm.dev-dependencies] lint = [ \"flake8\" , \"black\" ,] test = [ \"pytest\" , \"pytest-cov\" ,] doc = [ \"mkdocs\" ,] To install all of them: pdm install For more CLI usage, please refer to Manage Dependencies", "title": "Development dependencies groups"}, {"location": "pdm/#show-outdated-packages", "text": "pdm update --dry-run --unconstrained", "title": "Show outdated packages"}, {"location": "pdm/#console-scripts", "text": "The following content: [project.scripts] mycli = \"mycli.__main__:main\" will be translated to setuptools style: entry_points = { \"console_scripts\" : [ \"mycli=mycli.__main__:main\" ]} Also, [project.gui-scripts] will be translated to gui_scripts entry points group in setuptools style.", "title": "Console scripts"}, {"location": "pdm/#entry-points", "text": "Other types of entry points are given by [project.entry-points.<type>] section, with the same format of [project.scripts] : [project.entry-points.pytest11] myplugin = \"mypackage.plugin:pytest_plugin\"", "title": "Entry points"}, {"location": "pdm/#include-and-exclude-package-files", "text": "The way of specifying include and exclude files are simple, they are given as a list of glob patterns: includes = [ \"**/*.json\" , \"mypackage/\" ,] excludes = [ \"mypackage/_temp/*\" ,] In case you want some files to be included in sdist only, you use the source-includes field: includes = [...] excludes = [...] source-includes = [ \"tests/\" ] Note that the files defined in source-includes will be excluded automatically from non-sdist builds.", "title": "Include and exclude package files"}, {"location": "pdm/#default-values-for-includes-and-excludes", "text": "If you don't specify any of these fields, PDM also provides smart default values to fit the most common workflows. Top-level packages will be included. tests package will be excluded from non-sdist builds. src directory will be detected as the package-dir if it exists. If your project follows the above conventions you don't need to config any of these fields and it just works. Be aware PDM won't add PEP 420 implicit namespace packages automatically and they should always be specified in includes explicitly.", "title": "Default values for includes and excludes"}, {"location": "pdm/#determine-the-package-version-dynamically", "text": "The package version can be retrieved from the __version__ variable of a given file. To do this, put the following under the [tool.pdm] table: [tool.pdm.version] from = \"mypackage/__init__.py\" Remember set dynamic = [\"version\"] in [project] metadata. PDM can also read version from SCM tags. If you are using git or hg as the version control system, define the version as follows: [tool.pdm.version] use_scm = true In either case, you MUST delete the version field from the [project] table, and include version in the dynamic field, or the backend will raise an error: dynamic = [ \"version\" ,]", "title": "Determine the package version dynamically"}, {"location": "pdm/#cache-the-installation-of-wheels", "text": "If a package is required by many projects on the system, each project has to keep its own copy. This may become a waste of disk space especially for data science and machine learning libraries. PDM supports caching the installations of the same wheel by installing it into a centralized package repository and linking to that installation in different projects. To enabled it, run: pdm config feature.install_cache on It can be enabled on a project basis, by adding --local option to the command. The caches are located under $(pdm config cache_dir)/packages . One can view the cache usage by pdm cache info . But be noted the cached installations are managed automatically. They get deleted when not linked from any projects. Manually deleting the caches from the disk may break some projects on the system. Note: Only the installation of named requirements resolved from PyPI can be cached.", "title": "Cache the installation of wheels"}, {"location": "pdm/#working-with-a-virtualenv", "text": "Although PDM enforces PEP 582 by default, it also allows users to install packages into the virtualenv. It is controlled by the configuration item use_venv . When it is set to True (default), PDM will use the virtualenv if: A virtualenv is already activated. Any of venv , .venv , env is a valid virtualenv folder. Besides, when use-venv is on and the interpreter path given is a venv-like path, PDM will reuse that venv directory as well. For enhanced virtualenv support such as virtualenv management and auto-creation, please go for pdm-venv , which can be installed as a plugin.", "title": "Working with a virtualenv"}, {"location": "pdm/#use-pdm-in-continuous-integration", "text": "Fortunately, if you are using GitHub Action, there is pdm-project/setup-pdm to make this process easier. Here is an example workflow of GitHub Actions, while you can adapt it for other CI platforms. Testing : runs-on : ${{ matrix.os }} strategy : matrix : python-version : [ 3.7 , 3.8 , 3.9 , 3.10 ] os : [ ubuntu-latest , macOS-latest , windows-latest ] steps : - uses : actions/checkout@v1 - name : Set up PDM uses : pdm-project/setup-pdm@main with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | pdm sync -d -G testing - name : Run Tests run : | pdm run -v pytest tests Note: Tips for GitHub Action users, there is a known compatibility issue on Ubuntu virtual environment. If PDM parallel install is failed on that machine you should either set parallel_install to false or set env LD_PRELOAD=/lib/x86_64-linux-gnu/libgcc_s.so.1 . It is already handled by the pdm-project/setup-pdm action. Note: If your CI scripts run without a proper user set, you might get permission errors when PDM tries to create its cache directory. To work around this, you can set the HOME environment variable yourself, to a writable directory, for example: ```bash export HOME=/tmp/home ```", "title": "Use PDM in Continuous Integration"}, {"location": "pdm/#how-does-it-work", "text": "", "title": "How does it work"}, {"location": "pdm/#why-you-dont-need-to-use-virtualenvs", "text": "When you develop a Python project, you need to install the project's dependencies. For a long time, tutorials and articles have told you to use a virtual environment to isolate the project's dependencies. This way you don't contaminate the working set of other projects, or the global interpreter, to avoid possible version conflicts.", "title": "Why you don't need to use virtualenvs"}, {"location": "pdm/#problems-of-the-virtualenvs", "text": "Virtualenvs are confusing for people that are starting with python. They also use a lot of space, as many virtualenvs have their own copy of the same libraries. They help us isolate project dependencies though, but things get tricky when it comes to nested venvs. One installs the virtualenv manager(like Pipenv or Poetry) using a venv encapsulated Python, and creates more venvs using the tool which is based on an encapsulated Python. One day a minor release of Python is out and one has to check all those venvs and upgrade them if required before they can safely delete the out-dated Python version. Another scenario is global tools. There are many tools that are not tied to any specific virtualenv and are supposed to work with each of them. Examples are profiling tools and third-party REPLs. We also wish them to be installed in their own isolated environments. It's impossible to make them work with virtualenv, even if you have activated the virtualenv of the target project you want to work on because the tool is lying in its own virtualenv and it can only see the libraries installed in it. So we have to install the tool for each project. The solution has been existing for a long time. PEP 582 was originated in 2018 and is still a draft proposal till the time I copied this article. Say you have a project with the following structure: . \u251c\u2500\u2500 __pypackages__ \u2502 \u2514\u2500\u2500 3.8 \u2502 \u2514\u2500\u2500 lib \u2514\u2500\u2500 my_script.py As specified in the PEP 582, if you run python3.8 /path/to/my_script.py , __pypackages__/3.8/lib will be added to sys.path , and the libraries inside will become import-able in my_script.py . Now let's review the two problems mentioned above under PEP 582. For the first problem, the main cause is that the virtual environment is bound to a cloned Python interpreter on which the subsequent library searching based. It takes advantage of Python's existing mechanisms without any other complex changes but makes the entire virtual environment to become unavailable when the Python interpreter is stale. With the local packages directory, you don't have a Python interpreter any more, the library path is directly appended to sys.path , so you can freely move and copy it. For the second, once again, you just call the tool against the project you want to analyze, and the __pypackages__ sitting inside the project will be loaded automatically. This way you only need to keep one copy of the global tool and make it work with multiple projects. pdm installs dependencies into the local package directory __package__ and makes Python interpreters aware of it with a very simple setup .", "title": "Problems of the virtualenvs"}, {"location": "pdm/#how-we-make-pep-582-packages-available-to-the-python-interpreter", "text": "Thanks to the site packages loading on Python startup. It is possible to patch the sys.path by executing the sitecustomize.py shipped with PDM. The interpreter can search the directories for the nearest __pypackage__ folder and append it to the sys.path variable.", "title": "How we make PEP 582 packages available to the Python interpreter"}, {"location": "pdm/#plugins", "text": "PDM is aiming at being a community driven package manager. It is shipped with a full-featured plug-in system, with which you can: Develop a new command for PDM. Add additional options to existing PDM commands. Change PDM's behavior by reading dditional config items. Control the process of dependency resolution or installation. If you want to write a plugin, start here .", "title": "Plugins"}, {"location": "pdm/#issues", "text": "You can't still run mypy with pdm without virtualenvs. pawamoy created a patch that is supposed to work, but I'd rather use virtualenvs until it's supported. Once it's supported check the vim-test issue to see how to integrate it. It's not yet supported by dependabot . Once supported add it back to the cookiecutter template and spread it.", "title": "Issues"}, {"location": "pdm/#references", "text": "Git Docs", "title": "References"}, {"location": "pedal_pc/", "text": "The Pedal PC idea gathers crazy projects that try to use the energy of your pedaling while you are working on your PC. The most interesting is PedalPC , but still crazy. Pedal-Power is another similar project, although it looks unmaintained. References \u2691 PedalPC Blog", "title": "Pedal PC"}, {"location": "pedal_pc/#references", "text": "PedalPC Blog", "title": "References"}, {"location": "peek/", "text": "Peek is a simple animated GIF screen recorder with an easy to use interface. Installation \u2691 sudo apt-get install peek If you try to use it with i3, you're going to have a bad time, you'd need to install Compton , and then the elements may not even be clickable . With kitty it works though :) References \u2691 Git", "title": "Peek"}, {"location": "peek/#installation", "text": "sudo apt-get install peek If you try to use it with i3, you're going to have a bad time, you'd need to install Compton , and then the elements may not even be clickable . With kitty it works though :)", "title": "Installation"}, {"location": "peek/#references", "text": "Git", "title": "References"}, {"location": "personal_interruption_analysis/", "text": "This is the interruption analysis report applied to my personal life. I've identified the next interruption sources: Physical interruptions . Emails . Calls . Instant message applications . Calendar events . Other desktop notifications . Physical interruptions \u2691 The analysis is similar to the work physical interruptions . Emails \u2691 Email can be used as one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification of things that don't need to be acted upon immediately or when more powerful mechanisms are not available. In my case emails can be categorized as: Bill or bank receipt emails: I receive at least one per provider per month, the associated action is to download the attached pdf and remove the email. I've got it automated using a Python program that I need to manually run. In the future I expect it to be done automatically without my interaction . There is no urgency to act on them. General information: In the past I was subscribed to newsletters, now I prefer to use RSS. They don't usually require any direct action, so they can wait more than two days. Videogame deals: I was subscribed to Humblebundle, GOG and Steam notifications to be notified on the deals, but then I migrated to IsThereAnyDeal because it only sends the notifications of the deals that match a defined criteria (reducing the amount of emails), and monitors all sites in one place. I can act on them with one or two days of delay. Source code manager notifications: The web where I host my source code sends me emails when there are new pull requests or when there are comments on existent ones. I try to act on them daily. The CI sends notifications when some job fails. Unless it's a new pipeline or I'm actively working on it, a failed work job can wait many days broken before I need to interact with ithe. Infrastructure notifications: For example LetsEncrypt renewals or cloud provider notification or support cases. The related actions can wait a day or more. Monitorization notifications: I've configured Prometheus's alertmanager to send the notifications to the email as a fallback channel, but it's to be checked only if the main channel is down. Stranger emails: People whom I don't know that contacts me asking questions. These can be dealt with daily. In conclusion, I can check the personal emails twice a day, one after breakfast and another in the middle of the afternoon. So its safe to disable the notifications. I'm eager to start the email automation project so I can spend even less time and willpower managing the email. Calls \u2691 People are not used to call anymore, most of them prefer to chat. Even though it is much less efficient. I prefer to have less frequent calls where you have full focused interaction rather than many chat sessions. I categorize the calls in two groups: Social interactions: managed similar as the physical social interactions , with the people that I speak regularly, we arrange meetings that suit us both, the others I tell which are good time spans to call me. If the conversation allows it, I try to use headphones and simultaneously do mindless tasks such as folding the clothes or cleaning the kitchen. To prioritize and adjust the time between calls for each people I use relationship management processes . Spam callers: Hateful events where you can't dump all the frustration that they produce on the person that calls you as it's not their fault and they surely are not enjoying either the situation. They have the lowest priority and can be safely ignored and blocked. You can manually do it in the phone, although it's not very effective as they change numbers. A better approach is to add your number to do not call registries which legally allow you to scare them off. As calls are very rare and of high priority, I have my phone configured to ring on incoming calls. Instant messages \u2691 It's the main communication channel for most people, so it has a great volume of events but most have low priority. They can be categorized as: Asking for help through direct messages: We don't have many as we've agreed to use groups as much as possible . So they have high priority and I have the notifications enabled. Social interaction through direct messages: I don't have many as I try to arrange one on one calls instead , so they have a low priority. Team group or support rooms: We've defined the interruption role so I check them whenever an chosen interruption event comes. Information rooms: They have no priority and can be checked daily. In conclusion, I can check the personal chat applications three times per day, for example, after each meal. As I usually need them when I'm off the computer, I only have them configured at my mobile phone, with no sound notifications. That way I only check them when I want to. Desktop notifications \u2691 I have none but I've seen people have a notification each time the music player changes of song. It makes no sense at all.", "title": "Personal Interruption Analysis"}, {"location": "personal_interruption_analysis/#physical-interruptions", "text": "The analysis is similar to the work physical interruptions .", "title": "Physical interruptions"}, {"location": "personal_interruption_analysis/#emails", "text": "Email can be used as one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification of things that don't need to be acted upon immediately or when more powerful mechanisms are not available. In my case emails can be categorized as: Bill or bank receipt emails: I receive at least one per provider per month, the associated action is to download the attached pdf and remove the email. I've got it automated using a Python program that I need to manually run. In the future I expect it to be done automatically without my interaction . There is no urgency to act on them. General information: In the past I was subscribed to newsletters, now I prefer to use RSS. They don't usually require any direct action, so they can wait more than two days. Videogame deals: I was subscribed to Humblebundle, GOG and Steam notifications to be notified on the deals, but then I migrated to IsThereAnyDeal because it only sends the notifications of the deals that match a defined criteria (reducing the amount of emails), and monitors all sites in one place. I can act on them with one or two days of delay. Source code manager notifications: The web where I host my source code sends me emails when there are new pull requests or when there are comments on existent ones. I try to act on them daily. The CI sends notifications when some job fails. Unless it's a new pipeline or I'm actively working on it, a failed work job can wait many days broken before I need to interact with ithe. Infrastructure notifications: For example LetsEncrypt renewals or cloud provider notification or support cases. The related actions can wait a day or more. Monitorization notifications: I've configured Prometheus's alertmanager to send the notifications to the email as a fallback channel, but it's to be checked only if the main channel is down. Stranger emails: People whom I don't know that contacts me asking questions. These can be dealt with daily. In conclusion, I can check the personal emails twice a day, one after breakfast and another in the middle of the afternoon. So its safe to disable the notifications. I'm eager to start the email automation project so I can spend even less time and willpower managing the email.", "title": "Emails"}, {"location": "personal_interruption_analysis/#calls", "text": "People are not used to call anymore, most of them prefer to chat. Even though it is much less efficient. I prefer to have less frequent calls where you have full focused interaction rather than many chat sessions. I categorize the calls in two groups: Social interactions: managed similar as the physical social interactions , with the people that I speak regularly, we arrange meetings that suit us both, the others I tell which are good time spans to call me. If the conversation allows it, I try to use headphones and simultaneously do mindless tasks such as folding the clothes or cleaning the kitchen. To prioritize and adjust the time between calls for each people I use relationship management processes . Spam callers: Hateful events where you can't dump all the frustration that they produce on the person that calls you as it's not their fault and they surely are not enjoying either the situation. They have the lowest priority and can be safely ignored and blocked. You can manually do it in the phone, although it's not very effective as they change numbers. A better approach is to add your number to do not call registries which legally allow you to scare them off. As calls are very rare and of high priority, I have my phone configured to ring on incoming calls.", "title": "Calls"}, {"location": "personal_interruption_analysis/#instant-messages", "text": "It's the main communication channel for most people, so it has a great volume of events but most have low priority. They can be categorized as: Asking for help through direct messages: We don't have many as we've agreed to use groups as much as possible . So they have high priority and I have the notifications enabled. Social interaction through direct messages: I don't have many as I try to arrange one on one calls instead , so they have a low priority. Team group or support rooms: We've defined the interruption role so I check them whenever an chosen interruption event comes. Information rooms: They have no priority and can be checked daily. In conclusion, I can check the personal chat applications three times per day, for example, after each meal. As I usually need them when I'm off the computer, I only have them configured at my mobile phone, with no sound notifications. That way I only check them when I want to.", "title": "Instant messages"}, {"location": "personal_interruption_analysis/#desktop-notifications", "text": "I have none but I've seen people have a notification each time the music player changes of song. It makes no sense at all.", "title": "Desktop notifications"}, {"location": "pexpect/", "text": "pexpect is a pure Python module for spawning child applications; controlling them; and responding to expected patterns in their output. Pexpect works like Don Libes\u2019 Expect. Pexpect allows your script to spawn a child application and control it as if a human were typing commands. Installation \u2691 pip install pexpect Usage \u2691 import pexpect child = pexpect . spawn ( 'ftp ftp.openbsd.org' ) child . expect ( 'Name .*: ' ) child . sendline ( 'anonymous' ) If you're using it to spawn a program that asks something and then ends, you can catch the end with .expect_exact(pexpect.EOF) . tui = pexpect . spawn ( \"python source.py\" , timeout = 5 ) tui . expect ( \"Give me .*\" ) tui . sendline ( \"HI\" ) tui . expect_exact ( pexpect . EOF ) The timeout=5 is useful if the pexpect interaction is not well defined, so that the script is not hung forever. Send key presses \u2691 To simulate key presses, you can use prompt_toolkit keys with REVERSE_ANSI_SEQUENCES . from prompt_toolkit.input.ansi_escape_sequences import REVERSE_ANSI_SEQUENCES from prompt_toolkit.keys import Keys tui = pexpect . spawn ( \"python source.py\" , timeout = 5 ) tui . send ( REVERSE_ANSI_SEQUENCES [ Keys . ControlC ]) To make your code cleaner you can use a helper class : from prompt_toolkit.input.ansi_escape_sequences import REVERSE_ANSI_SEQUENCES from prompt_toolkit.keys import Keys class Keyboard ( str , Enum ): ControlH = REVERSE_ANSI_SEQUENCES [ Keys . ControlH ] Enter = \" \\r \" Esc = REVERSE_ANSI_SEQUENCES [ Keys . Escape ] # Equivalent keystrokes in terminals; see python-prompt-toolkit for # further explanations Alt = Esc Backspace = ControlH Read output of command \u2691 import sys import pexpect child = pexpect . spawn ( 'ls' ) child . logfile = sys . stdout child . expect ( pexpect . EOF ) For the tests, you can use the capsys fixture to do assertions on the content: out , err = capsys . readouterr () assert \"WARNING! you took 1 seconds to process the last element\" in out References \u2691 Docs", "title": "pexpect"}, {"location": "pexpect/#installation", "text": "pip install pexpect", "title": "Installation"}, {"location": "pexpect/#usage", "text": "import pexpect child = pexpect . spawn ( 'ftp ftp.openbsd.org' ) child . expect ( 'Name .*: ' ) child . sendline ( 'anonymous' ) If you're using it to spawn a program that asks something and then ends, you can catch the end with .expect_exact(pexpect.EOF) . tui = pexpect . spawn ( \"python source.py\" , timeout = 5 ) tui . expect ( \"Give me .*\" ) tui . sendline ( \"HI\" ) tui . expect_exact ( pexpect . EOF ) The timeout=5 is useful if the pexpect interaction is not well defined, so that the script is not hung forever.", "title": "Usage"}, {"location": "pexpect/#send-key-presses", "text": "To simulate key presses, you can use prompt_toolkit keys with REVERSE_ANSI_SEQUENCES . from prompt_toolkit.input.ansi_escape_sequences import REVERSE_ANSI_SEQUENCES from prompt_toolkit.keys import Keys tui = pexpect . spawn ( \"python source.py\" , timeout = 5 ) tui . send ( REVERSE_ANSI_SEQUENCES [ Keys . ControlC ]) To make your code cleaner you can use a helper class : from prompt_toolkit.input.ansi_escape_sequences import REVERSE_ANSI_SEQUENCES from prompt_toolkit.keys import Keys class Keyboard ( str , Enum ): ControlH = REVERSE_ANSI_SEQUENCES [ Keys . ControlH ] Enter = \" \\r \" Esc = REVERSE_ANSI_SEQUENCES [ Keys . Escape ] # Equivalent keystrokes in terminals; see python-prompt-toolkit for # further explanations Alt = Esc Backspace = ControlH", "title": "Send key presses"}, {"location": "pexpect/#read-output-of-command", "text": "import sys import pexpect child = pexpect . spawn ( 'ls' ) child . logfile = sys . stdout child . expect ( pexpect . EOF ) For the tests, you can use the capsys fixture to do assertions on the content: out , err = capsys . readouterr () assert \"WARNING! you took 1 seconds to process the last element\" in out", "title": "Read output of command"}, {"location": "pexpect/#references", "text": "Docs", "title": "References"}, {"location": "pilates/", "text": "Pilates is a physical fitness system based on controlled movements putting emphasis on alignment, breathing, developing a strong core, and improving coordination and balance. The core (or powerhouse), consisting of the muscles of the abdomen, low back, and hips, is thought to be the key to a person's stability. Pilates' system allows for different exercises to be modified in range of difficulty from beginner to advanced or to any other level, and also in terms of the instructor and practitioner's specific goals and/or limitations. Intensity can be increased over time as the body adapts itself to the exercises. You can think of yoga, but without the spiritual aspects. Principles \u2691 Breathing \u2691 The breathing in Pilates is meant the to be deeper, with full inhalations and complete exhalations. In order to keep the lower abdominals close to the spine, the breathing needs to be directed to the back and sides of the lower rib cage. When exhaling, you need to squeeze out the lungs as they would wring a wet towel dry. To do that you need to contract the deep abdominal and pelvic floor muscles, feeling your bellybutton going to your back and a little bit up. When inhaling you need to maintain this engagement to keep the core in control. The difficult part comes when you try to properly coordinate this breathing practice with the exercise movement, breathes out with the effort and in on the return. This technique is important as it increases the intake of oxygen and the circulation of this oxygenated blood to every part of the body, cleaning and invigorating it. Concentration \u2691 It demands intense focus, as you need to be aware of the position of each part of your body, and how they move to precisely do the exercise. Control \u2691 You don't see many quick movements, most of the exercises are anaerobic. The difficult relies on controlling your muscles to do what you want them to while they fight against gravity, springs and other torture tools. Flow \u2691 Pilates aims for elegant economy of movement, creating flow through the use of appropriate transitions. Once precision has been achieved, the exercises are intended to flow within and into each other in order to build strength and stamina. A smoothly doing a roll down (from seated position with your legs straight, slowly lay down, vertebrae by vertebrae) is a difficult challenge, as you need every muscle to coordinate to share the load of the weight. The muscles that we use more are stronger, and some of them are barely used, Pilates positions and slow transitions force you to use those weak, forgotten muscles, and when the load is transferred from the strong to the weak, your body starts shaking or breaks the movement rate thus breaking the flow. Even though it looks silly, it's tough. Postural alignment \u2691 Being more aware of your body by bringing to it's limits with each exercise, seeing where it fails, strengthening the weak muscles and practicing flow and control results in a better postural alignment. Precision \u2691 The focus is on doing one precise and perfect movement, rather than many halfhearted ones. The goal is for this precision to eventually become second nature and carry over into everyday life as grace and economy of movement. Relaxation \u2691 Correct muscle firing patterns and improved mental concentration are enhanced with relaxation. Stamina \u2691 Stamina is increased through the gradual strengthening of your body, and with the increasing precision of the motion, making them more efficient so there is less stress to perform the exercises. Positions \u2691 Feet in flex: your toes go away from your shins, so your foot follows your shin line. Exercises \u2691 I'm going to annotate the exercises I like most, probably the name is incorrect and the explanation not perfect. If you do them, please be careful, and in case of doubt ask a Pilates teacher. Swing from table \u2691 Lvl 0: Starting position (Inhale): Start at step 1 of the table . Step 1 (Exhale): Instead of going to the mat, when exhaling move your ass between your arms without touching the mat until it's behind them. Round your spine in the process. You'll feel a nice spine movement similar to the cat - cow movement. Return (Inhale): Slowly go back to starting position. Lvl 1: Starting position (Inhale): Start at step 1 of the table with one leg straight in the air, in the same line as your shoulders, hips and knee. Step 1 (Exhale): Similar to Lvl 0 but make sure that the heel of the foot doesn't touch the mat, feet in flex . Return (Inhale): Slowly go back to starting position, do X repetitions and then switch to the other foot. Lvl 2: Starting position (Inhale): Start at step 1 of the inverted plank. Step 1 (Exhale): Similar to Lvl 0. Return (Inhale): Slowly go back to starting position. Lvl 3: Similar to Lvl 2 with the leg up like Lvl 1. I've found that Lvl 2 and Lvl 3 give a less pleasant spine rub. Table \u2691 Starting position (Exhale): Sit in your mat with your legs parallel, knees bent and your feet at two or three fists from your ass, hands on the mat behind you, fingers pointing to your ass. If you have shoulder aches, you can point the fingers at 45 degrees or away from your ass. * Step 1 (Inhale): Slowly move your ass up until your shoulders, knees and hips are on the same line. To avoid neck pain, keep your chin down so you're looking at your knees. Your knees should be over your ankles and your arms should be extended. * Return (Exhale): Slowly come back to the starting position. References \u2691 Books \u2691 Pilates anatomy by Rael Isacowitz and Karen Clippinger : With gorgeous illustrations.", "title": "Pilates"}, {"location": "pilates/#principles", "text": "", "title": "Principles"}, {"location": "pilates/#breathing", "text": "The breathing in Pilates is meant the to be deeper, with full inhalations and complete exhalations. In order to keep the lower abdominals close to the spine, the breathing needs to be directed to the back and sides of the lower rib cage. When exhaling, you need to squeeze out the lungs as they would wring a wet towel dry. To do that you need to contract the deep abdominal and pelvic floor muscles, feeling your bellybutton going to your back and a little bit up. When inhaling you need to maintain this engagement to keep the core in control. The difficult part comes when you try to properly coordinate this breathing practice with the exercise movement, breathes out with the effort and in on the return. This technique is important as it increases the intake of oxygen and the circulation of this oxygenated blood to every part of the body, cleaning and invigorating it.", "title": "Breathing"}, {"location": "pilates/#concentration", "text": "It demands intense focus, as you need to be aware of the position of each part of your body, and how they move to precisely do the exercise.", "title": "Concentration"}, {"location": "pilates/#control", "text": "You don't see many quick movements, most of the exercises are anaerobic. The difficult relies on controlling your muscles to do what you want them to while they fight against gravity, springs and other torture tools.", "title": "Control"}, {"location": "pilates/#flow", "text": "Pilates aims for elegant economy of movement, creating flow through the use of appropriate transitions. Once precision has been achieved, the exercises are intended to flow within and into each other in order to build strength and stamina. A smoothly doing a roll down (from seated position with your legs straight, slowly lay down, vertebrae by vertebrae) is a difficult challenge, as you need every muscle to coordinate to share the load of the weight. The muscles that we use more are stronger, and some of them are barely used, Pilates positions and slow transitions force you to use those weak, forgotten muscles, and when the load is transferred from the strong to the weak, your body starts shaking or breaks the movement rate thus breaking the flow. Even though it looks silly, it's tough.", "title": "Flow"}, {"location": "pilates/#postural-alignment", "text": "Being more aware of your body by bringing to it's limits with each exercise, seeing where it fails, strengthening the weak muscles and practicing flow and control results in a better postural alignment.", "title": "Postural alignment"}, {"location": "pilates/#precision", "text": "The focus is on doing one precise and perfect movement, rather than many halfhearted ones. The goal is for this precision to eventually become second nature and carry over into everyday life as grace and economy of movement.", "title": "Precision"}, {"location": "pilates/#relaxation", "text": "Correct muscle firing patterns and improved mental concentration are enhanced with relaxation.", "title": "Relaxation"}, {"location": "pilates/#stamina", "text": "Stamina is increased through the gradual strengthening of your body, and with the increasing precision of the motion, making them more efficient so there is less stress to perform the exercises.", "title": "Stamina"}, {"location": "pilates/#positions", "text": "Feet in flex: your toes go away from your shins, so your foot follows your shin line.", "title": "Positions"}, {"location": "pilates/#exercises", "text": "I'm going to annotate the exercises I like most, probably the name is incorrect and the explanation not perfect. If you do them, please be careful, and in case of doubt ask a Pilates teacher.", "title": "Exercises"}, {"location": "pilates/#swing-from-table", "text": "Lvl 0: Starting position (Inhale): Start at step 1 of the table . Step 1 (Exhale): Instead of going to the mat, when exhaling move your ass between your arms without touching the mat until it's behind them. Round your spine in the process. You'll feel a nice spine movement similar to the cat - cow movement. Return (Inhale): Slowly go back to starting position. Lvl 1: Starting position (Inhale): Start at step 1 of the table with one leg straight in the air, in the same line as your shoulders, hips and knee. Step 1 (Exhale): Similar to Lvl 0 but make sure that the heel of the foot doesn't touch the mat, feet in flex . Return (Inhale): Slowly go back to starting position, do X repetitions and then switch to the other foot. Lvl 2: Starting position (Inhale): Start at step 1 of the inverted plank. Step 1 (Exhale): Similar to Lvl 0. Return (Inhale): Slowly go back to starting position. Lvl 3: Similar to Lvl 2 with the leg up like Lvl 1. I've found that Lvl 2 and Lvl 3 give a less pleasant spine rub.", "title": "Swing from table"}, {"location": "pilates/#table", "text": "Starting position (Exhale): Sit in your mat with your legs parallel, knees bent and your feet at two or three fists from your ass, hands on the mat behind you, fingers pointing to your ass. If you have shoulder aches, you can point the fingers at 45 degrees or away from your ass. * Step 1 (Inhale): Slowly move your ass up until your shoulders, knees and hips are on the same line. To avoid neck pain, keep your chin down so you're looking at your knees. Your knees should be over your ankles and your arms should be extended. * Return (Exhale): Slowly come back to the starting position.", "title": "Table"}, {"location": "pilates/#references", "text": "", "title": "References"}, {"location": "pilates/#books", "text": "Pilates anatomy by Rael Isacowitz and Karen Clippinger : With gorgeous illustrations.", "title": "Books"}, {"location": "pipenv/", "text": "Pipenv is a tool that aims to bring the best of all packaging worlds (bundler, composer, npm, cargo, yarn, etc.) to the Python world. It automatically creates and manages a virtualenv for your projects, as well as adds/removes packages from your Pipfile as you install/uninstall packages. It also generates the ever-important Pipfile.lock, which is used to produce deterministic builds. Features \u2691 Enables truly deterministic builds , while easily specifying only what you want . Generates and checks file hashes for locked dependencies. Automatically install required Pythons, if pyenv is available. Automatically finds your project home, recursively, by looking for a Pipfile . Automatically generates a Pipfile , if one doesn't exist. Automatically creates a virtualenv in a standard location. Automatically adds/removes packages to a Pipfile when they are un/installed. Automatically loads .env files, if they exist. The main commands are install , uninstall , and lock , which generates a Pipfile.lock . These are intended to replace $ pip install usage, as well as manual virtualenv management (to activate a virtualenv, run $ pipenv shell ). Basic Concepts \u2691 A virtualenv will automatically be created, when one doesn't exist. When no parameters are passed to install , all packages [packages] specified will be installed. Otherwise, whatever virtualenv defaults to will be the default. Other Commands \u2691 shell will spawn a shell with the virtualenv activated. run will run a given command from the virtualenv, with any arguments forwarded (e.g. $ pipenv run python ). check asserts that PEP 508 requirements are being met by the current environment. graph will print a pretty graph of all your installed dependencies. Installation \u2691 In Debian: apt-get install pipenv Or pip install pipenv . References \u2691 Git", "title": "Pipenv"}, {"location": "pipenv/#features", "text": "Enables truly deterministic builds , while easily specifying only what you want . Generates and checks file hashes for locked dependencies. Automatically install required Pythons, if pyenv is available. Automatically finds your project home, recursively, by looking for a Pipfile . Automatically generates a Pipfile , if one doesn't exist. Automatically creates a virtualenv in a standard location. Automatically adds/removes packages to a Pipfile when they are un/installed. Automatically loads .env files, if they exist. The main commands are install , uninstall , and lock , which generates a Pipfile.lock . These are intended to replace $ pip install usage, as well as manual virtualenv management (to activate a virtualenv, run $ pipenv shell ).", "title": "Features"}, {"location": "pipenv/#basic-concepts", "text": "A virtualenv will automatically be created, when one doesn't exist. When no parameters are passed to install , all packages [packages] specified will be installed. Otherwise, whatever virtualenv defaults to will be the default.", "title": "Basic Concepts"}, {"location": "pipenv/#other-commands", "text": "shell will spawn a shell with the virtualenv activated. run will run a given command from the virtualenv, with any arguments forwarded (e.g. $ pipenv run python ). check asserts that PEP 508 requirements are being met by the current environment. graph will print a pretty graph of all your installed dependencies.", "title": "Other Commands"}, {"location": "pipenv/#installation", "text": "In Debian: apt-get install pipenv Or pip install pipenv .", "title": "Installation"}, {"location": "pipenv/#references", "text": "Git", "title": "References"}, {"location": "pipx/", "text": "Pipx is a command line tool to install and run Python applications in isolated environments. Very useful not to pollute your user or device python environments. Installation \u2691 pip install pipx Usage \u2691 Now that you have pipx installed, you can install a program: pipx install PACKAGE for example pipx install pycowsay You can list programs installed: pipx list Or you can run a program without installing it: pipx run pycowsay moooo! You can view documentation for all commands by running pipx --help. Upgrade \u2691 You can use pipx upgrade-all to upgrade all your installed packages. If you want to just upgrade one, use pipx upgrade PACKAGE . If the package doesn't change the requirements of their dependencies so that the installed don't meet them, they won't be upgraded unless you use the --pip-args '--upgrade-strategy eager' flag. It uses the pip flag upgrade-strategy which can be one of: eager : dependencies are upgraded regardless of whether the currently installed version satisfies the requirements of the upgraded package(s). only-if-needed : dependencies are upgraded only when they do not satisfy the requirements of the upgraded package(s). This is the default value. References \u2691 Docs Git", "title": "Pipx"}, {"location": "pipx/#installation", "text": "pip install pipx", "title": "Installation"}, {"location": "pipx/#usage", "text": "Now that you have pipx installed, you can install a program: pipx install PACKAGE for example pipx install pycowsay You can list programs installed: pipx list Or you can run a program without installing it: pipx run pycowsay moooo! You can view documentation for all commands by running pipx --help.", "title": "Usage"}, {"location": "pipx/#upgrade", "text": "You can use pipx upgrade-all to upgrade all your installed packages. If you want to just upgrade one, use pipx upgrade PACKAGE . If the package doesn't change the requirements of their dependencies so that the installed don't meet them, they won't be upgraded unless you use the --pip-args '--upgrade-strategy eager' flag. It uses the pip flag upgrade-strategy which can be one of: eager : dependencies are upgraded regardless of whether the currently installed version satisfies the requirements of the upgraded package(s). only-if-needed : dependencies are upgraded only when they do not satisfy the requirements of the upgraded package(s). This is the default value.", "title": "Upgrade"}, {"location": "pipx/#references", "text": "Docs Git", "title": "References"}, {"location": "process_automation/", "text": "I understand process automation as the act of analyzing yourself and your interactions with the world to find the way to reduce the time or willpower spent on your life processes. Once you've covered some minimum life requirements (health, money or happiness), time is your most valued asset. It's sad to waste it doing stuff that we need but don't increase our happiness. I've also faced the problem of having so much stuff in my mind. Having background processes increase your brain load and are a constant sink of willpower. As a result, when you really need that CPU time, your brain is tired and doesn't work to it's full performance. Automating processes, as well as life logging and task management, allows you to delegate those worries. Process automation can lead to habit building , which reduces even more the willpower consumption of processes, at the same time it reduces the error rate. The marvelous xkcd comic has gathered the essence and pitfalls of process automation many times: Automating home chores \u2691 Using Grocy to maintain the house stock, shopping lists and meal plans.", "title": "Process Automation"}, {"location": "process_automation/#automating-home-chores", "text": "Using Grocy to maintain the house stock, shopping lists and meal plans.", "title": "Automating home chores"}, {"location": "profanity/", "text": "profanity is a console based XMPP client written in C using ncurses and libstrophe, inspired by Irssi. Installation \u2691 sudo apt-get install profanity Usage \u2691 Connect \u2691 To connect to an XMPP chat service: /connect user@server.com You will be prompted by the status bar to enter your password. Send one to one message \u2691 To open a new window and send a message use the /msg command: /msg mycontact@server.com Hello there! Profanity uses the contact's nickname by default, if one exists. For example: /msg Bob Are you there bob? Window navigation \u2691 To make a window visible in the main window area, use any of the following: Alt-1 to Alt-0 F1 to F10 Alt-left , Alt-right The /win command may also be used. Either the window number may be passed, or the window title: /win 4 /win someroom@chatserver.org /win MyBuddy To close the current window: /close Adding contacts \u2691 To add someone to your roster: /roster add newfriend@server.chat.com To subscribe to a contacts presence (to be notified when they are online/offline etc): /sub request newfriend@server.chat.com To approve a contact's request to subscribe to your presence: /sub allow newfriend@server.chat.com Giving contacts a nickname \u2691 /roster nick bob@company.org Bobster Logging out \u2691 To quit profanity: /quit Configure OMEMO \u2691 /omemo gen /carbons on References \u2691 Home Quickstart", "title": "Profanity"}, {"location": "profanity/#installation", "text": "sudo apt-get install profanity", "title": "Installation"}, {"location": "profanity/#usage", "text": "", "title": "Usage"}, {"location": "profanity/#connect", "text": "To connect to an XMPP chat service: /connect user@server.com You will be prompted by the status bar to enter your password.", "title": "Connect"}, {"location": "profanity/#send-one-to-one-message", "text": "To open a new window and send a message use the /msg command: /msg mycontact@server.com Hello there! Profanity uses the contact's nickname by default, if one exists. For example: /msg Bob Are you there bob?", "title": "Send one to one message"}, {"location": "profanity/#window-navigation", "text": "To make a window visible in the main window area, use any of the following: Alt-1 to Alt-0 F1 to F10 Alt-left , Alt-right The /win command may also be used. Either the window number may be passed, or the window title: /win 4 /win someroom@chatserver.org /win MyBuddy To close the current window: /close", "title": "Window navigation"}, {"location": "profanity/#adding-contacts", "text": "To add someone to your roster: /roster add newfriend@server.chat.com To subscribe to a contacts presence (to be notified when they are online/offline etc): /sub request newfriend@server.chat.com To approve a contact's request to subscribe to your presence: /sub allow newfriend@server.chat.com", "title": "Adding contacts"}, {"location": "profanity/#giving-contacts-a-nickname", "text": "/roster nick bob@company.org Bobster", "title": "Giving contacts a nickname"}, {"location": "profanity/#logging-out", "text": "To quit profanity: /quit", "title": "Logging out"}, {"location": "profanity/#configure-omemo", "text": "/omemo gen /carbons on", "title": "Configure OMEMO"}, {"location": "profanity/#references", "text": "Home Quickstart", "title": "References"}, {"location": "prompt_toolkit_fullscreen_applications/", "text": "Prompt toolkit can be used to build full screen interfaces . This section focuses in how to do it. If you want to build REPL applications instead go to this other article . Typically, an application consists of a layout (to describe the graphical part) and a set of key bindings. Every prompt_toolkit application is an instance of an Application object. from prompt_toolkit import Application app = Application ( full_screen = True ) app . run () When run() is called, the event loop will run until the application is done. An application will quit when exit() is called. The event loop is basically a while-true loop that waits for user input, and when it receives something (like a key press), it will send that to the appropriate handler, like for instance, a key binding. An application consists of several components. The most important are: I/O objects: the input and output device. The layout: this defines the graphical structure of the application. For instance, a text box on the left side, and a button on the right side. You can also think of the layout as a collection of \u2018widgets\u2019. A style: this defines what colors and underline/bold/italic styles are used everywhere. A set of key bindings. The layout \u2691 With the Layout object you define the graphical structure of the application, it accepts as argument a nested structure of Container objects, these arrange the layout by splitting the screen in many regions, while controls (children of UIControl , such as BufferControl or FormattedTextControl ) are responsible for generating the actual content. Some of the Container s you can use are: HSplit , Vsplit , FloatContainer , Window or ScrollablePane . The Window class itself is particular: it is a Container that can contain a UIControl . Thus, it\u2019s the adapter between the two. The Window class also takes care of scrolling the content and wrapping the lines if needed. Conditional Containers \u2691 Sometimes you only want to show containers when a condition is met, ConditionalContainers are meant to fulfill this case. They accept other containers, and a filter condition. You can read more about filters here , the simplest use case is if you have a boolean variable and you use the to_filter function. from prompt_toolkit.layout import ConditionalContainer from prompt_toolkit.filters.utils import to_filter show_header = True ConditionalContainer ( Label ( 'This is an optional text' ), filter = to_filter ( show_header ) ) Tables \u2691 Currently they are not supported :(, although there is an old PR . Controls \u2691 Focusing windows \u2691 Focusing something can be done by calling the focus() method. This method is very flexible and accepts a Window , a Buffer , a UIControl and more. In the following example, we use get_app() for getting the active application. from prompt_toolkit.application import get_app # This window was created earlier. w = Window () # ... # Now focus it. get_app () . layout . focus ( w ) To focus the next window in the layout you can use app.layout.focus_next() . [Key \u2691 bindings]( https://python-prompt-toolkit.readthedocs.io/en/master/pages/full_screen_apps.html#key-bindings ) In order to react to user actions, we need to create a KeyBindings object and pass that to our Application . There are two kinds of key bindings: Global key bindings , which are always active. Key bindings that belong to a certain UIControl and are only active when this control is focused. Both BufferControl and FormattedTextControl take a key_bindings argument. For complex keys you can always look at the Keys class . [Global key \u2691 bindings]( https://python-prompt-toolkit.readthedocs.io/en/master/pages/full_screen_apps.html#global-key-bindings ) Key bindings can be passed to the application as follows: from prompt_toolkit import Application from prompt_toolkit.key_binding.key_processor import KeyPressEvent kb = KeyBindings () app = Application ( key_bindings = kb ) app . run () To register a new keyboard shortcut, we can use the add() method as a decorator of the key handler: from prompt_toolkit import Application from prompt_toolkit.key_binding import KeyBindings from prompt_toolkit.key_binding.key_processor import KeyPressEvent kb = KeyBindings () @kb . add ( 'c-q' ) def exit_ ( event : KeyPressEvent ) -> None : \"\"\" Pressing Ctrl-Q will exit the user interface. Setting a return value means: quit the event loop that drives the user interface and return this value from the `Application.run()` call. \"\"\" event . app . exit () app = Application ( key_bindings = kb , full_screen = True ) app . run () Here you can read for more complex patterns with key bindings. A more programmatically way to add bindings is: from prompt_toolkit.key_binding import KeyBindings from prompt_toolkit.key_binding.bindings.focus import focus_next kb = KeyBindings () kb . add ( \"tab\" )( focus_next ) Pass more than one key \u2691 To map an action to two key presses use kb.add('g', 'g') . Styles \u2691 Many user interface controls, like Window accept a style argument which can be used to pass the formatting as a string. For instance, we can select a foreground color: fg:ansired : ANSI color palette fg:ansiblue : ANSI color palette fg:#ffaa33 : hexadecimal notation fg:darkred : named color Or a background color: bg:ansired : ANSI color palette bg:#ffaa33 : hexadecimal notation Like we do for web design, it is not a good habit to specify all styling inline. Instead, we can attach class names to UI controls and have a style sheet that refers to these class names. The Style can be passed as an argument to the Application . from prompt_toolkit.layout import VSplit , Window from prompt_toolkit.styles import Style layout = VSplit ([ Window ( BufferControl ( ... ), style = 'class:left' ), HSplit ([ Window ( BufferControl ( ... ), style = 'class:top' ), Window ( BufferControl ( ... ), style = 'class:bottom' ), ], style = 'class:right' ) ]) style = Style ([ ( 'left' , 'bg:ansired' ), ( 'top' , 'fg:#00aaaa' ), ( 'bottom' , 'underline bold' ), ]) You may need to define the 24bit color depths to see the colors you expect: from prompt_toolkit.output.color_depth import ColorDepth app = Application ( color_depth = ColorDepth . DEPTH_24_BIT , # ... ) If you want to see if a style is being applied in a component, set the style to bg:#dc322f and it will be highlighted in red. Dynamically changing the style \u2691 You'll need to create a widget, you can take as inspiration the package widgets . To create a row that changes color when it's focused use: from prompt_toolkit.layout.controls import FormattedTextControl from prompt_toolkit.layout.containers import Window from prompt_toolkit.application import get_app class Row : \"\"\"Define row. Args: text: text to print \"\"\" def __init__ ( self , text : str , ) -> None : \"\"\"Initialize the widget.\"\"\" self . text = text self . control = FormattedTextControl ( self . text , focusable = True , ) def get_style () -> str : if get_app () . layout . has_focus ( self ): return \"class:row.focused\" else : return \"class:row\" self . window = Window ( self . control , height = 1 , style = get_style , always_hide_cursor = True ) def __pt_container__ ( self ) -> Window : \"\"\"Return the window object. Mandatory to be considered a widget. \"\"\" return self . window An example of use would be: layout = HSplit ( [ Row ( \"Test1\" ), Row ( \"Test2\" ), Row ( \"Test3\" ), ] ) # Key bindings kb = KeyBindings () kb . add ( \"j\" )( focus_next ) kb . add ( \"k\" )( focus_previous ) @kb . add ( \"c-c\" , eager = True ) @kb . add ( \"q\" , eager = True ) def exit_ ( event : KeyPressEvent ) -> None : \"\"\"Exit the user interface.\"\"\" event . app . exit () # Styles style = Style ( [ ( \"row\" , \"bg:#073642 #657b83\" ), ( \"row.focused\" , \"bg:#002b36 #657b83\" ), ] ) # Application app = Application ( layout = Layout ( layout ), full_screen = True , key_bindings = kb , style = style , color_depth = ColorDepth . DEPTH_24_BIT , ) app . run () Examples \u2691 The best way to understand how it works is by running the examples in the repository, some interesting ones in increasing order of difficult are: Managing autocompletion Managing focus Managing floats , and floats with transparency Add margins , such as line number or a scroll bar. Managing styles . Wrapping lines in buffercontrol, and line prefixes A working REPL example . Interesting to see the SearchToolbar in use (press Ctrl+r ), and how to interact with other windows with handlers. A working editor : Complex application that shows how to build a working menu. pyvim : A rewrite of vim in python, it shows how to test, handle commands and a lot more, very interesting. pymux : Another full screen application example. Testing \u2691 Prompt toolkit application testing can be done at different levels: Component level: Useful to test how a component manages it's data by itself. Application level: Useful to test how a user interacts with the component. If you don't know how to test something, I suggest you check how prompt toolkit tests itself. You can also check how do third party packages do their tests too, such as prompt-toolkit-table or pyvim . Keep in mind that you don't usually want to check the result of the stdout or stderr directly, but the state of your component or the application itself. Component level \u2691 If your component accepts some input and does some magic on that input without the need to load the application, import the object directly and run tests changing the input directly and asserting the results of the output. Application level \u2691 If you want to test the interaction with your component at application level, for example what happens when a user presses a key, you need to instantiate a dummy application and play with it. Imagine we have a TableControl component we want to test that accepts some input in the form of data . We'll use the set_dummy_app function to configure an application that outputs to DummyOutput , and a helper function get_app_and_processor to return the active app and a processor to send key presses. def set_dummy_app ( data : Any ) -> Any : \"\"\"Return a context manager that starts the dummy application. This is important, because we need an `Application` with `is_done=False` flag, otherwise no keys will be processed. \"\"\" app : Application [ Any ] = Application ( layout = Layout ( Window ( TableControl ( data ))), output = DummyOutput (), input = create_pipe_input (), ) return set_app ( app ) def get_app_and_processor () -> Tuple [ Application [ Any ], KeyProcessor ]: \"\"\"Return the active application and it's key processor.\"\"\" app = get_app () key_bindings = app . layout . container . get_key_bindings () if key_bindings is None : key_bindings = KeyBindings () processor = KeyProcessor ( key_bindings ) return app , processor We've loaded the processor with the key bindings defined in the container. If you want other bindings change them there. For example prompt-toolkit uses a fixture to set them. Remember that you have the merge_key_bindings to join two key binding objects with: key_bindings = merge_key_bindings ([ key_bindings , control_bindings ]) Once the functions are set, you can make your test. Imagine that we want to check that if the user presses j , the variable _focused_row is incremented by 1. This variable will be used by the component internally to change the style of the rows so that the next element is highlighted. def test_j_moves_to_the_next_row ( self , pydantic_data : PydanticData ) -> None : \"\"\" Given: A well configured table When: j is press Then: the focus is moved to the next line \"\"\" with set_dummy_app ( pydantic_data ): app , processor = get_app_and_processor () processor . feed ( KeyPress ( \"j\" , \"j\" )) # act processor . process_keys () assert app . layout . container . content . _focused_row == 1 References \u2691 Docs Git Projects using prompt_toolkit", "title": "Full screen applications"}, {"location": "prompt_toolkit_fullscreen_applications/#the-layout", "text": "With the Layout object you define the graphical structure of the application, it accepts as argument a nested structure of Container objects, these arrange the layout by splitting the screen in many regions, while controls (children of UIControl , such as BufferControl or FormattedTextControl ) are responsible for generating the actual content. Some of the Container s you can use are: HSplit , Vsplit , FloatContainer , Window or ScrollablePane . The Window class itself is particular: it is a Container that can contain a UIControl . Thus, it\u2019s the adapter between the two. The Window class also takes care of scrolling the content and wrapping the lines if needed.", "title": "The layout"}, {"location": "prompt_toolkit_fullscreen_applications/#conditional-containers", "text": "Sometimes you only want to show containers when a condition is met, ConditionalContainers are meant to fulfill this case. They accept other containers, and a filter condition. You can read more about filters here , the simplest use case is if you have a boolean variable and you use the to_filter function. from prompt_toolkit.layout import ConditionalContainer from prompt_toolkit.filters.utils import to_filter show_header = True ConditionalContainer ( Label ( 'This is an optional text' ), filter = to_filter ( show_header ) )", "title": "Conditional Containers"}, {"location": "prompt_toolkit_fullscreen_applications/#tables", "text": "Currently they are not supported :(, although there is an old PR .", "title": "Tables"}, {"location": "prompt_toolkit_fullscreen_applications/#controls", "text": "", "title": "Controls"}, {"location": "prompt_toolkit_fullscreen_applications/#focusing-windows", "text": "Focusing something can be done by calling the focus() method. This method is very flexible and accepts a Window , a Buffer , a UIControl and more. In the following example, we use get_app() for getting the active application. from prompt_toolkit.application import get_app # This window was created earlier. w = Window () # ... # Now focus it. get_app () . layout . focus ( w ) To focus the next window in the layout you can use app.layout.focus_next() .", "title": "Focusing windows"}, {"location": "prompt_toolkit_fullscreen_applications/#key", "text": "bindings]( https://python-prompt-toolkit.readthedocs.io/en/master/pages/full_screen_apps.html#key-bindings ) In order to react to user actions, we need to create a KeyBindings object and pass that to our Application . There are two kinds of key bindings: Global key bindings , which are always active. Key bindings that belong to a certain UIControl and are only active when this control is focused. Both BufferControl and FormattedTextControl take a key_bindings argument. For complex keys you can always look at the Keys class .", "title": "[Key"}, {"location": "prompt_toolkit_fullscreen_applications/#global-key", "text": "bindings]( https://python-prompt-toolkit.readthedocs.io/en/master/pages/full_screen_apps.html#global-key-bindings ) Key bindings can be passed to the application as follows: from prompt_toolkit import Application from prompt_toolkit.key_binding.key_processor import KeyPressEvent kb = KeyBindings () app = Application ( key_bindings = kb ) app . run () To register a new keyboard shortcut, we can use the add() method as a decorator of the key handler: from prompt_toolkit import Application from prompt_toolkit.key_binding import KeyBindings from prompt_toolkit.key_binding.key_processor import KeyPressEvent kb = KeyBindings () @kb . add ( 'c-q' ) def exit_ ( event : KeyPressEvent ) -> None : \"\"\" Pressing Ctrl-Q will exit the user interface. Setting a return value means: quit the event loop that drives the user interface and return this value from the `Application.run()` call. \"\"\" event . app . exit () app = Application ( key_bindings = kb , full_screen = True ) app . run () Here you can read for more complex patterns with key bindings. A more programmatically way to add bindings is: from prompt_toolkit.key_binding import KeyBindings from prompt_toolkit.key_binding.bindings.focus import focus_next kb = KeyBindings () kb . add ( \"tab\" )( focus_next )", "title": "[Global key"}, {"location": "prompt_toolkit_fullscreen_applications/#pass-more-than-one-key", "text": "To map an action to two key presses use kb.add('g', 'g') .", "title": "Pass more than one key"}, {"location": "prompt_toolkit_fullscreen_applications/#styles", "text": "Many user interface controls, like Window accept a style argument which can be used to pass the formatting as a string. For instance, we can select a foreground color: fg:ansired : ANSI color palette fg:ansiblue : ANSI color palette fg:#ffaa33 : hexadecimal notation fg:darkred : named color Or a background color: bg:ansired : ANSI color palette bg:#ffaa33 : hexadecimal notation Like we do for web design, it is not a good habit to specify all styling inline. Instead, we can attach class names to UI controls and have a style sheet that refers to these class names. The Style can be passed as an argument to the Application . from prompt_toolkit.layout import VSplit , Window from prompt_toolkit.styles import Style layout = VSplit ([ Window ( BufferControl ( ... ), style = 'class:left' ), HSplit ([ Window ( BufferControl ( ... ), style = 'class:top' ), Window ( BufferControl ( ... ), style = 'class:bottom' ), ], style = 'class:right' ) ]) style = Style ([ ( 'left' , 'bg:ansired' ), ( 'top' , 'fg:#00aaaa' ), ( 'bottom' , 'underline bold' ), ]) You may need to define the 24bit color depths to see the colors you expect: from prompt_toolkit.output.color_depth import ColorDepth app = Application ( color_depth = ColorDepth . DEPTH_24_BIT , # ... ) If you want to see if a style is being applied in a component, set the style to bg:#dc322f and it will be highlighted in red.", "title": "Styles"}, {"location": "prompt_toolkit_fullscreen_applications/#dynamically-changing-the-style", "text": "You'll need to create a widget, you can take as inspiration the package widgets . To create a row that changes color when it's focused use: from prompt_toolkit.layout.controls import FormattedTextControl from prompt_toolkit.layout.containers import Window from prompt_toolkit.application import get_app class Row : \"\"\"Define row. Args: text: text to print \"\"\" def __init__ ( self , text : str , ) -> None : \"\"\"Initialize the widget.\"\"\" self . text = text self . control = FormattedTextControl ( self . text , focusable = True , ) def get_style () -> str : if get_app () . layout . has_focus ( self ): return \"class:row.focused\" else : return \"class:row\" self . window = Window ( self . control , height = 1 , style = get_style , always_hide_cursor = True ) def __pt_container__ ( self ) -> Window : \"\"\"Return the window object. Mandatory to be considered a widget. \"\"\" return self . window An example of use would be: layout = HSplit ( [ Row ( \"Test1\" ), Row ( \"Test2\" ), Row ( \"Test3\" ), ] ) # Key bindings kb = KeyBindings () kb . add ( \"j\" )( focus_next ) kb . add ( \"k\" )( focus_previous ) @kb . add ( \"c-c\" , eager = True ) @kb . add ( \"q\" , eager = True ) def exit_ ( event : KeyPressEvent ) -> None : \"\"\"Exit the user interface.\"\"\" event . app . exit () # Styles style = Style ( [ ( \"row\" , \"bg:#073642 #657b83\" ), ( \"row.focused\" , \"bg:#002b36 #657b83\" ), ] ) # Application app = Application ( layout = Layout ( layout ), full_screen = True , key_bindings = kb , style = style , color_depth = ColorDepth . DEPTH_24_BIT , ) app . run ()", "title": "Dynamically changing the style"}, {"location": "prompt_toolkit_fullscreen_applications/#examples", "text": "The best way to understand how it works is by running the examples in the repository, some interesting ones in increasing order of difficult are: Managing autocompletion Managing focus Managing floats , and floats with transparency Add margins , such as line number or a scroll bar. Managing styles . Wrapping lines in buffercontrol, and line prefixes A working REPL example . Interesting to see the SearchToolbar in use (press Ctrl+r ), and how to interact with other windows with handlers. A working editor : Complex application that shows how to build a working menu. pyvim : A rewrite of vim in python, it shows how to test, handle commands and a lot more, very interesting. pymux : Another full screen application example.", "title": "Examples"}, {"location": "prompt_toolkit_fullscreen_applications/#testing", "text": "Prompt toolkit application testing can be done at different levels: Component level: Useful to test how a component manages it's data by itself. Application level: Useful to test how a user interacts with the component. If you don't know how to test something, I suggest you check how prompt toolkit tests itself. You can also check how do third party packages do their tests too, such as prompt-toolkit-table or pyvim . Keep in mind that you don't usually want to check the result of the stdout or stderr directly, but the state of your component or the application itself.", "title": "Testing"}, {"location": "prompt_toolkit_fullscreen_applications/#component-level", "text": "If your component accepts some input and does some magic on that input without the need to load the application, import the object directly and run tests changing the input directly and asserting the results of the output.", "title": "Component level"}, {"location": "prompt_toolkit_fullscreen_applications/#application-level", "text": "If you want to test the interaction with your component at application level, for example what happens when a user presses a key, you need to instantiate a dummy application and play with it. Imagine we have a TableControl component we want to test that accepts some input in the form of data . We'll use the set_dummy_app function to configure an application that outputs to DummyOutput , and a helper function get_app_and_processor to return the active app and a processor to send key presses. def set_dummy_app ( data : Any ) -> Any : \"\"\"Return a context manager that starts the dummy application. This is important, because we need an `Application` with `is_done=False` flag, otherwise no keys will be processed. \"\"\" app : Application [ Any ] = Application ( layout = Layout ( Window ( TableControl ( data ))), output = DummyOutput (), input = create_pipe_input (), ) return set_app ( app ) def get_app_and_processor () -> Tuple [ Application [ Any ], KeyProcessor ]: \"\"\"Return the active application and it's key processor.\"\"\" app = get_app () key_bindings = app . layout . container . get_key_bindings () if key_bindings is None : key_bindings = KeyBindings () processor = KeyProcessor ( key_bindings ) return app , processor We've loaded the processor with the key bindings defined in the container. If you want other bindings change them there. For example prompt-toolkit uses a fixture to set them. Remember that you have the merge_key_bindings to join two key binding objects with: key_bindings = merge_key_bindings ([ key_bindings , control_bindings ]) Once the functions are set, you can make your test. Imagine that we want to check that if the user presses j , the variable _focused_row is incremented by 1. This variable will be used by the component internally to change the style of the rows so that the next element is highlighted. def test_j_moves_to_the_next_row ( self , pydantic_data : PydanticData ) -> None : \"\"\" Given: A well configured table When: j is press Then: the focus is moved to the next line \"\"\" with set_dummy_app ( pydantic_data ): app , processor = get_app_and_processor () processor . feed ( KeyPress ( \"j\" , \"j\" )) # act processor . process_keys () assert app . layout . container . content . _focused_row == 1", "title": "Application level"}, {"location": "prompt_toolkit_fullscreen_applications/#references", "text": "Docs Git Projects using prompt_toolkit", "title": "References"}, {"location": "prompt_toolkit_repl/", "text": "Prompt toolkit can be used to build REPL interfaces. This section focuses in how to do it. If you want to build full screen applications instead go to this other article . Testing \u2691 Testing prompt_toolkit or any text-based user interface (TUI) with python is not well documented. Some of the main developers suggest mocking it while others use pexpect . With the first approach you can test python functions and methods internally but it can lead you to the over mocking problem. The second will limit you to test functionality exposed through your program's command line, as it will spawn a process and interact it externally. Given that the TUIs are entrypoints to your program, it makes sense to test them in end-to-end tests, so I'm going to follow the second option. On the other hand, you may want to test a small part of your TUI in a unit test, if you want to follow this other path, I'd start with monkeypatch , although I didn't have good results with it. def test_method ( monkeypatch ): monkeypatch . setattr ( 'sys.stdin' , io . StringIO ( 'my input' )) Using pexpect \u2691 This method is useful to test end to end functions as you need to all the program as a command line. You can't use it to tests python functions internally. File: source.py from prompt_toolkit import prompt text = prompt ( \"Give me some input: \" ) with open ( \"/tmp/tui.txt\" , \"w\" ) as f : f . write ( text ) File: test_source.py ```python import pexpect def test_tui() -> None: tui = pexpect.spawn(\"python source.py\", timeout=5) tui.expect(\"Give me .*\") tui.sendline(\"HI\") tui.expect_exact(pexpect.EOF) with open(\"/tmp/tui.txt\", \"r\") as f: assert f.read() == \"HI\" ``` Where: The tui.expect_exact(pexpect.EOF) line is required so that the tests aren't run before the process has ended, otherwise the file might not exist yet. The timeout=5 is required in case that the pexpect interaction is not well defined, so that the test is not hung forever. Thank you Jairo Llopis for this solution. I've deduced the solution from his #260 PR into copier , and the comments of #1243", "title": "REPL"}, {"location": "prompt_toolkit_repl/#testing", "text": "Testing prompt_toolkit or any text-based user interface (TUI) with python is not well documented. Some of the main developers suggest mocking it while others use pexpect . With the first approach you can test python functions and methods internally but it can lead you to the over mocking problem. The second will limit you to test functionality exposed through your program's command line, as it will spawn a process and interact it externally. Given that the TUIs are entrypoints to your program, it makes sense to test them in end-to-end tests, so I'm going to follow the second option. On the other hand, you may want to test a small part of your TUI in a unit test, if you want to follow this other path, I'd start with monkeypatch , although I didn't have good results with it. def test_method ( monkeypatch ): monkeypatch . setattr ( 'sys.stdin' , io . StringIO ( 'my input' ))", "title": "Testing"}, {"location": "prompt_toolkit_repl/#using-pexpect", "text": "This method is useful to test end to end functions as you need to all the program as a command line. You can't use it to tests python functions internally. File: source.py from prompt_toolkit import prompt text = prompt ( \"Give me some input: \" ) with open ( \"/tmp/tui.txt\" , \"w\" ) as f : f . write ( text ) File: test_source.py ```python import pexpect def test_tui() -> None: tui = pexpect.spawn(\"python source.py\", timeout=5) tui.expect(\"Give me .*\") tui.sendline(\"HI\") tui.expect_exact(pexpect.EOF) with open(\"/tmp/tui.txt\", \"r\") as f: assert f.read() == \"HI\" ``` Where: The tui.expect_exact(pexpect.EOF) line is required so that the tests aren't run before the process has ended, otherwise the file might not exist yet. The timeout=5 is required in case that the pexpect interaction is not well defined, so that the test is not hung forever. Thank you Jairo Llopis for this solution. I've deduced the solution from his #260 PR into copier , and the comments of #1243", "title": "Using pexpect"}, {"location": "psu/", "text": "Power supply unit is the component of the computer that sources power from the primary source (the power coming from your wall outlet) and delivers it to its motherboard and all its components. Contrary to the common understanding, the PSU does not supply power to the computer; it instead converts the AC (Alternating Current) power from the source to the DC (Direct Current) power that the computer needs. There are two types of PSU: Linear and Switch-mode. Linear power supplies have a built-in transformer that steps down the voltage from the main to a usable one for the individual parts of the computer. The transformer makes the Linear PSU bulky, heavy, and expensive. Modern computers have switched to the switch-mode power supply, using switches instead of a transformer for voltage regulation. They\u2019re also more practical and economical to use because they\u2019re smaller, lighter, and cheaper than linear power supplies. PSU need to deliver at least the amount of power that each component requires, if it needs to deliver more, it simply won't work. Another puzzling question for most consumers is, \u201cDoes a PSU supply constant wattage to the computer?\u201d The answer is a flat No. The wattage you see on the PSUs casing or labels only indicates the maximum power it can supply to the system, theoretically. For example, by theory, a 500W PSU can supply a maximum of 500W to the computer. In reality, the PSU will draw a small portion of the power for itself and distributes power to each of the PC components according to its need. The amount of power the components need varies from 3.3V to 12V. If the total power of the components needs to add up to 250W, it would only use 250W of the 500W, giving you an overhead for additional components or future upgrades. Additionally, the amount of power the PSU supplies varies during peak periods and idle times. When the components are pushed to their limits, say when a video editor maximizes the GPU for graphics-intensive tasks, it would require more power than when the computer is used for simple tasks like web-browsing. The amount of power drawn from the PSU would depend on two things; the amount of power each component requires and the tasks that each component performs. Power supply efficiency \u2691 When PSU converts the AC power to DC, some of the power is wasted and is converted to heat. The more heat a PSU generates, the less efficient it is. Inefficient PSUs will likely damage the computer\u2019s components or shorten their lifespans in the long run. They also draw more power from the primary source resulting in higher electricity bills for consumers. You might\u2019ve seen 80 PLUS stickers on PSUs or its other variants like 80 PLUS Bronze, Silver, Gold, Platinum, and Titanium. 80 PLUS is the power supply\u2019s efficiency rating; the power supply must reach 80% efficiency to be certified. It\u2019s a voluntary standard, which means companies don\u2019t need to abide by the standard, but 80 PLUS certifications have become popular because a more efficient power supply can lessen the consumers\u2019 carbon footprint and help them save some bucks on their electric bills. Below is the efficiency rating that a PSU needs to achieve to get the desired rating. Certification Levels Efficiency at 10% Load Efficiency at 20% Load Efficiency at 50% Load Efficiency at 100% Load 80 PLUS (White) \u2014 80% 80% 80% 80 PLUS Bronze \u2014 82% 85% 82% 80 PLUS Silver \u2014 85% 88% 85% 80 PLUS Gold \u2014 87% 90% 87% 80 PLUS Platinum \u2014 90% 92% 89% 80 PLUS Titanium 90% 92% 94% 90% It\u2019s important to note that the 80% efficiency does not mean that the PSU will only supply 80% of its capacity to the computer. It means it will draw additional power from the primary source to only 20% of power is lost or generated as heat during the conversion. A 500W PSU will therefore draw 625W of power from the main to make it 80% efficient. Higher efficiency also means the internal components are subjected to less heat and are likely to have a longer lifespan. They may cost a bit more, but higher certified power supplies tend to be more reliable than others. Luckily, most manufacturers offer warranties. Power supply shopping tips \u2691 Determine wattage requirements : You don't need to purchase much more potential power capacity (wattage) than you\u2019ll ever use. You can calculate roughly how much power your new or upgraded system will draw from the wall and look for a capacity point that satisfies your demands. Several power supply sellers have calculators that will give you a rough estimate of your system's power needs. You can find a few below: PCPartPicker OuterVision PSU calculator be quiet! PSU Calculator Cooler Master Power Calculator Seasonic Wattage Calculator MSI PSU Calculator Newegg PSU Calculator Consider upcoming GPU power requirements : Although the best graphics cards are usually more power-efficient than previous generations, their power consumption increases overall. This is why the latest 12+4 pin connector that the upcoming generation graphics cards will use will provide up to 600 W of power. Currently, a pair of PCIe 6+2 pin connectors on dedicated cables are officially rated for up to 300W, and three of these connectors can deliver up to 450W safely. You should also add the up to 75W that the PCIe slot can provide in these numbers. What troubles today's power supplies is not the maximum sustained power consumption of a GPU but its power spikes, and this is why various manufacturers suggest strong PSUs for high-end graphics cards. If the PSU's over current and over power protection features are conservatively set, the PSU can shut down once the graphics card asks for increased power, even for very short periods ( nanoseconds range). This is why EVGA offers two different OPP features in its G6 and P6 units, called firmware and hardware OPP. The first triggers at lower loads, in the millisecond range, while the latter triggers at higher loads that last for some nanoseconds. This way, short power spikes from the graphics card are addressed without shutting down the system. If you add the increased power demands of modern high-end CPUs, you can quickly figure out why strong PSUs are necessary again. Please look at our GPU Benchmarks and CPU Benchmarks hierarchies to see how each of these chips perform relative to each other. Check the physical dimensions of your case before buying : If you have a standard ATX case, whether or not it is one of the best PC cases, an ATX power supply will fit. But many higher-wattage PSUs are longer than the typical 5.5 inches. So you'll want to be sure of your cases' PSU clearance. If you have an exceptionally small or slim PC case, it may require a less typical (and more compact) SFX power supply. Consider a modular power supply : If your case has lots of room behind the motherboard, or your chassis doesn't have a window or glass side, you can cable-wrap the wires you don't' need and stash them inside your rig. But if the system you're' building doesn't' have space for this, or there is no easy place to hide your cable mess, it's' worth paying extra for a modular power supply. Modular PSUs let you only plug in the power cables you need and leave the rest in the box. Market analysis \u2691 I'm searching for a power supply unit that can deliver 264W and can grow up to 373W. This means that the load is: Type 400W 450W 500W Min load 66% 59% 52% Max load 93% 83% 74% Given that PSU look to be more efficient when they have a load of 50%, the 450W or 500W would be better. Although if the efficiency goes over 80 PLUS Gold, the difference is almost negligible. I'd prioritize first the efficiency. But any PSU from 400W to 500W will work for me. Toms Hardware , PCGamer , IGN recommends: Corsair CX450: It has 80 PLUS Bronze, so I'll discard it XPG Pylon 450: It has 80 PLUS Bronze too... None of the suggested PSU are suited for my case. I'm going to select then which is the brands that are more suggested: Brand Tom's recommendations PCGamer recommendations IGN Corsair 6 2 3 Be quiet 1 1 1 Silverstone 1 1 1 Cooler Master 1 0 1 XPG 1 1 0 EVGA 1 0 0 Seasonic 0 1 0 It looks that the most popular are Corsair, Be quiet and Silverstone. Corsair doesn't have anyone with certification above Bronce. Silverstone Under or equal to 500 it has one gold, at 520 it has a platinum and on the 550W it has has 4 gold and another platinum. ET500-MG NJ520 Be quiet has both gold and platinum on the range of 550 and above. Gold and bronze on the 500-400W range. Pure Power 11 CM 500W Pure Power 11 500W SFX L Power 500W Straigt Power 11 450W It looks like I have to forget of efficiency above Gold unless I want to go with 520W. After a quick search on the provider I see that the most interesting in price are: Be Quiet! Straight Power 11 450W Be Quiet! Pure Power 11 CM 500W Be Quiet! Pure Power 11 500W I'm also going to trust Be Quiet on the CPU cooler so I'm happy with staying on the same brand for all fans. Type 400W 450W 500W Min load 66% 59% 52% Max load 93% 83% 74% Model Pure Power 11 CM 500W Pure Power 11 500W Straight Power 11 450W Continuous Power 500 500 450 Peak Power 550 550 500 Min load (my case) 52% 52% 59% Max load (my case) 74% 74% 83% Topology Active Clamp / SR / DC/DC Active Clamp / SR / DC/DC LLC / SR / DC/DC Fan dB(A) at 20% 9 9.3 9.4 Fan dB(A) at 50% 9.3 9.6 9.8 Fan dB(A) at 100% 18.8 21.6 12.3 dB(A) Min load 9.68 - 10.25 dB(A) Max load 13.86 - 11.45 SIP Protection Yes No Yes Efficiency Cert Gold Gold Gold Efficiency 20% 90.6 88.2 91 Efficiency 50% 92.1 91.3 93.1 Efficiency 100% 90.1 89.9 91.7 Cable management Semi-modular Fixed Modular Cable cm to mb 55 55 60 Max cable length 95 95 115 No. of cables 7 7 8 ATX-MB (20+4-pin) 1 1 1 P4+4 (CPU) 1 1 1 PCI-e 6+2 (GPU) 2 2 2 SATA 6 6 8 Dimensions 160 x 150 x 86 150 x 150 x 86 160 x 150 x 86 Warranty (Years) 5 5 5 Price (EUR) 85.93 79.02 99.60 I'd discard the Pure Power 11 500W because it: Has significantly worse efficiency Doesn't have SIP protection The fan is the loudest Between the other two Straight Power 11 has the advantages: 1% more efficiency. Will make 0.5dB more noise at min load but 2.41dB less at max load. So I expect it to be more silent Cables look better Has more cable length Has more SATA cables (equal to my number of drives) And the disadvantages: Is 13.66 EUR more expensive Has 50W less of power It doesn't look like I'm going to need the extra power, and if I need it (if I add a graphic card) then the 500W wouldn't work either. And the difference in money is not that big. Therefore I'll go with the Straight Power 11 450W References \u2691 Linuxhint article on PSU", "title": "Power Supply Unit"}, {"location": "psu/#power-supply-efficiency", "text": "When PSU converts the AC power to DC, some of the power is wasted and is converted to heat. The more heat a PSU generates, the less efficient it is. Inefficient PSUs will likely damage the computer\u2019s components or shorten their lifespans in the long run. They also draw more power from the primary source resulting in higher electricity bills for consumers. You might\u2019ve seen 80 PLUS stickers on PSUs or its other variants like 80 PLUS Bronze, Silver, Gold, Platinum, and Titanium. 80 PLUS is the power supply\u2019s efficiency rating; the power supply must reach 80% efficiency to be certified. It\u2019s a voluntary standard, which means companies don\u2019t need to abide by the standard, but 80 PLUS certifications have become popular because a more efficient power supply can lessen the consumers\u2019 carbon footprint and help them save some bucks on their electric bills. Below is the efficiency rating that a PSU needs to achieve to get the desired rating. Certification Levels Efficiency at 10% Load Efficiency at 20% Load Efficiency at 50% Load Efficiency at 100% Load 80 PLUS (White) \u2014 80% 80% 80% 80 PLUS Bronze \u2014 82% 85% 82% 80 PLUS Silver \u2014 85% 88% 85% 80 PLUS Gold \u2014 87% 90% 87% 80 PLUS Platinum \u2014 90% 92% 89% 80 PLUS Titanium 90% 92% 94% 90% It\u2019s important to note that the 80% efficiency does not mean that the PSU will only supply 80% of its capacity to the computer. It means it will draw additional power from the primary source to only 20% of power is lost or generated as heat during the conversion. A 500W PSU will therefore draw 625W of power from the main to make it 80% efficient. Higher efficiency also means the internal components are subjected to less heat and are likely to have a longer lifespan. They may cost a bit more, but higher certified power supplies tend to be more reliable than others. Luckily, most manufacturers offer warranties.", "title": "Power supply efficiency"}, {"location": "psu/#power-supply-shopping-tips", "text": "Determine wattage requirements : You don't need to purchase much more potential power capacity (wattage) than you\u2019ll ever use. You can calculate roughly how much power your new or upgraded system will draw from the wall and look for a capacity point that satisfies your demands. Several power supply sellers have calculators that will give you a rough estimate of your system's power needs. You can find a few below: PCPartPicker OuterVision PSU calculator be quiet! PSU Calculator Cooler Master Power Calculator Seasonic Wattage Calculator MSI PSU Calculator Newegg PSU Calculator Consider upcoming GPU power requirements : Although the best graphics cards are usually more power-efficient than previous generations, their power consumption increases overall. This is why the latest 12+4 pin connector that the upcoming generation graphics cards will use will provide up to 600 W of power. Currently, a pair of PCIe 6+2 pin connectors on dedicated cables are officially rated for up to 300W, and three of these connectors can deliver up to 450W safely. You should also add the up to 75W that the PCIe slot can provide in these numbers. What troubles today's power supplies is not the maximum sustained power consumption of a GPU but its power spikes, and this is why various manufacturers suggest strong PSUs for high-end graphics cards. If the PSU's over current and over power protection features are conservatively set, the PSU can shut down once the graphics card asks for increased power, even for very short periods ( nanoseconds range). This is why EVGA offers two different OPP features in its G6 and P6 units, called firmware and hardware OPP. The first triggers at lower loads, in the millisecond range, while the latter triggers at higher loads that last for some nanoseconds. This way, short power spikes from the graphics card are addressed without shutting down the system. If you add the increased power demands of modern high-end CPUs, you can quickly figure out why strong PSUs are necessary again. Please look at our GPU Benchmarks and CPU Benchmarks hierarchies to see how each of these chips perform relative to each other. Check the physical dimensions of your case before buying : If you have a standard ATX case, whether or not it is one of the best PC cases, an ATX power supply will fit. But many higher-wattage PSUs are longer than the typical 5.5 inches. So you'll want to be sure of your cases' PSU clearance. If you have an exceptionally small or slim PC case, it may require a less typical (and more compact) SFX power supply. Consider a modular power supply : If your case has lots of room behind the motherboard, or your chassis doesn't have a window or glass side, you can cable-wrap the wires you don't' need and stash them inside your rig. But if the system you're' building doesn't' have space for this, or there is no easy place to hide your cable mess, it's' worth paying extra for a modular power supply. Modular PSUs let you only plug in the power cables you need and leave the rest in the box.", "title": "Power supply shopping tips"}, {"location": "psu/#market-analysis", "text": "I'm searching for a power supply unit that can deliver 264W and can grow up to 373W. This means that the load is: Type 400W 450W 500W Min load 66% 59% 52% Max load 93% 83% 74% Given that PSU look to be more efficient when they have a load of 50%, the 450W or 500W would be better. Although if the efficiency goes over 80 PLUS Gold, the difference is almost negligible. I'd prioritize first the efficiency. But any PSU from 400W to 500W will work for me. Toms Hardware , PCGamer , IGN recommends: Corsair CX450: It has 80 PLUS Bronze, so I'll discard it XPG Pylon 450: It has 80 PLUS Bronze too... None of the suggested PSU are suited for my case. I'm going to select then which is the brands that are more suggested: Brand Tom's recommendations PCGamer recommendations IGN Corsair 6 2 3 Be quiet 1 1 1 Silverstone 1 1 1 Cooler Master 1 0 1 XPG 1 1 0 EVGA 1 0 0 Seasonic 0 1 0 It looks that the most popular are Corsair, Be quiet and Silverstone. Corsair doesn't have anyone with certification above Bronce. Silverstone Under or equal to 500 it has one gold, at 520 it has a platinum and on the 550W it has has 4 gold and another platinum. ET500-MG NJ520 Be quiet has both gold and platinum on the range of 550 and above. Gold and bronze on the 500-400W range. Pure Power 11 CM 500W Pure Power 11 500W SFX L Power 500W Straigt Power 11 450W It looks like I have to forget of efficiency above Gold unless I want to go with 520W. After a quick search on the provider I see that the most interesting in price are: Be Quiet! Straight Power 11 450W Be Quiet! Pure Power 11 CM 500W Be Quiet! Pure Power 11 500W I'm also going to trust Be Quiet on the CPU cooler so I'm happy with staying on the same brand for all fans. Type 400W 450W 500W Min load 66% 59% 52% Max load 93% 83% 74% Model Pure Power 11 CM 500W Pure Power 11 500W Straight Power 11 450W Continuous Power 500 500 450 Peak Power 550 550 500 Min load (my case) 52% 52% 59% Max load (my case) 74% 74% 83% Topology Active Clamp / SR / DC/DC Active Clamp / SR / DC/DC LLC / SR / DC/DC Fan dB(A) at 20% 9 9.3 9.4 Fan dB(A) at 50% 9.3 9.6 9.8 Fan dB(A) at 100% 18.8 21.6 12.3 dB(A) Min load 9.68 - 10.25 dB(A) Max load 13.86 - 11.45 SIP Protection Yes No Yes Efficiency Cert Gold Gold Gold Efficiency 20% 90.6 88.2 91 Efficiency 50% 92.1 91.3 93.1 Efficiency 100% 90.1 89.9 91.7 Cable management Semi-modular Fixed Modular Cable cm to mb 55 55 60 Max cable length 95 95 115 No. of cables 7 7 8 ATX-MB (20+4-pin) 1 1 1 P4+4 (CPU) 1 1 1 PCI-e 6+2 (GPU) 2 2 2 SATA 6 6 8 Dimensions 160 x 150 x 86 150 x 150 x 86 160 x 150 x 86 Warranty (Years) 5 5 5 Price (EUR) 85.93 79.02 99.60 I'd discard the Pure Power 11 500W because it: Has significantly worse efficiency Doesn't have SIP protection The fan is the loudest Between the other two Straight Power 11 has the advantages: 1% more efficiency. Will make 0.5dB more noise at min load but 2.41dB less at max load. So I expect it to be more silent Cables look better Has more cable length Has more SATA cables (equal to my number of drives) And the disadvantages: Is 13.66 EUR more expensive Has 50W less of power It doesn't look like I'm going to need the extra power, and if I need it (if I add a graphic card) then the 500W wouldn't work either. And the difference in money is not that big. Therefore I'll go with the Straight Power 11 450W", "title": "Market analysis"}, {"location": "psu/#references", "text": "Linuxhint article on PSU", "title": "References"}, {"location": "pydantic_factories/", "text": "Pydantic factories is a library offers powerful mock data generation capabilities for pydantic based models and dataclasses. It automatically creates FactoryBoy factories from a pydantic model. Example \u2691 from datetime import date , datetime from typing import List , Union from pydantic import BaseModel , UUID4 from pydantic_factories import ModelFactory class Person ( BaseModel ): id : UUID4 name : str hobbies : List [ str ] age : Union [ float , int ] birthday : Union [ datetime , date ] class PersonFactory ( ModelFactory [ Any ]): __model__ = Person result = PersonFactory . build () This is possible because of the typing information available on the pydantic model and model-fields, which are used as a source of truth for data generation. The factory parses the information stored in the pydantic model and generates a dictionary of kwargs that are passed to the Person class' init method. Installation \u2691 pip install pydantic-factories Basic Usage \u2691 Build Methods \u2691 The ModelFactory class exposes two build methods: .build(**kwargs) : builds a single instance of the factory's model. .batch(size: int, **kwargs) : build a list of size n instances. result = PersonFactory . build () # a single Person instance result = PersonFactory . batch ( size = 5 ) # list[Person, Person, Person, Person, Person] Any kwargs you pass to .build , .batch or any of the persistence methods, will take precedence over whatever defaults are defined on the factory class itself. Nested Models and Complex types \u2691 The automatic generation of mock data works for all types supported by pydantic, as well as nested classes that derive from BaseModel (including for 3 rd party libraries) and complex types. Defining Factory Attributes \u2691 The factory api is designed to be as semantic and simple as possible, lets look at several examples that assume we have the following models: from datetime import date , datetime from enum import Enum from pydantic import BaseModel , UUID4 from typing import Any , Dict , List , Union class Species ( str , Enum ): CAT = \"Cat\" DOG = \"Dog\" class Pet ( BaseModel ): name : str species : Species class Person ( BaseModel ): id : UUID4 name : str hobbies : List [ str ] age : Union [ float , int ] birthday : Union [ datetime , date ] pets : List [ Pet ] assets : List [ Dict [ str , Dict [ str , Any ]]] One way of defining defaults is to use hardcoded values: pet = Pet ( name = \"Roxy\" , sound = \"woof woof\" , species = Species . DOG ) class PersonFactory ( ModelFactory ): __model__ = Person pets = [ pet ] In this case when we call PersonFactory.build() the result will be randomly generated, except the pets list, which will be the hardcoded default we defined. Use field \u2691 This though is often not desirable. We could instead, define a factory for Pet where we restrict the choices to a range we like. For example: from enum import Enum from pydantic_factories import ModelFactory , Use from random import choice from .models import Pet , Person class Species ( str , Enum ): CAT = \"Cat\" DOG = \"Dog\" class PetFactory ( ModelFactory ): __model__ = Pet name = Use ( choice , [ \"Ralph\" , \"Roxy\" ]) species = Use ( choice , list ( Species )) class PersonFactory ( ModelFactory ): __model__ = Person pets = Use ( PetFactory . batch , size = 2 ) The signature for use is: cb: Callable, *args, **defaults , it can receive any sync callable. In the above example, we used the choice function from the standard library's random package, and the batch method of PetFactory . You do not need to use the Use field, you can place callables (including classes) as values for a factory's attribute directly, and these will be invoked at build-time. Thus, you could for example re-write the above PetFactory like so: class PetFactory ( ModelFactory ): __model__ = Pet name = lambda : choice ([ \"Ralph\" , \"Roxy\" ]) species = lambda : choice ( list ( Species )) Use is merely a semantic abstraction that makes the factory cleaner and simpler to understand. Ignore (field) \u2691 Ignore is another field exported by this library, and its used - as its name implies - to designate a given attribute as ignored: from typing import TypeVar from odmantic import EmbeddedModel , Model from pydantic_factories import ModelFactory , Ignore T = TypeVar ( \"T\" , Model , EmbeddedModel ) class OdmanticModelFactory ( ModelFactory [ T ]): id = Ignore () The above example is basically the extension included in pydantic-factories for the library ODMantic, which is a pydantic based mongo ODM. For ODMantic models, the id attribute should not be set by the factory, but rather handled by the odmantic logic itself. Thus, the id field is marked as ignored. When you ignore an attribute using Ignore , it will be completely ignored by the factory - that is, it will not be set as a kwarg passed to pydantic at all. Require (field) \u2691 The Require field in turn specifies that a particular attribute is a required kwarg. That is, if a kwarg with a value for this particular attribute is not passed when calling factory.build() , a MissingBuildKwargError will be raised. What is the use case for this? For example, lets say we have a document called Article which we store in some DB and is represented using a non-pydantic model. We then need to store in our pydantic object a reference to an id for this article. This value should not be some mock value, but must rather be an actual id passed to the factory. Thus, we can define this attribute as required: from pydantic import BaseModel from pydantic_factories import ModelFactory , Require from uuid import UUID class ArticleProxy ( BaseModel ): article_id : UUID ... class ArticleProxyFactory ( ModelFactory ): __model__ = ArticleProxy article_id = Require () If we call factory.build() without passing a value for article_id , an error will be raised. [Creating your custom \u2691 factories]( https://starlite-api.github.io/pydantic-factories/usage/7-handling-custom-types/?h=custom ) If your model has an attribute that is not supported by pydantic-factories and it depends on third party libraries, you can create your custom extension subclassing the ModelFactory , and overriding the get_mock_value method to add your logic. from pydantic_factories import ModelFactory class CustomFactory(ModelFactory[Any]): \"\"\"Tweak the ModelFactory to add our custom mocks.\"\"\" @classmethod def get_mock_value(cls, field_type: Any) -> Any: \"\"\"Add our custom mock value.\"\"\" if str(field_type) == \"my_super_rare_datetime_field\": return cls._get_faker().date_time_between() return super().get_mock_value(field_type) Where cls._get_faker() is a faker instance that you can use to build your returned value. Troubleshooting \u2691 Use pydantic-factories with pytest-freezegun \u2691 As pytest-freezegun overrides the datetime.datetime class, pydantic-factories is not able to find the correct generator for the datetime fields. It's solved by creating a custom factory from typing import Any from pydantic_factories import ModelFactory class CustomFactory ( ModelFactory [ Any ]): \"\"\"Tweak the ModelFactory to add our custom mocks.\"\"\" @classmethod def get_mock_value ( cls , field_type : Any ) -> Any : \"\"\"Add our custom mock value.\"\"\" if str ( field_type ) == \"<class 'datetime.datetime'>\" : return cls . _get_faker () . date_time_between () return super () . get_mock_value ( field_type ) References \u2691 Git Docs", "title": "Pydantic Factories"}, {"location": "pydantic_factories/#example", "text": "from datetime import date , datetime from typing import List , Union from pydantic import BaseModel , UUID4 from pydantic_factories import ModelFactory class Person ( BaseModel ): id : UUID4 name : str hobbies : List [ str ] age : Union [ float , int ] birthday : Union [ datetime , date ] class PersonFactory ( ModelFactory [ Any ]): __model__ = Person result = PersonFactory . build () This is possible because of the typing information available on the pydantic model and model-fields, which are used as a source of truth for data generation. The factory parses the information stored in the pydantic model and generates a dictionary of kwargs that are passed to the Person class' init method.", "title": "Example"}, {"location": "pydantic_factories/#installation", "text": "pip install pydantic-factories", "title": "Installation"}, {"location": "pydantic_factories/#basic-usage", "text": "", "title": "Basic Usage"}, {"location": "pydantic_factories/#build-methods", "text": "The ModelFactory class exposes two build methods: .build(**kwargs) : builds a single instance of the factory's model. .batch(size: int, **kwargs) : build a list of size n instances. result = PersonFactory . build () # a single Person instance result = PersonFactory . batch ( size = 5 ) # list[Person, Person, Person, Person, Person] Any kwargs you pass to .build , .batch or any of the persistence methods, will take precedence over whatever defaults are defined on the factory class itself.", "title": "Build Methods"}, {"location": "pydantic_factories/#nested-models-and-complex-types", "text": "The automatic generation of mock data works for all types supported by pydantic, as well as nested classes that derive from BaseModel (including for 3 rd party libraries) and complex types.", "title": "Nested Models and Complex types"}, {"location": "pydantic_factories/#defining-factory-attributes", "text": "The factory api is designed to be as semantic and simple as possible, lets look at several examples that assume we have the following models: from datetime import date , datetime from enum import Enum from pydantic import BaseModel , UUID4 from typing import Any , Dict , List , Union class Species ( str , Enum ): CAT = \"Cat\" DOG = \"Dog\" class Pet ( BaseModel ): name : str species : Species class Person ( BaseModel ): id : UUID4 name : str hobbies : List [ str ] age : Union [ float , int ] birthday : Union [ datetime , date ] pets : List [ Pet ] assets : List [ Dict [ str , Dict [ str , Any ]]] One way of defining defaults is to use hardcoded values: pet = Pet ( name = \"Roxy\" , sound = \"woof woof\" , species = Species . DOG ) class PersonFactory ( ModelFactory ): __model__ = Person pets = [ pet ] In this case when we call PersonFactory.build() the result will be randomly generated, except the pets list, which will be the hardcoded default we defined.", "title": "Defining Factory Attributes"}, {"location": "pydantic_factories/#use-field", "text": "This though is often not desirable. We could instead, define a factory for Pet where we restrict the choices to a range we like. For example: from enum import Enum from pydantic_factories import ModelFactory , Use from random import choice from .models import Pet , Person class Species ( str , Enum ): CAT = \"Cat\" DOG = \"Dog\" class PetFactory ( ModelFactory ): __model__ = Pet name = Use ( choice , [ \"Ralph\" , \"Roxy\" ]) species = Use ( choice , list ( Species )) class PersonFactory ( ModelFactory ): __model__ = Person pets = Use ( PetFactory . batch , size = 2 ) The signature for use is: cb: Callable, *args, **defaults , it can receive any sync callable. In the above example, we used the choice function from the standard library's random package, and the batch method of PetFactory . You do not need to use the Use field, you can place callables (including classes) as values for a factory's attribute directly, and these will be invoked at build-time. Thus, you could for example re-write the above PetFactory like so: class PetFactory ( ModelFactory ): __model__ = Pet name = lambda : choice ([ \"Ralph\" , \"Roxy\" ]) species = lambda : choice ( list ( Species )) Use is merely a semantic abstraction that makes the factory cleaner and simpler to understand.", "title": "Use field"}, {"location": "pydantic_factories/#ignore-field", "text": "Ignore is another field exported by this library, and its used - as its name implies - to designate a given attribute as ignored: from typing import TypeVar from odmantic import EmbeddedModel , Model from pydantic_factories import ModelFactory , Ignore T = TypeVar ( \"T\" , Model , EmbeddedModel ) class OdmanticModelFactory ( ModelFactory [ T ]): id = Ignore () The above example is basically the extension included in pydantic-factories for the library ODMantic, which is a pydantic based mongo ODM. For ODMantic models, the id attribute should not be set by the factory, but rather handled by the odmantic logic itself. Thus, the id field is marked as ignored. When you ignore an attribute using Ignore , it will be completely ignored by the factory - that is, it will not be set as a kwarg passed to pydantic at all.", "title": "Ignore (field)"}, {"location": "pydantic_factories/#require-field", "text": "The Require field in turn specifies that a particular attribute is a required kwarg. That is, if a kwarg with a value for this particular attribute is not passed when calling factory.build() , a MissingBuildKwargError will be raised. What is the use case for this? For example, lets say we have a document called Article which we store in some DB and is represented using a non-pydantic model. We then need to store in our pydantic object a reference to an id for this article. This value should not be some mock value, but must rather be an actual id passed to the factory. Thus, we can define this attribute as required: from pydantic import BaseModel from pydantic_factories import ModelFactory , Require from uuid import UUID class ArticleProxy ( BaseModel ): article_id : UUID ... class ArticleProxyFactory ( ModelFactory ): __model__ = ArticleProxy article_id = Require () If we call factory.build() without passing a value for article_id , an error will be raised.", "title": "Require (field)"}, {"location": "pydantic_factories/#creating-your-custom", "text": "factories]( https://starlite-api.github.io/pydantic-factories/usage/7-handling-custom-types/?h=custom ) If your model has an attribute that is not supported by pydantic-factories and it depends on third party libraries, you can create your custom extension subclassing the ModelFactory , and overriding the get_mock_value method to add your logic. from pydantic_factories import ModelFactory class CustomFactory(ModelFactory[Any]): \"\"\"Tweak the ModelFactory to add our custom mocks.\"\"\" @classmethod def get_mock_value(cls, field_type: Any) -> Any: \"\"\"Add our custom mock value.\"\"\" if str(field_type) == \"my_super_rare_datetime_field\": return cls._get_faker().date_time_between() return super().get_mock_value(field_type) Where cls._get_faker() is a faker instance that you can use to build your returned value.", "title": "[Creating your custom"}, {"location": "pydantic_factories/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "pydantic_factories/#use-pydantic-factories-with-pytest-freezegun", "text": "As pytest-freezegun overrides the datetime.datetime class, pydantic-factories is not able to find the correct generator for the datetime fields. It's solved by creating a custom factory from typing import Any from pydantic_factories import ModelFactory class CustomFactory ( ModelFactory [ Any ]): \"\"\"Tweak the ModelFactory to add our custom mocks.\"\"\" @classmethod def get_mock_value ( cls , field_type : Any ) -> Any : \"\"\"Add our custom mock value.\"\"\" if str ( field_type ) == \"<class 'datetime.datetime'>\" : return cls . _get_faker () . date_time_between () return super () . get_mock_value ( field_type )", "title": "Use pydantic-factories with pytest-freezegun"}, {"location": "pydantic_factories/#references", "text": "Git Docs", "title": "References"}, {"location": "pyment/", "text": "Pyment is a python3 program to automatically create, update or convert docstrings in existing Python files, managing several styles. Installation \u2691 pip install pyment Usage \u2691 $: pyment myfile.py # will generate a patch $: pyment -w myfile.py # will overwrite the file As of 2021-11-17, the program is not production ready yet for me, I've tested it in one of my projects and found some bugs that needed to be fixed before it's usable. Despite the number of stars, it looks like the development pace has dropped dramatically, so it needs our help to get better :). References \u2691 Git", "title": "Pyment"}, {"location": "pyment/#installation", "text": "pip install pyment", "title": "Installation"}, {"location": "pyment/#usage", "text": "$: pyment myfile.py # will generate a patch $: pyment -w myfile.py # will overwrite the file As of 2021-11-17, the program is not production ready yet for me, I've tested it in one of my projects and found some bugs that needed to be fixed before it's usable. Despite the number of stars, it looks like the development pace has dropped dramatically, so it needs our help to get better :).", "title": "Usage"}, {"location": "pyment/#references", "text": "Git", "title": "References"}, {"location": "pytest_httpserver/", "text": "pytest-httpserver is a python package which allows you to start a real HTTP server for your tests. The server can be configured programmatically to how to respond to requests. Installation \u2691 pip install pytest-httpserver Usage \u2691 import requests def test_json_client ( httpserver : HTTPServer ): httpserver . expect_request ( \"/foobar\" ) . respond_with_json ({ \"foo\" : \"bar\" }) assert requests . get ( httpserver . url_for ( \"/foobar\" )) . json () == { 'foo' : 'bar' } Specifying responses \u2691 Once you have set up the expected request, it is required to set up the response which will be returned to the client. In the example we used respond_with_json() but it is also possible to respond with an arbitrary content. respond_with_data ( \"Hello world!\" , content_type = \"text/plain\" ) In the example above, we are responding a text/plain content. You can specify the status also: respond_with_data ( \"Not found\" , status = 404 , content_type = \"text/plain\" ) Give a dynamic response \u2691 If you need to produce dynamic content, use the respond_with_handler method, which accepts a callable (eg. a python function): from werkzeug.wrappers import Response from werkzeug.wrappers.request import Request def my_handler ( request : Request ) -> Response : # here, examine the request object return Response ( \"Hello world!\" ) respond_with_handler ( my_handler ) References \u2691 Docs", "title": "Pytest-HttpServer"}, {"location": "pytest_httpserver/#installation", "text": "pip install pytest-httpserver", "title": "Installation"}, {"location": "pytest_httpserver/#usage", "text": "import requests def test_json_client ( httpserver : HTTPServer ): httpserver . expect_request ( \"/foobar\" ) . respond_with_json ({ \"foo\" : \"bar\" }) assert requests . get ( httpserver . url_for ( \"/foobar\" )) . json () == { 'foo' : 'bar' }", "title": "Usage"}, {"location": "pytest_httpserver/#specifying-responses", "text": "Once you have set up the expected request, it is required to set up the response which will be returned to the client. In the example we used respond_with_json() but it is also possible to respond with an arbitrary content. respond_with_data ( \"Hello world!\" , content_type = \"text/plain\" ) In the example above, we are responding a text/plain content. You can specify the status also: respond_with_data ( \"Not found\" , status = 404 , content_type = \"text/plain\" )", "title": "Specifying responses"}, {"location": "pytest_httpserver/#give-a-dynamic-response", "text": "If you need to produce dynamic content, use the respond_with_handler method, which accepts a callable (eg. a python function): from werkzeug.wrappers import Response from werkzeug.wrappers.request import Request def my_handler ( request : Request ) -> Response : # here, examine the request object return Response ( \"Hello world!\" ) respond_with_handler ( my_handler )", "title": "Give a dynamic response"}, {"location": "pytest_httpserver/#references", "text": "Docs", "title": "References"}, {"location": "python/", "text": "Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant indentation. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects. Install \u2691 apt-get install python Install a specific version \u2691 Install dependencies sudo apt install wget software-properties-common build-essential libnss3-dev zlib1g-dev libgdbm-dev libncurses5-dev libssl-dev libffi-dev libreadline-dev libsqlite3-dev libbz2-dev Select the version in https://www.python.org/ftp/python/ and download it wget https://www.python.org/ftp/python/3.9.2/Python-3.9.2.tgz cd Python-3.9.2/ ./configure --enable-optimizations sudo make altinstall Generators \u2691 Generator functions are a special kind of function that return a lazy iterator. These are objects that you can loop over like a list. However, unlike lists, lazy iterators do not store their contents in memory. An example would be an infinite sequence generator def infinite_sequence (): num = 0 while True : yield num num += 1 You can use it as a list: for i in infinite_sequence (): ... print ( i , end = \" \" ) ... 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 [ ... ] Instead of using a for loop, you can also call next() on the generator object directly. This is especially useful for testing a generator in the console:. >>> gen = infinite_sequence () >>> next ( gen ) 0 >>> next ( gen ) 1 >>> next ( gen ) 2 >>> next ( gen ) 3 Understanding Generators \u2691 Generator functions look and act just like regular functions, but with one defining characteristic. Generator functions use the Python yield keyword instead of return . yield indicates where a value is sent back to the caller, but unlike return , you don\u2019t exit the function afterward.Instead, the state of the function is remembered. That way, when next() is called on a generator object (either explicitly or implicitly within a for loop), the previously yielded variable num is incremented, and then yielded again. Interesting libraries to explore \u2691 di : a modern dependency injection system, modeled around the simplicity of FastAPI's dependency injection. humanize : This modest package contains various common humanization utilities, like turning a number into a fuzzy human-readable duration (\"3 minutes ago\") or into a human-readable size or throughput. tryceratops : A linter of exceptions. schedule : Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax. huey : a little task queue for python. textual : Textual is a TUI (Text User Interface) framework for Python using Rich as a renderer. parso : Parses Python code. kivi : Create android/Linux/iOS/Windows applications with python. Use it with kivimd to make it beautiful, check the examples and the docs . For beginner tutorials check the real python's and towards data science (and part 2 ). * apprise : Allows you to send a notification to almost all of the most popular notification services available to us today such as: Linux, Telegram, Discord, Slack, Amazon SNS, Gotify, etc. Look at all the supported notifications (\u00ac\u00ba-\u00b0)\u00ac . * aiomultiprocess : Presents a simple interface, while running a full AsyncIO event loop on each child process, enabling levels of concurrency never before seen in a Python application. Each child process can execute multiple coroutines at once, limited only by the workload and number of cores available. * twint : An advanced Twitter scraping & OSINT tool written in Python that doesn't use Twitter's API, allowing you to scrape a user's followers, following, Tweets and more while evading most API limitations. Maybe use snscrape (is below) if twint doesn't work. * snscrape : A social networking service scraper in Python. * tweepy : Twitter for Python. Interesting sources \u2691 Musa 550 looks like a nice way to learn how to process geolocation data.", "title": "Python"}, {"location": "python/#install", "text": "apt-get install python", "title": "Install"}, {"location": "python/#install-a-specific-version", "text": "Install dependencies sudo apt install wget software-properties-common build-essential libnss3-dev zlib1g-dev libgdbm-dev libncurses5-dev libssl-dev libffi-dev libreadline-dev libsqlite3-dev libbz2-dev Select the version in https://www.python.org/ftp/python/ and download it wget https://www.python.org/ftp/python/3.9.2/Python-3.9.2.tgz cd Python-3.9.2/ ./configure --enable-optimizations sudo make altinstall", "title": "Install a specific version"}, {"location": "python/#generators", "text": "Generator functions are a special kind of function that return a lazy iterator. These are objects that you can loop over like a list. However, unlike lists, lazy iterators do not store their contents in memory. An example would be an infinite sequence generator def infinite_sequence (): num = 0 while True : yield num num += 1 You can use it as a list: for i in infinite_sequence (): ... print ( i , end = \" \" ) ... 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 [ ... ] Instead of using a for loop, you can also call next() on the generator object directly. This is especially useful for testing a generator in the console:. >>> gen = infinite_sequence () >>> next ( gen ) 0 >>> next ( gen ) 1 >>> next ( gen ) 2 >>> next ( gen ) 3", "title": "Generators"}, {"location": "python/#understanding-generators", "text": "Generator functions look and act just like regular functions, but with one defining characteristic. Generator functions use the Python yield keyword instead of return . yield indicates where a value is sent back to the caller, but unlike return , you don\u2019t exit the function afterward.Instead, the state of the function is remembered. That way, when next() is called on a generator object (either explicitly or implicitly within a for loop), the previously yielded variable num is incremented, and then yielded again.", "title": "Understanding Generators"}, {"location": "python/#interesting-libraries-to-explore", "text": "di : a modern dependency injection system, modeled around the simplicity of FastAPI's dependency injection. humanize : This modest package contains various common humanization utilities, like turning a number into a fuzzy human-readable duration (\"3 minutes ago\") or into a human-readable size or throughput. tryceratops : A linter of exceptions. schedule : Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax. huey : a little task queue for python. textual : Textual is a TUI (Text User Interface) framework for Python using Rich as a renderer. parso : Parses Python code. kivi : Create android/Linux/iOS/Windows applications with python. Use it with kivimd to make it beautiful, check the examples and the docs . For beginner tutorials check the real python's and towards data science (and part 2 ). * apprise : Allows you to send a notification to almost all of the most popular notification services available to us today such as: Linux, Telegram, Discord, Slack, Amazon SNS, Gotify, etc. Look at all the supported notifications (\u00ac\u00ba-\u00b0)\u00ac . * aiomultiprocess : Presents a simple interface, while running a full AsyncIO event loop on each child process, enabling levels of concurrency never before seen in a Python application. Each child process can execute multiple coroutines at once, limited only by the workload and number of cores available. * twint : An advanced Twitter scraping & OSINT tool written in Python that doesn't use Twitter's API, allowing you to scrape a user's followers, following, Tweets and more while evading most API limitations. Maybe use snscrape (is below) if twint doesn't work. * snscrape : A social networking service scraper in Python. * tweepy : Twitter for Python.", "title": "Interesting libraries to explore"}, {"location": "python/#interesting-sources", "text": "Musa 550 looks like a nice way to learn how to process geolocation data.", "title": "Interesting sources"}, {"location": "python_elasticsearch/", "text": "Python elasticsearch is the Official low-level client for Elasticsearch. Its goal is to provide common ground for all Elasticsearch-related code in Python; because of this it tries to be opinion-free and very extendable. Installation \u2691 pip install elasticsearch Usage \u2691 Connect to the database \u2691 from elasticsearch import Elasticsearch client = Elasticsearch ( \"http://localhost:9200\" ) Get all indices \u2691 client . indices . get ( index = \"*\" ) Get all documents \u2691 resp = client . search ( index = \"test-index\" , query = { \"match_all\" : {}}) documents = resp . body [ \"hits\" ][ \"hits\" ] Use pprint to analyze the content of each document . Update a document \u2691 doc = { \"partial_document\" : \"value\" } resp = client . update ( index = INDEX , id = id_ , doc = doc ) References \u2691 Docs Source TowardsDataScience article", "title": "Elasticsearch"}, {"location": "python_elasticsearch/#installation", "text": "pip install elasticsearch", "title": "Installation"}, {"location": "python_elasticsearch/#usage", "text": "", "title": "Usage"}, {"location": "python_elasticsearch/#connect-to-the-database", "text": "from elasticsearch import Elasticsearch client = Elasticsearch ( \"http://localhost:9200\" )", "title": "Connect to the database"}, {"location": "python_elasticsearch/#get-all-indices", "text": "client . indices . get ( index = \"*\" )", "title": "Get all indices"}, {"location": "python_elasticsearch/#get-all-documents", "text": "resp = client . search ( index = \"test-index\" , query = { \"match_all\" : {}}) documents = resp . body [ \"hits\" ][ \"hits\" ] Use pprint to analyze the content of each document .", "title": "Get all documents"}, {"location": "python_elasticsearch/#update-a-document", "text": "doc = { \"partial_document\" : \"value\" } resp = client . update ( index = INDEX , id = id_ , doc = doc )", "title": "Update a document"}, {"location": "python_elasticsearch/#references", "text": "Docs Source TowardsDataScience article", "title": "References"}, {"location": "python_gnupg/", "text": "python-gnupg is a Python library to interact with gpg taking care of the internal details and allows its users to generate and manage keys, encrypt and decrypt data, and sign and verify messages. Installation \u2691 pip install python-gnupg Usage \u2691 You interface to the GnuPG functionality through an instance of the GPG class: gpg = gnupg . GPG ( gnupghome = \"/path/to/home/directory\" ) Decrypt a file: gpg . decrypt_file ( \"path/to/file\" ) Note: You can't pass Path arguments to decrypt_file . List private keys : >>> public_keys = gpg . list_keys () >>> private_keys = gpg . list_keys ( True ) References \u2691 Docs Source Issues", "title": "python-gnupg"}, {"location": "python_gnupg/#installation", "text": "pip install python-gnupg", "title": "Installation"}, {"location": "python_gnupg/#usage", "text": "You interface to the GnuPG functionality through an instance of the GPG class: gpg = gnupg . GPG ( gnupghome = \"/path/to/home/directory\" ) Decrypt a file: gpg . decrypt_file ( \"path/to/file\" ) Note: You can't pass Path arguments to decrypt_file . List private keys : >>> public_keys = gpg . list_keys () >>> private_keys = gpg . list_keys ( True )", "title": "Usage"}, {"location": "python_gnupg/#references", "text": "Docs Source Issues", "title": "References"}, {"location": "python_internationalization/", "text": "To make your code accessible to more people, you may want to support more than one language. It's not as easy as it looks as it's not enough to translate it but also it must look and feel local. The answer is internationalization. Internationalization (numeronymed as i18n) can be defined as the design process that ensures a program can be adapted to various languages and regions without requiring engineering changes to the source code. Common internationalization tasks include: Facilitating compliance with Unicode. Minimizing the use of concatenated strings. Accommodating support for double-byte languages (e.g. Japanese) and right-to-left languages (for example, Hebrew). Avoiding hard-coded text. Designing for independence from cultural conventions (e. g., date and time displays), limiting language, and character sets. Localization (l10n) refers to the adaptation of your program, once internationalized, to the local language and cultural habits. In theory it looks simple to implement. In practice though, it takes time and effort to provide the best Internationalization and Localization experience for your global audience. In Python, there is a specific bundled module for that and it\u2019s called gettext , which consists of a public API and a set of tools that help extract and generate message catalogs from the source code. References \u2691 Phrase blog on Localizing with GNU gettext Phrase blog on internationalization", "title": "Internationalization"}, {"location": "python_internationalization/#references", "text": "Phrase blog on Localizing with GNU gettext Phrase blog on internationalization", "title": "References"}, {"location": "python_jinja2/", "text": "Jinja2 is a modern and designer-friendly templating language for Python, modelled after Django\u2019s templates. It is fast, widely used and secure with the optional sandboxed template execution environment: <title>{% block title %}{% endblock %}</title> <ul> {% for user in users %} <li><a href=\"{{ user.url }}\">{{ user.username }}</a></li> {% endfor %} </ul> Features: Sandboxed execution. Powerful automatic HTML escaping system for XSS prevention. Template inheritance. Compiles down to the optimal python code just in time. Optional ahead-of-time template compilation. Easy to debug. Line numbers of exceptions directly point to the correct line in the template. Configurable syntax. Installation \u2691 pip install Jinja2 Usage \u2691 The most basic way to create a template and render it is through Template . This however is not the recommended way to work with it if your templates are not loaded from strings but the file system or another data source: >>> from jinja2 import Template >>> template = Template ( 'Hello {{ name }}!' ) >>> template . render ( name = 'John Doe' ) u 'Hello John Doe!' Jinja uses a central object called the template Environment . Instances of this class are used to store the configuration and global objects, and are used to load templates from the file system or other locations. The simplest way to configure Jinja to load templates for your application looks roughly like this: from jinja2 import Environment , PackageLoader , select_autoescape env = Environment ( loader = PackageLoader ( 'yourapplication' , 'templates' ), autoescape = select_autoescape ([ 'html' , 'xml' ]) ) This will create a template environment with the default settings and a loader that looks up the templates in the templates folder inside the yourapplication python package. Different loaders are available and you can also write your own if you want to load templates from a database or other resources. This also enables autoescaping for HTML and XML files. To load a template from this environment you just have to call the get_template() method which then returns the loaded Template: template = env . get_template ( 'mytemplate.html' ) To render it with some variables, just call the render() method: print ( template . render ( the = 'variables' , go = 'here' )) Template guidelines \u2691 Variables \u2691 Reference variables using {{ braces }} notation. Iteration/Loops \u2691 One in each line: {% for host in groups['tag_Function_logdb'] %} elasticsearch_discovery_zen_ping_unicast_hosts = {{ host }}:9300 {% endfor %} Inline: ALLOWED_HOSTS = [{% for domain in domains %}\" {{ domain }}\",{% endfor %}] Get the counter of the iteration \u2691 >>> from jinja2 import Template >>> s = \"{ % f or element in elements %}{{loop.index}} { % e ndfor %}\" >>> Template ( s ) . render ( elements = [ \"a\" , \"b\" , \"c\" , \"d\" ]) 1 2 3 4 Lookup \u2691 Get environmental variable \u2691 lookup('env','HOME') Join \u2691 lineinfile: dest=/etc/hosts line=\"{{ item.ip }} {{ item.aliases|join(' ') }}\" Map \u2691 Get elements of a dictionary set_fact : asg_instances=\"{{ instances.results | map(attribute='instances') | map('first') | map(attribute='public_ip_address') | list}}\" Default \u2691 Set default value of variable name : 'lol' new_name : \"{{ name | default('trol') }}\" Rejectattr \u2691 Exclude elements from a list. Filters a sequence of objects by applying a test to the specified attribute of each object, and rejecting the objects with the test succeeding. If no test is specified, the attribute's value will be evaluated as a boolean. {{ users|rejectattr(\"is_active\") }} {{ users|rejectattr(\"email\", \"none\") }} Regex \u2691 {{ 'ansible' | regex_replace('^a.*i(.*)$', 'a\\\\1') }} {{ 'foobar' | regex_replace('^f.*o(.*)$', '\\\\1') }} {{ 'localhost:80' | regex_replace('^(?P<host>.+):(?P<port>\\\\d+)$', '\\\\g<host>, \\\\g<port>') }} Slice string \u2691 {{ variable_name[:-8] }} Conditional \u2691 Conditional variable definition \u2691 {{ 'Update' if files else 'Continue' }} Check if variable is defined \u2691 { % if variable is defined % } Variable : {{ variable }} defined { % endif % } With two statements: \u2691 { % if (backend_environment == 'backend' and environment == 'Dev') : % } { % elif ... % } { % else % } { % endif % } Extract extension from file \u2691 s3_object : code/frontal/vodafone-v8.18.81.zip remote_clone_dir : \"{{deploy_dir}}/{{ s3_object | basename | splitext | first}}\" Comments \u2691 {# comment here #} Inheritance \u2691 For simple inclusions use include for more complex extend . Include \u2691 To include a snippet from another file you can use {% include '_post.html' %} Extend \u2691 To inherit from another document you can use the block control statement. Blocks are given a unique name, which derived templates can reference when they provide their content .base.html < html > < head > {% if title %} < title > {{ title }} - Microblog </ title > {% else %} < title > Welcome to Microblog </ title > {% endif %} </ head > < body > < div > Microblog: < a href = \"/index\" > Home </ a > </ div > < hr > {% block content %}{% endblock %} </ body > </ html > .index.html {% extends \"base.html\" %} {% block content %} < h1 > Hi </ h1 > {% endblock %} Execute a function and return the value to a variable \u2691 {% with messages = get_flashed_messages() %} {% if messages %} <ul> {% for message in messages %} <li>{{ message }}</li> {% endfor %} </ul> {% endif %} {% endwith %} Macros \u2691 Macros are comparable with functions in regular programming languages. They are useful to put often used idioms into reusable functions to not repeat yourself (\u201cDRY\u201d). {% macro input(name, value='', type='text', size=20) -%} <input type=\"{{ type }}\" name=\"{{ name }}\" value=\"{{ value|e }}\" size=\"{{ size }}\"> {%- endmacro %} The macro can then be called like a function in the namespace: <p>{{ input('username') }}</p> <p>{{ input('password', type='password') }}</p> Wrap long lines and indent \u2691 You can prepend the given string with a newline character, then use the wordwrap filter to wrap the text into multiple lines first, and use the replace filter to replace newline characters with newline plus ' ': {{ ('\\n' ~ item.comment) | wordwrap(76) | replace('\\n', '\\n ') }} The above assumes you want each line to be no more than 80 characters. Change 76 to your desired line width minus 4 to leave room for the indentation. Test if variable is None \u2691 Use the none test (not to be confused with Python's None object!): {% if p is not none %} {{ p.User['first_name'] }} {% else %} NONE {% endif %} References \u2691 Docs", "title": "Jinja2"}, {"location": "python_jinja2/#installation", "text": "pip install Jinja2", "title": "Installation"}, {"location": "python_jinja2/#usage", "text": "The most basic way to create a template and render it is through Template . This however is not the recommended way to work with it if your templates are not loaded from strings but the file system or another data source: >>> from jinja2 import Template >>> template = Template ( 'Hello {{ name }}!' ) >>> template . render ( name = 'John Doe' ) u 'Hello John Doe!' Jinja uses a central object called the template Environment . Instances of this class are used to store the configuration and global objects, and are used to load templates from the file system or other locations. The simplest way to configure Jinja to load templates for your application looks roughly like this: from jinja2 import Environment , PackageLoader , select_autoescape env = Environment ( loader = PackageLoader ( 'yourapplication' , 'templates' ), autoescape = select_autoescape ([ 'html' , 'xml' ]) ) This will create a template environment with the default settings and a loader that looks up the templates in the templates folder inside the yourapplication python package. Different loaders are available and you can also write your own if you want to load templates from a database or other resources. This also enables autoescaping for HTML and XML files. To load a template from this environment you just have to call the get_template() method which then returns the loaded Template: template = env . get_template ( 'mytemplate.html' ) To render it with some variables, just call the render() method: print ( template . render ( the = 'variables' , go = 'here' ))", "title": "Usage"}, {"location": "python_jinja2/#template-guidelines", "text": "", "title": "Template guidelines"}, {"location": "python_jinja2/#variables", "text": "Reference variables using {{ braces }} notation.", "title": "Variables"}, {"location": "python_jinja2/#iterationloops", "text": "One in each line: {% for host in groups['tag_Function_logdb'] %} elasticsearch_discovery_zen_ping_unicast_hosts = {{ host }}:9300 {% endfor %} Inline: ALLOWED_HOSTS = [{% for domain in domains %}\" {{ domain }}\",{% endfor %}]", "title": "Iteration/Loops"}, {"location": "python_jinja2/#get-the-counter-of-the-iteration", "text": ">>> from jinja2 import Template >>> s = \"{ % f or element in elements %}{{loop.index}} { % e ndfor %}\" >>> Template ( s ) . render ( elements = [ \"a\" , \"b\" , \"c\" , \"d\" ]) 1 2 3 4", "title": "Get the counter of the iteration"}, {"location": "python_jinja2/#lookup", "text": "", "title": "Lookup"}, {"location": "python_jinja2/#get-environmental-variable", "text": "lookup('env','HOME')", "title": "Get environmental variable"}, {"location": "python_jinja2/#join", "text": "lineinfile: dest=/etc/hosts line=\"{{ item.ip }} {{ item.aliases|join(' ') }}\"", "title": "Join"}, {"location": "python_jinja2/#map", "text": "Get elements of a dictionary set_fact : asg_instances=\"{{ instances.results | map(attribute='instances') | map('first') | map(attribute='public_ip_address') | list}}\"", "title": "Map"}, {"location": "python_jinja2/#default", "text": "Set default value of variable name : 'lol' new_name : \"{{ name | default('trol') }}\"", "title": "Default"}, {"location": "python_jinja2/#rejectattr", "text": "Exclude elements from a list. Filters a sequence of objects by applying a test to the specified attribute of each object, and rejecting the objects with the test succeeding. If no test is specified, the attribute's value will be evaluated as a boolean. {{ users|rejectattr(\"is_active\") }} {{ users|rejectattr(\"email\", \"none\") }}", "title": "Rejectattr"}, {"location": "python_jinja2/#regex", "text": "{{ 'ansible' | regex_replace('^a.*i(.*)$', 'a\\\\1') }} {{ 'foobar' | regex_replace('^f.*o(.*)$', '\\\\1') }} {{ 'localhost:80' | regex_replace('^(?P<host>.+):(?P<port>\\\\d+)$', '\\\\g<host>, \\\\g<port>') }}", "title": "Regex"}, {"location": "python_jinja2/#slice-string", "text": "{{ variable_name[:-8] }}", "title": "Slice string"}, {"location": "python_jinja2/#conditional", "text": "", "title": "Conditional"}, {"location": "python_jinja2/#conditional-variable-definition", "text": "{{ 'Update' if files else 'Continue' }}", "title": "Conditional variable definition"}, {"location": "python_jinja2/#check-if-variable-is-defined", "text": "{ % if variable is defined % } Variable : {{ variable }} defined { % endif % }", "title": "Check if variable is defined"}, {"location": "python_jinja2/#with-two-statements", "text": "{ % if (backend_environment == 'backend' and environment == 'Dev') : % } { % elif ... % } { % else % } { % endif % }", "title": "With two statements:"}, {"location": "python_jinja2/#extract-extension-from-file", "text": "s3_object : code/frontal/vodafone-v8.18.81.zip remote_clone_dir : \"{{deploy_dir}}/{{ s3_object | basename | splitext | first}}\"", "title": "Extract extension from file"}, {"location": "python_jinja2/#comments", "text": "{# comment here #}", "title": "Comments"}, {"location": "python_jinja2/#inheritance", "text": "For simple inclusions use include for more complex extend .", "title": "Inheritance"}, {"location": "python_jinja2/#include", "text": "To include a snippet from another file you can use {% include '_post.html' %}", "title": "Include"}, {"location": "python_jinja2/#extend", "text": "To inherit from another document you can use the block control statement. Blocks are given a unique name, which derived templates can reference when they provide their content .base.html < html > < head > {% if title %} < title > {{ title }} - Microblog </ title > {% else %} < title > Welcome to Microblog </ title > {% endif %} </ head > < body > < div > Microblog: < a href = \"/index\" > Home </ a > </ div > < hr > {% block content %}{% endblock %} </ body > </ html > .index.html {% extends \"base.html\" %} {% block content %} < h1 > Hi </ h1 > {% endblock %}", "title": "Extend"}, {"location": "python_jinja2/#execute-a-function-and-return-the-value-to-a-variable", "text": "{% with messages = get_flashed_messages() %} {% if messages %} <ul> {% for message in messages %} <li>{{ message }}</li> {% endfor %} </ul> {% endif %} {% endwith %}", "title": "Execute a function and return the value to a variable"}, {"location": "python_jinja2/#macros", "text": "Macros are comparable with functions in regular programming languages. They are useful to put often used idioms into reusable functions to not repeat yourself (\u201cDRY\u201d). {% macro input(name, value='', type='text', size=20) -%} <input type=\"{{ type }}\" name=\"{{ name }}\" value=\"{{ value|e }}\" size=\"{{ size }}\"> {%- endmacro %} The macro can then be called like a function in the namespace: <p>{{ input('username') }}</p> <p>{{ input('password', type='password') }}</p>", "title": "Macros"}, {"location": "python_jinja2/#wrap-long-lines-and-indent", "text": "You can prepend the given string with a newline character, then use the wordwrap filter to wrap the text into multiple lines first, and use the replace filter to replace newline characters with newline plus ' ': {{ ('\\n' ~ item.comment) | wordwrap(76) | replace('\\n', '\\n ') }} The above assumes you want each line to be no more than 80 characters. Change 76 to your desired line width minus 4 to leave room for the indentation.", "title": "Wrap long lines and indent"}, {"location": "python_jinja2/#test-if-variable-is-none", "text": "Use the none test (not to be confused with Python's None object!): {% if p is not none %} {{ p.User['first_name'] }} {% else %} NONE {% endif %}", "title": "Test if variable is None"}, {"location": "python_jinja2/#references", "text": "Docs", "title": "References"}, {"location": "python_logging/", "text": "Logging information in your Python programs makes it possible to debug problems when running. For command line application that the user is going to run directly, the logging module might be enough. For command line tools or APIs that are going to be run by a server, it might fall short. logging will write exceptions and breadcrumbs to a file, and unless you look at it directly most errors will pass unnoticed. To actively monitor and react to code exceptions use an application monitoring platform like sentry . They gather de data on your application and aggregate the errors in a user friendly way, such as: Showing the context of the error in a web interface. Gather both front and backend issues in one place. Show the trail of events that led to the exceptions with breadcrumbs. Show the probable commit that introduced the bug. Link problems with issue tracker issues. See the impact of each bug with the number of occurrences and users that are experiencing it. Visualize all the data in dashboards. Get notifications on the issues raised. Check the demo to see its features. You can self-host sentry , but it uses a docker-compose that depends on 12 services, including postgres, redis and kafka with a minimum requirements of 4 cores and 8 GB of RAM. So I've looked for a simple solution, and arrived to GlitchTip , a similar solution that even uses the sentry SDK, but has a smaller system footprint, and it's open sourced, while sentry is not anymore . Check it's documentation and source code .", "title": "Logging"}, {"location": "python_mysql/", "text": "Installation \u2691 pip install mysql-connector-python Usage \u2691 import mysql.connector # Connect to server cnx = mysql . connector . connect ( host = \"127.0.0.1\" , port = 3306 , user = \"mike\" , password = \"s3cre3t!\" ) # Get a cursor cur = cnx . cursor () # Execute a query cur . execute ( \"SELECT CURDATE()\" ) # Fetch one result row = cur . fetchone () print ( \"Current date is: {0} \" . format ( row [ 0 ])) # Close connection cnx . close () Iterate over the results of the cursor execution \u2691 cursor . execute ( show_db_query ) for db in cursor : print ( db ) References \u2691 Git Docs RealPython tutorial", "title": "Python Mysql"}, {"location": "python_mysql/#installation", "text": "pip install mysql-connector-python", "title": "Installation"}, {"location": "python_mysql/#usage", "text": "import mysql.connector # Connect to server cnx = mysql . connector . connect ( host = \"127.0.0.1\" , port = 3306 , user = \"mike\" , password = \"s3cre3t!\" ) # Get a cursor cur = cnx . cursor () # Execute a query cur . execute ( \"SELECT CURDATE()\" ) # Fetch one result row = cur . fetchone () print ( \"Current date is: {0} \" . format ( row [ 0 ])) # Close connection cnx . close ()", "title": "Usage"}, {"location": "python_mysql/#iterate-over-the-results-of-the-cursor-execution", "text": "cursor . execute ( show_db_query ) for db in cursor : print ( db )", "title": "Iterate over the results of the cursor execution"}, {"location": "python_mysql/#references", "text": "Git Docs RealPython tutorial", "title": "References"}, {"location": "python_optimization/", "text": "Optimization can be done through different metrics, such as, CPU performance (execution time) or memory footprint. Optimizing your code makes sense when you are sure that the business logic in the code is correct and not going to change soon. \"First make it work. Then make it right. Then make it fast.\" ~ Kent Beck Unless you're developing a performance-intensive product or a code dependency that is going to be used by other projects which might be performance-intensive, optimizing every aspect of the code can be overkill. For most of the scenarios, the 80-20 principle (80 percent of performance benefits may come from optimizing 20 percent of your code) will be more appropriate. Most of the time we make intuitive guesses on what the bottlenecks are, but more often than not, our guesses are either wrong or just approximately correct. So, it's always advisable to use profiling tools to identify how often a resource is used and who is using the resource. For instance, a profiler designed for profiling execution time will measure how often and for how various long parts of the code are executed. Using a profiling mechanism becomes a necessity when the codebase grows large, and you still want to maintain efficiency. Making Python command line fast \u2691 People like using software that feels fast, and Python programs tend to be slow to start running. What qualifies as fast is subjective, and varies by the type of tool and by the user's expectations. Roughly speaking, for a command line program, people expect results almost instantaneously. For a tool that appears to be doing a simple task a sub-second result is enough, but under 200ms is even better. Obviously to achieve this, your program actually has to be fast at doing its work. But what if you've written your code in Python, and it can take 800ms just to import your code, let alone start running it. How fast can a Python program be? \u2691 TBC with the next sources https://files.bemusement.org/talks/OSDC2008-FastPython/ https://files.bemusement.org/talks/OSDC2008-FastPython/ https://stackoverflow.com/questions/4177735/best-practice-for-lazy-loading-python-modules https://snarky.ca/lazy-importing-in-python-3-7/ https://levelup.gitconnected.com/python-trick-lazy-module-loading-df9b9dc111af Minimize the relative import statements on command line tools \u2691 When developing a library, it's common to expose the main objects into the package __init__.py under the variable __all__ . The problem with command line programs is that each time you run the command it will load those objects, which can mean an increase of 0.5s or even a second for each command, which is unacceptable. Following this string, if you manage to minimize the relative imports, you'll make your code faster. Python's wiki discusses different places to locate your import statements. If you put them on the top, the imports that you don't need for that command in particular will worsen your load time, if you add them inside the functions, if you run the function more than once, the performance drops too, and it's a common etiquete to have all your imports on the top. One step that you can do is to mark the imports required for type checking under a conditional: from typing import TYPE_CHECKING if TYPE_CHECKING : from model import Object This change can be negligible, and it will force you to use 'Object' , instead of Object in the typing information, which is not nice, so it may not be worth it. If you are still unable to make the loading time drop below an acceptable time, you can migrate to a server-client architecture, where all the logic is loaded by the backend (once as it's always running), and have a \"silly\" client that only does requests to the backend. Beware though, as you will add the network latency. Don't dynamically install the package \u2691 If you install the package with pip install -e . you will see an increase on the load time of ~0.2s. It is useful to develop the package, but when you use it, do so from a virtualenv that installs it directly without the -e flag. References \u2691 Satwik Kansal article on Scout APM", "title": "Optimization"}, {"location": "python_optimization/#making-python-command-line-fast", "text": "People like using software that feels fast, and Python programs tend to be slow to start running. What qualifies as fast is subjective, and varies by the type of tool and by the user's expectations. Roughly speaking, for a command line program, people expect results almost instantaneously. For a tool that appears to be doing a simple task a sub-second result is enough, but under 200ms is even better. Obviously to achieve this, your program actually has to be fast at doing its work. But what if you've written your code in Python, and it can take 800ms just to import your code, let alone start running it.", "title": "Making Python command line fast"}, {"location": "python_optimization/#how-fast-can-a-python-program-be", "text": "TBC with the next sources https://files.bemusement.org/talks/OSDC2008-FastPython/ https://files.bemusement.org/talks/OSDC2008-FastPython/ https://stackoverflow.com/questions/4177735/best-practice-for-lazy-loading-python-modules https://snarky.ca/lazy-importing-in-python-3-7/ https://levelup.gitconnected.com/python-trick-lazy-module-loading-df9b9dc111af", "title": "How fast can a Python program be?"}, {"location": "python_optimization/#minimize-the-relative-import-statements-on-command-line-tools", "text": "When developing a library, it's common to expose the main objects into the package __init__.py under the variable __all__ . The problem with command line programs is that each time you run the command it will load those objects, which can mean an increase of 0.5s or even a second for each command, which is unacceptable. Following this string, if you manage to minimize the relative imports, you'll make your code faster. Python's wiki discusses different places to locate your import statements. If you put them on the top, the imports that you don't need for that command in particular will worsen your load time, if you add them inside the functions, if you run the function more than once, the performance drops too, and it's a common etiquete to have all your imports on the top. One step that you can do is to mark the imports required for type checking under a conditional: from typing import TYPE_CHECKING if TYPE_CHECKING : from model import Object This change can be negligible, and it will force you to use 'Object' , instead of Object in the typing information, which is not nice, so it may not be worth it. If you are still unable to make the loading time drop below an acceptable time, you can migrate to a server-client architecture, where all the logic is loaded by the backend (once as it's always running), and have a \"silly\" client that only does requests to the backend. Beware though, as you will add the network latency.", "title": "Minimize the relative import statements on command line tools"}, {"location": "python_optimization/#dont-dynamically-install-the-package", "text": "If you install the package with pip install -e . you will see an increase on the load time of ~0.2s. It is useful to develop the package, but when you use it, do so from a virtualenv that installs it directly without the -e flag.", "title": "Don't dynamically install the package"}, {"location": "python_optimization/#references", "text": "Satwik Kansal article on Scout APM", "title": "References"}, {"location": "python_package_management/", "text": "Managing Python libraries is a nightmare for most developers, it has driven me crazy trying to keep all the requirements of the projects I maintain updated. I tried with pip-tools , but I was probably using it wrong. As package management has evolved a lot in the latest years, I'm going to compare Poetry , pipenv , pdm with my current workflow. Tool Stars Forks Latest commit Commits Issues Open/New/Closed PR Open/New/Merged Poetry 17.3k 1.4k 11h 1992 1.1k/58/80 149/13/77 Pipenv 22.5k 1.7k 5d 7226 555/12/54 32/0/22 pdm 1.3k 54 11h 1539 12/3/43 3/2/11 The New and Closed are taken from the Pulse insights of the last month. This data was taken on the 2021-11-30 so it will probably be outdated. Both Poetry and Pipenv are very popular, it looks that Poetry is more alive this last month, but they are both actively developed. pdm is actively developed but at other level. Pipenv has broad support. It is an official project of the Python Packaging Authority, alongside pip. It's also supported by the Heroku Python buildpack, which is useful for anyone with Heroku or Dokku-based deployment strategies. Poetry is a one-stop shop for dependency management and package management. It simplifies creating a package, managing its dependencies, and publishing it. Compared to Pipenv, Poetry's separate add and install commands are more explicit, and it's faster for everything except for a full dependency install. Solver \u2691 A Solver tries to find a working set of dependencies that all agree with each other. By looking back in time, it\u2019s happy to solve very old versions of packages if newer ones are supposed to be incompatible. This can be helpful, but is slow, and also means you can easily get a very ancient set of packages when you thought you were getting the latest versions. Pip\u2019s solver changed in version 20.3 to become significantly smarter. The old solver would ignore incompatible transitive requirements much more often than the new solver does. This means that an upper cap in a library might have been ignored before, but is much more likely to break things or change the solve now. Poetry has a unique and very strict (and slower) solver that goes even farther hunting for solutions. It forces you to cap Python if a dependency does. One key difference is that Poetry has the original environment specification to work with every time, while pip does not know what the original environment constraints were. This enables Poetry to roll back a dependency on a subsequent solve, while pip does not know what the original requirements were and so does not know if an older package is valid when it encounters a new cap. Poetry \u2691 Features I like: Stores program and development requirements in the pyproject.toml file. Don't need to manually edit requirements files to add new packages to the program or dev requirements, simply use poetry add . Easy initialization of the development environment with poetry install . Powerful dependency specification Installable packages with git dependencies??? Easy to specify local directory dependencies, even in editable mode. Specify different dependencies for different python versions It manage the building of your package, you don't need to manually configure sdist and wheel . Nice dependency view with poetry show . Nice dependency search interface with poetry search . Sync your environment packages with the lock file. Things I don't like that much: It does upper version capping by default, it even ignores your pins and adds the ^<new_version pin if you run poetry add <package>@latest https://github.com/python-poetry/poetry/issues/3503 . Given that upper version capping is becoming a big problem in the Python environment I'd stay away from poetry . This is specially useless when you add dependencies that follow CalVer . poetry add packaging will still do ^21 for the version it adds. You shouldn\u2019t be capping versions, but you really shouldn\u2019t be capping CalVer. It's equally troublesome that it upper pins the python version . Have their own dependency specification format similar to npm and incompatible with Python's PEP508 . No automatic process to update the dependencies constrains to match the latest version available. So if you have constrained a package to be <2.0.0 and 3.0.0 is out there, you will have to manually edit the pyproject.toml so that it accepts that new version. At least you can use poetry show --outdated and it will tell you which is the new version, and if the output is zero, you're sure you're on the last versions. PDM \u2691 Features I like: The pin strategy defaults to only add lower pins helping preventing the upper capping problem. It can't achieve dependency isolation without virtualenvs. Follows the Python's dependency specification format PEP508 . Supports different strategies to add and update dependencies. Command to update your requirements constrains when updating your packages. Sync your environment packages with the lock file. Easy to install package in editable mode. Easy to install local dependencies. You can force the installation of a package at your own risk even if it breaks the version constrains. (Useful if you're blocked by a third party upper bound) Changing the python version is as simple as running python use <python_version> . Plugin system where adding functionality is feasible (like the publish subcommand). Both global and local configuration. Nice interface to change the configuration. Automatic management of dependencies cache, where you only have one instance of each package version, and if no project needs it, it will be removed. Has a nice interface to see the cache usage Has the possibility of managing the global packages too. Allows the definition of scripts possibly removing the need of a makefile It's able to read the version of the program from a file, avoiding the duplication of the information. You can group your development dependencies in groups. Easy to define extra dependencies for your program. It has sensible defaults for includes and excludes when packaging. It's the fastest and most correct one. Downsides: They don't say how to configure your environment to work with vim . Summary \u2691 PDM offers the same features as Poetry with the additions of the possibility of selecting your version capping strategy, and doesn\u2019t cap as badly, and follows more PEP standards. References \u2691 PDM developer comparison John Franey comparison Frost Ming comparison (developer of PDM) Henry Schreiner analysis on Poetry", "title": "Package Management"}, {"location": "python_package_management/#solver", "text": "A Solver tries to find a working set of dependencies that all agree with each other. By looking back in time, it\u2019s happy to solve very old versions of packages if newer ones are supposed to be incompatible. This can be helpful, but is slow, and also means you can easily get a very ancient set of packages when you thought you were getting the latest versions. Pip\u2019s solver changed in version 20.3 to become significantly smarter. The old solver would ignore incompatible transitive requirements much more often than the new solver does. This means that an upper cap in a library might have been ignored before, but is much more likely to break things or change the solve now. Poetry has a unique and very strict (and slower) solver that goes even farther hunting for solutions. It forces you to cap Python if a dependency does. One key difference is that Poetry has the original environment specification to work with every time, while pip does not know what the original environment constraints were. This enables Poetry to roll back a dependency on a subsequent solve, while pip does not know what the original requirements were and so does not know if an older package is valid when it encounters a new cap.", "title": "Solver"}, {"location": "python_package_management/#poetry", "text": "Features I like: Stores program and development requirements in the pyproject.toml file. Don't need to manually edit requirements files to add new packages to the program or dev requirements, simply use poetry add . Easy initialization of the development environment with poetry install . Powerful dependency specification Installable packages with git dependencies??? Easy to specify local directory dependencies, even in editable mode. Specify different dependencies for different python versions It manage the building of your package, you don't need to manually configure sdist and wheel . Nice dependency view with poetry show . Nice dependency search interface with poetry search . Sync your environment packages with the lock file. Things I don't like that much: It does upper version capping by default, it even ignores your pins and adds the ^<new_version pin if you run poetry add <package>@latest https://github.com/python-poetry/poetry/issues/3503 . Given that upper version capping is becoming a big problem in the Python environment I'd stay away from poetry . This is specially useless when you add dependencies that follow CalVer . poetry add packaging will still do ^21 for the version it adds. You shouldn\u2019t be capping versions, but you really shouldn\u2019t be capping CalVer. It's equally troublesome that it upper pins the python version . Have their own dependency specification format similar to npm and incompatible with Python's PEP508 . No automatic process to update the dependencies constrains to match the latest version available. So if you have constrained a package to be <2.0.0 and 3.0.0 is out there, you will have to manually edit the pyproject.toml so that it accepts that new version. At least you can use poetry show --outdated and it will tell you which is the new version, and if the output is zero, you're sure you're on the last versions.", "title": "Poetry"}, {"location": "python_package_management/#pdm", "text": "Features I like: The pin strategy defaults to only add lower pins helping preventing the upper capping problem. It can't achieve dependency isolation without virtualenvs. Follows the Python's dependency specification format PEP508 . Supports different strategies to add and update dependencies. Command to update your requirements constrains when updating your packages. Sync your environment packages with the lock file. Easy to install package in editable mode. Easy to install local dependencies. You can force the installation of a package at your own risk even if it breaks the version constrains. (Useful if you're blocked by a third party upper bound) Changing the python version is as simple as running python use <python_version> . Plugin system where adding functionality is feasible (like the publish subcommand). Both global and local configuration. Nice interface to change the configuration. Automatic management of dependencies cache, where you only have one instance of each package version, and if no project needs it, it will be removed. Has a nice interface to see the cache usage Has the possibility of managing the global packages too. Allows the definition of scripts possibly removing the need of a makefile It's able to read the version of the program from a file, avoiding the duplication of the information. You can group your development dependencies in groups. Easy to define extra dependencies for your program. It has sensible defaults for includes and excludes when packaging. It's the fastest and most correct one. Downsides: They don't say how to configure your environment to work with vim .", "title": "PDM"}, {"location": "python_package_management/#summary", "text": "PDM offers the same features as Poetry with the additions of the possibility of selecting your version capping strategy, and doesn\u2019t cap as badly, and follows more PEP standards.", "title": "Summary"}, {"location": "python_package_management/#references", "text": "PDM developer comparison John Franey comparison Frost Ming comparison (developer of PDM) Henry Schreiner analysis on Poetry", "title": "References"}, {"location": "python_plugin_system/", "text": "When building Python applications, it's good to develop the core of your program, and allow extension via plugins. I still don't know how to do it, but I'm going to gather interesting references until I tackle it. Beets plugin system looks awesome.", "title": "Plugin System"}, {"location": "python_poetry/", "text": "Poetry is a command line program that helps you declare, manage and install dependencies of Python projects, ensuring you have the right stack everywhere. poetry saves all the information in the pyproject.toml file, including the project development and program dependencies, for example: [tool.poetry] name = \"poetry-demo\" version = \"0.1.0\" description = \"\" authors = [ \"S\u00e9bastien Eustace <sebastien@eustace.io>\" ] [tool.poetry.dependencies] python = \"*\" [tool.poetry.dev-dependencies] pytest = \"^3.4\" Installation \u2691 Although the official docs tell you to run: curl -sSL https://install.python-poetry.org | python3 - pip install poetry works too, which looks safer than executing arbitrary code from an url. To enable shell completion for zsh run: # Zsh poetry completions zsh > ~/.zfunc/_poetry # Oh-My-Zsh mkdir $ZSH_CUSTOM /plugins/poetry poetry completions zsh > $ZSH_CUSTOM /plugins/poetry/_poetry For zsh , you must then add the following line in your ~/.zshrc before compinit : fpath += ~/.zfunc For oh-my-zsh , you must then enable poetry in your ~/.zshrc plugins: plugins ( poetry ... ) Basic Usage \u2691 Initializing a pre-existing project \u2691 Instead of creating a new project, Poetry can be used to \u2018initialise\u2019 a pre-populated directory with poetry init . You can use the next options --name : Name of the package. --description : Description of the package. --author : Author of the package. --python : Compatible Python versions. --dependency : Package to require with a version constraint. Should be in format foo:1.0.0 . --dev-dependency : Development requirements, see --require . Installing dependencies \u2691 To install the defined dependencies for your project, just run the install command. poetry install When you run this command, one of two things may happen: Installing without poetry.lock : If you have never run the command before and there is also no poetry.lock file present, Poetry simply resolves all dependencies listed in your pyproject.toml file and downloads the latest version of their files. When Poetry has finished installing, it writes all of the packages and the exact versions of them that it downloaded to the poetry.lock file, locking the project to those specific versions. You should commit the poetry.lock file to your project repo so that all people working on the project are locked to the same versions of dependencies. Installing with poetry.lock : If there is already a poetry.lock file as well as a pyproject.toml , poetry resolves and installs all dependencies that you listed in pyproject.toml , but Poetry uses the exact versions listed in poetry.lock to ensure that the package versions are consistent for everyone working on your project. As a result you will have all dependencies requested by your pyproject.toml file, but they may not all be at the very latest available versions (some of the dependencies listed in the poetry.lock file may have released newer versions since the file was created). This is by design, it ensures that your project does not break because of unexpected changes in dependencies. The current project is installed in editable mode by default. If you don't want the development requirements use the --no-dev flag. To remove the untracked dependencies that are no longer in the lock file, use --remove-untracked . Updating dependencies to their latest versions \u2691 The poetry.lock file prevents you from automatically getting the latest versions of your dependencies. To update to the latest versions, use the update command. This will fetch the latest matching versions (according to your pyproject.toml file) and update the lock file with the new versions. (This is equivalent to deleting the poetry.lock file and running install again.) The main problem is that poetry add does upper pinning of dependencies by default, which is a really bad idea . And they don't plan to change . There is currently no way of updating your pyproject.toml dependency definitions so they match the latest version beyond your constrains. So if you have constrained a package to be <2.0.0 and 3.0.0 is out there, you will have to manually edit the pyproject.toml so that it accepts that new version. There is no automatic process that does this. At least you can use poetry show --outdated and it will tell you which is the new version, and if the output is zero, you're sure you're on the last versions. Some workarounds exists though, if you run poetry add dependency@latest it will update the lock to the latest. MousaZeidBaker made poetryup , a tool that is able to update the requirements to the latest version with poetryup --latest (although it still has some bugs ). Given that it uses poetry add <package>@latest behind the scenes, it will change your version pin to ^<new_version> , which as we've seen it's awful. Again, you should not be trying to do this, it's better to improve how you manage your dependencies . Debugging why a package is not updated to the latest version \u2691 Sometimes packages are not updated with poetry update or poetryup , to debug why, you need to understand if some package is setting a constrain that prevents the upgrade. To do that, first check the outdated packages with poetry show -o and for each of them: Check what packages are using the dependency . Search if there is an issue asking the maintainers to update their dependencies, if it doesn't exist, create it. Removing a dependency \u2691 poetry remove pendulum With the -D or --dev flag, it removes the dependency from the development ones. Building the package \u2691 Before you can actually publish your library, you will need to package it. poetry build This command will package your library in two different formats: sdist which is the source format, and wheel which is a compiled package. Once that\u2019s done you are ready to publish your library. Publishing to PyPI \u2691 Poetry will publish to PyPI by default. Anything that is published to PyPI is available automatically through Poetry. poetry publish This will package and publish the library to PyPI, at the condition that you are a registered user and you have configured your credentials properly . If you pass the --build flag, it will also build the package. Publishing to a private repository \u2691 Sometimes, you may want to keep your library private but also being accessible to your team. In this case, you will need to use a private repository. You will need to add it to your global list of repositories. Once this is done, you can actually publish to it like so: poetry publish -r my-repository Specifying dependencies \u2691 If you want to add dependencies to your project, you can specify them in the tool.poetry.dependencies section. [tool.poetry.dependencies] pendulum = \"^1.4\" As you can see, it takes a mapping of package names and version constraints. Poetry uses this information to search for the right set of files in package \u201crepositories\u201d that you register in the tool.poetry.repositories section, or on PyPI by default. Also, instead of modifying the pyproject.toml file by hand, you can use the add command. poetry add pendulum It will automatically find a suitable version constraint and install the package and subdependencies. If you want to add the dependency to the development ones, use the -D or --dev flag. Using your virtual environment \u2691 By default, poetry creates a virtual environment in {cache-dir}/virtualenvs . You can change the cache-dir value by editing the poetry config. Additionally, you can use the virtualenvs.in-project configuration variable to create virtual environment within your project directory. There are several ways to run commands within this virtual environment. To run your script simply use poetry run python your_script.py . Likewise if you have command line tools such as pytest or black you can run them using poetry run pytest . The easiest way to activate the virtual environment is to create a new shell with poetry shell . Version Management \u2691 poetry version shows the current version of the project. If you pass an argument, it will bump the version of the package, for example poetry version minor . But it doesn't read your commits to decide what kind of bump you apply, so I'd keep on using pip-compile . Dependency Specification \u2691 Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed. They don't follow Python's specification PEP508 Caret Requirements \u2691 Caret requirements allow SemVer compatible updates to a specified version. An update is allowed if the new version number does not modify the left-most non-zero digit in the major, minor, patch grouping. In this case, if we ran poetry update requests , poetry would update us to the next versions: Requirement Versions allowed ^1.2.3 >=1.2.3 <2.0.0 ^1.2 >=1.2.0 <2.0.0 ^1 >=1.0.0 <2.0.0 ^0.2.3 >=0.2.3 <0.3.0 ^0.0.3 >=0.0.3 <0.0.4 ^0.0 >=0.0.0 <0.1.0 ^0 >=0.0.0 <1.0.0 Tilde requirements \u2691 Tilde requirements specify a minimal version with some ability to update. If you specify a major, minor, and patch version or only a major and minor version, only patch-level changes are allowed. If you only specify a major version, then minor- and patch-level changes are allowed. Requirement Versions allowed ~1.2.3 >=1.2.3 <1.3.0 ~1.2 >=1.2.0 <1.3.0 ~1 >=1.0.0 <2.0.0 Wildcard requirements \u2691 Wildcard requirements allow for the latest (dependency dependent) version where the wildcard is positioned. Requirement Versions allowed * >=0.0.0 1.* >=1.0.0 <2.0.0 1.2.* >=1.2.0 <1.3.0 Inequality requirements \u2691 Inequality requirements allow manually specifying a version range or an exact version to depend on. Here are some examples of inequality requirements: >= 1.2.0 > 1 < 2 != 1.2.3 Exact requirements \u2691 You can specify the exact version of a package. This will tell Poetry to install this version and this version only. If other dependencies require a different version, the solver will ultimately fail and abort any install or update procedures. Multiple version requirements can also be separated with a comma, e.g. >= 1.2, < 1.5 . git dependencies \u2691 To depend on a library located in a git repository, the minimum information you need to specify is the location of the repository with the git key: [tool.poetry.dependencies] requests = { git = \"https://github.com/requests/requests.git\" } Since we haven\u2019t specified any other information, Poetry assumes that we intend to use the latest commit on the master branch to build our project. You can combine the git key with the branch key to use another branch. Alternatively, use rev or tag to pin a dependency to a specific commit hash or tagged ref, respectively. For example: [tool.poetry.dependencies] # Get the latest revision on the branch named \"next\" requests = { git = \"https://github.com/kennethreitz/requests.git\" , branch = \"next\" } # Get a revision by its commit hash flask = { git = \"https://github.com/pallets/flask.git\" , rev = \"38eb5d3b\" } # Get a revision by its tag numpy = { git = \"https://github.com/numpy/numpy.git\" , tag = \"v0.13.2\" } When using poetry add you can add: A https cloned repo: poetry add git+https://github.com/sdispater/pendulum.git A ssh cloned repo: poetry add git+ssh://git@github.com/sdispater/pendulum.git If you need to checkout a specific branch, tag or revision, you can specify it when using add: poetry add git+https://github.com/sdispater/pendulum.git#develop poetry add git+https://github.com/sdispater/pendulum.git#2.0.5 path dependencies \u2691 To depend on a library located in a local directory or file, you can use the path property: [tool.poetry.dependencies] # directory my-package = { path = \"../my-package/\" , develop = false } # file my-package = { path = \"../my-package/dist/my-package-0.1.0.tar.gz\" } When using poetry add , you can point them directly to the package or the file: poetry add ./my-package/ poetry add ../my-package/dist/my-package-0.1.0.tar.gz poetry add ../my-package/dist/my_package-0.1.0.whl If you want the dependency to be installed in editable mode you can specify it in the pyproject.toml file. It means that changes in the local directory will be reflected directly in environment. [tool.poetry.dependencies] my-package = { path = \"../my/path\" , develop = true } url dependencies \u2691 To depend on a library located on a remote archive, you can use the url property: [tool.poetry.dependencies] # directory my-package = { url = \"https://example.com/my-package-0.1.0.tar.gz\" } With the corresponding add call: poetry add https://example.com/my-package-0.1.0.tar.gz Python restricted dependencies \u2691 You can also specify that a dependency should be installed only for specific Python versions: [tool.poetry.dependencies] pathlib2 = { version = \"^2.2\" , python = \"~2.7\" } [tool.poetry.dependencies] pathlib2 = { version = \"^2.2\" , python = \"~2.7 || ^3.2\" } Multiple constraints dependencies \u2691 Sometimes, one of your dependency may have different version ranges depending on the target Python versions. Let\u2019s say you have a dependency on the package foo which is only compatible with Python <3.0 up to version 1.9 and compatible with Python 3.4+ from version 2.0 . You would declare it like so: [ tool . poetry . dependencies ] foo = [ { version = \"<=1.9\" , python = \"^2.7\" }, { version = \"^2.0\" , python = \"^3.4\" } ] Show the available packages \u2691 To list all of the available packages, you can use the show command. poetry show If you want to see the details of a certain package, you can pass the package name. poetry show pendulum name : pendulum version : 1 .4.2 description : Python datetimes made easy dependencies: - python-dateutil > = 2 .6.1 - tzlocal > = 1 .4 - pytzdata > = 2017 .2.2 By default it will print all the dependencies, if you pass --no-dev it will only show your package's ones. With the -l or --latest it will show the latest version of the packages, and with -o or --outdated it will show the latest version but only for packages that are outdated. Search for dependencies \u2691 This command searches for packages on a remote index. poetry search requests pendulum [Export requirements to \u2691 requirements.txt]( https://python-poetry.org/docs/cli/#export ) poetry export -f requirements.txt --output requirements.txt Project setup \u2691 If you don't already have a cookiecutter for your python projects, you can use poetry new poetry-demo , and it will create the poetry-demo directory with the following content: poetry-demo \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.rst \u251c\u2500\u2500 poetry_demo \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 test_poetry_demo.py If you want to use the src project structure, pass the --src flag. Checking what package is using a dependency \u2691 Even though poetry is supposed to show the information of which packages depend on a specific package with poetry show package , I don't see it. Luckily snejus made a small script that shows the information . Save it somewhere in your PATH . _RED = '\\\\\\\\e[1;31m&\\\\\\\\e[0m' _GREEN = '\\\\\\\\e[1;32m&\\\\\\\\e[0m' _YELLOW = '\\\\\\\\e[1;33m&\\\\\\\\e[0m' _format () { tr -d '\"' | sed \"s/ \\+>[^ ]* \\+<.*/ $_YELLOW /\" | # ~ / ^ / < >= ~ a window sed \"s/ \\+>[^ ]* * $ / $_GREEN /\" | # >= no upper limit sed \"/>/ !s/<.* $ / $_RED /\" | # < ~ upper limit sed \"/>\\|</ !s/ .*/ $_RED /\" # == ~ locked version } _requires () { sed -n \"/^name = \\\" $1 \\\"/I,/\\[\\[package\\]\\]/{ /\\[package.dep/,/^ $ /{ /^[^[]/ { s/= {version = \\(\\\"[^\\\"]*\\\"\\).*/, \\1/p; s/ =/,/gp }}}\" poetry.lock | sed \"/,.*,/!s/</,</; s/^[^<]\\+ $ /&,/\" | column -t -s , | _format } _required_by () { sed -n \"/\\[metadata\\]/,//d; /\\[package\\]\\|\\[package\\.depen/,/^ $ /H; /^name\\|^ $1 = /Ip\" poetry.lock | sed -n \"/^ $1 /I{x;G;p};h\" | sed 's/.*\"\\(.*\\)\".*/\\1/' | sed '$!N;s/\\n/ /' | column -t | _format } deps () { echo echo -e \"\\e[1mREQUIRES\\e[0m\" _requires \" $1 \" | xargs -i echo -e \"\\t{}\" echo echo -e \"\\e[1mREQUIRED BY\\e[0m\" _required_by \" $1 \" | xargs -i echo -e \"\\t{}\" echo } deps $1 Configuration \u2691 Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically be created when you first run that command. This file can typically be found in ~/.config/pypoetry . Poetry also provides the ability to have settings that are specific to a project by passing the --local option to the config command. poetry config virtualenvs.create false --local List the current configuration \u2691 To list the current configuration you can use the --list option of the config command: poetry config --list Which will give you something similar to this: cache-dir = \"/path/to/cache/directory\" virtualenvs.create = true virtualenvs.in-project = null virtualenvs.path = \"{cache-dir}/virtualenvs\" # /path/to/cache/directory/virtualenvs Adding or updating a configuration setting \u2691 To change or otherwise add a new configuration setting, you can pass a value after the setting\u2019s name: poetry config virtualenvs.path /path/to/cache/directory/virtualenvs For a full list of the supported settings see Available settings . Removing a specific setting \u2691 If you want to remove a previously set setting, you can use the --unset option: poetry config virtualenvs.path --unset Adding a repository \u2691 Adding a new repository is easy with the config command. poetry config repositories.foo https://foo.bar/simple/ This will set the url for repository foo to https://foo.bar/simple/ . Configuring credentials \u2691 If you want to store your credentials for a specific repository, you can do so easily: poetry config http-basic.foo username password If you do not specify the password you will be prompted to write it. To publish to PyPI, you can set your credentials for the repository named pypi . Note that it is recommended to use API tokens when uploading packages to PyPI. Once you have created a new token, you can tell Poetry to use it: poetry config pypi-token.pypi my-token If a system keyring is available and supported, the password is stored to and retrieved from the keyring. In the above example, the credential will be stored using the name poetry-repository-pypi . If access to keyring fails or is unsupported, this will fall back to writing the password to the auth.toml file along with the username. Keyring support is enabled using the keyring library. For more information on supported backends refer to the library documentation . It doesn't support pass by default, but Steffen Vogel created a specific keyring backend . Alternatively, you can use environment variables to provide the credentials: export POETRY_PYPI_TOKEN_PYPI = my-token export POETRY_HTTP_BASIC_PYPI_USERNAME = username export POETRY_HTTP_BASIC_PYPI_PASSWORD = password I've tried setting up the keyring but I get the next error: UploadError HTTP Error 403 : Invalid or non - existent authentication information . See https : // pypi . org / help / #invalid-auth for more information. at ~/. venvs / autodev / lib / python3 .9 / site - packages / poetry / publishing / uploader . py : 216 in _upload 212 \u2502 self . _register ( session , url ) 213 \u2502 except HTTPError as e : 214 \u2502 raise UploadError ( e ) 215 \u2502 \u2192 216 \u2502 raise UploadError ( e ) 217 \u2502 218 \u2502 def _do_upload ( 219 \u2502 self , session , url , dry_run = False 220 \u2502 ): # type: (requests.Session, str, Optional[bool]) -> None The keyring was configured with: poetry config pypi-token.pypi internet/pypi.token And I'm sure that the keyring works because python -m keyring get internet pypi.token works. I've also tried with the environmental variable POETRY_PYPI_TOKEN_PYPI but it didn't work either . And setting the configuration as poetry config http-basic.pypi __token__ internet/pypi.token . Finally I had to hardcode the token with poetry config pypi-token.pypi \"$(pass show internet/pypi.token) . Although I can't find where it's storing the value :S. References \u2691 Git Docs", "title": "Poetry"}, {"location": "python_poetry/#installation", "text": "Although the official docs tell you to run: curl -sSL https://install.python-poetry.org | python3 - pip install poetry works too, which looks safer than executing arbitrary code from an url. To enable shell completion for zsh run: # Zsh poetry completions zsh > ~/.zfunc/_poetry # Oh-My-Zsh mkdir $ZSH_CUSTOM /plugins/poetry poetry completions zsh > $ZSH_CUSTOM /plugins/poetry/_poetry For zsh , you must then add the following line in your ~/.zshrc before compinit : fpath += ~/.zfunc For oh-my-zsh , you must then enable poetry in your ~/.zshrc plugins: plugins ( poetry ... )", "title": "Installation"}, {"location": "python_poetry/#basic-usage", "text": "", "title": "Basic Usage"}, {"location": "python_poetry/#initializing-a-pre-existing-project", "text": "Instead of creating a new project, Poetry can be used to \u2018initialise\u2019 a pre-populated directory with poetry init . You can use the next options --name : Name of the package. --description : Description of the package. --author : Author of the package. --python : Compatible Python versions. --dependency : Package to require with a version constraint. Should be in format foo:1.0.0 . --dev-dependency : Development requirements, see --require .", "title": "Initializing a pre-existing project"}, {"location": "python_poetry/#installing-dependencies", "text": "To install the defined dependencies for your project, just run the install command. poetry install When you run this command, one of two things may happen: Installing without poetry.lock : If you have never run the command before and there is also no poetry.lock file present, Poetry simply resolves all dependencies listed in your pyproject.toml file and downloads the latest version of their files. When Poetry has finished installing, it writes all of the packages and the exact versions of them that it downloaded to the poetry.lock file, locking the project to those specific versions. You should commit the poetry.lock file to your project repo so that all people working on the project are locked to the same versions of dependencies. Installing with poetry.lock : If there is already a poetry.lock file as well as a pyproject.toml , poetry resolves and installs all dependencies that you listed in pyproject.toml , but Poetry uses the exact versions listed in poetry.lock to ensure that the package versions are consistent for everyone working on your project. As a result you will have all dependencies requested by your pyproject.toml file, but they may not all be at the very latest available versions (some of the dependencies listed in the poetry.lock file may have released newer versions since the file was created). This is by design, it ensures that your project does not break because of unexpected changes in dependencies. The current project is installed in editable mode by default. If you don't want the development requirements use the --no-dev flag. To remove the untracked dependencies that are no longer in the lock file, use --remove-untracked .", "title": "Installing dependencies"}, {"location": "python_poetry/#updating-dependencies-to-their-latest-versions", "text": "The poetry.lock file prevents you from automatically getting the latest versions of your dependencies. To update to the latest versions, use the update command. This will fetch the latest matching versions (according to your pyproject.toml file) and update the lock file with the new versions. (This is equivalent to deleting the poetry.lock file and running install again.) The main problem is that poetry add does upper pinning of dependencies by default, which is a really bad idea . And they don't plan to change . There is currently no way of updating your pyproject.toml dependency definitions so they match the latest version beyond your constrains. So if you have constrained a package to be <2.0.0 and 3.0.0 is out there, you will have to manually edit the pyproject.toml so that it accepts that new version. There is no automatic process that does this. At least you can use poetry show --outdated and it will tell you which is the new version, and if the output is zero, you're sure you're on the last versions. Some workarounds exists though, if you run poetry add dependency@latest it will update the lock to the latest. MousaZeidBaker made poetryup , a tool that is able to update the requirements to the latest version with poetryup --latest (although it still has some bugs ). Given that it uses poetry add <package>@latest behind the scenes, it will change your version pin to ^<new_version> , which as we've seen it's awful. Again, you should not be trying to do this, it's better to improve how you manage your dependencies .", "title": "Updating dependencies to their latest versions"}, {"location": "python_poetry/#debugging-why-a-package-is-not-updated-to-the-latest-version", "text": "Sometimes packages are not updated with poetry update or poetryup , to debug why, you need to understand if some package is setting a constrain that prevents the upgrade. To do that, first check the outdated packages with poetry show -o and for each of them: Check what packages are using the dependency . Search if there is an issue asking the maintainers to update their dependencies, if it doesn't exist, create it.", "title": "Debugging why a package is not updated to the latest version"}, {"location": "python_poetry/#removing-a-dependency", "text": "poetry remove pendulum With the -D or --dev flag, it removes the dependency from the development ones.", "title": "Removing a dependency"}, {"location": "python_poetry/#building-the-package", "text": "Before you can actually publish your library, you will need to package it. poetry build This command will package your library in two different formats: sdist which is the source format, and wheel which is a compiled package. Once that\u2019s done you are ready to publish your library.", "title": "Building the package"}, {"location": "python_poetry/#publishing-to-pypi", "text": "Poetry will publish to PyPI by default. Anything that is published to PyPI is available automatically through Poetry. poetry publish This will package and publish the library to PyPI, at the condition that you are a registered user and you have configured your credentials properly . If you pass the --build flag, it will also build the package.", "title": "Publishing to PyPI"}, {"location": "python_poetry/#publishing-to-a-private-repository", "text": "Sometimes, you may want to keep your library private but also being accessible to your team. In this case, you will need to use a private repository. You will need to add it to your global list of repositories. Once this is done, you can actually publish to it like so: poetry publish -r my-repository", "title": "Publishing to a private repository"}, {"location": "python_poetry/#specifying-dependencies", "text": "If you want to add dependencies to your project, you can specify them in the tool.poetry.dependencies section. [tool.poetry.dependencies] pendulum = \"^1.4\" As you can see, it takes a mapping of package names and version constraints. Poetry uses this information to search for the right set of files in package \u201crepositories\u201d that you register in the tool.poetry.repositories section, or on PyPI by default. Also, instead of modifying the pyproject.toml file by hand, you can use the add command. poetry add pendulum It will automatically find a suitable version constraint and install the package and subdependencies. If you want to add the dependency to the development ones, use the -D or --dev flag.", "title": "Specifying dependencies"}, {"location": "python_poetry/#using-your-virtual-environment", "text": "By default, poetry creates a virtual environment in {cache-dir}/virtualenvs . You can change the cache-dir value by editing the poetry config. Additionally, you can use the virtualenvs.in-project configuration variable to create virtual environment within your project directory. There are several ways to run commands within this virtual environment. To run your script simply use poetry run python your_script.py . Likewise if you have command line tools such as pytest or black you can run them using poetry run pytest . The easiest way to activate the virtual environment is to create a new shell with poetry shell .", "title": "Using your virtual environment"}, {"location": "python_poetry/#version-management", "text": "poetry version shows the current version of the project. If you pass an argument, it will bump the version of the package, for example poetry version minor . But it doesn't read your commits to decide what kind of bump you apply, so I'd keep on using pip-compile .", "title": "Version Management"}, {"location": "python_poetry/#dependency-specification", "text": "Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed. They don't follow Python's specification PEP508", "title": "Dependency Specification"}, {"location": "python_poetry/#caret-requirements", "text": "Caret requirements allow SemVer compatible updates to a specified version. An update is allowed if the new version number does not modify the left-most non-zero digit in the major, minor, patch grouping. In this case, if we ran poetry update requests , poetry would update us to the next versions: Requirement Versions allowed ^1.2.3 >=1.2.3 <2.0.0 ^1.2 >=1.2.0 <2.0.0 ^1 >=1.0.0 <2.0.0 ^0.2.3 >=0.2.3 <0.3.0 ^0.0.3 >=0.0.3 <0.0.4 ^0.0 >=0.0.0 <0.1.0 ^0 >=0.0.0 <1.0.0", "title": "Caret Requirements"}, {"location": "python_poetry/#tilde-requirements", "text": "Tilde requirements specify a minimal version with some ability to update. If you specify a major, minor, and patch version or only a major and minor version, only patch-level changes are allowed. If you only specify a major version, then minor- and patch-level changes are allowed. Requirement Versions allowed ~1.2.3 >=1.2.3 <1.3.0 ~1.2 >=1.2.0 <1.3.0 ~1 >=1.0.0 <2.0.0", "title": "Tilde requirements"}, {"location": "python_poetry/#wildcard-requirements", "text": "Wildcard requirements allow for the latest (dependency dependent) version where the wildcard is positioned. Requirement Versions allowed * >=0.0.0 1.* >=1.0.0 <2.0.0 1.2.* >=1.2.0 <1.3.0", "title": "Wildcard requirements"}, {"location": "python_poetry/#inequality-requirements", "text": "Inequality requirements allow manually specifying a version range or an exact version to depend on. Here are some examples of inequality requirements: >= 1.2.0 > 1 < 2 != 1.2.3", "title": "Inequality requirements"}, {"location": "python_poetry/#exact-requirements", "text": "You can specify the exact version of a package. This will tell Poetry to install this version and this version only. If other dependencies require a different version, the solver will ultimately fail and abort any install or update procedures. Multiple version requirements can also be separated with a comma, e.g. >= 1.2, < 1.5 .", "title": "Exact requirements"}, {"location": "python_poetry/#git-dependencies", "text": "To depend on a library located in a git repository, the minimum information you need to specify is the location of the repository with the git key: [tool.poetry.dependencies] requests = { git = \"https://github.com/requests/requests.git\" } Since we haven\u2019t specified any other information, Poetry assumes that we intend to use the latest commit on the master branch to build our project. You can combine the git key with the branch key to use another branch. Alternatively, use rev or tag to pin a dependency to a specific commit hash or tagged ref, respectively. For example: [tool.poetry.dependencies] # Get the latest revision on the branch named \"next\" requests = { git = \"https://github.com/kennethreitz/requests.git\" , branch = \"next\" } # Get a revision by its commit hash flask = { git = \"https://github.com/pallets/flask.git\" , rev = \"38eb5d3b\" } # Get a revision by its tag numpy = { git = \"https://github.com/numpy/numpy.git\" , tag = \"v0.13.2\" } When using poetry add you can add: A https cloned repo: poetry add git+https://github.com/sdispater/pendulum.git A ssh cloned repo: poetry add git+ssh://git@github.com/sdispater/pendulum.git If you need to checkout a specific branch, tag or revision, you can specify it when using add: poetry add git+https://github.com/sdispater/pendulum.git#develop poetry add git+https://github.com/sdispater/pendulum.git#2.0.5", "title": "git dependencies"}, {"location": "python_poetry/#path-dependencies", "text": "To depend on a library located in a local directory or file, you can use the path property: [tool.poetry.dependencies] # directory my-package = { path = \"../my-package/\" , develop = false } # file my-package = { path = \"../my-package/dist/my-package-0.1.0.tar.gz\" } When using poetry add , you can point them directly to the package or the file: poetry add ./my-package/ poetry add ../my-package/dist/my-package-0.1.0.tar.gz poetry add ../my-package/dist/my_package-0.1.0.whl If you want the dependency to be installed in editable mode you can specify it in the pyproject.toml file. It means that changes in the local directory will be reflected directly in environment. [tool.poetry.dependencies] my-package = { path = \"../my/path\" , develop = true }", "title": "path dependencies"}, {"location": "python_poetry/#url-dependencies", "text": "To depend on a library located on a remote archive, you can use the url property: [tool.poetry.dependencies] # directory my-package = { url = \"https://example.com/my-package-0.1.0.tar.gz\" } With the corresponding add call: poetry add https://example.com/my-package-0.1.0.tar.gz", "title": "url dependencies"}, {"location": "python_poetry/#python-restricted-dependencies", "text": "You can also specify that a dependency should be installed only for specific Python versions: [tool.poetry.dependencies] pathlib2 = { version = \"^2.2\" , python = \"~2.7\" } [tool.poetry.dependencies] pathlib2 = { version = \"^2.2\" , python = \"~2.7 || ^3.2\" }", "title": "Python restricted dependencies"}, {"location": "python_poetry/#multiple-constraints-dependencies", "text": "Sometimes, one of your dependency may have different version ranges depending on the target Python versions. Let\u2019s say you have a dependency on the package foo which is only compatible with Python <3.0 up to version 1.9 and compatible with Python 3.4+ from version 2.0 . You would declare it like so: [ tool . poetry . dependencies ] foo = [ { version = \"<=1.9\" , python = \"^2.7\" }, { version = \"^2.0\" , python = \"^3.4\" } ]", "title": "Multiple constraints dependencies"}, {"location": "python_poetry/#show-the-available-packages", "text": "To list all of the available packages, you can use the show command. poetry show If you want to see the details of a certain package, you can pass the package name. poetry show pendulum name : pendulum version : 1 .4.2 description : Python datetimes made easy dependencies: - python-dateutil > = 2 .6.1 - tzlocal > = 1 .4 - pytzdata > = 2017 .2.2 By default it will print all the dependencies, if you pass --no-dev it will only show your package's ones. With the -l or --latest it will show the latest version of the packages, and with -o or --outdated it will show the latest version but only for packages that are outdated.", "title": "Show the available packages"}, {"location": "python_poetry/#search-for-dependencies", "text": "This command searches for packages on a remote index. poetry search requests pendulum", "title": "Search for dependencies"}, {"location": "python_poetry/#export-requirements-to", "text": "requirements.txt]( https://python-poetry.org/docs/cli/#export ) poetry export -f requirements.txt --output requirements.txt", "title": "[Export requirements to"}, {"location": "python_poetry/#project-setup", "text": "If you don't already have a cookiecutter for your python projects, you can use poetry new poetry-demo , and it will create the poetry-demo directory with the following content: poetry-demo \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.rst \u251c\u2500\u2500 poetry_demo \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 test_poetry_demo.py If you want to use the src project structure, pass the --src flag.", "title": "Project setup"}, {"location": "python_poetry/#checking-what-package-is-using-a-dependency", "text": "Even though poetry is supposed to show the information of which packages depend on a specific package with poetry show package , I don't see it. Luckily snejus made a small script that shows the information . Save it somewhere in your PATH . _RED = '\\\\\\\\e[1;31m&\\\\\\\\e[0m' _GREEN = '\\\\\\\\e[1;32m&\\\\\\\\e[0m' _YELLOW = '\\\\\\\\e[1;33m&\\\\\\\\e[0m' _format () { tr -d '\"' | sed \"s/ \\+>[^ ]* \\+<.*/ $_YELLOW /\" | # ~ / ^ / < >= ~ a window sed \"s/ \\+>[^ ]* * $ / $_GREEN /\" | # >= no upper limit sed \"/>/ !s/<.* $ / $_RED /\" | # < ~ upper limit sed \"/>\\|</ !s/ .*/ $_RED /\" # == ~ locked version } _requires () { sed -n \"/^name = \\\" $1 \\\"/I,/\\[\\[package\\]\\]/{ /\\[package.dep/,/^ $ /{ /^[^[]/ { s/= {version = \\(\\\"[^\\\"]*\\\"\\).*/, \\1/p; s/ =/,/gp }}}\" poetry.lock | sed \"/,.*,/!s/</,</; s/^[^<]\\+ $ /&,/\" | column -t -s , | _format } _required_by () { sed -n \"/\\[metadata\\]/,//d; /\\[package\\]\\|\\[package\\.depen/,/^ $ /H; /^name\\|^ $1 = /Ip\" poetry.lock | sed -n \"/^ $1 /I{x;G;p};h\" | sed 's/.*\"\\(.*\\)\".*/\\1/' | sed '$!N;s/\\n/ /' | column -t | _format } deps () { echo echo -e \"\\e[1mREQUIRES\\e[0m\" _requires \" $1 \" | xargs -i echo -e \"\\t{}\" echo echo -e \"\\e[1mREQUIRED BY\\e[0m\" _required_by \" $1 \" | xargs -i echo -e \"\\t{}\" echo } deps $1", "title": "Checking what package is using a dependency"}, {"location": "python_poetry/#configuration", "text": "Poetry can be configured via the config command (see more about its usage here) or directly in the config.toml file that will be automatically be created when you first run that command. This file can typically be found in ~/.config/pypoetry . Poetry also provides the ability to have settings that are specific to a project by passing the --local option to the config command. poetry config virtualenvs.create false --local", "title": "Configuration"}, {"location": "python_poetry/#list-the-current-configuration", "text": "To list the current configuration you can use the --list option of the config command: poetry config --list Which will give you something similar to this: cache-dir = \"/path/to/cache/directory\" virtualenvs.create = true virtualenvs.in-project = null virtualenvs.path = \"{cache-dir}/virtualenvs\" # /path/to/cache/directory/virtualenvs", "title": "List the current configuration"}, {"location": "python_poetry/#adding-or-updating-a-configuration-setting", "text": "To change or otherwise add a new configuration setting, you can pass a value after the setting\u2019s name: poetry config virtualenvs.path /path/to/cache/directory/virtualenvs For a full list of the supported settings see Available settings .", "title": "Adding or updating a configuration setting"}, {"location": "python_poetry/#removing-a-specific-setting", "text": "If you want to remove a previously set setting, you can use the --unset option: poetry config virtualenvs.path --unset", "title": "Removing a specific setting"}, {"location": "python_poetry/#adding-a-repository", "text": "Adding a new repository is easy with the config command. poetry config repositories.foo https://foo.bar/simple/ This will set the url for repository foo to https://foo.bar/simple/ .", "title": "Adding a repository"}, {"location": "python_poetry/#configuring-credentials", "text": "If you want to store your credentials for a specific repository, you can do so easily: poetry config http-basic.foo username password If you do not specify the password you will be prompted to write it. To publish to PyPI, you can set your credentials for the repository named pypi . Note that it is recommended to use API tokens when uploading packages to PyPI. Once you have created a new token, you can tell Poetry to use it: poetry config pypi-token.pypi my-token If a system keyring is available and supported, the password is stored to and retrieved from the keyring. In the above example, the credential will be stored using the name poetry-repository-pypi . If access to keyring fails or is unsupported, this will fall back to writing the password to the auth.toml file along with the username. Keyring support is enabled using the keyring library. For more information on supported backends refer to the library documentation . It doesn't support pass by default, but Steffen Vogel created a specific keyring backend . Alternatively, you can use environment variables to provide the credentials: export POETRY_PYPI_TOKEN_PYPI = my-token export POETRY_HTTP_BASIC_PYPI_USERNAME = username export POETRY_HTTP_BASIC_PYPI_PASSWORD = password I've tried setting up the keyring but I get the next error: UploadError HTTP Error 403 : Invalid or non - existent authentication information . See https : // pypi . org / help / #invalid-auth for more information. at ~/. venvs / autodev / lib / python3 .9 / site - packages / poetry / publishing / uploader . py : 216 in _upload 212 \u2502 self . _register ( session , url ) 213 \u2502 except HTTPError as e : 214 \u2502 raise UploadError ( e ) 215 \u2502 \u2192 216 \u2502 raise UploadError ( e ) 217 \u2502 218 \u2502 def _do_upload ( 219 \u2502 self , session , url , dry_run = False 220 \u2502 ): # type: (requests.Session, str, Optional[bool]) -> None The keyring was configured with: poetry config pypi-token.pypi internet/pypi.token And I'm sure that the keyring works because python -m keyring get internet pypi.token works. I've also tried with the environmental variable POETRY_PYPI_TOKEN_PYPI but it didn't work either . And setting the configuration as poetry config http-basic.pypi __token__ internet/pypi.token . Finally I had to hardcode the token with poetry config pypi-token.pypi \"$(pass show internet/pypi.token) . Although I can't find where it's storing the value :S.", "title": "Configuring credentials"}, {"location": "python_poetry/#references", "text": "Git Docs", "title": "References"}, {"location": "python_profiling/", "text": "Profiling is to find out where your code spends its time. Profilers can collect several types of information: timing, function calls, interruptions or cache faults. It can be useful to identify bottlenecks, which should be the first step when trying to optimize some code, or to study the evolution of the performance of your code. Profiling types \u2691 There are two types of profiling: Deterministic Profiling All events are monitored. It provides accurate information but has a big impact on performance (overhead). It means the code runs slower under profiling. Its use in production systems is often impractical. This type of profiling is suitable for small functions. Statistical profiling Sampling the execution state at regular intervals to compute indicators. This method is less accurate, but it also reduces the overhead. Profiling tools \u2691 The profiling tools you should use vary with the code you are working on. If you are writing a single algorithm or a small program, you should use a simple profiler like cProfile or even a fine-grained tool like line_profiler . In contrast, when you are optimizing a whole program, you may want to use a statistical profiler to avoid overhead, such as pyinstrument , or if you're debugging a running process, using py-spy . Deterministic Profiling \u2691 cProfile \u2691 Python comes with two built-in modules for deterministic profiling: cProfile and profile. Both are different implementations of the same interface. The former is a C extension with relatively small overhead, and the latter is a pure Python module. As the official documentation says, the module profile would be suitable when we want to extend the profiler in some way. Otherwise, cProfile is preferred for long-running programs. Unfortunately, there is no built-in module for statistical profiling, but we will see some external packages for it. $: python3 -m cProfile script.py 58 function calls in 9 .419 seconds Ordered by: standard namen calls tottime percall cumtime percall filename:lineno ( function ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:1 ( <module> ) 51 9 .419 0 .185 9 .419 0 .185 part1.py:1 ( computation ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:10 ( function1 ) 1 0 .000 0 .000 9 .243 9 .243 part1.py:15 ( function2 ) 1 0 .000 0 .000 0 .176 0 .176 part1.py:20 ( function3 ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:24 ( main ) Where: ncalls Is the number of calls. We should try to optimize functions that have a lot of calls or consume too much time per call. tottime The total time spent in the function itself, excluding sub calls. This is where we should look closely at. We can see that the function computation is called 51 times, and each time consumes 0.185s. cumtime Cumulative time. It includes sub calls. percall We have two \u201cper call\u201d metrics. The first one: total time per call, and the second one: cumulative time per call. Again, we should focus on the total time metric. We can also sort the functions by some criteria, for example python3 -m cProfile -s tottime script.py . Statistical profiling \u2691 Py-spy \u2691 Py-Spy is a statistical (sampling) profiler that lets you visualize the time each function consumes during the execution. An important feature is that you can attach the profiler without restarting the program or modifying the code, and has a low overhead. This makes the tool highly suitable for production code. To install it, just type: pip install py-spy To test the performance of a file use: py-spy top python3 script.py To assess the performance of a runnin process, specify it's PID: py-spy top --pid $PID They will show a top like interface showing the following data: GIL: 100.00%, Active: 100.00%, Threads: 1 %Own %Total OwnTime TotalTime Function (filename:line) 61.00% 61.00% 10.50s 10.50s computation (script.py:7) 39.00% 39.00% 7.50s 7.50s computation (script.py:6) 0.00% 100.00% 0.000s 18.00s <module> (script.py:30) 0.00% 100.00% 0.000s 18.00s function2 (script.py:18) 0.00% 100.00% 0.000s 18.00s main (script.py:26) 0.00% 100.00% 0.000s 18.00s function1 (script.py:12) pyinstrument \u2691 It is similar to cProfile in the sense that we can\u2019t attach the profiler to a running program, but that is where the similarities end, as pyinstrument doesn't track every function call that your program makes. Instead, it's recording the call stack every 1ms. Install it with: pip install pyinstrument Use: The advantages are that: The output is far more attractive. It has less overhead, so it distorts less the results. Doesn't show the internal calls that make cProfiling result reading difficult. It uses wall-clock time instead of CPU time. So it takes into account the IO time. $: pyinstrument script.py _ ._ __/__ _ _ _ _ _/_ Recorded: 15 :45:20 Samples: 51 /_//_/// /_ \\ / //_// / //_ ' / // Duration: 4 .517 CPU time: 4 .516 / _/ v3.3.0 Program: script.py 4 .516 <module> script.py:2 \u2514\u2500 4 .516 main script.py:25 \u2514\u2500 4 .516 function1 script.py:11 \u251c\u2500 4 .425 function2 script.py:16 \u2502 \u2514\u2500 4 .425 computation script.py:2 \u2514\u2500 0 .092 function3 script.py:21 \u2514\u2500 0 .092 computation script.py:2 With the possibility to generate an HTML report. The disadvantages are that it's only easy to profile python script files, not full packages. You can also profile a chunk of code , which can be useful when developing or for writing performance tests. from pyinstrument import Profiler profiler = Profiler () profiler . start () # code you want to profile profiler . stop () print ( profiler . output_text ( unicode = True , color = True )) To explore the profile in a web browser, use profiler.open_in_browser() . To save this HTML for later, use profiler.output_html() . Introduce profiling in your test workflow \u2691 I run out of time, so here are the starting points: Niklas Meinzer post Pypi page of pytest-benchmark , Docs , Git Docs of pytest-profiling uwpce guide on using pstats The idea is to develop the following ideas: How to integrate profiling with pytest. How to compare benchmark results between CI runs. Some guidelines on writing performance tests And memray looks very promising. References \u2691 Antonio Molner article on Python Profiling", "title": "Profiling"}, {"location": "python_profiling/#profiling-types", "text": "There are two types of profiling: Deterministic Profiling All events are monitored. It provides accurate information but has a big impact on performance (overhead). It means the code runs slower under profiling. Its use in production systems is often impractical. This type of profiling is suitable for small functions. Statistical profiling Sampling the execution state at regular intervals to compute indicators. This method is less accurate, but it also reduces the overhead.", "title": "Profiling types"}, {"location": "python_profiling/#profiling-tools", "text": "The profiling tools you should use vary with the code you are working on. If you are writing a single algorithm or a small program, you should use a simple profiler like cProfile or even a fine-grained tool like line_profiler . In contrast, when you are optimizing a whole program, you may want to use a statistical profiler to avoid overhead, such as pyinstrument , or if you're debugging a running process, using py-spy .", "title": "Profiling tools"}, {"location": "python_profiling/#deterministic-profiling", "text": "", "title": "Deterministic Profiling"}, {"location": "python_profiling/#cprofile", "text": "Python comes with two built-in modules for deterministic profiling: cProfile and profile. Both are different implementations of the same interface. The former is a C extension with relatively small overhead, and the latter is a pure Python module. As the official documentation says, the module profile would be suitable when we want to extend the profiler in some way. Otherwise, cProfile is preferred for long-running programs. Unfortunately, there is no built-in module for statistical profiling, but we will see some external packages for it. $: python3 -m cProfile script.py 58 function calls in 9 .419 seconds Ordered by: standard namen calls tottime percall cumtime percall filename:lineno ( function ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:1 ( <module> ) 51 9 .419 0 .185 9 .419 0 .185 part1.py:1 ( computation ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:10 ( function1 ) 1 0 .000 0 .000 9 .243 9 .243 part1.py:15 ( function2 ) 1 0 .000 0 .000 0 .176 0 .176 part1.py:20 ( function3 ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:24 ( main ) Where: ncalls Is the number of calls. We should try to optimize functions that have a lot of calls or consume too much time per call. tottime The total time spent in the function itself, excluding sub calls. This is where we should look closely at. We can see that the function computation is called 51 times, and each time consumes 0.185s. cumtime Cumulative time. It includes sub calls. percall We have two \u201cper call\u201d metrics. The first one: total time per call, and the second one: cumulative time per call. Again, we should focus on the total time metric. We can also sort the functions by some criteria, for example python3 -m cProfile -s tottime script.py .", "title": "cProfile"}, {"location": "python_profiling/#statistical-profiling", "text": "", "title": "Statistical profiling"}, {"location": "python_profiling/#py-spy", "text": "Py-Spy is a statistical (sampling) profiler that lets you visualize the time each function consumes during the execution. An important feature is that you can attach the profiler without restarting the program or modifying the code, and has a low overhead. This makes the tool highly suitable for production code. To install it, just type: pip install py-spy To test the performance of a file use: py-spy top python3 script.py To assess the performance of a runnin process, specify it's PID: py-spy top --pid $PID They will show a top like interface showing the following data: GIL: 100.00%, Active: 100.00%, Threads: 1 %Own %Total OwnTime TotalTime Function (filename:line) 61.00% 61.00% 10.50s 10.50s computation (script.py:7) 39.00% 39.00% 7.50s 7.50s computation (script.py:6) 0.00% 100.00% 0.000s 18.00s <module> (script.py:30) 0.00% 100.00% 0.000s 18.00s function2 (script.py:18) 0.00% 100.00% 0.000s 18.00s main (script.py:26) 0.00% 100.00% 0.000s 18.00s function1 (script.py:12)", "title": "Py-spy"}, {"location": "python_profiling/#pyinstrument", "text": "It is similar to cProfile in the sense that we can\u2019t attach the profiler to a running program, but that is where the similarities end, as pyinstrument doesn't track every function call that your program makes. Instead, it's recording the call stack every 1ms. Install it with: pip install pyinstrument Use: The advantages are that: The output is far more attractive. It has less overhead, so it distorts less the results. Doesn't show the internal calls that make cProfiling result reading difficult. It uses wall-clock time instead of CPU time. So it takes into account the IO time. $: pyinstrument script.py _ ._ __/__ _ _ _ _ _/_ Recorded: 15 :45:20 Samples: 51 /_//_/// /_ \\ / //_// / //_ ' / // Duration: 4 .517 CPU time: 4 .516 / _/ v3.3.0 Program: script.py 4 .516 <module> script.py:2 \u2514\u2500 4 .516 main script.py:25 \u2514\u2500 4 .516 function1 script.py:11 \u251c\u2500 4 .425 function2 script.py:16 \u2502 \u2514\u2500 4 .425 computation script.py:2 \u2514\u2500 0 .092 function3 script.py:21 \u2514\u2500 0 .092 computation script.py:2 With the possibility to generate an HTML report. The disadvantages are that it's only easy to profile python script files, not full packages. You can also profile a chunk of code , which can be useful when developing or for writing performance tests. from pyinstrument import Profiler profiler = Profiler () profiler . start () # code you want to profile profiler . stop () print ( profiler . output_text ( unicode = True , color = True )) To explore the profile in a web browser, use profiler.open_in_browser() . To save this HTML for later, use profiler.output_html() .", "title": "pyinstrument"}, {"location": "python_profiling/#introduce-profiling-in-your-test-workflow", "text": "I run out of time, so here are the starting points: Niklas Meinzer post Pypi page of pytest-benchmark , Docs , Git Docs of pytest-profiling uwpce guide on using pstats The idea is to develop the following ideas: How to integrate profiling with pytest. How to compare benchmark results between CI runs. Some guidelines on writing performance tests And memray looks very promising.", "title": "Introduce profiling in your test workflow"}, {"location": "python_profiling/#references", "text": "Antonio Molner article on Python Profiling", "title": "References"}, {"location": "python_properties/", "text": "The @property is the pythonic way to use getters and setters in object-oriented programming. It can be used to make methods look like attributes. The property decorator returns an object that proxies any request to set or access the attribute value through the methods we have specified. class Foo : @property def foo ( self ): return 'bar' We can specify a setter function for the new property class Foo : @property def foo ( self ): return self . _foo @foo . setter def foo ( self , value ): self . _foo = value We first decorate the foo method a as getter. Then we decorate a second method with exactly the same name by applying the setter attribute of the originally decorated foo method. The property function returns an object; this object always comes with its own setter attribute, which can then be applied as a decorator to other functions. Using the same name for the get and set methods is not required, but it does help group the multiple methods that access one property together. We can also specify a deletion function with @foo.deleter . We cannot specify a docstring using property decorators, so we need to rely on the property copying the docstring from the initial getter method class Silly : @property def silly ( self ): \"This is a silly property\" print ( \"You are getting silly\" ) return self . _silly @silly . setter def silly ( self , value ): print ( \"You are making silly {} \" . format ( value )) self . _silly = value @silly . deleter def silly ( self ): print ( \"Whoah, you kicked silly!\" ) del self . silly >>> s = Silly () >>> s . silly = \"funny\" You are making silly funny >>> s . silly You are getting silly 'funny' >>> del s . silly Whoah , you kicked silly ! When to use properties \u2691 The most common use of a property is when we have some data on a class that we later want to add behavior to. The fact that methods are just callable attributes, and properties are just customizable attributes can help us make the decision. Methods should typically represent actions; things that can be done to, or performed by, the object. When you call a method, even with only one argument, it should do something. Method names a generally verbs. Once confirming that an attribute is not an action, we need to decide between standard data attributes and properties. In general, always use a standard attribute until you need to control access to that property in some way. In either case, your attribute is usually a noun . The only difference between an attribute and a property is that we can invoke custom actions automatically when a property is retrieved, set, or deleted Cache expensive data \u2691 A common need for custom behavior is caching a value that is difficult to calculate or expensive to look up. We can do this with a custom getter on the property. The first time the value is retrieved, we perform the lookup or calculation. Then we could locally cache the value as a private attribute on our object, and the next time the value is requested, we return the stored data. from urlib.request import urlopen class Webpage : def __init__ ( self , url ): self . url = url self . _content = None @property def content ( self ): if not self . _content : print ( \"Retrieving New Page..\" ) self . _content = urlopen ( self . url ) . read () return self . _content >>> import time >>> webpage = Webpage ( \"http://ccphillips.net/\" ) >>> now = time . time () >>> content1 = webpage . content Retrieving new Page ... >>> time . time () - now 22.43316 >>> now = time . time () >>> content2 = webpage . content >>> time . time () - now 1.926645 >>> content1 == content2 True Attributes calculated on the fly \u2691 Custom getters are also useful for attributes that need to be calculated on the fly, based on other object attributes. clsas AverageList ( list ): @property def average ( self ): return sum ( self ) / len ( self ) >>> a = AverageList ([ 1 , 2 , 3 , 4 ]) >>> a . average 2.5 Of course we could have made this a method instead, but then we should call it calculate_average() , since methods represent actions. But a property called average is more suitable, both easier to type, and easier to read. Abstract properties \u2691 Sometimes you want to define properties in your abstract classes, to do that, use: from abc import ABC , abstractmethod class C ( ABC ): @property @abstractmethod def my_abstract_property ( self ): ... If you want to use an abstract setter, you'll encounter the mypy Decorated property not supported error, you'll need to add a # type: ignore until this issue is solved .", "title": "Properties"}, {"location": "python_properties/#when-to-use-properties", "text": "The most common use of a property is when we have some data on a class that we later want to add behavior to. The fact that methods are just callable attributes, and properties are just customizable attributes can help us make the decision. Methods should typically represent actions; things that can be done to, or performed by, the object. When you call a method, even with only one argument, it should do something. Method names a generally verbs. Once confirming that an attribute is not an action, we need to decide between standard data attributes and properties. In general, always use a standard attribute until you need to control access to that property in some way. In either case, your attribute is usually a noun . The only difference between an attribute and a property is that we can invoke custom actions automatically when a property is retrieved, set, or deleted", "title": "When to use properties"}, {"location": "python_properties/#cache-expensive-data", "text": "A common need for custom behavior is caching a value that is difficult to calculate or expensive to look up. We can do this with a custom getter on the property. The first time the value is retrieved, we perform the lookup or calculation. Then we could locally cache the value as a private attribute on our object, and the next time the value is requested, we return the stored data. from urlib.request import urlopen class Webpage : def __init__ ( self , url ): self . url = url self . _content = None @property def content ( self ): if not self . _content : print ( \"Retrieving New Page..\" ) self . _content = urlopen ( self . url ) . read () return self . _content >>> import time >>> webpage = Webpage ( \"http://ccphillips.net/\" ) >>> now = time . time () >>> content1 = webpage . content Retrieving new Page ... >>> time . time () - now 22.43316 >>> now = time . time () >>> content2 = webpage . content >>> time . time () - now 1.926645 >>> content1 == content2 True", "title": "Cache expensive data"}, {"location": "python_properties/#attributes-calculated-on-the-fly", "text": "Custom getters are also useful for attributes that need to be calculated on the fly, based on other object attributes. clsas AverageList ( list ): @property def average ( self ): return sum ( self ) / len ( self ) >>> a = AverageList ([ 1 , 2 , 3 , 4 ]) >>> a . average 2.5 Of course we could have made this a method instead, but then we should call it calculate_average() , since methods represent actions. But a property called average is more suitable, both easier to type, and easier to read.", "title": "Attributes calculated on the fly"}, {"location": "python_properties/#abstract-properties", "text": "Sometimes you want to define properties in your abstract classes, to do that, use: from abc import ABC , abstractmethod class C ( ABC ): @property @abstractmethod def my_abstract_property ( self ): ... If you want to use an abstract setter, you'll encounter the mypy Decorated property not supported error, you'll need to add a # type: ignore until this issue is solved .", "title": "Abstract properties"}, {"location": "python_sh/", "text": "sh is a full-fledged subprocess replacement so beautiful that makes you want to cry. It allows you to call any program as if it were a function: from sh import ifconfig print ( ifconfig ( \"wlan0\" )) Output: wlan0 Link encap : Ethernet HWaddr 00 : 00 : 00 : 00 : 00 : 00 inet addr : 192.168.1.100 Bcast : 192.168.1.255 Mask : 255.255.255.0 inet6 addr : ffff :: ffff : ffff : ffff : fff / 64 Scope : Link UP BROADCAST RUNNING MULTICAST MTU : 1500 Metric : 1 RX packets : 0 errors : 0 dropped : 0 overruns : 0 frame : 0 TX packets : 0 errors : 0 dropped : 0 overruns : 0 carrier : 0 collisions : 0 txqueuelen : 1000 RX bytes : 0 ( 0 GB ) TX bytes : 0 ( 0 GB ) Note that these aren't Python functions, these are running the binary commands on your system by dynamically resolving your $PATH, much like Bash does, and then wrapping the binary in a function. In this way, all the programs on your system are available to you from within Python. Installation \u2691 pip install sh Usage \u2691 Passing arguments \u2691 sh . ls ( \"-l\" , \"/tmp\" , color = \"never\" ) If the command gives you a syntax error (like pass ), you can use bash. sh . bash ( \"-c\" , \"pass\" ) Handling exceptions \u2691 Normal processes exit with exit code 0. You can access the program return code with RunningCommand.exit_code : output = ls ( \"/\" ) print ( output . exit_code ) # should be 0 If a process terminates, and the exit code is not 0, sh generates an exception dynamically. This lets you catch a specific return code, or catch all error return codes through the base class ErrorReturnCode : try : print ( ls ( \"/some/non-existant/folder\" )) except sh . ErrorReturnCode_2 : print ( \"folder doesn't exist!\" ) create_the_folder () except sh . ErrorReturnCode : print ( \"unknown error\" ) The exception object is an sh command object, which has, between other , the stderr and stdout bytes attributes with the errors. To show them use: except sh . ErrorReturnCode as error : print ( str ( error . stderr , 'utf8' )) Redirecting output \u2691 sh . ifconfig ( _out = \"/tmp/interfaces\" ) Running in background \u2691 By default, each running command blocks until completion. If you have a long-running command, you can put it in the background with the _bg=True special kwarg: # blocks sleep ( 3 ) print ( \"...3 seconds later\" ) # doesn't block p = sleep ( 3 , _bg = True ) print ( \"prints immediately!\" ) p . wait () print ( \"...and 3 seconds later\" ) You\u2019ll notice that you need to call RunningCommand.wait() in order to exit after your command exits. Commands launched in the background ignore SIGHUP , meaning that when their controlling process (the session leader, if there is a controlling terminal) exits, they will not be signalled by the kernel. But because sh commands launch their processes in their own sessions by default, meaning they are their own session leaders, ignoring SIGHUP will normally have no impact. So the only time ignoring SIGHUP will do anything is if you use _new_session=False , in which case the controlling process will probably be the shell from which you launched python, and exiting that shell would normally send a SIGHUP to all child processes. If you want to terminate the process use p.kill() . Output callbacks \u2691 In combination with _bg=True , sh can use callbacks to process output incrementally by passing a callable function to _out and/or _err . This callable will be called for each line (or chunk) of data that your command outputs: from sh import tail def process_output ( line ): print ( line ) p = tail ( \"-f\" , \"/var/log/some_log_file.log\" , _out = process_output , _bg = True ) p . wait () To \u201cquit\u201d your callback, simply return True . This tells the command not to call your callback anymore. This does not kill the process though see Interactive callbacks for how to kill a process from a callback. The line or chunk received by the callback can either be of type str or bytes. If the output could be decoded using the provided encoding, a str will be passed to the callback, otherwise it would be raw bytes. Interactive callbacks \u2691 Commands may communicate with the underlying process interactively through a specific callback signature. Each command launched through sh has an internal STDIN queue.Queue that can be used from callbacks: def interact ( line , stdin ): if line == \"What... is the air-speed velocity of an unladen swallow?\" : stdin . put ( \"What do you mean? An African or European swallow?\" ) elif line == \"Huh? I... I don't know that....AAAAGHHHHHH\" : cross_bridge () return True else : stdin . put ( \"I don't know....AAGGHHHHH\" ) return True p = sh . bridgekeeper ( _out = interact , _bg = True ) p . wait () You can also kill or terminate your process (or send any signal, really) from your callback by adding a third argument to receive the process object: def process_output ( line , stdin , process ): print ( line ) if \"ERROR\" in line : process . kill () return True p = tail ( \"-f\" , \"/var/log/some_log_file.log\" , _out = process_output , _bg = True ) The above code will run, printing lines from some_log_file.log until the word ERROR appears in a line, at which point the tail process will be killed and the script will end. Interacting with programs that ask input from the user \u2691 Note Check the interactive callbacks or this issue , as it looks like a cleaner solution. sh allows you to interact with programs that asks for user input. The documentation is not clear on how to do it, but between the function callbacks documentation, and the example on how to enter an SSH password we can deduce how to do it. Imagine we've got a python script that asks the user to enter a username so it can save it in a file. File: /tmp/script.py answer = input ( \"Enter username: \" ) with open ( \"/tmp/user.txt\" , \"w+\" ) as f : f . write ( answer ) When we run it in the terminal we get prompted and answer with lyz : $: /tmp/script.py Enter username: lyz $: cat /tmp/user.txt lyz To achieve the same goal automatically with sh we'll need to use the function callbacks. They are functions we pass to the sh command through the _out argument. import sys import re aggregated = \"\" def interact ( char , stdin ): global aggregated sys . stdout . write ( char . encode ()) sys . stdout . flush () aggregated += char if re . search ( r \"Enter username: \" , aggregated , re . MULTILINE ): stdin . put ( \"lyz \\n \" ) sh . bash ( \"-c\" , \"/tmp/script.py\" , _out = interact , _out_bufsize = 0 ) In the example above we've created an interact function that will get called on each character of the stdout of the command. It will be called on each character because we passed the argument _out_bufsize=0 . Check the ssh password example to see why we need that. As it's run on each character, and we need to input the username once the program is expecting us to enter the input and not before, we need to keep track of all the printed characters through the global aggregated variable. Once the regular expression matches what we want, sh will inject the desired value. Remember to add the \\n at the end of the string you want to inject. If the output never matches the regular expression, you'll enter an endless loop, so you need to know before hand all the possible user input prompts. Testing \u2691 sh can be patched in your tests the typical way, with unittest.mock.patch() : from unittest.mock import patch import sh def get_something (): return sh . pwd () @patch ( \"sh.pwd\" , create = True ) def test_something ( pwd ): pwd . return_value = \"/\" assert get_something () == \"/\" The important thing to note here is that create=True is set. This is required because sh is a bit magical and patch will fail to find the pwd command as an attribute on the sh module. You may also patch the Command class: from unittest.mock import patch import sh def get_something (): pwd = sh . Command ( \"pwd\" ) return pwd () @patch ( \"sh.Command\" ) def test_something ( Command ): Command () . return_value = \"/\" assert get_something () == \"/\" Notice here we do not need create=True , because Command is not an automatically generated object on the sh module (it actually exists). Tips \u2691 Avoid exception logging when killing a background process \u2691 In order to catch this exception execute your process with _bg_exec=False and execute p.wait() if you want to handle the exception. Otherwise don't use the p.wait() . p = sh . sleep ( 100 , _bg = True , _bg_exc = False ) try : p . kill () p . wait () except sh . SignalException_SIGKILL as err : print ( \"foo\" ) foo References \u2691 Docs Git", "title": "sh"}, {"location": "python_sh/#installation", "text": "pip install sh", "title": "Installation"}, {"location": "python_sh/#usage", "text": "", "title": "Usage"}, {"location": "python_sh/#passing-arguments", "text": "sh . ls ( \"-l\" , \"/tmp\" , color = \"never\" ) If the command gives you a syntax error (like pass ), you can use bash. sh . bash ( \"-c\" , \"pass\" )", "title": "Passing arguments"}, {"location": "python_sh/#handling-exceptions", "text": "Normal processes exit with exit code 0. You can access the program return code with RunningCommand.exit_code : output = ls ( \"/\" ) print ( output . exit_code ) # should be 0 If a process terminates, and the exit code is not 0, sh generates an exception dynamically. This lets you catch a specific return code, or catch all error return codes through the base class ErrorReturnCode : try : print ( ls ( \"/some/non-existant/folder\" )) except sh . ErrorReturnCode_2 : print ( \"folder doesn't exist!\" ) create_the_folder () except sh . ErrorReturnCode : print ( \"unknown error\" ) The exception object is an sh command object, which has, between other , the stderr and stdout bytes attributes with the errors. To show them use: except sh . ErrorReturnCode as error : print ( str ( error . stderr , 'utf8' ))", "title": "Handling exceptions"}, {"location": "python_sh/#redirecting-output", "text": "sh . ifconfig ( _out = \"/tmp/interfaces\" )", "title": "Redirecting output"}, {"location": "python_sh/#running-in-background", "text": "By default, each running command blocks until completion. If you have a long-running command, you can put it in the background with the _bg=True special kwarg: # blocks sleep ( 3 ) print ( \"...3 seconds later\" ) # doesn't block p = sleep ( 3 , _bg = True ) print ( \"prints immediately!\" ) p . wait () print ( \"...and 3 seconds later\" ) You\u2019ll notice that you need to call RunningCommand.wait() in order to exit after your command exits. Commands launched in the background ignore SIGHUP , meaning that when their controlling process (the session leader, if there is a controlling terminal) exits, they will not be signalled by the kernel. But because sh commands launch their processes in their own sessions by default, meaning they are their own session leaders, ignoring SIGHUP will normally have no impact. So the only time ignoring SIGHUP will do anything is if you use _new_session=False , in which case the controlling process will probably be the shell from which you launched python, and exiting that shell would normally send a SIGHUP to all child processes. If you want to terminate the process use p.kill() .", "title": "Running in background"}, {"location": "python_sh/#output-callbacks", "text": "In combination with _bg=True , sh can use callbacks to process output incrementally by passing a callable function to _out and/or _err . This callable will be called for each line (or chunk) of data that your command outputs: from sh import tail def process_output ( line ): print ( line ) p = tail ( \"-f\" , \"/var/log/some_log_file.log\" , _out = process_output , _bg = True ) p . wait () To \u201cquit\u201d your callback, simply return True . This tells the command not to call your callback anymore. This does not kill the process though see Interactive callbacks for how to kill a process from a callback. The line or chunk received by the callback can either be of type str or bytes. If the output could be decoded using the provided encoding, a str will be passed to the callback, otherwise it would be raw bytes.", "title": "Output callbacks"}, {"location": "python_sh/#interactive-callbacks", "text": "Commands may communicate with the underlying process interactively through a specific callback signature. Each command launched through sh has an internal STDIN queue.Queue that can be used from callbacks: def interact ( line , stdin ): if line == \"What... is the air-speed velocity of an unladen swallow?\" : stdin . put ( \"What do you mean? An African or European swallow?\" ) elif line == \"Huh? I... I don't know that....AAAAGHHHHHH\" : cross_bridge () return True else : stdin . put ( \"I don't know....AAGGHHHHH\" ) return True p = sh . bridgekeeper ( _out = interact , _bg = True ) p . wait () You can also kill or terminate your process (or send any signal, really) from your callback by adding a third argument to receive the process object: def process_output ( line , stdin , process ): print ( line ) if \"ERROR\" in line : process . kill () return True p = tail ( \"-f\" , \"/var/log/some_log_file.log\" , _out = process_output , _bg = True ) The above code will run, printing lines from some_log_file.log until the word ERROR appears in a line, at which point the tail process will be killed and the script will end.", "title": "Interactive callbacks"}, {"location": "python_sh/#interacting-with-programs-that-ask-input-from-the-user", "text": "Note Check the interactive callbacks or this issue , as it looks like a cleaner solution. sh allows you to interact with programs that asks for user input. The documentation is not clear on how to do it, but between the function callbacks documentation, and the example on how to enter an SSH password we can deduce how to do it. Imagine we've got a python script that asks the user to enter a username so it can save it in a file. File: /tmp/script.py answer = input ( \"Enter username: \" ) with open ( \"/tmp/user.txt\" , \"w+\" ) as f : f . write ( answer ) When we run it in the terminal we get prompted and answer with lyz : $: /tmp/script.py Enter username: lyz $: cat /tmp/user.txt lyz To achieve the same goal automatically with sh we'll need to use the function callbacks. They are functions we pass to the sh command through the _out argument. import sys import re aggregated = \"\" def interact ( char , stdin ): global aggregated sys . stdout . write ( char . encode ()) sys . stdout . flush () aggregated += char if re . search ( r \"Enter username: \" , aggregated , re . MULTILINE ): stdin . put ( \"lyz \\n \" ) sh . bash ( \"-c\" , \"/tmp/script.py\" , _out = interact , _out_bufsize = 0 ) In the example above we've created an interact function that will get called on each character of the stdout of the command. It will be called on each character because we passed the argument _out_bufsize=0 . Check the ssh password example to see why we need that. As it's run on each character, and we need to input the username once the program is expecting us to enter the input and not before, we need to keep track of all the printed characters through the global aggregated variable. Once the regular expression matches what we want, sh will inject the desired value. Remember to add the \\n at the end of the string you want to inject. If the output never matches the regular expression, you'll enter an endless loop, so you need to know before hand all the possible user input prompts.", "title": "Interacting with programs that ask input from the user"}, {"location": "python_sh/#testing", "text": "sh can be patched in your tests the typical way, with unittest.mock.patch() : from unittest.mock import patch import sh def get_something (): return sh . pwd () @patch ( \"sh.pwd\" , create = True ) def test_something ( pwd ): pwd . return_value = \"/\" assert get_something () == \"/\" The important thing to note here is that create=True is set. This is required because sh is a bit magical and patch will fail to find the pwd command as an attribute on the sh module. You may also patch the Command class: from unittest.mock import patch import sh def get_something (): pwd = sh . Command ( \"pwd\" ) return pwd () @patch ( \"sh.Command\" ) def test_something ( Command ): Command () . return_value = \"/\" assert get_something () == \"/\" Notice here we do not need create=True , because Command is not an automatically generated object on the sh module (it actually exists).", "title": "Testing"}, {"location": "python_sh/#tips", "text": "", "title": "Tips"}, {"location": "python_sh/#avoid-exception-logging-when-killing-a-background-process", "text": "In order to catch this exception execute your process with _bg_exec=False and execute p.wait() if you want to handle the exception. Otherwise don't use the p.wait() . p = sh . sleep ( 100 , _bg = True , _bg_exc = False ) try : p . kill () p . wait () except sh . SignalException_SIGKILL as err : print ( \"foo\" ) foo", "title": "Avoid exception logging when killing a background process"}, {"location": "python_sh/#references", "text": "Docs Git", "title": "References"}, {"location": "python_vlc/", "text": "Python VLC is a library to control vlc from python. There is not usable online documentation, you'll have to go through the help(<component>) inside the python console. Installation \u2691 pip install python-vlc Usage \u2691 Basic usage \u2691 You can create an instance of the vlc player with: import vlc player = vlc . MediaPlayer ( 'path/to/file.mp4' ) The player has the next interesting methods: play() : Opens the program and starts playing, if you've used pause it resumes playing. pause() : Pauses the video stop() : Closes the player. set_fulscreen(1) : Sets to fullscreen if you pass 0 as argument it returns from fullscreen. set_media(vlc.Media('path/to/another/file.mp4')) : Change the reproduced file. It can even play pictures! If you want more control, it's better to use an vlc.Instance object to work with. Configure the instance \u2691 instance = Instance ( '--loop' ) Reproduce many files \u2691 First you need to create a media list: media_list = instance . media_list_new () path = \"/path/to/directory\" files = os . listdir ( path ) for file_ in files : media_list . add_media ( instance . media_new ( os . path . join ( path , s ))) Then create the player: media_player = instance . media_list_player_new () media_player . set_media_list ( media_list ) Now you can use player.next() and player.previous() . Set playback mode \u2691 media_player . set_playback_mode ( vlc . PlaybackMode . loop ) There are the next playback modes: default loop repeat References \u2691 Home Source", "title": "Python VLC"}, {"location": "python_vlc/#installation", "text": "pip install python-vlc", "title": "Installation"}, {"location": "python_vlc/#usage", "text": "", "title": "Usage"}, {"location": "python_vlc/#basic-usage", "text": "You can create an instance of the vlc player with: import vlc player = vlc . MediaPlayer ( 'path/to/file.mp4' ) The player has the next interesting methods: play() : Opens the program and starts playing, if you've used pause it resumes playing. pause() : Pauses the video stop() : Closes the player. set_fulscreen(1) : Sets to fullscreen if you pass 0 as argument it returns from fullscreen. set_media(vlc.Media('path/to/another/file.mp4')) : Change the reproduced file. It can even play pictures! If you want more control, it's better to use an vlc.Instance object to work with.", "title": "Basic usage"}, {"location": "python_vlc/#configure-the-instance", "text": "instance = Instance ( '--loop' )", "title": "Configure the instance"}, {"location": "python_vlc/#reproduce-many-files", "text": "First you need to create a media list: media_list = instance . media_list_new () path = \"/path/to/directory\" files = os . listdir ( path ) for file_ in files : media_list . add_media ( instance . media_new ( os . path . join ( path , s ))) Then create the player: media_player = instance . media_list_player_new () media_player . set_media_list ( media_list ) Now you can use player.next() and player.previous() .", "title": "Reproduce many files"}, {"location": "python_vlc/#set-playback-mode", "text": "media_player . set_playback_mode ( vlc . PlaybackMode . loop ) There are the next playback modes: default loop repeat", "title": "Set playback mode"}, {"location": "python_vlc/#references", "text": "Home Source", "title": "References"}, {"location": "pythonping/", "text": "pythonping is simple way to ping in Python. With it, you can send ICMP Probes to remote devices like you would do from the terminal. Warning: Since using pythonping requires root permissions or granting cap_net_raw capability to the python interpreter, try to measure the latency to a server by other means such as using requests . Installation \u2691 pip install pythonping By default it requires root permissions to run because Operating systems are designed to require root for creating raw IP packets, and sniffing the traffic on the network card. These actions are required to do the ping. If you don't want to run your script with root, you can use the capabilities framework . You can give Python the same capabilities as /bin/ping by doing: sudo setcap cap_net_raw+ep $( readlink -f $( which python )) This will allow Python to capture raw packets, without having to give it full root permission. If you want to remove the permissions you can do: sudo setcap -r $( readlink -f $( which python )) You can check that you've removed it with: sudo getcap $( readlink -f $( which python )) If it doesn't return any output is that it doesn't have any capabilities. Usage \u2691 If you want to see the output immediately, emulating what happens on the terminal, use the verbose flag as below. Otherwise it won't show any information on the stdout . from pythonping import ping ping ( \"127.0.0.1\" , verbose = True ) This will yield the following result. Reply from 127.0.0.1, 9 bytes in 0.17ms Reply from 127.0.0.1, 9 bytes in 0.14ms Reply from 127.0.0.1, 9 bytes in 0.12ms Reply from 127.0.0.1, 9 bytes in 0.12ms Regardless of the verbose mode, the ping function will always return a ResponseList object. This is a special iterable object, containing a list of Response items. In each Response , you can find the packet received and some meta information, like: error_message : contains a string describing the error this response represents. For example, an error could be \u201cNetwork Unreachable\u201d or \u201cFragmentation Required\u201d. If you got a successful response, this property is None.. success : is a bool indicating if the response is successful. time_elapsed : and time_elapsed_ms indicate how long it took to receive this response, respectively in seconds and milliseconds.. On top of that, ResponseList adds some intelligence you can access from its own members. The fields are self-explanatory: rtt_min and rtt_min_ms . rtt_max and rtt_max_ms . rtt_avg and rtt_avg_ms . You can also tune your ping by using some of its additional parameters: size : is an integer that allows you to specify the size of the ICMP payload you desire. timeout : is the number of seconds you wish to wait for a response, before assuming the target is unreachable. payload : allows you to use a specific payload (bytes). count : specify allows you to define how many ICMP packets to send. interval : the time to wait between pings, in seconds. sweep_start and sweep_end : allows you to perform a ping sweep, starting from payload size defined in sweep_start and growing up to size defined in sweep_end. Here, we repeat the payload you provided to match the desired size, or we generate a random one if no payload was provided. Note that if you defined size, these two fields will be ignored. df is a flag that, if set to True, will enable the Don't Fragment flag in the IP header verbose enables the verbose mode, printing output to a stream (see out) out is the target stream of verbose mode. If you enable the verbose mode and do not provide out, verbose output will be send to the sys.stdout stream. You may want to use a file here. match : is a flag that, if set to True, will enable payload matching between a ping request and reply (default behaviour follows that of Windows which counts a successful reply by a matched packet identifier only; Linux behaviour counts a non equivalent payload with a matched packet identifier in reply as fail, such as when pinging 8.8.8.8 with 1000 bytes and the reply is truncated to only the first 74 of request payload with a matching packet identifier). References \u2691 Git ictshore article on pythonping", "title": "pythonping"}, {"location": "pythonping/#installation", "text": "pip install pythonping By default it requires root permissions to run because Operating systems are designed to require root for creating raw IP packets, and sniffing the traffic on the network card. These actions are required to do the ping. If you don't want to run your script with root, you can use the capabilities framework . You can give Python the same capabilities as /bin/ping by doing: sudo setcap cap_net_raw+ep $( readlink -f $( which python )) This will allow Python to capture raw packets, without having to give it full root permission. If you want to remove the permissions you can do: sudo setcap -r $( readlink -f $( which python )) You can check that you've removed it with: sudo getcap $( readlink -f $( which python )) If it doesn't return any output is that it doesn't have any capabilities.", "title": "Installation"}, {"location": "pythonping/#usage", "text": "If you want to see the output immediately, emulating what happens on the terminal, use the verbose flag as below. Otherwise it won't show any information on the stdout . from pythonping import ping ping ( \"127.0.0.1\" , verbose = True ) This will yield the following result. Reply from 127.0.0.1, 9 bytes in 0.17ms Reply from 127.0.0.1, 9 bytes in 0.14ms Reply from 127.0.0.1, 9 bytes in 0.12ms Reply from 127.0.0.1, 9 bytes in 0.12ms Regardless of the verbose mode, the ping function will always return a ResponseList object. This is a special iterable object, containing a list of Response items. In each Response , you can find the packet received and some meta information, like: error_message : contains a string describing the error this response represents. For example, an error could be \u201cNetwork Unreachable\u201d or \u201cFragmentation Required\u201d. If you got a successful response, this property is None.. success : is a bool indicating if the response is successful. time_elapsed : and time_elapsed_ms indicate how long it took to receive this response, respectively in seconds and milliseconds.. On top of that, ResponseList adds some intelligence you can access from its own members. The fields are self-explanatory: rtt_min and rtt_min_ms . rtt_max and rtt_max_ms . rtt_avg and rtt_avg_ms . You can also tune your ping by using some of its additional parameters: size : is an integer that allows you to specify the size of the ICMP payload you desire. timeout : is the number of seconds you wish to wait for a response, before assuming the target is unreachable. payload : allows you to use a specific payload (bytes). count : specify allows you to define how many ICMP packets to send. interval : the time to wait between pings, in seconds. sweep_start and sweep_end : allows you to perform a ping sweep, starting from payload size defined in sweep_start and growing up to size defined in sweep_end. Here, we repeat the payload you provided to match the desired size, or we generate a random one if no payload was provided. Note that if you defined size, these two fields will be ignored. df is a flag that, if set to True, will enable the Don't Fragment flag in the IP header verbose enables the verbose mode, printing output to a stream (see out) out is the target stream of verbose mode. If you enable the verbose mode and do not provide out, verbose output will be send to the sys.stdout stream. You may want to use a file here. match : is a flag that, if set to True, will enable payload matching between a ping request and reply (default behaviour follows that of Windows which counts a successful reply by a matched packet identifier only; Linux behaviour counts a non equivalent payload with a matched packet identifier in reply as fail, such as when pinging 8.8.8.8 with 1000 bytes and the reply is truncated to only the first 74 of request payload with a matching packet identifier).", "title": "Usage"}, {"location": "pythonping/#references", "text": "Git ictshore article on pythonping", "title": "References"}, {"location": "questionary/", "text": "questionary is a Python library based on Prompt Toolkit to effortlessly building pretty command line interfaces. It makes it very easy to query your user for input. Installation \u2691 pip install questionary Usage \u2691 Asking a single question \u2691 Questionary ships with a lot of different Question Types to provide the right prompt for the right question. All of them work in the same way though. import questionary answer = questionary . text ( \"What's your first name\" ) . ask () Since our question is a text prompt, answer will contain the text the user typed after they submitted it. Asking Multiple Questions \u2691 You can use the form() function to ask a collection of Questions . The questions will be asked in the order they are passed to questionary.form . import questionary answers = questionary . form ( first = questionary . confirm ( \"Would you like the next question?\" , default = True ), second = questionary . select ( \"Select item\" , choices = [ \"item1\" , \"item2\" , \"item3\" ]), ) . ask () The output will have the following format: { ' f irs t ' : True , 'seco n d' : 'i te m 2 ' } The prompt() function also allows you to ask a collection of questions, however instead of taking Question instances, it takes a dictionary: import questionary questions = [ { \"type\" : \"confirm\" , \"name\" : \"first\" , \"message\" : \"Would you like the next question?\" , \"default\" : True , }, { \"type\" : \"select\" , \"name\" : \"second\" , \"message\" : \"Select item\" , \"choices\" : [ \"item1\" , \"item2\" , \"item3\" ], }, ] questionary . prompt ( questions ) Conditionally skip questions \u2691 Sometimes it is helpful to be able to skip a question based on a condition. To avoid the need for an if around the question, you can pass the condition when you create the question: import questionary DISABLED = True response = questionary . confirm ( \"Are you amazed?\" ) . skip_if ( DISABLED , default = True ) . ask () If the condition (in this case DISABLED ) is True , the question will be skipped and the default value gets returned, otherwise the user will be prompted as usual and the default value will be ignored. Exit when using control + c \u2691 If you want the question to exit when it receives a KeyboardInterrupt event, use unsafe_ask instead of ask . Question types \u2691 The different question types are meant to cover different use cases. The parameters and configuration options are explained in detail for each type. But before we get into to many details, here is a cheatsheet with the available question types: Use Text to ask for free text input. Use Password to ask for free text where the text is hidden. Use File Path to ask for a file or directory path with autocompletion. Use Confirmation to ask a yes or no question. >>> questionary . confirm ( \"Are you amazed?\" ) . ask () ? Are you amazed ? Yes True Use Select to ask the user to select one item from a beautiful list. Use Raw Select to ask the user to select one item from a list. Use Checkbox to ask the user to select any number of items from a list. Use Autocomplete to ask for free text with autocomplete help. Check the examples to see them in action and how to use them. Styling \u2691 Don't highlight the selected option by default \u2691 If you don't want to highlight the default choice in the select question use the next style: from questionary import Style choice = select ( \"Question title: \" , choices = [ \"a\" , \"b\" , \"c\" ], default = \"a\" , style = Style ([( \"selected\" , \"noreverse\" )]), ) . ask () Testing \u2691 To test questionary code, follow the guidelines of testing prompt_toolkit . References \u2691 Docs Git", "title": "questionary"}, {"location": "questionary/#installation", "text": "pip install questionary", "title": "Installation"}, {"location": "questionary/#usage", "text": "", "title": "Usage"}, {"location": "questionary/#asking-a-single-question", "text": "Questionary ships with a lot of different Question Types to provide the right prompt for the right question. All of them work in the same way though. import questionary answer = questionary . text ( \"What's your first name\" ) . ask () Since our question is a text prompt, answer will contain the text the user typed after they submitted it.", "title": "Asking a single question"}, {"location": "questionary/#asking-multiple-questions", "text": "You can use the form() function to ask a collection of Questions . The questions will be asked in the order they are passed to questionary.form . import questionary answers = questionary . form ( first = questionary . confirm ( \"Would you like the next question?\" , default = True ), second = questionary . select ( \"Select item\" , choices = [ \"item1\" , \"item2\" , \"item3\" ]), ) . ask () The output will have the following format: { ' f irs t ' : True , 'seco n d' : 'i te m 2 ' } The prompt() function also allows you to ask a collection of questions, however instead of taking Question instances, it takes a dictionary: import questionary questions = [ { \"type\" : \"confirm\" , \"name\" : \"first\" , \"message\" : \"Would you like the next question?\" , \"default\" : True , }, { \"type\" : \"select\" , \"name\" : \"second\" , \"message\" : \"Select item\" , \"choices\" : [ \"item1\" , \"item2\" , \"item3\" ], }, ] questionary . prompt ( questions )", "title": "Asking Multiple Questions"}, {"location": "questionary/#conditionally-skip-questions", "text": "Sometimes it is helpful to be able to skip a question based on a condition. To avoid the need for an if around the question, you can pass the condition when you create the question: import questionary DISABLED = True response = questionary . confirm ( \"Are you amazed?\" ) . skip_if ( DISABLED , default = True ) . ask () If the condition (in this case DISABLED ) is True , the question will be skipped and the default value gets returned, otherwise the user will be prompted as usual and the default value will be ignored.", "title": "Conditionally skip questions"}, {"location": "questionary/#exit-when-using-control-c", "text": "If you want the question to exit when it receives a KeyboardInterrupt event, use unsafe_ask instead of ask .", "title": "Exit when using control + c"}, {"location": "questionary/#question-types", "text": "The different question types are meant to cover different use cases. The parameters and configuration options are explained in detail for each type. But before we get into to many details, here is a cheatsheet with the available question types: Use Text to ask for free text input. Use Password to ask for free text where the text is hidden. Use File Path to ask for a file or directory path with autocompletion. Use Confirmation to ask a yes or no question. >>> questionary . confirm ( \"Are you amazed?\" ) . ask () ? Are you amazed ? Yes True Use Select to ask the user to select one item from a beautiful list. Use Raw Select to ask the user to select one item from a list. Use Checkbox to ask the user to select any number of items from a list. Use Autocomplete to ask for free text with autocomplete help. Check the examples to see them in action and how to use them.", "title": "Question types"}, {"location": "questionary/#styling", "text": "", "title": "Styling"}, {"location": "questionary/#dont-highlight-the-selected-option-by-default", "text": "If you don't want to highlight the default choice in the select question use the next style: from questionary import Style choice = select ( \"Question title: \" , choices = [ \"a\" , \"b\" , \"c\" ], default = \"a\" , style = Style ([( \"selected\" , \"noreverse\" )]), ) . ask ()", "title": "Don't highlight the selected option by default"}, {"location": "questionary/#testing", "text": "To test questionary code, follow the guidelines of testing prompt_toolkit .", "title": "Testing"}, {"location": "questionary/#references", "text": "Docs Git", "title": "References"}, {"location": "qwik/", "text": "Qwik is a new kind of web framework that can deliver instantly load web applications at any size or complexity. Your sites and apps can boot with about 1kb of JS (regardless of application complexity), and achieve consistent performance at scale. You can see a good overview in the Qwik presentation . References \u2691 Home Git", "title": "Qwik"}, {"location": "qwik/#references", "text": "Home Git", "title": "References"}, {"location": "ram/", "text": "RAM is a form of computer memory that can be read and changed in any order, typically used to store working data and machine code. A random-access memory device allows data items to be read or written in almost the same amount of time irrespective of the physical location of data inside the memory, in contrast with other direct-access data storage media (such as hard disks), where the time required to read and write data items varies significantly depending on their physical locations on the recording medium, due to mechanical limitations such as media rotation speeds and arm movement. They are the faster device either to read or to write your data. Properties \u2691 RAM sticks vary on: Size: the amount of data that they can hold, measured in GB. Frequency: how often the RAM is accessed per second, measured in MHz. Clock latency (CL): number of cycles before the RAM responds. Type: as the technology evolves there are different types, such as DDR4. Form factor: There are different types of RAM in regards of the devices they'll fit in: 260-pin SO-DIMMs: laptop RAM, shorter, slower, more expensive, and won't fit in a desktop system. 288-pin DIMMs: desktop RAM - required for most desktop motherboards. ECC : Whether it has error correction code. Speed \u2691 RAM's speed is measured as a combination of frequency and clock latency. More cycles per second means the RAM is 'faster', but you also have to consider latency as well. If you compare MHz and CL, you can get an idea of actual speed. For example, 3600 MHz CL18 and 3200 MHz CL16 are the same speed on paper since the faster 3600 MHz takes more clocks to respond, but there are more clocks per second, so the response time is actually the same. !!! note In reality, faster RAM will be a little bit faster in modern architectures. Also, Ryzen specifically prefers 3600 MHz RAM because of how its FCLK works it likes whole-number multipliers, so if it can run at 1800 MHz (x2 = 3600 MHz with the RAM), then it will run 2-3% faster than equivalent 3200 MHz RAM. Summing up, the higher the speed, and the lower the CL, the better the overall performance. RAM latency (lower the better) = (CAS Latency (CL) x 2000 ) / Frequency (MHz) ECC \u2691 Error correction code memory (ECC memory) is a type of computer data storage that uses an error correction code to detect and correct n-bit data corruption which occurs in memory. ECC memory is used in most computers where data corruption cannot be tolerated, for example when using zfs . How to choose a RAM stick \u2691 CPU brand \u2691 Depending on your CPU brand you need to take into account the next advices: Intel: Intel CPUs aren\u2019t massively reliant on the performance of memory while running, which might explain why RAM speed support has historically been rather limited outside of Intel\u2019s enthusiast chipsets (Z-Series) and capped to 2666Mhz (at least until recently). If you\u2019re the owner of an Intel CPU we certainly suggest getting a good quality RAM kit, but the speed of that RAM isn\u2019t as important. Save your money for other components or a RAM capacity upgrade if required. AMD: In stark contrast to Intel, AMD\u2019s more recent \u2018Zen\u2019 line of CPUs has RAM speed almost baked into the architecture of the CPU. AMD\u2019s infinity fabric technology uses the speed of the RAM to pass information across sections of the CPU. This means that better memory will serve to boost the CPU performance as well as helping in those intense applications we mentioned earlier. Motherboard \u2691 Many manufacturers list specific RAM kits as \u2018verified\u2019 with their products, meaning that the manufacturer has tested the motherboard model in question with a specific RAM kit and has confirmed full support for that kit, at its advertised speed and CAS latency. Try to purchase RAM listed on your motherboard\u2019s QVL where possible, for the best compatibility. However, this is almost always impractical given the availability of exact RAM kits at any given time. Achieving stability \u2691 Speed, CAS latency, module size, and module quantity; in order to avoid running into problems you should balance these factors when considering your purchase. For example, 16GB of 3600MHz CL16 memory is much more likely to be stable than 32GB of the same modules, even if the settings in BIOS remain the same. Consider another example - you may want to run 64GB of RAM at 3600MHz, but to get it to run properly you need to lower the speed to 3000MHz. Conclusion \u2691 In summary, a high-performance 3600MHz memory kit is ideal for AMD Ryzen CPUs. Decide the size, speed, if you need ECC and make sure which type of RAM does your CPU and motherboard combo support (ie, DDR3, DDR4, or DDR5), and that you're choosing the correct form factor. Then, buy a kit that is in line with your budget. You're probably looking for DDR4, probably 2x8 = 16 GB . The sweet spot there will likely be 3600 MHz CL18 or 3200 MHz CL16 for $55 or so. Technically, you should check your motherboard's QVL (list of RAM that is guaranteed to work), but most big-name brands should work. There are other things to consider - like, does your cooler interfere with RAM? But, generally only top-tier coolers have RAM fitment issues. References \u2691 How to choose RAM: Speed vs Capacity", "title": "RAM"}, {"location": "ram/#properties", "text": "RAM sticks vary on: Size: the amount of data that they can hold, measured in GB. Frequency: how often the RAM is accessed per second, measured in MHz. Clock latency (CL): number of cycles before the RAM responds. Type: as the technology evolves there are different types, such as DDR4. Form factor: There are different types of RAM in regards of the devices they'll fit in: 260-pin SO-DIMMs: laptop RAM, shorter, slower, more expensive, and won't fit in a desktop system. 288-pin DIMMs: desktop RAM - required for most desktop motherboards. ECC : Whether it has error correction code.", "title": "Properties"}, {"location": "ram/#speed", "text": "RAM's speed is measured as a combination of frequency and clock latency. More cycles per second means the RAM is 'faster', but you also have to consider latency as well. If you compare MHz and CL, you can get an idea of actual speed. For example, 3600 MHz CL18 and 3200 MHz CL16 are the same speed on paper since the faster 3600 MHz takes more clocks to respond, but there are more clocks per second, so the response time is actually the same. !!! note In reality, faster RAM will be a little bit faster in modern architectures. Also, Ryzen specifically prefers 3600 MHz RAM because of how its FCLK works it likes whole-number multipliers, so if it can run at 1800 MHz (x2 = 3600 MHz with the RAM), then it will run 2-3% faster than equivalent 3200 MHz RAM. Summing up, the higher the speed, and the lower the CL, the better the overall performance. RAM latency (lower the better) = (CAS Latency (CL) x 2000 ) / Frequency (MHz)", "title": "Speed"}, {"location": "ram/#ecc", "text": "Error correction code memory (ECC memory) is a type of computer data storage that uses an error correction code to detect and correct n-bit data corruption which occurs in memory. ECC memory is used in most computers where data corruption cannot be tolerated, for example when using zfs .", "title": "ECC"}, {"location": "ram/#how-to-choose-a-ram-stick", "text": "", "title": "How to choose a RAM stick"}, {"location": "ram/#cpu-brand", "text": "Depending on your CPU brand you need to take into account the next advices: Intel: Intel CPUs aren\u2019t massively reliant on the performance of memory while running, which might explain why RAM speed support has historically been rather limited outside of Intel\u2019s enthusiast chipsets (Z-Series) and capped to 2666Mhz (at least until recently). If you\u2019re the owner of an Intel CPU we certainly suggest getting a good quality RAM kit, but the speed of that RAM isn\u2019t as important. Save your money for other components or a RAM capacity upgrade if required. AMD: In stark contrast to Intel, AMD\u2019s more recent \u2018Zen\u2019 line of CPUs has RAM speed almost baked into the architecture of the CPU. AMD\u2019s infinity fabric technology uses the speed of the RAM to pass information across sections of the CPU. This means that better memory will serve to boost the CPU performance as well as helping in those intense applications we mentioned earlier.", "title": "CPU brand"}, {"location": "ram/#motherboard", "text": "Many manufacturers list specific RAM kits as \u2018verified\u2019 with their products, meaning that the manufacturer has tested the motherboard model in question with a specific RAM kit and has confirmed full support for that kit, at its advertised speed and CAS latency. Try to purchase RAM listed on your motherboard\u2019s QVL where possible, for the best compatibility. However, this is almost always impractical given the availability of exact RAM kits at any given time.", "title": "Motherboard"}, {"location": "ram/#achieving-stability", "text": "Speed, CAS latency, module size, and module quantity; in order to avoid running into problems you should balance these factors when considering your purchase. For example, 16GB of 3600MHz CL16 memory is much more likely to be stable than 32GB of the same modules, even if the settings in BIOS remain the same. Consider another example - you may want to run 64GB of RAM at 3600MHz, but to get it to run properly you need to lower the speed to 3000MHz.", "title": "Achieving stability"}, {"location": "ram/#conclusion", "text": "In summary, a high-performance 3600MHz memory kit is ideal for AMD Ryzen CPUs. Decide the size, speed, if you need ECC and make sure which type of RAM does your CPU and motherboard combo support (ie, DDR3, DDR4, or DDR5), and that you're choosing the correct form factor. Then, buy a kit that is in line with your budget. You're probably looking for DDR4, probably 2x8 = 16 GB . The sweet spot there will likely be 3600 MHz CL18 or 3200 MHz CL16 for $55 or so. Technically, you should check your motherboard's QVL (list of RAM that is guaranteed to work), but most big-name brands should work. There are other things to consider - like, does your cooler interfere with RAM? But, generally only top-tier coolers have RAM fitment issues.", "title": "Conclusion"}, {"location": "ram/#references", "text": "How to choose RAM: Speed vs Capacity", "title": "References"}, {"location": "redox/", "text": "Redox Installation \u2691 First flash: Download the hex from the via website Try to flash it many times reseting the promicros. sudo avrdude -b 57600 -p m32u4 -P /dev/ttyACM0 -c avr109 -D -U flash:w:redox_rev1_base_via.hex Once the write has finished install via: https://github.com/the-via/releases/releases Reboot the computer launch it with via-nativia . References \u2691 Git", "title": "Redox"}, {"location": "redox/#installation", "text": "First flash: Download the hex from the via website Try to flash it many times reseting the promicros. sudo avrdude -b 57600 -p m32u4 -P /dev/ttyACM0 -c avr109 -D -U flash:w:redox_rev1_base_via.hex Once the write has finished install via: https://github.com/the-via/releases/releases Reboot the computer launch it with via-nativia .", "title": "Installation"}, {"location": "redox/#references", "text": "Git", "title": "References"}, {"location": "refinement_template/", "text": "Refinement \u2691 Doubts \u2691 Expected current sprint undone tasks \u2691 Review the proposed Kanban board \u2691 Sprint Goals \u2691 With this proposed plan we'll: *", "title": "Refinement Template"}, {"location": "refinement_template/#refinement", "text": "", "title": "Refinement"}, {"location": "refinement_template/#doubts", "text": "", "title": "Doubts"}, {"location": "refinement_template/#expected-current-sprint-undone-tasks", "text": "", "title": "Expected current sprint undone tasks"}, {"location": "refinement_template/#review-the-proposed-kanban-board", "text": "", "title": "Review the proposed Kanban board"}, {"location": "refinement_template/#sprint-goals", "text": "With this proposed plan we'll: *", "title": "Sprint Goals"}, {"location": "regicide/", "text": "Regicide is a wonderful cooperative card game for 1 to 4 players. It's awesome how they've built such a rich game dynamic with a normal deck of cards. Even if you can play it with any deck, I suggest to buy the deck they sell because their cards are magnificent and they deserve the money for their impressive game. Another thing I love about them is that even if you can't or don't want to pay for the game, they give the rules for free . If you don't like reading the rules directly from their pdf (although it's quite short), they explain them in this video . Variations \u2691 Each game is different and challenging despite your experience, even so, I've thought that to make it even more varied and rich, the players can use one or many of the next expansions: Situation modifiers . Player modifiers . Each of the expansions make the game both more different and more complex, so it's not suggested to use them all at the same time, try one, and once the players are used to it, add another one. These expansions are yet untested ideas, so they might break the playability. If you have any suggestion please contact me or open a pull request. Throughout the next sections you'll encounter the 1 x level notation to define the times an action will take place. It's assumed that the level of the enemies is: Card Level J 1 Q 2 K 3 Situation modifiers \u2691 You can spice up each game by making each round different by applying situation modifiers. Once a new enemy arrives the scene, roll up a dice to choose one of the next situations: Disloyal : The fighters you use in this round that match the enemy's suit will be disloyal to you and will join the enemy's ranks in the next round. The player will receive damage of both the enemy and their minions. Players will need to kill their minions before they hit the royal enemy. Cornered : You're cornered and the enemy archers are firing you. At the start of each player turn, it takes 2 x level amount of damage. Exterminator : When it deals damage, 1 x level of the player discarded cards are taken out of the game. Enemy ambush : When the enemy enters the scene, it deals 2 x level amount of damage to the players. Enemy Spy : It has moles inside your ranks. When it comes to scene, you drop 1 x level amount of cards of that suit. Necromancer : When it hits the first player, the smallest discarded card goes to the enemy ranks instead of the discarded pile. Players need to kill this minion before hitting the enemy. Represor Enemy : It kills 1 x level people from the tavern at the start of each player's turn. Dexterous Enemy : It has a 20% x level of chances to dodge the player's attack. Quick Enemy : The enemy hits you in the first phase of your turn, instead of the last. Stronger Enemy : It deals 2 x level more damage. Tougher Enemy : It has 3 x level more health. Blind Enemy : It attacks to any player that makes a noise in addition to the player that is currently fighting it. Random Enemy : There is no logic in it's actions, instead of attacking the player who is playing, it attacks a random one. Uneventful round : Nothing happens, play the round as the vanilla game. Softer Enemy : It has 3 x level less health. Weaker Enemy : It deals 2 x level less damage. Clumsy Enemy : It has a 20% x level of chances to fail when hitting the players. Resistance Spies : You have moles inside the enemy ranks that removes their suit invulnerability. Ambush : When the enemy enters the scene, you all deal 2 x level amount of damage to the enemy. Communist/Anarchist \"enemy\" : It really is on your side to bring down the monarchy, so you all get a 1 x level to all the cards you play, and a 2 x level to it's suit. Enemy Cornered : You cornered the enemy, and your archers are firing them. At the start of each player turn, the enemy takes 2 x level amount of damage. Loyal : The fighters you use in this round that match the enemy's suit will be loyal to you and won't be moved to the discarded pile at the end of the round. On the first round of each player, they'll use both the card in the table and the one that they use. Player modifiers \u2691 At the start of the game players can decide their suit, they will get a bonus on the played cards of their suit, and a penalization on the opposite suit. The opposite suits are: \u2660 opposite of \u2665 \u2663 opposite of \u2666 The bonus depends on the level of the enemy being: J: +1 or -1 Q: +2 or -2 K: +3 or -3 Imagine that I've chosen \u2666 as my suit, if I were to play: The 8\u2666 against a J\u2665, I'd draw 8+1 cards from the deck, and deal 8+1 damage The 7\u2663 against a Q\u2660, I'd deal 10 of damage (7-2) * 2 . The 4\u26664\u2665 against a K\u2663, I'd heal and draw 11 cards (4+4+3) . The 4\u26664\u2663 against a K\u2660, I'd draw 8 cards (4+4+3-3) and deal 16 of damage. I haven't decide yet if the bonus should apply at the time of receiving damage, we played one without counting and it was playable, will test the other soon. Player classes \u2691 Create player classes: Each player class has abilities that can use X amount of times in the match: Archer: Healer: Paladin: Warrior: Wizard: References \u2691 Homepage", "title": "Regicide"}, {"location": "regicide/#variations", "text": "Each game is different and challenging despite your experience, even so, I've thought that to make it even more varied and rich, the players can use one or many of the next expansions: Situation modifiers . Player modifiers . Each of the expansions make the game both more different and more complex, so it's not suggested to use them all at the same time, try one, and once the players are used to it, add another one. These expansions are yet untested ideas, so they might break the playability. If you have any suggestion please contact me or open a pull request. Throughout the next sections you'll encounter the 1 x level notation to define the times an action will take place. It's assumed that the level of the enemies is: Card Level J 1 Q 2 K 3", "title": "Variations"}, {"location": "regicide/#situation-modifiers", "text": "You can spice up each game by making each round different by applying situation modifiers. Once a new enemy arrives the scene, roll up a dice to choose one of the next situations: Disloyal : The fighters you use in this round that match the enemy's suit will be disloyal to you and will join the enemy's ranks in the next round. The player will receive damage of both the enemy and their minions. Players will need to kill their minions before they hit the royal enemy. Cornered : You're cornered and the enemy archers are firing you. At the start of each player turn, it takes 2 x level amount of damage. Exterminator : When it deals damage, 1 x level of the player discarded cards are taken out of the game. Enemy ambush : When the enemy enters the scene, it deals 2 x level amount of damage to the players. Enemy Spy : It has moles inside your ranks. When it comes to scene, you drop 1 x level amount of cards of that suit. Necromancer : When it hits the first player, the smallest discarded card goes to the enemy ranks instead of the discarded pile. Players need to kill this minion before hitting the enemy. Represor Enemy : It kills 1 x level people from the tavern at the start of each player's turn. Dexterous Enemy : It has a 20% x level of chances to dodge the player's attack. Quick Enemy : The enemy hits you in the first phase of your turn, instead of the last. Stronger Enemy : It deals 2 x level more damage. Tougher Enemy : It has 3 x level more health. Blind Enemy : It attacks to any player that makes a noise in addition to the player that is currently fighting it. Random Enemy : There is no logic in it's actions, instead of attacking the player who is playing, it attacks a random one. Uneventful round : Nothing happens, play the round as the vanilla game. Softer Enemy : It has 3 x level less health. Weaker Enemy : It deals 2 x level less damage. Clumsy Enemy : It has a 20% x level of chances to fail when hitting the players. Resistance Spies : You have moles inside the enemy ranks that removes their suit invulnerability. Ambush : When the enemy enters the scene, you all deal 2 x level amount of damage to the enemy. Communist/Anarchist \"enemy\" : It really is on your side to bring down the monarchy, so you all get a 1 x level to all the cards you play, and a 2 x level to it's suit. Enemy Cornered : You cornered the enemy, and your archers are firing them. At the start of each player turn, the enemy takes 2 x level amount of damage. Loyal : The fighters you use in this round that match the enemy's suit will be loyal to you and won't be moved to the discarded pile at the end of the round. On the first round of each player, they'll use both the card in the table and the one that they use.", "title": "Situation modifiers"}, {"location": "regicide/#player-modifiers", "text": "At the start of the game players can decide their suit, they will get a bonus on the played cards of their suit, and a penalization on the opposite suit. The opposite suits are: \u2660 opposite of \u2665 \u2663 opposite of \u2666 The bonus depends on the level of the enemy being: J: +1 or -1 Q: +2 or -2 K: +3 or -3 Imagine that I've chosen \u2666 as my suit, if I were to play: The 8\u2666 against a J\u2665, I'd draw 8+1 cards from the deck, and deal 8+1 damage The 7\u2663 against a Q\u2660, I'd deal 10 of damage (7-2) * 2 . The 4\u26664\u2665 against a K\u2663, I'd heal and draw 11 cards (4+4+3) . The 4\u26664\u2663 against a K\u2660, I'd draw 8 cards (4+4+3-3) and deal 16 of damage. I haven't decide yet if the bonus should apply at the time of receiving damage, we played one without counting and it was playable, will test the other soon.", "title": "Player modifiers"}, {"location": "regicide/#player-classes", "text": "Create player classes: Each player class has abilities that can use X amount of times in the match: Archer: Healer: Paladin: Warrior: Wizard:", "title": "Player classes"}, {"location": "regicide/#references", "text": "Homepage", "title": "References"}, {"location": "relationship_management/", "text": "I try to keep my mind as empty as possible of non relevant processes, that's why I use a task manager to handle my tasks and meetings. This system has a side effect, if there isn't something reminding you that you have to do something, you fail to do it. That principle applied to human relationships means that if you don't stumble that person in your daily life, it doesn't exist for you and you will probably not take enough care that the person deserves. To solve that problem I started creating periodic tasks to call these people or hang out. I've also used those tasks to keep a diary of the interactions. Recently I've found Monica a popular open source personal CRM that helps in the same direction. So I'm going to migrate all my information to the system and see how it goes.", "title": "Relationship Management"}, {"location": "remote_work/", "text": "Remote working is a work arrangement in which employees do not commute or travel (e.g. by bus, bicycle or car, etc.) to a central place of work, such as an office building, warehouse, or store. As a side effect, we're spending a lot of time in front of our computers, so we should be careful that our working environment helps us to stay healthy. For example we could: Use an external monitor: Your laptop's screen is usually not big enough and will force you to look down instead of look straight which can lead to neck pain. Some prefer super big monitors (48 inches) while others feel that 24 inches is more than enough so you don't have to turn your head to reach each side of the screen. For me the sweet spot is having two terminals with 100 characters of width one beside the other. If you use a tiling window manager like i3wm , that should be enough. Some people valued that the screen was not fixed, so it could be tilted or it's height could be changed. Adjust the screen to your eye level: The center of the monitor should be at eye level, if the monitor height adjustment is not enough, you can use some old books or buy a screen support. Use an external keyboard: Sometimes the keys of the laptop keyboards have a cheap feedback or a weird key disposition, which leads to finger and wrist aches. The use of an external keyboard (better if it's a mechanical one) can help with this issue. The chair should support your back and don't be too hard to hurt your butt, nor too soft. Your legs should not be hanging in the air, that will add unnecessary pressure on your thighs which can lead to tingling. If you're in this situation, a leg support comes handy. The legs shouldn't be crossed either in front or below you, they should be straight with a 90 degree angle between your thighs and your calves, with a waist level separation between the feet. The table height should be enough to have a 90 degree angle between your forearms and your biceps , and your shoulders are in a relaxed stance. Small people may need a table with no drawers between your elbows and your legs, or you wont be able to fulfill the arm's requirement. The table height should be low enough to fulfill the leg's requirement above. Sometimes they are too high to be able to have a 90 degree angle between the thighs and calves even with feet support, in that case, change the desk or cut it's legs. Think about using a standing desk. Desk's with variable height are quite expensive, but there is always the option to buy a laptop support that let's you stand. Your hands should be at the same level as your forearms, you could use a wrist support for that and also to soften the contact of your forearms with the desk. If you're a heavy mouse user, think of using a vertical mouse instead of the traditional to prevent the metacarpal syndrome. And try not to use it! learn how to use a tiling window manager and Vim shortcuts for everything, such as using tridactyl for Firefox. Keep your working place between 19 and 21 degrees centigrades, otherwise you may unawarely contract your body. Use blue filter either in your glasses or in your screen. Have enough light so you don't need to strain your eyes. Having your monitor screen as your only source of light is harmful. Try to promote initiatives that increase the social interaction between your fellow workers. Stand up and walk around at least once each two hours. Meetings are a good moment to do an outside walk. Other tips non related with your work environment but with the consequences of your work experience can be: Don't remain static, doing exercise daily is a must. As you don't need to go out, it's quite easy to fall into the routine of waking up, sit in your computer, eat and go back to sleep. Both anaerobic (pilates, yoga or stretching) and aerobic (running, biking or dancing) give different benefits. Drink enough water, around 8 to 10 glasses per day. Use the extra time that remote working gives you to strengthen your outside work social relationships. Try not to be exposed to any screen light for an hour before you go to sleep. If you use an e-book, don't rely on their builtin light, use an external source instead. If you have a remote work contract, make sure that your employer pays for any upgrades, it's their responsibility.", "title": "Remote working"}, {"location": "renovate/", "text": "Renovate is a program that does automated dependency updates. Multi-platform and multi-language. Why use Renovate? Get pull requests to update your dependencies and lock files. Reduce noise by scheduling when Renovate creates PRs. Renovate finds relevant package files automatically, including in monorepos. You can customize the bot's behavior with configuration files. Share your configuration with ESLint-like config presets. Get replacement PRs to migrate from a deprecated dependency to the community suggested replacement (npm packages only). Open source. Popular (more than 9.7k stars and 1.3k forks) Beautifully integrate with main Git web applications (Gitea, Gitlab, Github). It supports most important languages: Python, Docker, Kubernetes, Terraform, Ansible, Node, ... Behind the scenes \u2691 How Renovate updates a package file \u2691 Renovate: Scans your repositories to detect package files and their dependencies. Checks if any newer versions exist. Raises Pull Requests for available updates. The Pull Requests patch the package files directly, and include Release Notes for the newer versions (if they are available). By default: You'll get separate Pull Requests for each dependency. Major updates are kept separate from non-major updates. References \u2691 Docs", "title": "renovate"}, {"location": "renovate/#behind-the-scenes", "text": "", "title": "Behind the scenes"}, {"location": "renovate/#how-renovate-updates-a-package-file", "text": "Renovate: Scans your repositories to detect package files and their dependencies. Checks if any newer versions exist. Raises Pull Requests for available updates. The Pull Requests patch the package files directly, and include Release Notes for the newer versions (if they are available). By default: You'll get separate Pull Requests for each dependency. Major updates are kept separate from non-major updates.", "title": "How Renovate updates a package file"}, {"location": "renovate/#references", "text": "Docs", "title": "References"}, {"location": "requests/", "text": "Requests is an elegant and simple HTTP library for Python, built for human beings. Installation \u2691 pip install requests Usage \u2691 Download file \u2691 url = \"http://beispiel.dort/ichbineinbild.jpg\" filename = url . split ( \"/\" )[ - 1 ] r = requests . get ( url , timeout = 0.5 ) if r . status_code == 200 : with open ( filename , 'wb' ) as f : f . write ( r . content ) Encode url \u2691 requests . utils . quote ( '/test' , safe = '' ) Get \u2691 requests . get ( '{{ url }}' ) Put url \u2691 requests . put ({{ url }}) Put json data url \u2691 data = { \"key\" : \"value\" } requests . put ({{ url }} json = data ) Use cookies between requests \u2691 You can use Session objects to persists cookies or default data across all requests. s = requests . Session () s . get ( 'https://httpbin.org/cookies/set/sessioncookie/123456789' ) r = s . get ( 'https://httpbin.org/cookies' ) print ( r . text ) # '{\"cookies\": {\"sessioncookie\": \"123456789\"}}' s . auth = ( 'user' , 'pass' ) s . headers . update ({ 'x-test' : 'true' }) # both 'x-test' and 'x-test2' are sent s . get ( 'https://httpbin.org/headers' , headers = { 'x-test2' : 'true' }) References \u2691 Docs", "title": "Requests"}, {"location": "requests/#installation", "text": "pip install requests", "title": "Installation"}, {"location": "requests/#usage", "text": "", "title": "Usage"}, {"location": "requests/#download-file", "text": "url = \"http://beispiel.dort/ichbineinbild.jpg\" filename = url . split ( \"/\" )[ - 1 ] r = requests . get ( url , timeout = 0.5 ) if r . status_code == 200 : with open ( filename , 'wb' ) as f : f . write ( r . content )", "title": "Download file"}, {"location": "requests/#encode-url", "text": "requests . utils . quote ( '/test' , safe = '' )", "title": "Encode url"}, {"location": "requests/#get", "text": "requests . get ( '{{ url }}' )", "title": "Get"}, {"location": "requests/#put-url", "text": "requests . put ({{ url }})", "title": "Put url"}, {"location": "requests/#put-json-data-url", "text": "data = { \"key\" : \"value\" } requests . put ({{ url }} json = data )", "title": "Put json data url"}, {"location": "requests/#use-cookies-between-requests", "text": "You can use Session objects to persists cookies or default data across all requests. s = requests . Session () s . get ( 'https://httpbin.org/cookies/set/sessioncookie/123456789' ) r = s . get ( 'https://httpbin.org/cookies' ) print ( r . text ) # '{\"cookies\": {\"sessioncookie\": \"123456789\"}}' s . auth = ( 'user' , 'pass' ) s . headers . update ({ 'x-test' : 'true' }) # both 'x-test' and 'x-test2' are sent s . get ( 'https://httpbin.org/headers' , headers = { 'x-test2' : 'true' })", "title": "Use cookies between requests"}, {"location": "requests/#references", "text": "Docs", "title": "References"}, {"location": "rich/", "text": "Rich is a Python library for rich text and beautiful formatting in the terminal. Installation \u2691 pip install rich Usage \u2691 Progress display \u2691 Rich can display continuously updated information regarding the progress of long running tasks / file copies etc. The information displayed is configurable, the default will display a description of the \u2018task\u2019, a progress bar, percentage complete, and estimated time remaining. Rich progress display supports multiple tasks, each with a bar and progress information. You can use this to track concurrent tasks where the work is happening in threads or processes. It's beautiful, check it out with python -m rich.progress . Basic Usage \u2691 For basic usage call the track() function, which accepts a sequence (such as a list or range object) and an optional description of the job you are working on. The track method will yield values from the sequence and update the progress information on each iteration. Here\u2019s an example: from rich.progress import track for n in track ( range ( n ), description = \"Processing...\" ): do_work ( n ) Tables \u2691 from rich.console import Console from rich.table import Table table = Table ( title = \"Star Wars Movies\" ) table . add_column ( \"Released\" , justify = \"right\" , style = \"cyan\" , no_wrap = True ) table . add_column ( \"Title\" , style = \"magenta\" ) table . add_column ( \"Box Office\" , justify = \"right\" , style = \"green\" ) table . add_row ( \"Dec 20, 2019\" , \"Star Wars: The Rise of Skywalker\" , \"$952,110,690\" ) table . add_row ( \"May 25, 2018\" , \"Solo: A Star Wars Story\" , \"$393,151,347\" ) table . add_row ( \"Dec 15, 2017\" , \"Star Wars Ep. V111: The Last Jedi\" , \"$1,332,539,889\" ) table . add_row ( \"Dec 16, 2016\" , \"Rogue One: A Star Wars Story\" , \"$1,332,439,889\" ) console = Console () console . print ( table ) Rich text \u2691 from rich.console import Console from rich.text import Text console = Console () text = Text . assemble (( \"Hello\" , \"bold magenta\" ), \" World!\" ) console . print ( text ) Live display text \u2691 import time from rich.live import Live with Live ( \"Test\" ) as live : for row in range ( 12 ): live . update ( f \"Test { row } \" ) time . sleep ( 0.4 ) If you don't want the text to have the default colors, you can embed it all in a Text object. References \u2691 Git Docs", "title": "rich"}, {"location": "rich/#installation", "text": "pip install rich", "title": "Installation"}, {"location": "rich/#usage", "text": "", "title": "Usage"}, {"location": "rich/#progress-display", "text": "Rich can display continuously updated information regarding the progress of long running tasks / file copies etc. The information displayed is configurable, the default will display a description of the \u2018task\u2019, a progress bar, percentage complete, and estimated time remaining. Rich progress display supports multiple tasks, each with a bar and progress information. You can use this to track concurrent tasks where the work is happening in threads or processes. It's beautiful, check it out with python -m rich.progress .", "title": "Progress display"}, {"location": "rich/#basic-usage", "text": "For basic usage call the track() function, which accepts a sequence (such as a list or range object) and an optional description of the job you are working on. The track method will yield values from the sequence and update the progress information on each iteration. Here\u2019s an example: from rich.progress import track for n in track ( range ( n ), description = \"Processing...\" ): do_work ( n )", "title": "Basic Usage"}, {"location": "rich/#tables", "text": "from rich.console import Console from rich.table import Table table = Table ( title = \"Star Wars Movies\" ) table . add_column ( \"Released\" , justify = \"right\" , style = \"cyan\" , no_wrap = True ) table . add_column ( \"Title\" , style = \"magenta\" ) table . add_column ( \"Box Office\" , justify = \"right\" , style = \"green\" ) table . add_row ( \"Dec 20, 2019\" , \"Star Wars: The Rise of Skywalker\" , \"$952,110,690\" ) table . add_row ( \"May 25, 2018\" , \"Solo: A Star Wars Story\" , \"$393,151,347\" ) table . add_row ( \"Dec 15, 2017\" , \"Star Wars Ep. V111: The Last Jedi\" , \"$1,332,539,889\" ) table . add_row ( \"Dec 16, 2016\" , \"Rogue One: A Star Wars Story\" , \"$1,332,439,889\" ) console = Console () console . print ( table )", "title": "Tables"}, {"location": "rich/#rich-text", "text": "from rich.console import Console from rich.text import Text console = Console () text = Text . assemble (( \"Hello\" , \"bold magenta\" ), \" World!\" ) console . print ( text )", "title": "Rich text"}, {"location": "rich/#live-display-text", "text": "import time from rich.live import Live with Live ( \"Test\" ) as live : for row in range ( 12 ): live . update ( f \"Test { row } \" ) time . sleep ( 0.4 ) If you don't want the text to have the default colors, you can embed it all in a Text object.", "title": "Live display text"}, {"location": "rich/#references", "text": "Git Docs", "title": "References"}, {"location": "rtorrent/", "text": "Debugging \u2691 Get into the docker with docker exec -it docker_name bash cd /home/nobody Open the rtorrent.sh file add set -x above the line you think is starting your rtorrent and set +x below to fetch the command that is launching your rtorrent instance, for example: /usr/bin/tmux new-session -d -s rt -n rtorrent /usr/bin/rtorrent -b 12.5.232.12 -o ip=232.234.324.211 If you manually run /usr/bin/rtorrent -b 12.5.232.12 -o ip=232.234.324.211 you'll get more information on why rtorrent is not starting.", "title": "Rtorrent"}, {"location": "rtorrent/#debugging", "text": "Get into the docker with docker exec -it docker_name bash cd /home/nobody Open the rtorrent.sh file add set -x above the line you think is starting your rtorrent and set +x below to fetch the command that is launching your rtorrent instance, for example: /usr/bin/tmux new-session -d -s rt -n rtorrent /usr/bin/rtorrent -b 12.5.232.12 -o ip=232.234.324.211 If you manually run /usr/bin/rtorrent -b 12.5.232.12 -o ip=232.234.324.211 you'll get more information on why rtorrent is not starting.", "title": "Debugging"}, {"location": "scrum/", "text": "Scrum is an agile framework for developing, delivering, and sustaining complex products, with an initial emphasis on software development, although it has been used in other fields such as personal task management. It is designed for teams of ten or fewer members, who break their work into goals that can be completed within time-boxed iterations, called sprints, no longer than one month and most commonly two weeks. The Scrum Team track progress in 15-minute time-boxed daily meetings, called daily scrums. At the end of the sprint, the team holds sprint review, to demonstrate the work done, a sprint retrospective to improve continuously, and a sprint planning to prepare next sprint's tasks. For my personal scrum workflow and in the DevOps and DevSecOps teams I've found that Sprint goals are not operative, as multiple unrelated tasks need to be done, so it doesn't make sense to define just one goal. The meetings \u2691 Scrum tries to minimize the time spent in meetings while keeping a clearly defined direction and a healthy environment between all the people involved in the project. To achieve that is uses four types of meetings: Daily . Refinement . Retros . Reviews . Plannings . Daily meetings \u2691 Dailies or weeklies are the meetings where the development team exposes at high level of detail the current work. Similar to the dailies in the scrum terms, in the meeting each development team member exposes: The advances in the assigned tasks, with special interest in the encountered problems and deviations from the steps defined in the refinement. An estimation of the tasks that are going to be left unfinished by the end of the sprint. The goals of the meeting are: Get a general knowledge of what everyone else is doing. Learn from the experience gained by the others while doing their tasks. Get a clear idea of where we stand in terms of completing the sprint tasks. As opposed to what it may seem, this meeting is not meant to keep track of the productivity of each of us, we work based on trust, and know that each of us is working our best. Refinement meetings \u2691 Refinement are the meetings where the development team reviews the issues in the backlog and prepares the tasks that will probably be done in the following sprint. The goals of the meeting are: Next sprint tasks are ready to be worked upon in the next sprint. That means each task: Meets the Definition of Ready. All disambiguation in task description, validation criteria and steps is solved. Make the Planning meeting more dynamic. The meeting is composed of the following phases: Scrum master preparation. Development team refinement. Product owner refinement. Refinement preparation \u2691 To prepare the refinement, the scrum master has to: Make a copy of the Refinement document template . Open the OKRs document if you have one and for category in OKR categories: Select the category label in the issue tracker and select the milestone of the semester. Review which of those issues might enter the next sprint, and set the sprint project on them. Remove the milestone from the issue filter to see if there are interesting issues without the milestone set. Go to the next sprint Kanban board: Order the issues by priority. Make sure there are tasks with the Good first issue label. Make sure that there are more tasks than we can probably do so we can remove some instead of need to review the backlog and add more in the refinement. Fill up the sprint goals section of the refinement document. Create the Refinement developer team and product owner meeting calendar events. Development team refinement meeting \u2691 In this meeting the development team with the help of the scrum master, reviews the tasks to be added to the next sprint. The steps are defined in the refinement template. Product owner refinement meeting \u2691 In this meeting the product owner with the help of the scrum master reviews the tasks to be added to the next sprint. With the refinement document as reference: The expected current sprint undone tasks are reviewed. The sprint goals are discussed, modified and agreed. If there are many changes, we might think of setting the goals together in next sprints. The scrum master does a quick description of each issue. Each task priority is discussed and updated. Retro meetings \u2691 Retrospectives or Retros are the meetings where the scrum team plan ways to increase the quality and effectiveness of the team. The scrum master conducts different dynamics to help the rest of the scrum team inspect how the last Sprint went with regards to individuals, interactions, processes, tools, and their Definition of Done and Ready. Assumptions that led them astray are identified and their origins explored. The most impactful improvements are addressed as soon as possible. They may even be added to the backlog for the next sprint. Although improvements may be implemented at any time, the sprint retrospective provides a formal opportunity to focus on inspection and adaptation. The sprint retrospective concludes the sprint. The meeting consists of five phases, all of them conducted by the scrum master: Set the stage : There is an opening dynamic to give people time to \u201carrive\u201d and get into the right mood. Gather Data : Help everyone remember. Create a shared pool of information (everybody sees the world differently). There is an initial dynamic to measure the general feeling of the team and the issues to analyze further. Generate insights : Analyze why did things happen the way they did, identify patterns and see the big picture. Decide what to do : Pick a few issues to work on and create concrete action plans of how you\u2019ll address them. Adding the as issues in the scrum board. Close the retrospective : Clarify follow-ups, show appreciations, leave the meeting with a general good feeling, and analyze how could the retrospectives improve. If you have no idea how to conduct this meeting, you can take ideas from retromat . The goals of the meeting are: Analyze and draft a plan to iteratively improve the team's well-being, quality and efficiency. Review meetings \u2691 Reviews are the meetings where the product owner presents the sprint work to the rest of the team and the stakeholders. The idea of what is going to be done in the next sprint is also defined in this meeting. The meeting goes as follows: The product owner explains what items have been \u201cDone\u201d and what has not been \u201cDone\u201d. The product owner discuss what went well during the sprint, what problems they ran into, and how those problems were solved. The developers demonstrate the work that it has \u201cDone\u201d and answers questions. The product owner discusses the Product Backlog as it stands in terms of the semester OKRs. The entire group collaborates on what to do next, so that the Sprint Review provides valuable input to subsequent Sprint Planning. As the target audience are the stakeholders, the language must be changed accordingly, we should give overall ideas and not get caught in complicated high tech detailed explanations unless they ask them. The goals of the meeting are: Increase the transparency on what the team has done in the sprint. By explaining to the stake holders: What has been done. The reasons why we've implemented the specific outcomes for the tasks. The deviation from the expected plan of action. The status of the unfinished tasks with an explanation of why weren't they closed. The meaning of the work done in terms of the semester OKRs. Increase the transparency on what the team plans to do for the following sprint by explaining to the stakeholders: What do we plan to do in the next semester. How we plan to do it. The meaning of the plan in terms of the semester OKRs. Get the feedback from the stakeholders. We expect to gather and process their feedback by processing their opinions both of the work done of the past sprint and the work to be done in the next one. It will be gathered by the scrum master and persisted in the board on the planning meetings. Incorporate the stakeholders in the decision making process of the team. By inviting them to define with the rest of the scrum team the tasks for the next sprint. Planning meetings \u2691 Plannings are the meetings where the scrum team decides what it's going to do in the following sprint. The decision is made with the information gathered in the refinement, retro and review sessions. Conducted by the scrum master, usually only the members of the scrum team (developers, product owner and scrum master) are present, but stakeholders can also be invited. If the job has been done in the previous sessions, the backlog should be priorized and refined, so we should only add the newest issues gathered in the retro and review, refine them and decide what we want to do this sprint. The meeting goes as follows: We add the issues raised in the review to the backlog. We analyze the tasks on the top of the backlog, add them to the sprint board without assigning it to any developer. Once all tasks are added, we the stats of past sprints to see if the scope is realistic. The goals of the meeting are: Assert that the tasks added to the sprint follow the global path defined by the semester OKRs. All team has a clear view of what needs to be done. The team makes a realistic work commitment. The roles \u2691 There are three roles required in the scrum team: Product owner. Scrum master. Developer. Product owner \u2691 Scrum product owner is accountable for maximizing the value of the product resulting from the work of the scrum team. It's roles are: Assist the scrum master with: Priorization of the semester OKRs. Monitorization of the status of the semester OKRs on reviews and plannings. Priorization of the sprint tasks. Conduct the daily meetings: Show the Kanban board in the meeting Remind the number of weeks left until the review meeting. Make sure that the team is aware of what tasks are going to be left undone at the end of the sprint. Inform the affected stakeholders of the possible delay. Prepare and conduct the review meeting: With the help of the scrum master, prepare the reports: Create the report of the sprint, including: Make sure that the Definition of Done is met for the closed tasks. Explanation of the done tasks. Status of uncompleted tasks, and reason why they weren't complete. The meaning of the work done in terms of the semester OKRs. Create the report of the proposed next sprint's planning, with arguments behind why we do each task. Conduct the review meeting presenting the reports to the stakeholders. Attend the daily, review, retro and planning meetings. Scrum master \u2691 Scrum master is accountable for establishing Scrum as defined in this document. This position is going to be rotated between the members of the scrum team with a period of two sprints. It's roles are: Monitoring the status of the semester OKRs on reviews and plannings. Create new tasks required to meet the objectives. Refining the backlog: Adjust priority. Refine the tasks that are going to enter next sprint. Organize the required meetings to refine the backlog with the team members. Delete deprecated tasks. Assert that issues that are going to enter the new sprint meet the Definition of Ready . Arrange, prepare the daily meetings: Update the calendar events according to the week needs. Arrange, prepare and conduct the review meeting: Create the calendar event inviting the scrum team and the stakeholders. With the help of the product owner, prepare the reports: Create the report of the sprint, including: Make sure that the Definition of Done is met for the closed tasks. Explanation of the done tasks. Status of uncompleted tasks, and reason why they weren't complete. The meaning of the work done in terms of the semester OKRs. Create the report of the proposed next sprint's planning, with arguments behind why we do each task. Update the planning with the requirements of the stakeholders. Upload the review reports to the documentation repository. Arrange, prepare and conduct the refinement meetings: Prepare the tasks that need to be refined: Adjust the priority of the backlog tasks. Select the tasks that are most probably going to enter the next sprint. Expand the description of those tasks so it's understandable by any team member. If the task need some steps to be done before it can be worked upon, do them or create a task to do them before the original task. Create the required refinement calendar events inviting the members of the scrum team. Conduct the refinement meeting. Update the tasks with the outcome of the meeting. Prepare the next sprint's Kanban board. Arrange, prepare and conduct the retro meeting: Prepare the dynamics of the meeting. Create the retro calendar event inviting the members of the scrum team. Conduct the retro meeting. Update the tasks with the outcome of the meeting. Upload the retro reports to the documentation repository. Arrange, prepare and conduct the planning meeting: Make sure that you've done the required refinement sessions to have the tasks and Kanban board ready for the next sprint. Create the planning calendar event inviting the members of the scrum team. Conduct the planning meeting. Update the tasks with the outcome of the meeting and start the sprint. Developer \u2691 Developers are the people in the scrum team that are committed to creating any aspect of a usable increment each sprint. It's roles are: Attend the daily, refinement, review, retro and planning meetings. Focus on completing the assigned sprint tasks. Do the required work or be responsible to coordinate the work that others do for the task to be complete. Make sure that the Definition of Done is met before closing the task. Inter team workflow \u2691 To improve the communication between the teams, you can: Present more clearly the team objectives and reasons behind our tasks, and make the rest of the teams part of the decision making. Be aware of the other team's needs and tasks. To solve the first point, you can offer the rest of the teams different solutions depending the time they want to invest in staying informed: You can invite the other team members to the sprint reviews, where you show the sprint's work and present what you plan to do in the next sprint. This could be the best way to stay informed, as you'll try to sum up everything they need to know in the shortest time. For those that want to be more involved with the decision making inside the team, they could be invited to the planning sessions and even the refinement ones where they are involved. For those that don't want to attend the review, they can either get a summary from other members of their team that did attend, or they can read the meeting notes that you publish after each one. The second point means that your team members become more involved in the other team's work. The different levels of involvement are linked to the amount of time invested and the quality of the interaction. The highest level of involvement would be that a member of your team is also part of the other team. This is easier for those teams that already use Scrum as their agile framework, that means: Attending the team's meetings (retro, review, planning and refinement). Inform the rest of your team of the outcomes of those meetings in the daily meeting. Focus on doing that team's sprint tasks. Populate and refine the tasks related to your team in the other team issue tracker. For those teams that are smaller or don't use Scrum as their agile framework, a your team members could accompany them by: Setting periodic meetings (weekly/biweekly/monthly) to discuss what are they doing, what do they plan to do and how. Create the team related tasks in your backlog, coordinating with the scrum master to refine and prioritize them. Definitions \u2691 Definition of Ready \u2691 The Definition of Ready (DoR) is a list of criteria which must be met before any task can be added to a sprint. It is agreed by the whole scrum team and reviewed in the planning sessions. Expected Benefits \u2691 Avoids beginning work on features that do not have clearly defined completion criteria, which usually translates into costly back-and-forth discussion or rework. Provides the team with an explicit agreement allowing it to \u201cpush back\u201d on accepting ill-defined features to work on. The Definition of Ready provides a checklist which usefully guides pre-implementation activities: discussion, estimation, design. Example of a Definition of Ready \u2691 A task needs to meet the following criteria before being added to a sprint. Have a short title that summarizes the goal of the task. Have a description clear enough so any team member can understand why we need to do the task Have a validation criteria for the task to be done Have a checklist of steps required to meet the validation criteria, clear enough so that any team member can understand them. Have a scope that can be met in one sprint. Have the Priority: label set. If other teams are involved in the task, add the Team: labels. If it's associated to an OKR set the OKR: label. Definition of Done \u2691 The Definition of Done (DoD) is a list of criteria which must be met before any task can be closed. It is agreed by the whole scrum team and reviewed in the planning sessions. Expected Benefits \u2691 The Definition of Done limits the cost of rework once a feature has been accepted as \u201cdone\u201d. Having an explicit contract limits the risk of misunderstanding and conflict between the development team and the customer or product owner. Common Pitfalls \u2691 Obsessing over the list of criteria can be counter-productive; the list needs to define the minimum work generally required to get a product increment to the \u201cdone\u201d state. Individual features or user stories may have specific \u201cdone\u201d criteria in addition to the ones that apply to work in general. If the definition of done is merely a shared understanding, rather than spelled out and displayed on a wall, it may lose much of its effectiveness; a good part of its value lies in being an explicit contract known to all members of the team. Example of a Definition of Done \u2691 A task needs to meet the following criteria before being closed. All changes must be documented. All related pull requests must be merged.", "title": "Scrum"}, {"location": "scrum/#the-meetings", "text": "Scrum tries to minimize the time spent in meetings while keeping a clearly defined direction and a healthy environment between all the people involved in the project. To achieve that is uses four types of meetings: Daily . Refinement . Retros . Reviews . Plannings .", "title": "The meetings"}, {"location": "scrum/#daily-meetings", "text": "Dailies or weeklies are the meetings where the development team exposes at high level of detail the current work. Similar to the dailies in the scrum terms, in the meeting each development team member exposes: The advances in the assigned tasks, with special interest in the encountered problems and deviations from the steps defined in the refinement. An estimation of the tasks that are going to be left unfinished by the end of the sprint. The goals of the meeting are: Get a general knowledge of what everyone else is doing. Learn from the experience gained by the others while doing their tasks. Get a clear idea of where we stand in terms of completing the sprint tasks. As opposed to what it may seem, this meeting is not meant to keep track of the productivity of each of us, we work based on trust, and know that each of us is working our best.", "title": "Daily meetings"}, {"location": "scrum/#refinement-meetings", "text": "Refinement are the meetings where the development team reviews the issues in the backlog and prepares the tasks that will probably be done in the following sprint. The goals of the meeting are: Next sprint tasks are ready to be worked upon in the next sprint. That means each task: Meets the Definition of Ready. All disambiguation in task description, validation criteria and steps is solved. Make the Planning meeting more dynamic. The meeting is composed of the following phases: Scrum master preparation. Development team refinement. Product owner refinement.", "title": "Refinement meetings"}, {"location": "scrum/#refinement-preparation", "text": "To prepare the refinement, the scrum master has to: Make a copy of the Refinement document template . Open the OKRs document if you have one and for category in OKR categories: Select the category label in the issue tracker and select the milestone of the semester. Review which of those issues might enter the next sprint, and set the sprint project on them. Remove the milestone from the issue filter to see if there are interesting issues without the milestone set. Go to the next sprint Kanban board: Order the issues by priority. Make sure there are tasks with the Good first issue label. Make sure that there are more tasks than we can probably do so we can remove some instead of need to review the backlog and add more in the refinement. Fill up the sprint goals section of the refinement document. Create the Refinement developer team and product owner meeting calendar events.", "title": "Refinement preparation"}, {"location": "scrum/#development-team-refinement-meeting", "text": "In this meeting the development team with the help of the scrum master, reviews the tasks to be added to the next sprint. The steps are defined in the refinement template.", "title": "Development team refinement meeting"}, {"location": "scrum/#product-owner-refinement-meeting", "text": "In this meeting the product owner with the help of the scrum master reviews the tasks to be added to the next sprint. With the refinement document as reference: The expected current sprint undone tasks are reviewed. The sprint goals are discussed, modified and agreed. If there are many changes, we might think of setting the goals together in next sprints. The scrum master does a quick description of each issue. Each task priority is discussed and updated.", "title": "Product owner refinement meeting"}, {"location": "scrum/#retro-meetings", "text": "Retrospectives or Retros are the meetings where the scrum team plan ways to increase the quality and effectiveness of the team. The scrum master conducts different dynamics to help the rest of the scrum team inspect how the last Sprint went with regards to individuals, interactions, processes, tools, and their Definition of Done and Ready. Assumptions that led them astray are identified and their origins explored. The most impactful improvements are addressed as soon as possible. They may even be added to the backlog for the next sprint. Although improvements may be implemented at any time, the sprint retrospective provides a formal opportunity to focus on inspection and adaptation. The sprint retrospective concludes the sprint. The meeting consists of five phases, all of them conducted by the scrum master: Set the stage : There is an opening dynamic to give people time to \u201carrive\u201d and get into the right mood. Gather Data : Help everyone remember. Create a shared pool of information (everybody sees the world differently). There is an initial dynamic to measure the general feeling of the team and the issues to analyze further. Generate insights : Analyze why did things happen the way they did, identify patterns and see the big picture. Decide what to do : Pick a few issues to work on and create concrete action plans of how you\u2019ll address them. Adding the as issues in the scrum board. Close the retrospective : Clarify follow-ups, show appreciations, leave the meeting with a general good feeling, and analyze how could the retrospectives improve. If you have no idea how to conduct this meeting, you can take ideas from retromat . The goals of the meeting are: Analyze and draft a plan to iteratively improve the team's well-being, quality and efficiency.", "title": "Retro meetings"}, {"location": "scrum/#review-meetings", "text": "Reviews are the meetings where the product owner presents the sprint work to the rest of the team and the stakeholders. The idea of what is going to be done in the next sprint is also defined in this meeting. The meeting goes as follows: The product owner explains what items have been \u201cDone\u201d and what has not been \u201cDone\u201d. The product owner discuss what went well during the sprint, what problems they ran into, and how those problems were solved. The developers demonstrate the work that it has \u201cDone\u201d and answers questions. The product owner discusses the Product Backlog as it stands in terms of the semester OKRs. The entire group collaborates on what to do next, so that the Sprint Review provides valuable input to subsequent Sprint Planning. As the target audience are the stakeholders, the language must be changed accordingly, we should give overall ideas and not get caught in complicated high tech detailed explanations unless they ask them. The goals of the meeting are: Increase the transparency on what the team has done in the sprint. By explaining to the stake holders: What has been done. The reasons why we've implemented the specific outcomes for the tasks. The deviation from the expected plan of action. The status of the unfinished tasks with an explanation of why weren't they closed. The meaning of the work done in terms of the semester OKRs. Increase the transparency on what the team plans to do for the following sprint by explaining to the stakeholders: What do we plan to do in the next semester. How we plan to do it. The meaning of the plan in terms of the semester OKRs. Get the feedback from the stakeholders. We expect to gather and process their feedback by processing their opinions both of the work done of the past sprint and the work to be done in the next one. It will be gathered by the scrum master and persisted in the board on the planning meetings. Incorporate the stakeholders in the decision making process of the team. By inviting them to define with the rest of the scrum team the tasks for the next sprint.", "title": "Review meetings"}, {"location": "scrum/#planning-meetings", "text": "Plannings are the meetings where the scrum team decides what it's going to do in the following sprint. The decision is made with the information gathered in the refinement, retro and review sessions. Conducted by the scrum master, usually only the members of the scrum team (developers, product owner and scrum master) are present, but stakeholders can also be invited. If the job has been done in the previous sessions, the backlog should be priorized and refined, so we should only add the newest issues gathered in the retro and review, refine them and decide what we want to do this sprint. The meeting goes as follows: We add the issues raised in the review to the backlog. We analyze the tasks on the top of the backlog, add them to the sprint board without assigning it to any developer. Once all tasks are added, we the stats of past sprints to see if the scope is realistic. The goals of the meeting are: Assert that the tasks added to the sprint follow the global path defined by the semester OKRs. All team has a clear view of what needs to be done. The team makes a realistic work commitment.", "title": "Planning meetings"}, {"location": "scrum/#the-roles", "text": "There are three roles required in the scrum team: Product owner. Scrum master. Developer.", "title": "The roles"}, {"location": "scrum/#product-owner", "text": "Scrum product owner is accountable for maximizing the value of the product resulting from the work of the scrum team. It's roles are: Assist the scrum master with: Priorization of the semester OKRs. Monitorization of the status of the semester OKRs on reviews and plannings. Priorization of the sprint tasks. Conduct the daily meetings: Show the Kanban board in the meeting Remind the number of weeks left until the review meeting. Make sure that the team is aware of what tasks are going to be left undone at the end of the sprint. Inform the affected stakeholders of the possible delay. Prepare and conduct the review meeting: With the help of the scrum master, prepare the reports: Create the report of the sprint, including: Make sure that the Definition of Done is met for the closed tasks. Explanation of the done tasks. Status of uncompleted tasks, and reason why they weren't complete. The meaning of the work done in terms of the semester OKRs. Create the report of the proposed next sprint's planning, with arguments behind why we do each task. Conduct the review meeting presenting the reports to the stakeholders. Attend the daily, review, retro and planning meetings.", "title": "Product owner"}, {"location": "scrum/#scrum-master", "text": "Scrum master is accountable for establishing Scrum as defined in this document. This position is going to be rotated between the members of the scrum team with a period of two sprints. It's roles are: Monitoring the status of the semester OKRs on reviews and plannings. Create new tasks required to meet the objectives. Refining the backlog: Adjust priority. Refine the tasks that are going to enter next sprint. Organize the required meetings to refine the backlog with the team members. Delete deprecated tasks. Assert that issues that are going to enter the new sprint meet the Definition of Ready . Arrange, prepare the daily meetings: Update the calendar events according to the week needs. Arrange, prepare and conduct the review meeting: Create the calendar event inviting the scrum team and the stakeholders. With the help of the product owner, prepare the reports: Create the report of the sprint, including: Make sure that the Definition of Done is met for the closed tasks. Explanation of the done tasks. Status of uncompleted tasks, and reason why they weren't complete. The meaning of the work done in terms of the semester OKRs. Create the report of the proposed next sprint's planning, with arguments behind why we do each task. Update the planning with the requirements of the stakeholders. Upload the review reports to the documentation repository. Arrange, prepare and conduct the refinement meetings: Prepare the tasks that need to be refined: Adjust the priority of the backlog tasks. Select the tasks that are most probably going to enter the next sprint. Expand the description of those tasks so it's understandable by any team member. If the task need some steps to be done before it can be worked upon, do them or create a task to do them before the original task. Create the required refinement calendar events inviting the members of the scrum team. Conduct the refinement meeting. Update the tasks with the outcome of the meeting. Prepare the next sprint's Kanban board. Arrange, prepare and conduct the retro meeting: Prepare the dynamics of the meeting. Create the retro calendar event inviting the members of the scrum team. Conduct the retro meeting. Update the tasks with the outcome of the meeting. Upload the retro reports to the documentation repository. Arrange, prepare and conduct the planning meeting: Make sure that you've done the required refinement sessions to have the tasks and Kanban board ready for the next sprint. Create the planning calendar event inviting the members of the scrum team. Conduct the planning meeting. Update the tasks with the outcome of the meeting and start the sprint.", "title": "Scrum master"}, {"location": "scrum/#developer", "text": "Developers are the people in the scrum team that are committed to creating any aspect of a usable increment each sprint. It's roles are: Attend the daily, refinement, review, retro and planning meetings. Focus on completing the assigned sprint tasks. Do the required work or be responsible to coordinate the work that others do for the task to be complete. Make sure that the Definition of Done is met before closing the task.", "title": "Developer"}, {"location": "scrum/#inter-team-workflow", "text": "To improve the communication between the teams, you can: Present more clearly the team objectives and reasons behind our tasks, and make the rest of the teams part of the decision making. Be aware of the other team's needs and tasks. To solve the first point, you can offer the rest of the teams different solutions depending the time they want to invest in staying informed: You can invite the other team members to the sprint reviews, where you show the sprint's work and present what you plan to do in the next sprint. This could be the best way to stay informed, as you'll try to sum up everything they need to know in the shortest time. For those that want to be more involved with the decision making inside the team, they could be invited to the planning sessions and even the refinement ones where they are involved. For those that don't want to attend the review, they can either get a summary from other members of their team that did attend, or they can read the meeting notes that you publish after each one. The second point means that your team members become more involved in the other team's work. The different levels of involvement are linked to the amount of time invested and the quality of the interaction. The highest level of involvement would be that a member of your team is also part of the other team. This is easier for those teams that already use Scrum as their agile framework, that means: Attending the team's meetings (retro, review, planning and refinement). Inform the rest of your team of the outcomes of those meetings in the daily meeting. Focus on doing that team's sprint tasks. Populate and refine the tasks related to your team in the other team issue tracker. For those teams that are smaller or don't use Scrum as their agile framework, a your team members could accompany them by: Setting periodic meetings (weekly/biweekly/monthly) to discuss what are they doing, what do they plan to do and how. Create the team related tasks in your backlog, coordinating with the scrum master to refine and prioritize them.", "title": "Inter team workflow"}, {"location": "scrum/#definitions", "text": "", "title": "Definitions"}, {"location": "scrum/#definition-of-ready", "text": "The Definition of Ready (DoR) is a list of criteria which must be met before any task can be added to a sprint. It is agreed by the whole scrum team and reviewed in the planning sessions.", "title": "Definition of Ready"}, {"location": "scrum/#expected-benefits", "text": "Avoids beginning work on features that do not have clearly defined completion criteria, which usually translates into costly back-and-forth discussion or rework. Provides the team with an explicit agreement allowing it to \u201cpush back\u201d on accepting ill-defined features to work on. The Definition of Ready provides a checklist which usefully guides pre-implementation activities: discussion, estimation, design.", "title": "Expected Benefits"}, {"location": "scrum/#example-of-a-definition-of-ready", "text": "A task needs to meet the following criteria before being added to a sprint. Have a short title that summarizes the goal of the task. Have a description clear enough so any team member can understand why we need to do the task Have a validation criteria for the task to be done Have a checklist of steps required to meet the validation criteria, clear enough so that any team member can understand them. Have a scope that can be met in one sprint. Have the Priority: label set. If other teams are involved in the task, add the Team: labels. If it's associated to an OKR set the OKR: label.", "title": "Example of a Definition of Ready"}, {"location": "scrum/#definition-of-done", "text": "The Definition of Done (DoD) is a list of criteria which must be met before any task can be closed. It is agreed by the whole scrum team and reviewed in the planning sessions.", "title": "Definition of Done"}, {"location": "scrum/#expected-benefits_1", "text": "The Definition of Done limits the cost of rework once a feature has been accepted as \u201cdone\u201d. Having an explicit contract limits the risk of misunderstanding and conflict between the development team and the customer or product owner.", "title": "Expected Benefits"}, {"location": "scrum/#common-pitfalls", "text": "Obsessing over the list of criteria can be counter-productive; the list needs to define the minimum work generally required to get a product increment to the \u201cdone\u201d state. Individual features or user stories may have specific \u201cdone\u201d criteria in addition to the ones that apply to work in general. If the definition of done is merely a shared understanding, rather than spelled out and displayed on a wall, it may lose much of its effectiveness; a good part of its value lies in being an explicit contract known to all members of the team.", "title": "Common Pitfalls"}, {"location": "scrum/#example-of-a-definition-of-done", "text": "A task needs to meet the following criteria before being closed. All changes must be documented. All related pull requests must be merged.", "title": "Example of a Definition of Done"}, {"location": "selenium/", "text": "Selenium is a portable framework for testing web applications. It also provides a test domain-specific language (Selenese) to write tests in a number of popular programming languages. Web driver backends \u2691 Selenium can be used with many browsers, such as Firefox , Chrome or PhantomJS . But first, install selenium : pip install selenium Firefox \u2691 Assuming you've got firefox already installed, you need to download the geckodriver , unpack the tar and add the geckodriver binary somewhere in your PATH . from selenium import webdriver driver = webdriver . Firefox () driver . get ( \"https://duckduckgo.com/\" ) If you need to get the status code of the requests use Chrome instead There is an issue with Firefox that doesn't support this feature. Chrome \u2691 We're going to use Chromium instead of Chrome. Download the chromedriver of the same version as your Chromium, unpack the tar and add the chromedriver binary somewhere in your PATH . from selenium import webdriver from selenium.webdriver.chrome.options import Options opts = Options () opts . binary_location = '/usr/bin/chromium' driver = webdriver . Chrome ( options = opts ) driver . get ( \"https://duckduckgo.com/\" ) If you don't want to see the browser, you can run it in headless mode adding the next line when defining the options : opts . add_argument ( \"--headless\" ) PhantomJS \u2691 PhantomJS is abandoned -> Don't use it The development stopped in 2018 PhantomJS is a headless Webkit, in conjunction with Selenium WebDriver, it can be used to run tests directly from the command line. Since PhantomJS eliminates the need for a graphical browser, tests run much faster. Don't install phantomjs from the official repos as it's not a working release -.-. npm install -g phantomjs didn't work either. I had to download the tar from the downloads page , which didn't work either. The project is abandoned , so don't use this. Usage \u2691 Assuming that you've got a configured driver , to get the url you're in after javascript has done it's magic use the driver.current_url method. To return the HTML of the page use driver.page_source . Open a URL \u2691 driver . get ( \"https://duckduckgo.com/\" ) Get page source \u2691 driver . page_source Get current url \u2691 driver . current_url Click on element \u2691 Once you've opened the page you want to interact with driver.get() , you need to get the Xpath of the element to click on. You can do that by using your browser inspector, to select the element, and once on the code if you right click there is a \"Copy XPath\" Once that is done you should have something like this when you paste it down. //* [ @id = \u201d react - root \u201d ] / section / main / article / div [ 2 ] / div [ 2 ] / p / a Similarly it is the same process for the input fields for username, password, and login button. We can go ahead and do that on the current page. We can store these xpaths as strings in our code to make it readable. We should have three xpaths from this page and one from the initial login. first_login = '//*[@id=\u201dreact-root\u201d]/section/main/article/div[2]/div[2]/p/a' username_input = '//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[2]/div/label/input' password_input = '//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[3]/div/label/input' login_submit = '//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[4]/button/div' Now that we have the xpaths defined we can now tell Selenium webdriver to click and send some keys over for the input fields. from selenium.webdriver.common.by import By driver . find_element ( By . XPATH , first_login ) . click () driver . find_element ( By . XPATH , username_input ) . send_keys ( \"username\" ) driver . find_element ( By . XPATH , password_input ) . send_keys ( \"password\" ) driver . find_element ( By . XPATH , login_submit ) . click () Note Many pages suggest to use methods like find_element_by_name , find_element_by_xpath or find_element_by_id . These are deprecated now . You should use find_element(By. instead. So, instead of: driver . find_element_by_xpath ( \"your_xpath\" ) It should be now: driver . find_element ( By . XPATH , \"your_xpath\" ) Where By is imported with from selenium.webdriver.common.by import By . Close the browser \u2691 driver . close () Change browser configuration \u2691 You can pass options to the initialization of the chromedriver to tweak how does the browser behave. To get a list of the actual prefs you can go to chrome://prefs-internals , there you can get the code you need to tweak. Disable loading of images \u2691 options = ChromeOptions () options . add_experimental_option ( \"prefs\" , { \"profile.default_content_setting_values.images\" : 2 , \"profile.default_content_setting_values.cookies\" : 2 , }, ) Disable site cookies \u2691 options = ChromeOptions () options . add_experimental_option ( \"prefs\" , { \"profile.default_content_setting_values.cookies\" : 2 , }, ) Bypass Selenium detectors \u2691 Sometimes web servers react differently if they notice that you're using selenium. Browsers can be detected through different ways and some commonly used mechanisms are as follows: Implementing captcha / recaptcha to detect the automatic bots. Non-human behaviour (browsing too fast, not scrolling to the visible elements, ...) Using an IP that's flagged as suspicious (VPN, VPS, Tor...) Detecting the term HeadlessChrome within headless Chrome UserAgent Using Bot Management service from Distil Networks , Akamai , Datadome . They do it through different mechanisms: Use undetected-chromedriver Use Selenium stealth Rotate the user agent Changing browser properties Predefined Javascript variables Don't use selenium If you've already been detected, you might get blocked for a plethora of other reasons even after using these methods. So you may have to try accessing the site that was detecting you using a VPN, different user-agent, etc. Use undetected-chromedriver \u2691 undetected-chromedriver is a python library that uses an optimized Selenium Chromedriver patch which does not trigger anti-bot services like Distill Network / Imperva / DataDome / Botprotect.io Automatically downloads the driver binary and patches it. Installation \u2691 pip install undetected-chromedriver Usage \u2691 import undetected_chromedriver.v2 as uc driver = uc . Chrome () driver . get ( 'https://nowsecure.nl' ) # my own test test site with max anti-bot protection If you want to specify the path to the browser use uc.Chrome(browser_executable_path=\"/path/to/your/file\") . Use Selenium Stealth \u2691 selenium-stealth is a python package to prevent detection (by doing most of the steps of this guide) by making selenium more stealthy. Note It's less maintained than undetected-chromedriver so I'd use that other instead. I leave the section in case it's helpful if the other fails for you. Installation \u2691 pip install selenium-stealth Usage \u2691 from selenium import webdriver from selenium_stealth import stealth import time options = webdriver . ChromeOptions () options . add_argument ( \"start-maximized\" ) # options.add_argument(\"--headless\") options . add_experimental_option ( \"excludeSwitches\" , [ \"enable-automation\" ]) options . add_experimental_option ( 'useAutomationExtension' , False ) driver = webdriver . Chrome ( options = options , executable_path = r \"C:\\Users\\DIPRAJ\\Programming\\adclick_bot\\chromedriver.exe\" ) stealth ( driver , languages = [ \"en-US\" , \"en\" ], vendor = \"Google Inc.\" , platform = \"Win32\" , webgl_vendor = \"Intel Inc.\" , renderer = \"Intel Iris OpenGL Engine\" , fix_hairline = True , ) url = \"https://bot.sannysoft.com/\" driver . get ( url ) time . sleep ( 5 ) driver . quit () You can test it with antibot . Rotate the user agent \u2691 Rotating the UserAgent in every execution of your Test Suite using fake_useragent module as follows: from selenium import webdriver from selenium.webdriver.chrome.options import Options from fake_useragent import UserAgent options = Options () ua = UserAgent () userAgent = ua . random print ( userAgent ) options . add_argument ( f 'user-agent= { userAgent } ' ) driver = webdriver . Chrome ( chrome_options = options ) driver . get ( \"https://www.google.co.in\" ) driver . quit () You can also rotate it with execute_cdp_cmd : from selenium import webdriver driver = webdriver . Chrome ( executable_path = r 'C:\\WebDrivers\\chromedriver.exe' ) print ( driver . execute_script ( \"return navigator.userAgent;\" )) # Setting user agent as Chrome/83.0.4103.97 driver . execute_cdp_cmd ( 'Network.setUserAgentOverride' , { \"userAgent\" : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36' }) print ( driver . execute_script ( \"return navigator.userAgent;\" )) Changing browser properties \u2691 Changing the property value of navigator for webdriver to undefined as follows: driver . execute_cdp_cmd ( \"Page.addScriptToEvaluateOnNewDocument\" , { \"source\" : \"\"\" Object.defineProperty(navigator, 'webdriver', { get: () => undefined }) \"\"\" }) You can find a relevant detailed discussion in Selenium webdriver: Modifying navigator.webdriver flag to prevent selenium detection Changing the values of navigator.plugins, navigator.languages, WebGL, hairline feature, missing image, etc. You can find a relevant detailed discussion in Is there a version of selenium webdriver that is not detectable? Changing the conventional Viewport You can find a relevant detailed discussion in How to bypass Google captcha with Selenium and python? Predefined Javascript variables \u2691 One way of detecting Selenium is by checking for predefined JavaScript variables which appear when running with Selenium. The bot detection scripts usually look anything containing word selenium , webdriver in any of the variables (on window object), and also document variables called $cdc_ and $wdc_ . Of course, all of this depends on which browser you are on. All the different browsers expose different things. In Chrome, what people had to do was to ensure that $cdc_ didn't exist as a document variable. You don't need to go compile the chromedriver yourself, if you open the file with vim and execute :%s/cdc_/dog_/g where dog can be any three characters that will work. With perl you can achieve the same result with: perl -pi -e 's/cdc_/dog_/g' /path/to/chromedriver Don't use selenium \u2691 Even with undetected-chromedriver , sometimes servers are able to detect that you're using selenium . A uglier but maybe efective way to go is not using selenium and do a combination of working directly with the chrome devtools protocol with pycdp (using this maintained fork ) and doing the clicks with pyautogui . See an example on this answer . Keep in mind though that these tools don't look to be actively maintained, and that the approach is quite brittle to site changes. Is there really not other way to achieve what you want? Set timeout of a response \u2691 For Firefox and Chromedriver: driver . set_page_load_timeout ( 30 ) The rest: driver . implicitly_wait ( 30 ) This will throw a TimeoutException whenever the page load takes more than 30 seconds. Get the status code of a response \u2691 Surprisingly this is not as easy as with requests, there is no status_code method on the driver, you need to dive into the browser log to get it. Firefox has an open issue since 2016 that prevents you from getting this information . Use Chromium if you need this functionality. from selenium.webdriver.common.desired_capabilities import DesiredCapabilities capabilities = DesiredCapabilities . CHROME . copy () capabilities [ 'goog:loggingPrefs' ] = { 'performance' : 'ALL' } driver = webdriver . Chrome ( desired_capabilities = capabilities ) driver . get ( \"https://duckduckgo.com/\" ) logs = driver . get_log ( \"performance\" ) status_code = get_status ( driver . current_url , logs ) Where get_status is: def get_status ( url : str , logs : List [ Dict [ str , Any ]]) -> int : \"\"\"Get the url response status code. Args: url: url to search logs: Browser driver logs Returns: The status code. \"\"\" for log in logs : if log [ \"message\" ]: data = json . loads ( log [ \"message\" ]) with suppress ( KeyError ): if data [ \"message\" ][ \"params\" ][ \"response\" ][ \"url\" ] == url : return data [ \"message\" ][ \"params\" ][ \"response\" ][ \"status\" ] raise ValueError ( f \"Error retrieving the status code for url { url } \" ) You have to use driver.current_url to handle well urls that redirect to other urls. If your url is not catched and you get a ValueError , use the next snippet inside the with suppress(KeyError) statement. content_type = ( \"text/html\" in data [ \"message\" ][ \"params\" ][ \"response\" ][ \"headers\" ][ \"content-type\" ] ) response_received = ( data [ \"message\" ][ \"method\" ] == \"Network.responseReceived\" ) if content_type and response_received : __import__ ( \"pdb\" ) . set_trace () # XXX BREAKPOINT pass And try to see why url != data[\"message\"][\"params\"][\"response\"][\"url\"] . Sometimes servers redirect the user to a url without the www. . Troubleshooting \u2691 Chromedriver hangs up unexpectedly \u2691 Some say that adding the DBUS_SESSION_BUS_ADDRESS environmental variable fixes it: os . environ [ \"DBUS_SESSION_BUS_ADDRESS\" ] = \"/dev/null\" But it still hangs for me. Right now the only solution I see is to assume it's going to hang and add functionality in your program to resume the work instead of starting from scratch. Ugly I know... Issues \u2691 Firefox driver doesn't have access to the log : Update the section above and start using Firefox instead of Chrome when you need to get the status code of the responses.", "title": "Selenium"}, {"location": "selenium/#web-driver-backends", "text": "Selenium can be used with many browsers, such as Firefox , Chrome or PhantomJS . But first, install selenium : pip install selenium", "title": "Web driver backends"}, {"location": "selenium/#firefox", "text": "Assuming you've got firefox already installed, you need to download the geckodriver , unpack the tar and add the geckodriver binary somewhere in your PATH . from selenium import webdriver driver = webdriver . Firefox () driver . get ( \"https://duckduckgo.com/\" ) If you need to get the status code of the requests use Chrome instead There is an issue with Firefox that doesn't support this feature.", "title": "Firefox"}, {"location": "selenium/#chrome", "text": "We're going to use Chromium instead of Chrome. Download the chromedriver of the same version as your Chromium, unpack the tar and add the chromedriver binary somewhere in your PATH . from selenium import webdriver from selenium.webdriver.chrome.options import Options opts = Options () opts . binary_location = '/usr/bin/chromium' driver = webdriver . Chrome ( options = opts ) driver . get ( \"https://duckduckgo.com/\" ) If you don't want to see the browser, you can run it in headless mode adding the next line when defining the options : opts . add_argument ( \"--headless\" )", "title": "Chrome"}, {"location": "selenium/#phantomjs", "text": "PhantomJS is abandoned -> Don't use it The development stopped in 2018 PhantomJS is a headless Webkit, in conjunction with Selenium WebDriver, it can be used to run tests directly from the command line. Since PhantomJS eliminates the need for a graphical browser, tests run much faster. Don't install phantomjs from the official repos as it's not a working release -.-. npm install -g phantomjs didn't work either. I had to download the tar from the downloads page , which didn't work either. The project is abandoned , so don't use this.", "title": "PhantomJS"}, {"location": "selenium/#usage", "text": "Assuming that you've got a configured driver , to get the url you're in after javascript has done it's magic use the driver.current_url method. To return the HTML of the page use driver.page_source .", "title": "Usage"}, {"location": "selenium/#open-a-url", "text": "driver . get ( \"https://duckduckgo.com/\" )", "title": "Open a URL"}, {"location": "selenium/#get-page-source", "text": "driver . page_source", "title": "Get page source"}, {"location": "selenium/#get-current-url", "text": "driver . current_url", "title": "Get current url"}, {"location": "selenium/#click-on-element", "text": "Once you've opened the page you want to interact with driver.get() , you need to get the Xpath of the element to click on. You can do that by using your browser inspector, to select the element, and once on the code if you right click there is a \"Copy XPath\" Once that is done you should have something like this when you paste it down. //* [ @id = \u201d react - root \u201d ] / section / main / article / div [ 2 ] / div [ 2 ] / p / a Similarly it is the same process for the input fields for username, password, and login button. We can go ahead and do that on the current page. We can store these xpaths as strings in our code to make it readable. We should have three xpaths from this page and one from the initial login. first_login = '//*[@id=\u201dreact-root\u201d]/section/main/article/div[2]/div[2]/p/a' username_input = '//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[2]/div/label/input' password_input = '//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[3]/div/label/input' login_submit = '//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[4]/button/div' Now that we have the xpaths defined we can now tell Selenium webdriver to click and send some keys over for the input fields. from selenium.webdriver.common.by import By driver . find_element ( By . XPATH , first_login ) . click () driver . find_element ( By . XPATH , username_input ) . send_keys ( \"username\" ) driver . find_element ( By . XPATH , password_input ) . send_keys ( \"password\" ) driver . find_element ( By . XPATH , login_submit ) . click () Note Many pages suggest to use methods like find_element_by_name , find_element_by_xpath or find_element_by_id . These are deprecated now . You should use find_element(By. instead. So, instead of: driver . find_element_by_xpath ( \"your_xpath\" ) It should be now: driver . find_element ( By . XPATH , \"your_xpath\" ) Where By is imported with from selenium.webdriver.common.by import By .", "title": "Click on element"}, {"location": "selenium/#close-the-browser", "text": "driver . close ()", "title": "Close the browser"}, {"location": "selenium/#change-browser-configuration", "text": "You can pass options to the initialization of the chromedriver to tweak how does the browser behave. To get a list of the actual prefs you can go to chrome://prefs-internals , there you can get the code you need to tweak.", "title": "Change browser configuration"}, {"location": "selenium/#disable-loading-of-images", "text": "options = ChromeOptions () options . add_experimental_option ( \"prefs\" , { \"profile.default_content_setting_values.images\" : 2 , \"profile.default_content_setting_values.cookies\" : 2 , }, )", "title": "Disable loading of images"}, {"location": "selenium/#disable-site-cookies", "text": "options = ChromeOptions () options . add_experimental_option ( \"prefs\" , { \"profile.default_content_setting_values.cookies\" : 2 , }, )", "title": "Disable site cookies"}, {"location": "selenium/#bypass-selenium-detectors", "text": "Sometimes web servers react differently if they notice that you're using selenium. Browsers can be detected through different ways and some commonly used mechanisms are as follows: Implementing captcha / recaptcha to detect the automatic bots. Non-human behaviour (browsing too fast, not scrolling to the visible elements, ...) Using an IP that's flagged as suspicious (VPN, VPS, Tor...) Detecting the term HeadlessChrome within headless Chrome UserAgent Using Bot Management service from Distil Networks , Akamai , Datadome . They do it through different mechanisms: Use undetected-chromedriver Use Selenium stealth Rotate the user agent Changing browser properties Predefined Javascript variables Don't use selenium If you've already been detected, you might get blocked for a plethora of other reasons even after using these methods. So you may have to try accessing the site that was detecting you using a VPN, different user-agent, etc.", "title": "Bypass Selenium detectors"}, {"location": "selenium/#use-undetected-chromedriver", "text": "undetected-chromedriver is a python library that uses an optimized Selenium Chromedriver patch which does not trigger anti-bot services like Distill Network / Imperva / DataDome / Botprotect.io Automatically downloads the driver binary and patches it.", "title": "Use undetected-chromedriver"}, {"location": "selenium/#installation", "text": "pip install undetected-chromedriver", "title": "Installation"}, {"location": "selenium/#usage_1", "text": "import undetected_chromedriver.v2 as uc driver = uc . Chrome () driver . get ( 'https://nowsecure.nl' ) # my own test test site with max anti-bot protection If you want to specify the path to the browser use uc.Chrome(browser_executable_path=\"/path/to/your/file\") .", "title": "Usage"}, {"location": "selenium/#use-selenium-stealth", "text": "selenium-stealth is a python package to prevent detection (by doing most of the steps of this guide) by making selenium more stealthy. Note It's less maintained than undetected-chromedriver so I'd use that other instead. I leave the section in case it's helpful if the other fails for you.", "title": "Use Selenium Stealth"}, {"location": "selenium/#installation_1", "text": "pip install selenium-stealth", "title": "Installation"}, {"location": "selenium/#usage_2", "text": "from selenium import webdriver from selenium_stealth import stealth import time options = webdriver . ChromeOptions () options . add_argument ( \"start-maximized\" ) # options.add_argument(\"--headless\") options . add_experimental_option ( \"excludeSwitches\" , [ \"enable-automation\" ]) options . add_experimental_option ( 'useAutomationExtension' , False ) driver = webdriver . Chrome ( options = options , executable_path = r \"C:\\Users\\DIPRAJ\\Programming\\adclick_bot\\chromedriver.exe\" ) stealth ( driver , languages = [ \"en-US\" , \"en\" ], vendor = \"Google Inc.\" , platform = \"Win32\" , webgl_vendor = \"Intel Inc.\" , renderer = \"Intel Iris OpenGL Engine\" , fix_hairline = True , ) url = \"https://bot.sannysoft.com/\" driver . get ( url ) time . sleep ( 5 ) driver . quit () You can test it with antibot .", "title": "Usage"}, {"location": "selenium/#rotate-the-user-agent", "text": "Rotating the UserAgent in every execution of your Test Suite using fake_useragent module as follows: from selenium import webdriver from selenium.webdriver.chrome.options import Options from fake_useragent import UserAgent options = Options () ua = UserAgent () userAgent = ua . random print ( userAgent ) options . add_argument ( f 'user-agent= { userAgent } ' ) driver = webdriver . Chrome ( chrome_options = options ) driver . get ( \"https://www.google.co.in\" ) driver . quit () You can also rotate it with execute_cdp_cmd : from selenium import webdriver driver = webdriver . Chrome ( executable_path = r 'C:\\WebDrivers\\chromedriver.exe' ) print ( driver . execute_script ( \"return navigator.userAgent;\" )) # Setting user agent as Chrome/83.0.4103.97 driver . execute_cdp_cmd ( 'Network.setUserAgentOverride' , { \"userAgent\" : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36' }) print ( driver . execute_script ( \"return navigator.userAgent;\" ))", "title": "Rotate the user agent"}, {"location": "selenium/#changing-browser-properties", "text": "Changing the property value of navigator for webdriver to undefined as follows: driver . execute_cdp_cmd ( \"Page.addScriptToEvaluateOnNewDocument\" , { \"source\" : \"\"\" Object.defineProperty(navigator, 'webdriver', { get: () => undefined }) \"\"\" }) You can find a relevant detailed discussion in Selenium webdriver: Modifying navigator.webdriver flag to prevent selenium detection Changing the values of navigator.plugins, navigator.languages, WebGL, hairline feature, missing image, etc. You can find a relevant detailed discussion in Is there a version of selenium webdriver that is not detectable? Changing the conventional Viewport You can find a relevant detailed discussion in How to bypass Google captcha with Selenium and python?", "title": "Changing browser properties"}, {"location": "selenium/#predefined-javascript-variables", "text": "One way of detecting Selenium is by checking for predefined JavaScript variables which appear when running with Selenium. The bot detection scripts usually look anything containing word selenium , webdriver in any of the variables (on window object), and also document variables called $cdc_ and $wdc_ . Of course, all of this depends on which browser you are on. All the different browsers expose different things. In Chrome, what people had to do was to ensure that $cdc_ didn't exist as a document variable. You don't need to go compile the chromedriver yourself, if you open the file with vim and execute :%s/cdc_/dog_/g where dog can be any three characters that will work. With perl you can achieve the same result with: perl -pi -e 's/cdc_/dog_/g' /path/to/chromedriver", "title": "Predefined Javascript variables"}, {"location": "selenium/#dont-use-selenium", "text": "Even with undetected-chromedriver , sometimes servers are able to detect that you're using selenium . A uglier but maybe efective way to go is not using selenium and do a combination of working directly with the chrome devtools protocol with pycdp (using this maintained fork ) and doing the clicks with pyautogui . See an example on this answer . Keep in mind though that these tools don't look to be actively maintained, and that the approach is quite brittle to site changes. Is there really not other way to achieve what you want?", "title": "Don't use selenium"}, {"location": "selenium/#set-timeout-of-a-response", "text": "For Firefox and Chromedriver: driver . set_page_load_timeout ( 30 ) The rest: driver . implicitly_wait ( 30 ) This will throw a TimeoutException whenever the page load takes more than 30 seconds.", "title": "Set timeout of a response"}, {"location": "selenium/#get-the-status-code-of-a-response", "text": "Surprisingly this is not as easy as with requests, there is no status_code method on the driver, you need to dive into the browser log to get it. Firefox has an open issue since 2016 that prevents you from getting this information . Use Chromium if you need this functionality. from selenium.webdriver.common.desired_capabilities import DesiredCapabilities capabilities = DesiredCapabilities . CHROME . copy () capabilities [ 'goog:loggingPrefs' ] = { 'performance' : 'ALL' } driver = webdriver . Chrome ( desired_capabilities = capabilities ) driver . get ( \"https://duckduckgo.com/\" ) logs = driver . get_log ( \"performance\" ) status_code = get_status ( driver . current_url , logs ) Where get_status is: def get_status ( url : str , logs : List [ Dict [ str , Any ]]) -> int : \"\"\"Get the url response status code. Args: url: url to search logs: Browser driver logs Returns: The status code. \"\"\" for log in logs : if log [ \"message\" ]: data = json . loads ( log [ \"message\" ]) with suppress ( KeyError ): if data [ \"message\" ][ \"params\" ][ \"response\" ][ \"url\" ] == url : return data [ \"message\" ][ \"params\" ][ \"response\" ][ \"status\" ] raise ValueError ( f \"Error retrieving the status code for url { url } \" ) You have to use driver.current_url to handle well urls that redirect to other urls. If your url is not catched and you get a ValueError , use the next snippet inside the with suppress(KeyError) statement. content_type = ( \"text/html\" in data [ \"message\" ][ \"params\" ][ \"response\" ][ \"headers\" ][ \"content-type\" ] ) response_received = ( data [ \"message\" ][ \"method\" ] == \"Network.responseReceived\" ) if content_type and response_received : __import__ ( \"pdb\" ) . set_trace () # XXX BREAKPOINT pass And try to see why url != data[\"message\"][\"params\"][\"response\"][\"url\"] . Sometimes servers redirect the user to a url without the www. .", "title": "Get the status code of a response"}, {"location": "selenium/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "selenium/#chromedriver-hangs-up-unexpectedly", "text": "Some say that adding the DBUS_SESSION_BUS_ADDRESS environmental variable fixes it: os . environ [ \"DBUS_SESSION_BUS_ADDRESS\" ] = \"/dev/null\" But it still hangs for me. Right now the only solution I see is to assume it's going to hang and add functionality in your program to resume the work instead of starting from scratch. Ugly I know...", "title": "Chromedriver hangs up unexpectedly"}, {"location": "selenium/#issues", "text": "Firefox driver doesn't have access to the log : Update the section above and start using Firefox instead of Chrome when you need to get the status code of the responses.", "title": "Issues"}, {"location": "semantic_versioning/", "text": "Semantic Versioning is a way to define your program's version based on the type of changes you've introduced. It's defined as a three-number string (separated with a period) in the format of MAJOR.MINOR.PATCH . Usually, it starts with 0.0.0. Then depending on the type of change you make to the library, you increment one of these and set subsequent numbers to zero: MAJOR version if you make backward-incompatible changes. MINOR version if you add a new feature. PATCH version if you fix bugs. The version number in this context is used as a contract between the library developer and the systems pulling it in about how freely they can upgrade. For example, if you wrote your web server against Django 3 , you should be good to go with all Django 3 releases that are at least as new as your current one. This allows you to express your Django dependency in the format of Django >= 3.0.2, <4 . In addition, we have to take into account the following considerations: A normal version number MUST take the form X.Y.Z where X, Y, and Z are non-negative integers, and MUST NOT contain leading zeroes. Once a versioned package has been released, the contents of that version MUST NOT be modified. Any modifications MUST be released as a new version. Major version zero (0.y.z) is for initial development. Anything may change at any time. The public API should not be considered stable. But don't fall into using ZeroVer instead. Releasing the version 1.0.0 is a declaration of intentions to your users that the code is to be considered stable. Patch version Z (x.y.Z | x > 0) MUST be incremented if only backwards compatible bug fixes are introduced. A bug fix is defined as an internal change that fixes incorrect behavior. Minor version Y (x.Y.z | x > 0) MUST be incremented if new, backwards compatible functionality is introduced to the public API. It MUST be incremented if any public API functionality is marked as deprecated. It MAY be incremented if substantial new functionality or improvements are introduced within the private code. It MAY include patch level changes. Patch version MUST be reset to 0 when minor version is incremented. Major version X (X.y.z | X > 0) MUST be incremented if any backwards incompatible changes are introduced to the public API. It MAY include minor and patch level changes. Patch and minor version MUST be reset to 0 when major version is incremented. !!! note \"Encoding this information in the version is just an extremely lossy, but very fast to parse and interpret, which may lead into issues By using this format whenever you rebuild your application, you\u2019ll automatically pull in any new feature/bugfix/security releases of Django, enabling you to use the latest and best version that still in theory guarantees to works with your project. This is great because: You enable automatic, compatible security fixes. It automatically pulls in bug fixes on the library side. Your application will keep building and working in the future as it did today because the significant version pin protects you from pulling in versions whose API would not match. Commit message guidelines \u2691 If you like the idea behind Semantic Versioning, it makes sense to follow the Angular commit convention to automate the changelog maintenance and the program version bumping. Each commit message consists of a header, a body and a footer. The header has a defined format that includes a type, a scope and a subject: <type>(<scope>): <subject> <BLANK LINE> <body> <BLANK LINE> <footer> The header is mandatory and the scope of the header is optional. Any line of the commit message cannot be longer 100 characters. The footer could contain a closing reference to an issue . Samples: docs(changelog): update changelog to beta.5 fix(release): need to depend on latest rxjs and zone.js The version in our package.json gets copied to the one we publish, and users need the latest of these. docs(router): fix typo 'containa' to 'contains' (#36764) Closes #36763 PR Close #36764 Change types \u2691 Must be one of the following: feat : A new feature. fix : A bug fix. test : Adding missing tests or correcting existing tests. docs : Documentation changes. chore : A package maintenance change such as updating the requirements. bump : A commit to mark the increase of the version number. style : Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc). ci : Changes to our CI configuration files and scripts. perf : A code change that improves performance. refactor : A code change that neither fixes a bug nor adds a feature. build : Changes that affect the build system or external dependencies. Subject \u2691 The subject contains a succinct description of the change: Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". Don't capitalize the first letter. No dot (.) at the end. Body \u2691 Same as in the subject, use the imperative present tense. The body should include the motivation for the change and contrast this with previous behavior. Footer \u2691 The footer should contain any information about Breaking Changes and is also the place to reference issues that this commit Closes. Breaking Changes should start with the word BREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this. Revert \u2691 If the commit reverts a previous commit, it should begin with revert: , followed by the header of the reverted commit. In the body it should say: This reverts commit <hash>. , where the hash is the SHA of the commit to revert. Helpers \u2691 Use tool to bump your program version \u2691 You can use the commitizen tool to: Automatically detect which type of change you're introducing and decide which should be the next version number. Update the changelog By running cz bump --changelog --no-verify . The --no-verify part is required if you use pre-commit hooks . Whenever you want to release 1.0.0 , use cz bump --changelog --no-verify --increment MAJOR . If you are on a version 0.X.Y , and you introduced a breaking change but don't want to upgrade to 1.0.0 , use the --increment MINOR flag. Use tool to create the commit messages \u2691 To get used to make correct commit messages, you can use the commitizen tool, that guides you through the steps of making a good commit message. Once you're used to the system though, it makes more sense to ditch the tool and write the messages yourself. In Vim, if you're using Vim fugitive you can change the configuration to: nnoremap <leader>gc :terminal cz c<CR> nnoremap <leader>gr :terminal cz c --retry<CR> \" Open terminal mode in insert mode if has('nvim') autocmd TermOpen term://* startinsert endif autocmd BufLeave term://* stopinsert If some pre-commit hook fails, make the changes and then use <leader>gr to repeat the same commit message. Pre-commit \u2691 To ensure that your project follows these guidelines, add the following to your pre-commit configuration : File: .pre-commit-config.yaml - repo : https://github.com/commitizen-tools/commitizen rev : master hooks : - id : commitizen stages : [ commit-msg ] When to do a major release \u2691 Following the Semantic Versioning idea of a major update is problematic because: You can quickly get into the high version number problem. The fact that any change may break the users code makes the definition of when a change should be major blurry . Often the change that triggered the major change only affects a low percentage of your users (usually those using that one feature you changed in an incompatible fashion). Does dropping Python 2 require a major release? Many (most) packages did this, but the general answer is ironically no, it is not an addition or a breaking change, the version solver will ensure the correct version is used (unless the Requires-Python metadata slot is empty or not updated). If you mark a feature as deprecated (almost always in a minor release), you can remove that feature in a future minor release. You have to define in your library documentation what the deprecation period is. For example, NumPy and Python use three minor releases. Sometimes is useful to implement deprecations based on a period of time. SemVer purists argue that this makes minor releases into major releases, but as we've seen it\u2019s not that simple. The deprecation period ensures the \u201cnext\u201d version works, which is really useful, and usually gives you time to adjust before the removal happens. It\u2019s a great balance for projects that are well kept up using libraries that move forward at a reasonable pace. If you make sure you can see deprecations, you will almost always work with the next several versions. Semantic versioning system problems \u2691 On paper, semantic versioning seems to be addressing all we need to encode the evolution and state of our library. When implementation time comes some issues are raised though. !!! note \"The pitfalls mentioned below don't invalidate the Semantic Versioning system, you just need to be aware of them.\" Maintaining different versions \u2691 Version numbers are just a mapping of a sequence of digits to our branching strategy in source control. For instance, if you are doing SemVer then your X.Y.Z version maps a branch to X.Y branch where you're doing your current feature work, an X.Y.Z+1 branch for any bugfixes, and potentially an X+1.0.0 branch where you doing some crazy new stuff. So you got your next branch, main branch, and bugfix branch. And all three of those branches are alive and receiving updates. For projects that have those 3 kinds of branches going, the concept of SemVer makes much more sense, but how many projects are doing that? You have to be a pretty substantial project typically to have the throughput to justify that much project overhead. There are a lot more projects that have a single bugfix branch and a main branch which has all feature work, whether it be massively backwards-incompatible or not. In that case why carry around two version numbers? This is how you end up with ZeroVer . If you're doing that why not just drop a digit and have your version be X.Y ? PEP 440 supports it, and it would more truthfully represent your branching strategy appropriately in your version number. However, most library maintainers/developers out there don\u2019t have enough resources to maintain even two branches. Maintaining a library is very time-consuming, and most libraries have just a few active maintainers available that maintain other many libraries. To complicate matters even further, for most maintainers this is not a full-time job, but something on the side, part of their free time. Given the scarce human resources to maintain a library, in practice, there\u2019s a single supported version for any library at any given point in time: the latest one . Any version before that (be that major, minor, patch) is in essence abandoned: If you want security updates, you need to move to the latest version. If you want a bug to be fixed, you need to move to the newest version. If you want a new feature, it is only going to be available in the latest version. If the only maintained version is the latest, you really just have an X version number that is monotonically increasing. Once again PEP 440 supports it, so why not! It still communicates your branch strategy of there being only a single branch at any one time. Now I know this is a bit too unconventional for some people, and you may get into the high version number problem , then maybe it makes sense to use calendar versioning to use the version number to indicate the release date to signify just how old of a version you\u2019re using, but if stuff is working does that really matter? High version numbers \u2691 Another major argument is that people inherently judge a project based on what it\u2019s version number is. They\u2019ll implicitly assume that foo 2.0 is better than bar 1.0 (and frob 3.0 is better still) because the version numbers are higher. However, there is a limit to this, if you go too high too quickly, people assume your project is unstable and shouldn\u2019t really be used, even if the reason that your project is so high is because you removed some tiny edge cases that nobody actually used and didn\u2019t actually impact many people, if any, at all. These are two different expressions of the same thing. The first is that people will look down on a project for not having a high enough version compared to its competitors. While it\u2019s true that some people will do this, it's not a significant reason to throw away the communication benefits of your version number. Ultimately, no matter what you do, people who judge a project as inferior because of something as shallow as \u201csmaller version number\u201d will find some other, equally shallow, reason to pick between projects. The other side of this is a bit different. When you have a large major version, like 42.0.0 , people assume that your library is not stable and that you regularly break compatibility and if you follow SemVer strictly, it does actually mean that you regularly break compatibility. There are two general cases: The true positives : where a project that does routinely break it\u2019s public API in meaningful ways. The false positives : Projects that strictly follow semantic versioning were each change which is not backwards compatible requires bumping a major version. This means that if you remove some function that nobody actually uses you need to increase your major version. Do it again and you need to increase your major version again. Do this enough times, for even very small changes and you can quickly get into a large version number 6 . This case is a false positive for the \u201cstability\u201d test, because the reality is that your project is actually quite stable. Difference in change categorization \u2691 Here's a thought experiment: you need to add a new warning to your Python package that tries to follow SemVer. Would that single change cause you to increase the major, minor, or patch version number? You might think a patch number bump since it isn't a new feature or breaking anything. You might think it's a minor version bump because it isn't exactly a bugfix. And you might think it's a major version bump because if you ran your Python code with -W error you suddenly introduced a new exception which could break people's code. Brett Cannon did a poll , answered by 231 people with the results: Patch/Bugfix : 47.2% Minor/enhancement : 44.2% Major/breaking : 8.7% That speaks volumes to why SemVer does not inherently work: someone's bugfix may be someone else's breaking change. Because in Python we can't statically define what an API change is there will always be a disagreement between you and your dependencies as to what a \"feature\" or \"bugfix\" truly is. That builds one of the arguments for CalVer . Because SemVer is imperfect at describing if a particular change will break someone upgrading the software, that we should instead throw it out and replace it with something that doesn\u2019t purport to tell us that information. Unintended changes \u2691 A major version bump must happen not only when you rewrite an entire library with its complete API, but also when you\u2019re just renaming a single rarely used function (which some may erroneously view as a minor change). Or even worse, it\u2019s not always clear what\u2019s part of the public API and what\u2019s not. You have a library with some incidental, undocumented, and unspecified behavior that you consider to be obviously not part of the public interface. You change it to solve what seems like a bug to you, and make a patch release, only to find that you have angry hordes at the gate who, thanks to Hyrum\u2019s Law , depend on the old behavior. With a sufficient number of users of an API, it does not matter what you promise in the contract. All observable behaviors of your system will be depended on by somebody. Which has been represented perfectly by the people behind xkcd . While every maintainer would like to believe they\u2019ve thought of every use case up-front and created the best API for everything. In practice it's impossible to think on every impact your changes will make. Even if you were very diligent/broad with your interpretation to avoid accidentally breaking people with a bugfix release, bugs can still happen in a bugfix release. It obviously isn't intentional, but it does happen which means SemVer can't protect you from having to test your code to see if a patch version is compatible with your code. This makes \u201ctrue\u201d SemVer pointless. Minor releases are impossible, and patch releases are nearly impossible. If you fix a bug, someone could be depending on the buggy behaviour. Using ZeroVer \u2691 ZeroVer is a joke versioning system similar to Semantic Versioning with the sole difference that MAJOR is always 0 . From the specification, as long as you are in the 0.X.Y versions, you can introduce incompatible changes at any point. It intended to make fun of people who use \u201csemantic versioning\u201d but never make a 1.0 release, thus defeating the purpose of semver. This one of the consequences of trying to strictly follow Semantic Versioning, because once you give the leap to 1.0 you need to increase the major on each change quickly leading to the problem of high version numbers . The best way to fight this behaviour is to remember the often overlooked SemVer 2.0 FAQ guideline : If your software is being used in production, it should probably already be 1.0.0. If you have a stable API on which users have come to depend, you should be 1.0.0. If you\u2019re worrying a lot about backwards compatibility, you should probably already be 1.0.0. When to use it \u2691 Check the Deciding what version system to use for your programs article section. References \u2691 Home Bernat post on versioning Why I don't like SemVer anymore by Snarky Versioning Software by donald stufft Libraries \u2691 These libraries can be used to interact with a git history of commits that follow the semantic versioning commit guidelines . python-semantic-release", "title": "Semantic Versioning"}, {"location": "semantic_versioning/#commit-message-guidelines", "text": "If you like the idea behind Semantic Versioning, it makes sense to follow the Angular commit convention to automate the changelog maintenance and the program version bumping. Each commit message consists of a header, a body and a footer. The header has a defined format that includes a type, a scope and a subject: <type>(<scope>): <subject> <BLANK LINE> <body> <BLANK LINE> <footer> The header is mandatory and the scope of the header is optional. Any line of the commit message cannot be longer 100 characters. The footer could contain a closing reference to an issue . Samples: docs(changelog): update changelog to beta.5 fix(release): need to depend on latest rxjs and zone.js The version in our package.json gets copied to the one we publish, and users need the latest of these. docs(router): fix typo 'containa' to 'contains' (#36764) Closes #36763 PR Close #36764", "title": "Commit message guidelines"}, {"location": "semantic_versioning/#change-types", "text": "Must be one of the following: feat : A new feature. fix : A bug fix. test : Adding missing tests or correcting existing tests. docs : Documentation changes. chore : A package maintenance change such as updating the requirements. bump : A commit to mark the increase of the version number. style : Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc). ci : Changes to our CI configuration files and scripts. perf : A code change that improves performance. refactor : A code change that neither fixes a bug nor adds a feature. build : Changes that affect the build system or external dependencies.", "title": "Change types"}, {"location": "semantic_versioning/#subject", "text": "The subject contains a succinct description of the change: Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". Don't capitalize the first letter. No dot (.) at the end.", "title": "Subject"}, {"location": "semantic_versioning/#body", "text": "Same as in the subject, use the imperative present tense. The body should include the motivation for the change and contrast this with previous behavior.", "title": "Body"}, {"location": "semantic_versioning/#footer", "text": "The footer should contain any information about Breaking Changes and is also the place to reference issues that this commit Closes. Breaking Changes should start with the word BREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this.", "title": "Footer"}, {"location": "semantic_versioning/#revert", "text": "If the commit reverts a previous commit, it should begin with revert: , followed by the header of the reverted commit. In the body it should say: This reverts commit <hash>. , where the hash is the SHA of the commit to revert.", "title": "Revert"}, {"location": "semantic_versioning/#helpers", "text": "", "title": "Helpers"}, {"location": "semantic_versioning/#use-tool-to-bump-your-program-version", "text": "You can use the commitizen tool to: Automatically detect which type of change you're introducing and decide which should be the next version number. Update the changelog By running cz bump --changelog --no-verify . The --no-verify part is required if you use pre-commit hooks . Whenever you want to release 1.0.0 , use cz bump --changelog --no-verify --increment MAJOR . If you are on a version 0.X.Y , and you introduced a breaking change but don't want to upgrade to 1.0.0 , use the --increment MINOR flag.", "title": "Use tool to bump your program version"}, {"location": "semantic_versioning/#use-tool-to-create-the-commit-messages", "text": "To get used to make correct commit messages, you can use the commitizen tool, that guides you through the steps of making a good commit message. Once you're used to the system though, it makes more sense to ditch the tool and write the messages yourself. In Vim, if you're using Vim fugitive you can change the configuration to: nnoremap <leader>gc :terminal cz c<CR> nnoremap <leader>gr :terminal cz c --retry<CR> \" Open terminal mode in insert mode if has('nvim') autocmd TermOpen term://* startinsert endif autocmd BufLeave term://* stopinsert If some pre-commit hook fails, make the changes and then use <leader>gr to repeat the same commit message.", "title": "Use tool to create the commit messages"}, {"location": "semantic_versioning/#pre-commit", "text": "To ensure that your project follows these guidelines, add the following to your pre-commit configuration : File: .pre-commit-config.yaml - repo : https://github.com/commitizen-tools/commitizen rev : master hooks : - id : commitizen stages : [ commit-msg ]", "title": "Pre-commit"}, {"location": "semantic_versioning/#when-to-do-a-major-release", "text": "Following the Semantic Versioning idea of a major update is problematic because: You can quickly get into the high version number problem. The fact that any change may break the users code makes the definition of when a change should be major blurry . Often the change that triggered the major change only affects a low percentage of your users (usually those using that one feature you changed in an incompatible fashion). Does dropping Python 2 require a major release? Many (most) packages did this, but the general answer is ironically no, it is not an addition or a breaking change, the version solver will ensure the correct version is used (unless the Requires-Python metadata slot is empty or not updated). If you mark a feature as deprecated (almost always in a minor release), you can remove that feature in a future minor release. You have to define in your library documentation what the deprecation period is. For example, NumPy and Python use three minor releases. Sometimes is useful to implement deprecations based on a period of time. SemVer purists argue that this makes minor releases into major releases, but as we've seen it\u2019s not that simple. The deprecation period ensures the \u201cnext\u201d version works, which is really useful, and usually gives you time to adjust before the removal happens. It\u2019s a great balance for projects that are well kept up using libraries that move forward at a reasonable pace. If you make sure you can see deprecations, you will almost always work with the next several versions.", "title": "When to do a major release"}, {"location": "semantic_versioning/#semantic-versioning-system-problems", "text": "On paper, semantic versioning seems to be addressing all we need to encode the evolution and state of our library. When implementation time comes some issues are raised though. !!! note \"The pitfalls mentioned below don't invalidate the Semantic Versioning system, you just need to be aware of them.\"", "title": "Semantic versioning system problems"}, {"location": "semantic_versioning/#maintaining-different-versions", "text": "Version numbers are just a mapping of a sequence of digits to our branching strategy in source control. For instance, if you are doing SemVer then your X.Y.Z version maps a branch to X.Y branch where you're doing your current feature work, an X.Y.Z+1 branch for any bugfixes, and potentially an X+1.0.0 branch where you doing some crazy new stuff. So you got your next branch, main branch, and bugfix branch. And all three of those branches are alive and receiving updates. For projects that have those 3 kinds of branches going, the concept of SemVer makes much more sense, but how many projects are doing that? You have to be a pretty substantial project typically to have the throughput to justify that much project overhead. There are a lot more projects that have a single bugfix branch and a main branch which has all feature work, whether it be massively backwards-incompatible or not. In that case why carry around two version numbers? This is how you end up with ZeroVer . If you're doing that why not just drop a digit and have your version be X.Y ? PEP 440 supports it, and it would more truthfully represent your branching strategy appropriately in your version number. However, most library maintainers/developers out there don\u2019t have enough resources to maintain even two branches. Maintaining a library is very time-consuming, and most libraries have just a few active maintainers available that maintain other many libraries. To complicate matters even further, for most maintainers this is not a full-time job, but something on the side, part of their free time. Given the scarce human resources to maintain a library, in practice, there\u2019s a single supported version for any library at any given point in time: the latest one . Any version before that (be that major, minor, patch) is in essence abandoned: If you want security updates, you need to move to the latest version. If you want a bug to be fixed, you need to move to the newest version. If you want a new feature, it is only going to be available in the latest version. If the only maintained version is the latest, you really just have an X version number that is monotonically increasing. Once again PEP 440 supports it, so why not! It still communicates your branch strategy of there being only a single branch at any one time. Now I know this is a bit too unconventional for some people, and you may get into the high version number problem , then maybe it makes sense to use calendar versioning to use the version number to indicate the release date to signify just how old of a version you\u2019re using, but if stuff is working does that really matter?", "title": "Maintaining different versions"}, {"location": "semantic_versioning/#high-version-numbers", "text": "Another major argument is that people inherently judge a project based on what it\u2019s version number is. They\u2019ll implicitly assume that foo 2.0 is better than bar 1.0 (and frob 3.0 is better still) because the version numbers are higher. However, there is a limit to this, if you go too high too quickly, people assume your project is unstable and shouldn\u2019t really be used, even if the reason that your project is so high is because you removed some tiny edge cases that nobody actually used and didn\u2019t actually impact many people, if any, at all. These are two different expressions of the same thing. The first is that people will look down on a project for not having a high enough version compared to its competitors. While it\u2019s true that some people will do this, it's not a significant reason to throw away the communication benefits of your version number. Ultimately, no matter what you do, people who judge a project as inferior because of something as shallow as \u201csmaller version number\u201d will find some other, equally shallow, reason to pick between projects. The other side of this is a bit different. When you have a large major version, like 42.0.0 , people assume that your library is not stable and that you regularly break compatibility and if you follow SemVer strictly, it does actually mean that you regularly break compatibility. There are two general cases: The true positives : where a project that does routinely break it\u2019s public API in meaningful ways. The false positives : Projects that strictly follow semantic versioning were each change which is not backwards compatible requires bumping a major version. This means that if you remove some function that nobody actually uses you need to increase your major version. Do it again and you need to increase your major version again. Do this enough times, for even very small changes and you can quickly get into a large version number 6 . This case is a false positive for the \u201cstability\u201d test, because the reality is that your project is actually quite stable.", "title": "High version numbers"}, {"location": "semantic_versioning/#difference-in-change-categorization", "text": "Here's a thought experiment: you need to add a new warning to your Python package that tries to follow SemVer. Would that single change cause you to increase the major, minor, or patch version number? You might think a patch number bump since it isn't a new feature or breaking anything. You might think it's a minor version bump because it isn't exactly a bugfix. And you might think it's a major version bump because if you ran your Python code with -W error you suddenly introduced a new exception which could break people's code. Brett Cannon did a poll , answered by 231 people with the results: Patch/Bugfix : 47.2% Minor/enhancement : 44.2% Major/breaking : 8.7% That speaks volumes to why SemVer does not inherently work: someone's bugfix may be someone else's breaking change. Because in Python we can't statically define what an API change is there will always be a disagreement between you and your dependencies as to what a \"feature\" or \"bugfix\" truly is. That builds one of the arguments for CalVer . Because SemVer is imperfect at describing if a particular change will break someone upgrading the software, that we should instead throw it out and replace it with something that doesn\u2019t purport to tell us that information.", "title": "Difference in change categorization"}, {"location": "semantic_versioning/#unintended-changes", "text": "A major version bump must happen not only when you rewrite an entire library with its complete API, but also when you\u2019re just renaming a single rarely used function (which some may erroneously view as a minor change). Or even worse, it\u2019s not always clear what\u2019s part of the public API and what\u2019s not. You have a library with some incidental, undocumented, and unspecified behavior that you consider to be obviously not part of the public interface. You change it to solve what seems like a bug to you, and make a patch release, only to find that you have angry hordes at the gate who, thanks to Hyrum\u2019s Law , depend on the old behavior. With a sufficient number of users of an API, it does not matter what you promise in the contract. All observable behaviors of your system will be depended on by somebody. Which has been represented perfectly by the people behind xkcd . While every maintainer would like to believe they\u2019ve thought of every use case up-front and created the best API for everything. In practice it's impossible to think on every impact your changes will make. Even if you were very diligent/broad with your interpretation to avoid accidentally breaking people with a bugfix release, bugs can still happen in a bugfix release. It obviously isn't intentional, but it does happen which means SemVer can't protect you from having to test your code to see if a patch version is compatible with your code. This makes \u201ctrue\u201d SemVer pointless. Minor releases are impossible, and patch releases are nearly impossible. If you fix a bug, someone could be depending on the buggy behaviour.", "title": "Unintended changes"}, {"location": "semantic_versioning/#using-zerover", "text": "ZeroVer is a joke versioning system similar to Semantic Versioning with the sole difference that MAJOR is always 0 . From the specification, as long as you are in the 0.X.Y versions, you can introduce incompatible changes at any point. It intended to make fun of people who use \u201csemantic versioning\u201d but never make a 1.0 release, thus defeating the purpose of semver. This one of the consequences of trying to strictly follow Semantic Versioning, because once you give the leap to 1.0 you need to increase the major on each change quickly leading to the problem of high version numbers . The best way to fight this behaviour is to remember the often overlooked SemVer 2.0 FAQ guideline : If your software is being used in production, it should probably already be 1.0.0. If you have a stable API on which users have come to depend, you should be 1.0.0. If you\u2019re worrying a lot about backwards compatibility, you should probably already be 1.0.0.", "title": "Using ZeroVer"}, {"location": "semantic_versioning/#when-to-use-it", "text": "Check the Deciding what version system to use for your programs article section.", "title": "When to use it"}, {"location": "semantic_versioning/#references", "text": "Home Bernat post on versioning Why I don't like SemVer anymore by Snarky Versioning Software by donald stufft", "title": "References"}, {"location": "semantic_versioning/#libraries", "text": "These libraries can be used to interact with a git history of commits that follow the semantic versioning commit guidelines . python-semantic-release", "title": "Libraries"}, {"location": "signal/", "text": "Signal is a cross-platform centralized encrypted messaging service developed by the Signal Technology Foundation and Signal Messenger LLC. It uses the Internet to send one-to-one and group messages, which can include files, voice notes, images and videos. It can also be used to make one-to-one and group voice and video calls. Signal uses standard cellular telephone numbers as identifiers and secures all communications to other Signal users with end-to-end encryption. The apps include mechanisms by which users can independently verify the identity of their contacts and the integrity of the data channel. Signal's software is free and open-source. Its clients are published under the GPLv3 license, while the server code is published under the AGPLv3 license. The official Android app generally uses the proprietary Google Play Services (installed on most Android devices), though it is designed to still work without them installed. Signal also has an official client app for iOS and desktop apps for Windows, MacOS and Linux. Pros and cons \u2691 Pros: Good security by default. Easy to use for non technical users. Good multi-device support. Cons: Uses phones to identify users. Centralized. Not available in F-droid . Backup extraction \u2691 I'd first try to use signal-black . References \u2691 Home", "title": "Signal"}, {"location": "signal/#pros-and-cons", "text": "Pros: Good security by default. Easy to use for non technical users. Good multi-device support. Cons: Uses phones to identify users. Centralized. Not available in F-droid .", "title": "Pros and cons"}, {"location": "signal/#backup-extraction", "text": "I'd first try to use signal-black .", "title": "Backup extraction"}, {"location": "signal/#references", "text": "Home", "title": "References"}, {"location": "sleep/", "text": "Sleep is a naturally recurring state of mind and body, characterized by altered consciousness, relatively inhibited sensory activity, reduced muscle activity and inhibition of nearly all voluntary muscles during rapid eye movement (REM) sleep,and reduced interactions with surroundings. Distinguished from wakefulness by a decreased ability to react to stimuli. Most of the content of this article is extracted from the Why we sleep book by Matthew Walker Consequences of lack of sleep \u2691 Sleeping less than six or seven hours a night can produce these consequences: Demolishing of the immune system. Doubling your risk of cancer. Is a key lifestyle factor determining and worsening the development of the Alzheimer's disease. Disruption of blood sugar levels so profoundly that you would be classified as pre-diabetic. Increase the likelihood of block and brittle of your coronary arteries. Setting you on a path toward cardiovascular disease, stroke, and congestive heart failure. Contributes to all major psychiatric conditions, including depression, anxiety, and suicidality. Swelling concentrations of a hormone that makes you feel hungry while suppressing a companion hormone that otherwise signals food satisfaction. Thwart the ability to learn and memorize. A balanced diet and exercise are of vital importance, but we now see sleep as the key factor in health. The physical and mental impairments caused by one night of bad sleep dwarf those caused by an equivalent absence of food or exercise. Therefore, the shorter you sleep, the shorter your life span . Sleep benefits \u2691 We sleep for a lot of nighttime benefits that service both our brains and our body. There does not seem to be one major organ within the body, or process within the brain, that isn't optimally enhanced by sleep. Within the brain, sleep enriches our ability to learn, memorize and make logical decisions and choices. It recalibrates our emotional brain circuits, allowing us to navigate next day social and psychological challenges with cool-headed composture. Downstairs in the body, sleep: Restocks the armory of our immune system: helping fight malignancy, preventing infection, and warding off sickness. Reforms the body's metabolic state by fine-tuning the balance of insulin and circulating glucose. Regulates our appetite, helping control body weight through healthy food selection rather than rash impulsivity. Maintains a flourishing microbiome within your gut essential to our nutritional health being. Is tied to the fitness of our cardiovascular system, lowering blood pressure while keeping our hearts in fine condition. Dreaming produces a neurochemical bath that mollifies painful memories and a virtual reality space in which the brain melds past and present knowledge, inspiring creativity. Therefore, Sleep is the single most effective thing we can do to reset our brain and body health each day . Sleep physiological effects \u2691 There are two main factors that determine when you want to sleep or stay awake: The signal sent by the suprachiasmatic nucleus following the circadian rhythm. Sleep pressure: The brain builds up a chemical substance that creates the \"sleep pressure\". The longer you've been awake, the more that chemical sleep pressure accumulates, and consequentially, the sleepier you feel. The circadian rhythm \u2691 We have an internal clock deep within the brain, called the suprachiasmatic nucleus, that creates a cycling, day-night rhythm, known as circadian rhythm, that makes you feel tired or alert at regular times of night and day, respectively. The circadian rhythm determines: When you want to be awake or asleep. Your timed preferences for eating and drinking. Your moods and emotions The amount of urine you produce. Your core body temperature. Your metabolic rate. The release of numerous hormones. Contrary to common belief, circadian rhythm is not defined by the daylight sun cycle. As Kleitman and Richardson demonstrated in 1938: When cut off from the daily cycle of light and dark, the body keeps on maintaining the rhythm. The period of the circadian rhythm is different for each person, but has an average of 24 hours and 15 minutes. Even if it's not defined by the sun light, it corrects those 15 minutes of delay to stay in sync with it. The suprachiasmatic nucleus can readjust by about one hour each day, that is why jet lag can be spawn through multiple days. That reset does not come free. Studies in airplane cabin crews who frequently fly on long haul routes and have little chance to recover have registered: The part of the brains related to learning and memory had physically shrunk, suggesting the destruction of brain cells caused by the biological stress of timezone travel. Their short term memory was significantly impaired. They had far higher rates of cancer and type 2 diabetes than the general population. The peak and valley points of wakefulness or sleepiness vary too between people, it's known as their chronotype and it's strongly determined by genetics. The chronotype defines three types of people: Morning types : They have their peak of wakefulness early in the day and the sleepiness early at night. They prefer to wake at or around dawn, and function optimally at this time of day. Evening types : They prefer going to bed late and subsequently wake up late the following morning, or even in the afternoon. In between : The remaining people fall somewhere in between, with a slight leaning towards eveningness. Melatonin \u2691 The suprachiasmatic nucleus communicates its repeating signal of day and night to your brain and body by releasing melatonin into the bloodstream from the pineal gland. Soon after dusk, the suprachiasmatic nucleus starts increasing the levels of this hormone, telling the rest of the body that it's time to sleep. But melatonin has little influence on the generation of sleep itself. Once sleep is under way, melatonin decreases in concentration across the night and into the morning hours. With dawn, as sunlight enters the brain through the eyes (even through the closed lids), the pineal gland is instructed to stop releasing melatonin. The absence of circulating melatonin now informs the brain and body that it's time to return to a wakefulness active state for the rest of the day Sleep pressure \u2691 While you are awake, the brain is releasing a chemical called adenosine. One consequence of the increasing accumulation of adenosine is the increase of the desire to sleep by turning down the \"volume\" of wake promoting regions in the brain and turning up the sleep inducing ones. Most people fall to the pressure after twelve to sixteen hours of being awake. Caffeine \u2691 You can artificially mute the sleep signal of adenosine by using a chemical that makes you feel more alert and awake, such as caffeine. Caffeine works by battling with adenosine for the privilege of latching on to adenosine receptors in the brain. Once caffeine occupies these receptors, it does not stimulate them like adenosine, making you sleepy. Rather, caffeine blocks and effectively inactivates the receptors acting as a masking agent. Levels of caffeine peak around thirty minutes after ingestion. What is problematic, though, is the persistence of caffeine in your system. It takes between five to seven hours to remove 50 percent of the caffeine concentration from your body. An enzyme within your liver removes caffeine from your system. Based in large part on genetics, some people have a more efficient version of the enzyme that degrades caffeine, allowing the liver to clear it from the bloodstream. Age is also a variable to take into account, the older we are the longer it takes our brain and body to remove it. When your liver evicts the caffeine from your system, you encounter the caffeine crash . Your energy levels plummet rapidly, finding difficult to function and concentrate, with a strong sense of sleepiness once again. For the entire time that caffeine is in your system, the adenosine keeps on building up. Your brain is not aware of this rising tide of sleep encouraging chemical because the wall of caffeine is holding it back from your perception. Once your liver dismantles the barricade, you feel a vicious backlash: you are hit with the sleepiness you had experienced two or three hours ago before you drank that cup of coffee plus all the extra adenosine that has accumulated in the hours in between. Relationship between the circadian rhythm and the sleep pressure \u2691 The two governing forces that regulate your sleep are ignorant of each other. Although they are not coupled, they are usually aligned. Starting on the far left of the figure, the circadian rhythm begins to increase its activity a few hours before you wake up. It infuses the brain and body with an alerting energy signal. At first, the signal is faint, but gradually it builds with time. By early afternoon, the activating signal from the circadian rhythm peaks. Now let's look at the sleep pressure pattern. By mid to late morning, you have only been awake for a half of hours. As a result, adenosine concentrations have increased a little. Furthermore, the circadian rhythm is on its powerful upswing of alertness. This combination of strong activating output from the circadian rhythm together with low levels of adenosine result in a delightful sensation of being wide awake. The distance between the curved lines above will be a direct reflection of your desire to sleep. By eleven pm, you've been awake for fifteen hours, and your brain is drenched in high concentrations of adenosine. Additionally, the circadian rhythm line is descending, powering down your activity and alertness levels. This powerful combination triggers a strong desire for sleep. During sleep, a mass evacuation of adenosine gets under way as the brain has the chance to degrade and remove it. After eight hours of healthy sleep, the adenosine purge is complete. As this process is ending, the circadian activity rhythm has returned, and its energizing influence starts to approach, therefore naturally waking us up. All-nighters \u2691 Scientists can demonstrate that the two forces determining when you want to be awake and sleep are independent and can be decoupled from their normal lockstep. When you skip one night's sleep and remain awake throughout the following day, By remaining awake, and blocking access to the adenosine drain that sleep opens up, the brain is unable to rid itself of the chemical sleep pressure. The mounting adenosine levels continue to rise. This should mean that the longer you are awake, the sleepier you feel. But that's not true. Though you will feel increasingly sleepy throughout the nighttime phase, hitting a low point in your alertness around five or six in the morning, thereafter, you'll start to be more awake. This effect is answered by the energy return of the circadian rhythm. Unlike sleep pressure, the circadian rhythm pays no attention to whether you are asleep or awake. Am I getting enough sleep? \u2691 When you don't sleep enough, one consequence among many is that adenosine concentrations remain too high, so the next morning you continue to accumulate sleep debt If after waking up you could fall asleep at ten or eleven in the morning, it means that you're likely not getting enough sleep quantity or quality. The same can be said if you can't function optimally without caffeine before noon, you'll be most likely self-medicating your state of chronic sleep deprivation. Other sleep indicators can be if you would sleep more if you didn't set an alarm clock, or if you find yourself at your computer screen reading and then rereading the same sentence. Of course, even if you are giving yourself plenty of time to get a full night of shut-eye, next-day fatigue and sleepiness can still occur because you are suffering from an undiagnosed sleep disorder. The sleep cycle \u2691 Humans cycle through two types of sleep in a regular pattern throughout the night with a period of 90 minutes. They were called non-rapid eye movement (NREM) and rapid eye movement (REM). Later, NREM sleep was further subdivided into four separate stages, named from 1 to 4 (all awful names (\u0482\u2323\u0300_\u2323\u0301) ). Stages 3 and 4 are the deepest stages of NREM sleep, meaning that it's more difficult to wake you up in comparison with stages 1 and 2. In REM sleep, your eyes rapidly move from side to side underneath the lids. This movement are accompanied by active brainwaves, almost identical to those observed when you are awake. On the other hand, eyes remain still and the brainwaves also calm down in the NREM phases. Even though we switch from sleep phases each 90 minutes, the ratio of NREM to REM sleep throughout the night changes across the night. In the first half of the night, the vast majority of time is spent in deep NREM and very little REM. But as we transition through the second half of the night, REM starts dominating. Although there is no scientific consensus, the need to remodel and update our neural circuits at night can explain this repeatable but asymmetric pattern. Throughout the day, the new memories are stored in the RAM of your brain, when you start to sleep, the brain needs to move the important ones to the hard drive, for long term retrieval. The brain needs to solve an optimization problem: The hard drive and the RAM have limited capacity. The RAM needs to be cleaned to be able to register the next day's memories. The brain needs RAM to do the analysis of which memories to keep and which to remove. A key function of NREM sleep is to remove unnecessary neural connections, while REM sleep plays a role in strengthening those connections. The different roles and the capacity limits explains why the brain needs to switch between them. The asymmetry can be explained with the simile of creating a sculpture from a block of clay. At the beginning of the night, the long phases of NREM extensively removes unneeded material, with short REM phases to define the basic form. With each cycle, less material needs to be strongly removed and more enhancing of the details is required, thus the increase of REM sleep. A danger resides in this sleep profile. Since your brain desires most of its REM sleep in the last part of the night, if you wake up early, sleeping 6 hours instead of 8, you can be losing between 60 to 90% of all your REM sleep, even though you are losing 25% of your total sleep time. It works both ways, if you instead go to sleep two hours late, you'll loose a significant amount of deep NREM sleep. Preventing the brain to have the required REM or NREM daily rations results in many physical and mental issues. Sleeping time and sense distortions \u2691 When you're asleep, you loose awareness of the outside world. Your ears are still hearing, your eyes, though closed, are still seeing, and the rest of the organs keep on working too. All these signals still flood into the center of your brain, but it's there, in the sensory convergence zone, where they end. The thalamus is the sensory gate to the brain that blocks them. If it lets them pass, they travel to the cortex at the top of your brain, where they are consciously perceived. By locking its gates shut when you're asleep, the thalamus imposes a sensory blackout in the brain. As a result, you are no longer consciously aware of the information transmitted from your sense organs. Another consequence of sleeping is a sense of time distortion experienced in two contradictory ways. While you loose your conscious mapping during sleep, at a non-conscious level, the brain keeps track of time with incredible precision. To distort it even more, you sense a time dilation in dreams. The signature patterns of brain-cell activity that occurs as you learn, gets recurrently repeated during sleep. That is, memories are being replayed at the level of brain-cell activity as you sleep. During REM sleep, the memories are replayed at half or quarter the speed in comparison of the activity when you're awake. This slow neural recounting may be the reason why we have that time dilation. How your brain generates sleep \u2691 Brainwave activity of REM sleep looks similar to the one you have when you're awake. They cycle (going up and down) at a fast frequency of thirty or forty times per second in an unreliable pattern. This behaviour is explained by the fact that different parts of your waking brain are processing different pieces of information at different moments in time and in different ways. References \u2691 Why we sleep book by Matthew Walker", "title": "Sleep"}, {"location": "sleep/#consequences-of-lack-of-sleep", "text": "Sleeping less than six or seven hours a night can produce these consequences: Demolishing of the immune system. Doubling your risk of cancer. Is a key lifestyle factor determining and worsening the development of the Alzheimer's disease. Disruption of blood sugar levels so profoundly that you would be classified as pre-diabetic. Increase the likelihood of block and brittle of your coronary arteries. Setting you on a path toward cardiovascular disease, stroke, and congestive heart failure. Contributes to all major psychiatric conditions, including depression, anxiety, and suicidality. Swelling concentrations of a hormone that makes you feel hungry while suppressing a companion hormone that otherwise signals food satisfaction. Thwart the ability to learn and memorize. A balanced diet and exercise are of vital importance, but we now see sleep as the key factor in health. The physical and mental impairments caused by one night of bad sleep dwarf those caused by an equivalent absence of food or exercise. Therefore, the shorter you sleep, the shorter your life span .", "title": "Consequences of lack of sleep"}, {"location": "sleep/#sleep-benefits", "text": "We sleep for a lot of nighttime benefits that service both our brains and our body. There does not seem to be one major organ within the body, or process within the brain, that isn't optimally enhanced by sleep. Within the brain, sleep enriches our ability to learn, memorize and make logical decisions and choices. It recalibrates our emotional brain circuits, allowing us to navigate next day social and psychological challenges with cool-headed composture. Downstairs in the body, sleep: Restocks the armory of our immune system: helping fight malignancy, preventing infection, and warding off sickness. Reforms the body's metabolic state by fine-tuning the balance of insulin and circulating glucose. Regulates our appetite, helping control body weight through healthy food selection rather than rash impulsivity. Maintains a flourishing microbiome within your gut essential to our nutritional health being. Is tied to the fitness of our cardiovascular system, lowering blood pressure while keeping our hearts in fine condition. Dreaming produces a neurochemical bath that mollifies painful memories and a virtual reality space in which the brain melds past and present knowledge, inspiring creativity. Therefore, Sleep is the single most effective thing we can do to reset our brain and body health each day .", "title": "Sleep benefits"}, {"location": "sleep/#sleep-physiological-effects", "text": "There are two main factors that determine when you want to sleep or stay awake: The signal sent by the suprachiasmatic nucleus following the circadian rhythm. Sleep pressure: The brain builds up a chemical substance that creates the \"sleep pressure\". The longer you've been awake, the more that chemical sleep pressure accumulates, and consequentially, the sleepier you feel.", "title": "Sleep physiological effects"}, {"location": "sleep/#the-circadian-rhythm", "text": "We have an internal clock deep within the brain, called the suprachiasmatic nucleus, that creates a cycling, day-night rhythm, known as circadian rhythm, that makes you feel tired or alert at regular times of night and day, respectively. The circadian rhythm determines: When you want to be awake or asleep. Your timed preferences for eating and drinking. Your moods and emotions The amount of urine you produce. Your core body temperature. Your metabolic rate. The release of numerous hormones. Contrary to common belief, circadian rhythm is not defined by the daylight sun cycle. As Kleitman and Richardson demonstrated in 1938: When cut off from the daily cycle of light and dark, the body keeps on maintaining the rhythm. The period of the circadian rhythm is different for each person, but has an average of 24 hours and 15 minutes. Even if it's not defined by the sun light, it corrects those 15 minutes of delay to stay in sync with it. The suprachiasmatic nucleus can readjust by about one hour each day, that is why jet lag can be spawn through multiple days. That reset does not come free. Studies in airplane cabin crews who frequently fly on long haul routes and have little chance to recover have registered: The part of the brains related to learning and memory had physically shrunk, suggesting the destruction of brain cells caused by the biological stress of timezone travel. Their short term memory was significantly impaired. They had far higher rates of cancer and type 2 diabetes than the general population. The peak and valley points of wakefulness or sleepiness vary too between people, it's known as their chronotype and it's strongly determined by genetics. The chronotype defines three types of people: Morning types : They have their peak of wakefulness early in the day and the sleepiness early at night. They prefer to wake at or around dawn, and function optimally at this time of day. Evening types : They prefer going to bed late and subsequently wake up late the following morning, or even in the afternoon. In between : The remaining people fall somewhere in between, with a slight leaning towards eveningness.", "title": "The circadian rhythm"}, {"location": "sleep/#melatonin", "text": "The suprachiasmatic nucleus communicates its repeating signal of day and night to your brain and body by releasing melatonin into the bloodstream from the pineal gland. Soon after dusk, the suprachiasmatic nucleus starts increasing the levels of this hormone, telling the rest of the body that it's time to sleep. But melatonin has little influence on the generation of sleep itself. Once sleep is under way, melatonin decreases in concentration across the night and into the morning hours. With dawn, as sunlight enters the brain through the eyes (even through the closed lids), the pineal gland is instructed to stop releasing melatonin. The absence of circulating melatonin now informs the brain and body that it's time to return to a wakefulness active state for the rest of the day", "title": "Melatonin"}, {"location": "sleep/#sleep-pressure", "text": "While you are awake, the brain is releasing a chemical called adenosine. One consequence of the increasing accumulation of adenosine is the increase of the desire to sleep by turning down the \"volume\" of wake promoting regions in the brain and turning up the sleep inducing ones. Most people fall to the pressure after twelve to sixteen hours of being awake.", "title": "Sleep pressure"}, {"location": "sleep/#caffeine", "text": "You can artificially mute the sleep signal of adenosine by using a chemical that makes you feel more alert and awake, such as caffeine. Caffeine works by battling with adenosine for the privilege of latching on to adenosine receptors in the brain. Once caffeine occupies these receptors, it does not stimulate them like adenosine, making you sleepy. Rather, caffeine blocks and effectively inactivates the receptors acting as a masking agent. Levels of caffeine peak around thirty minutes after ingestion. What is problematic, though, is the persistence of caffeine in your system. It takes between five to seven hours to remove 50 percent of the caffeine concentration from your body. An enzyme within your liver removes caffeine from your system. Based in large part on genetics, some people have a more efficient version of the enzyme that degrades caffeine, allowing the liver to clear it from the bloodstream. Age is also a variable to take into account, the older we are the longer it takes our brain and body to remove it. When your liver evicts the caffeine from your system, you encounter the caffeine crash . Your energy levels plummet rapidly, finding difficult to function and concentrate, with a strong sense of sleepiness once again. For the entire time that caffeine is in your system, the adenosine keeps on building up. Your brain is not aware of this rising tide of sleep encouraging chemical because the wall of caffeine is holding it back from your perception. Once your liver dismantles the barricade, you feel a vicious backlash: you are hit with the sleepiness you had experienced two or three hours ago before you drank that cup of coffee plus all the extra adenosine that has accumulated in the hours in between.", "title": "Caffeine"}, {"location": "sleep/#relationship-between-the-circadian-rhythm-and-the-sleep-pressure", "text": "The two governing forces that regulate your sleep are ignorant of each other. Although they are not coupled, they are usually aligned. Starting on the far left of the figure, the circadian rhythm begins to increase its activity a few hours before you wake up. It infuses the brain and body with an alerting energy signal. At first, the signal is faint, but gradually it builds with time. By early afternoon, the activating signal from the circadian rhythm peaks. Now let's look at the sleep pressure pattern. By mid to late morning, you have only been awake for a half of hours. As a result, adenosine concentrations have increased a little. Furthermore, the circadian rhythm is on its powerful upswing of alertness. This combination of strong activating output from the circadian rhythm together with low levels of adenosine result in a delightful sensation of being wide awake. The distance between the curved lines above will be a direct reflection of your desire to sleep. By eleven pm, you've been awake for fifteen hours, and your brain is drenched in high concentrations of adenosine. Additionally, the circadian rhythm line is descending, powering down your activity and alertness levels. This powerful combination triggers a strong desire for sleep. During sleep, a mass evacuation of adenosine gets under way as the brain has the chance to degrade and remove it. After eight hours of healthy sleep, the adenosine purge is complete. As this process is ending, the circadian activity rhythm has returned, and its energizing influence starts to approach, therefore naturally waking us up.", "title": "Relationship between the circadian rhythm and the sleep pressure"}, {"location": "sleep/#all-nighters", "text": "Scientists can demonstrate that the two forces determining when you want to be awake and sleep are independent and can be decoupled from their normal lockstep. When you skip one night's sleep and remain awake throughout the following day, By remaining awake, and blocking access to the adenosine drain that sleep opens up, the brain is unable to rid itself of the chemical sleep pressure. The mounting adenosine levels continue to rise. This should mean that the longer you are awake, the sleepier you feel. But that's not true. Though you will feel increasingly sleepy throughout the nighttime phase, hitting a low point in your alertness around five or six in the morning, thereafter, you'll start to be more awake. This effect is answered by the energy return of the circadian rhythm. Unlike sleep pressure, the circadian rhythm pays no attention to whether you are asleep or awake.", "title": "All-nighters"}, {"location": "sleep/#am-i-getting-enough-sleep", "text": "When you don't sleep enough, one consequence among many is that adenosine concentrations remain too high, so the next morning you continue to accumulate sleep debt If after waking up you could fall asleep at ten or eleven in the morning, it means that you're likely not getting enough sleep quantity or quality. The same can be said if you can't function optimally without caffeine before noon, you'll be most likely self-medicating your state of chronic sleep deprivation. Other sleep indicators can be if you would sleep more if you didn't set an alarm clock, or if you find yourself at your computer screen reading and then rereading the same sentence. Of course, even if you are giving yourself plenty of time to get a full night of shut-eye, next-day fatigue and sleepiness can still occur because you are suffering from an undiagnosed sleep disorder.", "title": "Am I getting enough sleep?"}, {"location": "sleep/#the-sleep-cycle", "text": "Humans cycle through two types of sleep in a regular pattern throughout the night with a period of 90 minutes. They were called non-rapid eye movement (NREM) and rapid eye movement (REM). Later, NREM sleep was further subdivided into four separate stages, named from 1 to 4 (all awful names (\u0482\u2323\u0300_\u2323\u0301) ). Stages 3 and 4 are the deepest stages of NREM sleep, meaning that it's more difficult to wake you up in comparison with stages 1 and 2. In REM sleep, your eyes rapidly move from side to side underneath the lids. This movement are accompanied by active brainwaves, almost identical to those observed when you are awake. On the other hand, eyes remain still and the brainwaves also calm down in the NREM phases. Even though we switch from sleep phases each 90 minutes, the ratio of NREM to REM sleep throughout the night changes across the night. In the first half of the night, the vast majority of time is spent in deep NREM and very little REM. But as we transition through the second half of the night, REM starts dominating. Although there is no scientific consensus, the need to remodel and update our neural circuits at night can explain this repeatable but asymmetric pattern. Throughout the day, the new memories are stored in the RAM of your brain, when you start to sleep, the brain needs to move the important ones to the hard drive, for long term retrieval. The brain needs to solve an optimization problem: The hard drive and the RAM have limited capacity. The RAM needs to be cleaned to be able to register the next day's memories. The brain needs RAM to do the analysis of which memories to keep and which to remove. A key function of NREM sleep is to remove unnecessary neural connections, while REM sleep plays a role in strengthening those connections. The different roles and the capacity limits explains why the brain needs to switch between them. The asymmetry can be explained with the simile of creating a sculpture from a block of clay. At the beginning of the night, the long phases of NREM extensively removes unneeded material, with short REM phases to define the basic form. With each cycle, less material needs to be strongly removed and more enhancing of the details is required, thus the increase of REM sleep. A danger resides in this sleep profile. Since your brain desires most of its REM sleep in the last part of the night, if you wake up early, sleeping 6 hours instead of 8, you can be losing between 60 to 90% of all your REM sleep, even though you are losing 25% of your total sleep time. It works both ways, if you instead go to sleep two hours late, you'll loose a significant amount of deep NREM sleep. Preventing the brain to have the required REM or NREM daily rations results in many physical and mental issues.", "title": "The sleep cycle"}, {"location": "sleep/#sleeping-time-and-sense-distortions", "text": "When you're asleep, you loose awareness of the outside world. Your ears are still hearing, your eyes, though closed, are still seeing, and the rest of the organs keep on working too. All these signals still flood into the center of your brain, but it's there, in the sensory convergence zone, where they end. The thalamus is the sensory gate to the brain that blocks them. If it lets them pass, they travel to the cortex at the top of your brain, where they are consciously perceived. By locking its gates shut when you're asleep, the thalamus imposes a sensory blackout in the brain. As a result, you are no longer consciously aware of the information transmitted from your sense organs. Another consequence of sleeping is a sense of time distortion experienced in two contradictory ways. While you loose your conscious mapping during sleep, at a non-conscious level, the brain keeps track of time with incredible precision. To distort it even more, you sense a time dilation in dreams. The signature patterns of brain-cell activity that occurs as you learn, gets recurrently repeated during sleep. That is, memories are being replayed at the level of brain-cell activity as you sleep. During REM sleep, the memories are replayed at half or quarter the speed in comparison of the activity when you're awake. This slow neural recounting may be the reason why we have that time dilation.", "title": "Sleeping time and sense distortions"}, {"location": "sleep/#how-your-brain-generates-sleep", "text": "Brainwave activity of REM sleep looks similar to the one you have when you're awake. They cycle (going up and down) at a fast frequency of thirty or forty times per second in an unreliable pattern. This behaviour is explained by the fact that different parts of your waking brain are processing different pieces of information at different moments in time and in different ways.", "title": "How your brain generates sleep"}, {"location": "sleep/#references", "text": "Why we sleep book by Matthew Walker", "title": "References"}, {"location": "sponsor/", "text": "It may arrive the moment in your life where someone wants to sponsor you . There are many sponsoring platforms you can use, each has their advantages and disadvantages. Liberapay. Ko-fi. Buy me a coffee. Github Sponsor. Liberapay Ko-fi Buy Me a Coffee Github Sponsor Non-profit Yes No No No! (Microsoft!) Monthly fee No No No No Donation Commission 0% 0% 5% Not clear Paid plan No Yes No No Payment Processors Stripe, Paypal Stripe, Paypal Stripe, Standard Payout Stripe One time donations Possible but not user friendly Yes Yes Yes Membership Yes Yes Yes Yes Shop/Sales No Yes No No Based in France ? United States United States? Pay delay Instant Instant Instant Until 100$ User friendliness OK Good Good Good Liberapay is the only non-profit recurrent donations platform. It's been the most recommended platform from the people I know from the open-source, activist environment. Ko-fi would be my next choice, as they don't do commissions on the donations and they support more features (that I don't need right now) than Liberapay. Each of these platforms use different payment processors such as: Stripe Paypal Usually Stripe takes less commissions than Paypal, also Paypal is known for closing user accounts and keeping their money. Conclusion \u2691 If you just want something simple and don't mind the difficulties of doing one time donations of Liberapay, go with it, it's also the most ethical. If you want something more powerful, then Ko-fi is the best solution. You can even have both. Try to avoid Paypal and use Stripe for both platforms. Github integration \u2691 Ko-fi Github integration \u2691 Add ko_fi: your_account_id to the .github/FUNDING.yml file. Add a widget to your README.md . Liberapay Github integration \u2691 Add liberapay: your_user to the .github/FUNDING.yml file. Add a widget to your README.md . You can get them on the Widgets section of your settings.", "title": "Sponsor"}, {"location": "sponsor/#conclusion", "text": "If you just want something simple and don't mind the difficulties of doing one time donations of Liberapay, go with it, it's also the most ethical. If you want something more powerful, then Ko-fi is the best solution. You can even have both. Try to avoid Paypal and use Stripe for both platforms.", "title": "Conclusion"}, {"location": "sponsor/#github-integration", "text": "", "title": "Github integration"}, {"location": "sponsor/#ko-fi-github-integration", "text": "Add ko_fi: your_account_id to the .github/FUNDING.yml file. Add a widget to your README.md .", "title": "Ko-fi Github integration"}, {"location": "sponsor/#liberapay-github-integration", "text": "Add liberapay: your_user to the .github/FUNDING.yml file. Add a widget to your README.md . You can get them on the Widgets section of your settings.", "title": "Liberapay Github integration"}, {"location": "sqlite/", "text": "SQLite is a relational database management system (RDBMS) contained in a C library. In contrast to many other database management systems, SQLite is not a client\u2013server database engine. Rather, it is embedded into the end program. SQLite is ACID-compliant and implements most of the SQL standard, generally following PostgreSQL syntax. However, SQLite uses a dynamically and weakly typed SQL syntax that does not guarantee the domain integrity.[7] This means that one can, for example, insert a string into a column defined as an integer. SQLite will attempt to convert data between formats where appropriate, the string \"123\" into an integer in this case, but does not guarantee such conversions and will store the data as-is if such a conversion is not possible. SQLite is a popular choice as embedded database software for local/client storage in application software such as web browsers. It is arguably the most widely deployed database engine, as it is used today by several widespread browsers, operating systems, and embedded systems (such as mobile phones), among others. Operators and statements \u2691 Upsert statements \u2691 UPSERT is a special syntax addition to INSERT that causes the INSERT to behave as an UPDATE or a no-op if the INSERT would violate a uniqueness constraint. UPSERT is not standard SQL. UPSERT in SQLite follows the syntax established by PostgreSQL. The syntax that occurs in between the \"ON CONFLICT\" and \"DO\" keywords is called the \"conflict target\". The conflict target specifies a specific uniqueness constraint that will trigger the upsert. The conflict target is required for DO UPDATE upserts, but is optional for DO NOTHING. When the conflict target is omitted, the upsert behavior is triggered by a violation of any uniqueness constraint on the table of the INSERT. If the insert operation would cause the uniqueness constraint identified by the conflict-target clause to fail, then the insert is omitted and either the DO NOTHING or DO UPDATE operation is performed instead. In the case of a multi-row insert, this decision is made separately for each row of the insert. The special UPSERT processing happens only for uniqueness constraint on the table that is receiving the INSERT. A \"uniqueness constraint\" is an explicit UNIQUE or PRIMARY KEY constraint within the CREATE TABLE statement, or a unique index. UPSERT does not intervene for failed NOT NULL or foreign key constraints or for constraints that are implemented using triggers. Column names in the expressions of a DO UPDATE refer to the original unchanged value of the column, before the attempted INSERT. To use the value that would have been inserted had the constraint not failed, add the special \"excluded.\" table qualifier to the column name. CREATE TABLE phonebook2 ( name TEXT PRIMARY KEY , phonenumber TEXT , validDate DATE ); INSERT INTO phonebook2 ( name , phonenumber , validDate ) VALUES ( 'Alice' , '704-555-1212' , '2018-05-08' ) ON CONFLICT ( name ) DO UPDATE SET phonenumber = excluded . phonenumber , validDate = excluded . validDate REGEXP \u2691 The REGEXP operator is a special syntax for the regexp() user function. No regexp() user function is defined by default and so use of the REGEXP operator will normally result in an error message. If an application-defined SQL function named regexp is added at run-time, then the X REGEXP Y operator will be implemented as a call to regexp(Y,X) . If you're using sqlite3 , you can check how to create the regexp function . Snippets \u2691 Get the columns of a database \u2691 PRAGMA table_info(table_name); Troubleshooting \u2691 [Integer autoincrement not \u2691 working]( https://stackoverflow.com/questions/16832401/sqlite-auto-increment-not-working ) Rename the column type from INT to INTEGER and it starts working. From this: CREATE TABLE IF NOT EXISTS foo ( id INT PRIMARY KEY , bar INT ) to this: CREATE TABLE IF NOT EXISTS foo ( id INTEGER PRIMARY KEY , bar INT ) References \u2691 Home rqlite : is a lightweight, distributed relational database, which uses SQLite as its storage engine. Forming a cluster is very straightforward, it gracefully handles leader elections, and tolerates failures of machines, including the leader.", "title": "SQLite"}, {"location": "sqlite/#operators-and-statements", "text": "", "title": "Operators and statements"}, {"location": "sqlite/#upsert-statements", "text": "UPSERT is a special syntax addition to INSERT that causes the INSERT to behave as an UPDATE or a no-op if the INSERT would violate a uniqueness constraint. UPSERT is not standard SQL. UPSERT in SQLite follows the syntax established by PostgreSQL. The syntax that occurs in between the \"ON CONFLICT\" and \"DO\" keywords is called the \"conflict target\". The conflict target specifies a specific uniqueness constraint that will trigger the upsert. The conflict target is required for DO UPDATE upserts, but is optional for DO NOTHING. When the conflict target is omitted, the upsert behavior is triggered by a violation of any uniqueness constraint on the table of the INSERT. If the insert operation would cause the uniqueness constraint identified by the conflict-target clause to fail, then the insert is omitted and either the DO NOTHING or DO UPDATE operation is performed instead. In the case of a multi-row insert, this decision is made separately for each row of the insert. The special UPSERT processing happens only for uniqueness constraint on the table that is receiving the INSERT. A \"uniqueness constraint\" is an explicit UNIQUE or PRIMARY KEY constraint within the CREATE TABLE statement, or a unique index. UPSERT does not intervene for failed NOT NULL or foreign key constraints or for constraints that are implemented using triggers. Column names in the expressions of a DO UPDATE refer to the original unchanged value of the column, before the attempted INSERT. To use the value that would have been inserted had the constraint not failed, add the special \"excluded.\" table qualifier to the column name. CREATE TABLE phonebook2 ( name TEXT PRIMARY KEY , phonenumber TEXT , validDate DATE ); INSERT INTO phonebook2 ( name , phonenumber , validDate ) VALUES ( 'Alice' , '704-555-1212' , '2018-05-08' ) ON CONFLICT ( name ) DO UPDATE SET phonenumber = excluded . phonenumber , validDate = excluded . validDate", "title": "Upsert statements"}, {"location": "sqlite/#regexp", "text": "The REGEXP operator is a special syntax for the regexp() user function. No regexp() user function is defined by default and so use of the REGEXP operator will normally result in an error message. If an application-defined SQL function named regexp is added at run-time, then the X REGEXP Y operator will be implemented as a call to regexp(Y,X) . If you're using sqlite3 , you can check how to create the regexp function .", "title": "REGEXP"}, {"location": "sqlite/#snippets", "text": "", "title": "Snippets"}, {"location": "sqlite/#get-the-columns-of-a-database", "text": "PRAGMA table_info(table_name);", "title": "Get the columns of a database"}, {"location": "sqlite/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "sqlite/#integer-autoincrement-not", "text": "working]( https://stackoverflow.com/questions/16832401/sqlite-auto-increment-not-working ) Rename the column type from INT to INTEGER and it starts working. From this: CREATE TABLE IF NOT EXISTS foo ( id INT PRIMARY KEY , bar INT ) to this: CREATE TABLE IF NOT EXISTS foo ( id INTEGER PRIMARY KEY , bar INT )", "title": "[Integer autoincrement not"}, {"location": "sqlite/#references", "text": "Home rqlite : is a lightweight, distributed relational database, which uses SQLite as its storage engine. Forming a cluster is very straightforward, it gracefully handles leader elections, and tolerates failures of machines, including the leader.", "title": "References"}, {"location": "sqlite3/", "text": "SQLite3 is a python library that provides an SQL interface compliant with the DB-API 2.0 specification described by PEP 249. Usage \u2691 To use the module, you must first create a Connection object that represents the database, and a Cursor one to interact with it. Here the data will be stored in the example.db file: import sqlite3 conn = sqlite3 . connect ( 'example.db' ) cursor = conn . cursor () Once we have a cursor we can execute the different SQL statements and the save them with the commit method of the Connection object. Finally we can close the connection with close . # Create table cursor . execute ( '''CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)''' ) # Insert a row of data cursor . execute ( \"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\" ) # Save (commit) the changes conn . commit () # We can also close the connection if we are done with it. # Just be sure any changes have been committed or they will be lost. conn . close () Get columns of a query \u2691 cursor = connection . execute ( 'select * from bar' ) names = [ description [ 0 ] for description in cursor . description ] Get a list of the tables \u2691 sql_query = \"\"\"SELECT name FROM sqlite_master WHERE type='table';\"\"\" cursor = sqliteConnection . cursor () cursor . execute ( sql_query ) print ( cursor . fetchall ()) Regexp \u2691 SQLite needs the user to define a regexp function to be able to use the filter. import sqlite3 import re def regexp ( expr , item ): reg = re . compile ( expr ) return reg . search ( item ) is not None conn = sqlite3 . connect ( ':memory:' ) conn . create_function ( \"REGEXP\" , 2 , regexp ) cursor = conn . cursor () References \u2691 Docs", "title": "sqlite3"}, {"location": "sqlite3/#usage", "text": "To use the module, you must first create a Connection object that represents the database, and a Cursor one to interact with it. Here the data will be stored in the example.db file: import sqlite3 conn = sqlite3 . connect ( 'example.db' ) cursor = conn . cursor () Once we have a cursor we can execute the different SQL statements and the save them with the commit method of the Connection object. Finally we can close the connection with close . # Create table cursor . execute ( '''CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)''' ) # Insert a row of data cursor . execute ( \"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\" ) # Save (commit) the changes conn . commit () # We can also close the connection if we are done with it. # Just be sure any changes have been committed or they will be lost. conn . close ()", "title": "Usage"}, {"location": "sqlite3/#get-columns-of-a-query", "text": "cursor = connection . execute ( 'select * from bar' ) names = [ description [ 0 ] for description in cursor . description ]", "title": "Get columns of a query"}, {"location": "sqlite3/#get-a-list-of-the-tables", "text": "sql_query = \"\"\"SELECT name FROM sqlite_master WHERE type='table';\"\"\" cursor = sqliteConnection . cursor () cursor . execute ( sql_query ) print ( cursor . fetchall ())", "title": "Get a list of the tables"}, {"location": "sqlite3/#regexp", "text": "SQLite needs the user to define a regexp function to be able to use the filter. import sqlite3 import re def regexp ( expr , item ): reg = re . compile ( expr ) return reg . search ( item ) is not None conn = sqlite3 . connect ( ':memory:' ) conn . create_function ( \"REGEXP\" , 2 , regexp ) cursor = conn . cursor ()", "title": "Regexp"}, {"location": "sqlite3/#references", "text": "Docs", "title": "References"}, {"location": "storage/", "text": "I have a server at home to host some services for my closest ones. The server is an Intel NUC which is super awesome in terms of electric consumption, CPU and RAM versus cost. The downside is that it has no hard drive to store the services data. It does have some USB ports to connect external hard drives though. As the data kept growing I started buying bigger drives. While it was affordable I purchased two so as to have one to store the backup of the data. The problem came when it became unaffordable for me. Then I took the good idea to assume that I could only have one drive of 16TB with my data. Obviously the inevitable happened. The hard drive died and those 10TB of data that were not stored in any backup were lost. Luckily enough, it was not unique data like personal photos. The data could be regenerated by manual processes at the cost of precious time (I'm still suffering this :( ). But every cloud has a silver lining, this failure gave me the energy and motivation to improve my home architecture. To prevent this from happening again, the solution needs to be: Robust: If disks die I will have time to replace them before data is lost. Flexible: It needs to expand as the data grows. Not very expensive. Easy to maintain. There are two types of solutions to store data: On one host: All disks are attached to a server and the storage capacity is shared to other devices by the local network. Distributed: The disks are attached to many servers and they work together to provide the storage through the local network. A NAS server represents the first solution, while systems like Ceph or GlusterFS over Odroid HC4 fall into the second. Both are robust and flexible but I'm more inclined towards building a NAS because it can hold the amount of data that I need, it's easier to maintain and the underlying technology has been more battle proven throughout the years.", "title": "Storage"}, {"location": "strategy/", "text": "Strategy is a general plan to achieve one or more long-term or overall goals under conditions of uncertainty. Strategy is important because the resources available to achieve goals are usually limited. Strategy generally involves setting goals and priorities, determining actions to achieve the goals, and mobilizing resources to execute the actions. A strategy describes how the ends (goals) will be achieved by the means (resources). Strategy can be intended or can emerge as a pattern of activity as the person or organization adapts to its environment. It typically involves two major processes: Formulation : Involves analyzing the environment or situation, making a diagnosis, and developing guiding policies. It includes such activities as strategic planning and strategic thinking . Implementation : Refers to the action plans taken to achieve the goals established by the guiding policy. Strategic planning \u2691 Strategic planning is an organization's process of defining its strategy, or direction, and making decisions on allocating its resources to pursue this strategy. It helps coordinate the two processes required by the strategy, formulation and implementation. However, strategic planning is analytical in nature (i.e., it involves \"finding the dots\"); strategy formation itself involves synthesis (i.e., \"connecting the dots\") via strategic thinking. As such, strategic planning occurs around the strategy formation activity. Strategic thinking \u2691 Strategic thinking is defined as a mental or thinking process applied by an individual in the context of achieving a goal or set of goals in a game or other endeavor. As a cognitive activity, it produces thought. Strategic thinking includes finding and developing a strategic foresight capacity for an organization or individual, by exploring all possible futures, and challenging conventional thinking to foster decision making today. The strategist must have a great capacity for both analysis and synthesis; analysis is necessary to assemble the data on which he makes his diagnosis, synthesis in order to produce from these data the diagnosis itself\u2014and the diagnosis in fact amounts to a choice between alternative courses of action.", "title": "Strategy"}, {"location": "strategy/#strategic-planning", "text": "Strategic planning is an organization's process of defining its strategy, or direction, and making decisions on allocating its resources to pursue this strategy. It helps coordinate the two processes required by the strategy, formulation and implementation. However, strategic planning is analytical in nature (i.e., it involves \"finding the dots\"); strategy formation itself involves synthesis (i.e., \"connecting the dots\") via strategic thinking. As such, strategic planning occurs around the strategy formation activity.", "title": "Strategic planning"}, {"location": "strategy/#strategic-thinking", "text": "Strategic thinking is defined as a mental or thinking process applied by an individual in the context of achieving a goal or set of goals in a game or other endeavor. As a cognitive activity, it produces thought. Strategic thinking includes finding and developing a strategic foresight capacity for an organization or individual, by exploring all possible futures, and challenging conventional thinking to foster decision making today. The strategist must have a great capacity for both analysis and synthesis; analysis is necessary to assemble the data on which he makes his diagnosis, synthesis in order to produce from these data the diagnosis itself\u2014and the diagnosis in fact amounts to a choice between alternative courses of action.", "title": "Strategic thinking"}, {"location": "sudokus/", "text": "Sudoku is a logic-based, combinatorial number-placement puzzle. In classic Sudoku, the objective is to fill a 9 \u00d7 9 grid with digits so that each column, each row, and each of the nine 3 \u00d7 3 subgrids that compose the grid (also called \"boxes\", \"blocks\", or \"regions\") contain all of the digits from 1 to 9. The puzzle setter provides a partially completed grid, which for a well-posed puzzle has a single solution. Sudoku strategies \u2691 Hidden pairs . Hidden triplets . Naked triplets .", "title": "Sudoku"}, {"location": "sudokus/#sudoku-strategies", "text": "Hidden pairs . Hidden triplets . Naked triplets .", "title": "Sudoku strategies"}, {"location": "tahoe/", "text": "Tahoe-LAFS is a free and open, secure, decentralized, fault-tolerant, distributed data store and distributed file system. Tahoe-LAFS is a system that helps you to store files. You run a client program on your computer, which talks to one or more storage servers on other computers. When you tell your client to store a file, it will encrypt that file, encode it into multiple pieces, then spread those pieces out among multiple servers. The pieces are all encrypted and protected against modifications. Later, when you ask your client to retrieve the file, it will find the necessary pieces, make sure they haven\u2019t been corrupted, reassemble them, and decrypt the result. The client creates more pieces (or \u201cshares\u201d) than it will eventually need, so even if some of the servers fail, you can still get your data back. Corrupt shares are detected and ignored, so the system can tolerate server-side hard-drive errors. All files are encrypted (with a unique key) before uploading, so even a malicious server operator cannot read your data. The only thing you ask of the servers is that they can (usually) provide the shares when you ask for them: you aren\u2019t relying upon them for confidentiality, integrity, or absolute availability. Tahoe does not provide locking of mutable files and directories . If there is more than one simultaneous attempt to change a mutable file or directory, then an UncoordinatedWriteError may result. This might, in rare cases, cause the file or directory contents to be accidentally deleted. The user is expected to ensure that there is at most one outstanding write or update request for a given file or directory at a time. One convenient way to accomplish this is to make a different file or directory for each person or process that wants to write. If mutable parts of a file store are accessed via sshfs, only a single sshfs mount should be used. There may be data loss if mutable files or directories are accessed via two sshfs mounts, or written both via sshfs and from other clients. Installation \u2691 apt-get install tahoe-lafs Or if you want the latest version pip install tahoe-lafs If you plan to connect to servers protected through Tor, use pip install tahoe-lafs[tor] instead. Troubleshooting \u2691 pkg_resources.DistributionNotFound: The idna distribution was not found and is required by Twisted \u2691 apt-get install python-idna References \u2691 Git Docs Issues", "title": "Tahoe-LAFS"}, {"location": "tahoe/#installation", "text": "apt-get install tahoe-lafs Or if you want the latest version pip install tahoe-lafs If you plan to connect to servers protected through Tor, use pip install tahoe-lafs[tor] instead.", "title": "Installation"}, {"location": "tahoe/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "tahoe/#pkg_resourcesdistributionnotfound-the-idna-distribution-was-not-found-and-is-required-by-twisted", "text": "apt-get install python-idna", "title": "pkg_resources.DistributionNotFound: The idna  distribution was not found and is required by Twisted"}, {"location": "tahoe/#references", "text": "Git Docs Issues", "title": "References"}, {"location": "talkey/", "text": "Talkey is a Simple Text-To-Speech (TTS) interface library with multi-language and multi-engine support. Installation \u2691 pip install talkey You need to install the TTS engines by yourself. Talkey supports: Flite SVOX Pico Festival eSpeak mbrola via eSpeak I've tried SVOX Pico, Festival and eSpeak. I've discarded Flite because it's not in the official repositories. Of those three the one that gives the most natural support is SVOX Pico. To install it execute: sudo apt-get install libttspico-utils It also supports the following networked TTS Engines: MaryTTS (needs hosting). Google TTS (cloud hosted) I obviously discard Google for privacy reasons, and MaryTTS too because it needs you to run a server, which is inconvenient for most users and pico gives us enough quality. Usage \u2691 At its simplest use case: import talkey tts = talkey . Talkey () tts . say ( \"I've been really busy being dead. You know, after you murdered me.\" ) It automatically detects languages without any further configuration: tts . say ( \"La cabra siempre tira al monte\" ) References \u2691 Git Docs", "title": "Talkey"}, {"location": "talkey/#installation", "text": "pip install talkey You need to install the TTS engines by yourself. Talkey supports: Flite SVOX Pico Festival eSpeak mbrola via eSpeak I've tried SVOX Pico, Festival and eSpeak. I've discarded Flite because it's not in the official repositories. Of those three the one that gives the most natural support is SVOX Pico. To install it execute: sudo apt-get install libttspico-utils It also supports the following networked TTS Engines: MaryTTS (needs hosting). Google TTS (cloud hosted) I obviously discard Google for privacy reasons, and MaryTTS too because it needs you to run a server, which is inconvenient for most users and pico gives us enough quality.", "title": "Installation"}, {"location": "talkey/#usage", "text": "At its simplest use case: import talkey tts = talkey . Talkey () tts . say ( \"I've been really busy being dead. You know, after you murdered me.\" ) It automatically detects languages without any further configuration: tts . say ( \"La cabra siempre tira al monte\" )", "title": "Usage"}, {"location": "talkey/#references", "text": "Git Docs", "title": "References"}, {"location": "task_management/", "text": "Task management is the process of managing a task through its life cycle. It involves planning, testing, tracking, and reporting. Task management can help either individual achieve goals, or groups of individuals collaborate and share knowledge for the accomplishment of collective goals. You can address task management at different levels. High level management ensures that you choose your tasks in order to accomplish a goal, low level management helps you get things done. When you do task management well, you benefit from: Reducing your mental load, so you can use those resources doing productive work. Improving your efficiency. Making more realistic estimations, thus meeting the commited deadlines. Finishing what you start. Knowing you're working towards your ultimate goals Stop feeling lost or overburdened. Make context switches cheaper. On the other side, task management done wrong can consume your willpower in the exchange of lost time and a confused mind. The tricky reality is that the factors that decide if you do it right or wrong are different for every person, and even for a person it may change over the time or mood states. That's why I follow the thought that task management is a tool that is meant to help you. If it's not, you need to change your system until it does. A side effect is that you have to tailor your task management system yourself. It doesn't matter how good the systems you find in the internet are, until you start getting your hands dirty, you won't know if they works for you. So instead of trying to discover the perfect solution, start with one that introduces the least friction in your current workflow, and evolve from that point guided by the faults you find. Forget about instant solutions, this is a never ending marathon. Make sure that each step is small and easy, otherwise you will get tired too soon. I haven't written a guide yet on how to give your first steps, but you could start by following a simple workflow with simple tools .", "title": "Task Management"}, {"location": "task_tools/", "text": "I currently use two tools to manage my tasks: the inbox and the task manager . Inbox \u2691 The inbox does not refer only to your e-mail inbox. It is a broader concept that includes all the elements you have collected in different ways: tasks you have to do, ideas you have thought of, notes, bills, business cards, etc\u2026 To achieve a stress-free productivity, emptying the inbox should be a daily activity. Note that this does not mean doing things, it just means identifying things and deciding what to do with them, when you get it done, your situation is as follows: You have eliminated every thing you do not need. You have completed small actions that require no more than two minutes. You have delegated some actions that you do not have to do. You have sorted in your task manager the actions you will do when appropriate, because they require more than 2 minutes. You have sorted in your task manager or calendar the tasks that have a due date. There have been only a few minutes, but you feel pretty good. Everything is where it should be. I've developed pynbox to automate the management of the inbox. Help out if you like it! Task manager \u2691 If you've never used a task manager, start with the simplest one and see what do you feel its lacking. Choose then a better task manager based on your needs. In the past I've used taskwarrior , but its limitations led me to start creating pydo although it's still not a usable project :(. The simplest task manager \u2691 The simplest task manager is a markdown file in your computer with a list of tasks to do. Annotate only the actionable tasks that you need to do today, otherwise it can quickly grow to be unmanageable. When you add a new item, choose it's location relative to the existent one based on its priority. Being the top tasks are the ones that need to be done first. * Task with a high priority * Task with low priority The advantages of using a plaintext file over a physical notebook is that you can use your editor skills to manage the elements more efficiently. For example by reordering them or changing the description. Add task state sections \u2691 You'll soon encounter tasks that become blocked but need your monitoring. You can add a # Blocked section and move those tasks under it. You can optionally add the reasons why it's blocked indented below the element. * Unblocked task # Blocked * Blocked task * Waiting for Y to happen Divide a task in small steps \u2691 One of the main benefits of a task manager is that you free your mind of what you need to do next, so you can focus on the task at hand. When a task is big split it in smaller doable steps that drive to its completion. If the steps are also big split them further with more indentation levels. * Complex task * Do X * Do Y * Do Z * Do W Web based task manager \u2691 Life happened and the development of pydo has fallen behind in my priority list. I've also reached a point where simplest one is no longer suitable for my workflow because: I loose a lot of time in the reviews. I loose a lot of time when doing the different plannings (year, trimester, month, week, day). I find it hard to organize and refine the backlog. As pydo is not ready yet and I need a solution that works today better than the simplest task manager, I've done an analysis of the state of the art of self-hosted applications of all of them the two that were more promising were Taiga and OpenProject . Taiga \u2691 An Open source project with a lot of functionality. If you want to try it, you can create an account at Disroot (an awesome collective by the way). They have set up an instance where you can check if you like it. Some facts made me finally not choose it, for example: Subtasks can't have subtasks. Something I've found myself having quite often. Specially if you refine your tasks in great detail. When browsing the backlog or the boards, you can't edit a task in that window, you need to open it in another tab. I don't understand very well the different components, the difference between tasks and issues for example. OpenProject \u2691 Check the OpenProject page to see the analysis of the tool. In the end I went with this option. References \u2691 GTD time management framework.", "title": "Task Management tools"}, {"location": "task_tools/#inbox", "text": "The inbox does not refer only to your e-mail inbox. It is a broader concept that includes all the elements you have collected in different ways: tasks you have to do, ideas you have thought of, notes, bills, business cards, etc\u2026 To achieve a stress-free productivity, emptying the inbox should be a daily activity. Note that this does not mean doing things, it just means identifying things and deciding what to do with them, when you get it done, your situation is as follows: You have eliminated every thing you do not need. You have completed small actions that require no more than two minutes. You have delegated some actions that you do not have to do. You have sorted in your task manager the actions you will do when appropriate, because they require more than 2 minutes. You have sorted in your task manager or calendar the tasks that have a due date. There have been only a few minutes, but you feel pretty good. Everything is where it should be. I've developed pynbox to automate the management of the inbox. Help out if you like it!", "title": "Inbox"}, {"location": "task_tools/#task-manager", "text": "If you've never used a task manager, start with the simplest one and see what do you feel its lacking. Choose then a better task manager based on your needs. In the past I've used taskwarrior , but its limitations led me to start creating pydo although it's still not a usable project :(.", "title": "Task manager"}, {"location": "task_tools/#the-simplest-task-manager", "text": "The simplest task manager is a markdown file in your computer with a list of tasks to do. Annotate only the actionable tasks that you need to do today, otherwise it can quickly grow to be unmanageable. When you add a new item, choose it's location relative to the existent one based on its priority. Being the top tasks are the ones that need to be done first. * Task with a high priority * Task with low priority The advantages of using a plaintext file over a physical notebook is that you can use your editor skills to manage the elements more efficiently. For example by reordering them or changing the description.", "title": "The simplest task manager"}, {"location": "task_tools/#add-task-state-sections", "text": "You'll soon encounter tasks that become blocked but need your monitoring. You can add a # Blocked section and move those tasks under it. You can optionally add the reasons why it's blocked indented below the element. * Unblocked task # Blocked * Blocked task * Waiting for Y to happen", "title": "Add task state sections"}, {"location": "task_tools/#divide-a-task-in-small-steps", "text": "One of the main benefits of a task manager is that you free your mind of what you need to do next, so you can focus on the task at hand. When a task is big split it in smaller doable steps that drive to its completion. If the steps are also big split them further with more indentation levels. * Complex task * Do X * Do Y * Do Z * Do W", "title": "Divide a task in small steps"}, {"location": "task_tools/#web-based-task-manager", "text": "Life happened and the development of pydo has fallen behind in my priority list. I've also reached a point where simplest one is no longer suitable for my workflow because: I loose a lot of time in the reviews. I loose a lot of time when doing the different plannings (year, trimester, month, week, day). I find it hard to organize and refine the backlog. As pydo is not ready yet and I need a solution that works today better than the simplest task manager, I've done an analysis of the state of the art of self-hosted applications of all of them the two that were more promising were Taiga and OpenProject .", "title": "Web based task manager"}, {"location": "task_tools/#taiga", "text": "An Open source project with a lot of functionality. If you want to try it, you can create an account at Disroot (an awesome collective by the way). They have set up an instance where you can check if you like it. Some facts made me finally not choose it, for example: Subtasks can't have subtasks. Something I've found myself having quite often. Specially if you refine your tasks in great detail. When browsing the backlog or the boards, you can't edit a task in that window, you need to open it in another tab. I don't understand very well the different components, the difference between tasks and issues for example.", "title": "Taiga"}, {"location": "task_tools/#openproject", "text": "Check the OpenProject page to see the analysis of the tool. In the end I went with this option.", "title": "OpenProject"}, {"location": "task_tools/#references", "text": "GTD time management framework.", "title": "References"}, {"location": "task_workflows/", "text": "Hype flow versus a defined plan \u2691 I've found two ways to work on my tasks: following a plan and following the hype flow. The first one helps you finish what you started, and directs your efforts towards big goals. The side effect is that it achieves it by setting constrains on what to do, so you sometimes end up in the position of doing tasks that you don't want to at the moment, and suppressing yourself not to do the ones that you want. The second one takes advantage of letting you work on wherever you want at the moment, which boosts your creativity and productivity. This way imposes less constrains on you and is more pleasant because surfing the hype is awesome. The side effect is that if you have many interests, you can move forward very quickly on many directions leaving a lot of projects half done, instead of pushing in the direction of your big goals. The art here is to combine both at need, if you have a good plan, you may be able to start surfing the hype, and when the time constrains start to press you, switch to a stricter plan to be able to deliver value in time. This makes more sense in work environments, at personal level I usually just surf the hype unless I have a clear objective with a due date to reach. Planning workflows \u2691 Task management can be done at different levels. All of them help you in different ways to reduce the mental load, each also gives you extra benefits that can't be gained by the others. Going from lowest to highest abstraction level we have: Task plan. Pomodoro. Day plan. Week plan. Fortnight plan. Month plan. Trimester plan. Year plan. If you're starting your task management career, start with the first level. Once you're comfortable, move one step up until you reach the sweet spot between time invested in management and the profit it returns. Each of the plans defined below describe the most complete process, use them as a starting point to define the plan that works for you depending on your needs and how much time you want to invest at that particular moment. Even I don't follow them strictly. As they change over time, it's useful to find a way to be able to follow them without thinking too much on what are the specific steps, for example having a checklist or a script. Task plan \u2691 The task plan defines the steps required to finish a task. It's your most basic roadmap to address a task, and a good starting point if you feel overwhelmed when faced with an assignment. When done well, you'll better understand what you need to do, it will prevent you from wasting time at dead ends as you'll think before acting, and you'll develop the invaluable skill of breaking big problems into smaller ones. To define a task plan, follow the next steps: Decide what do you want to achieve when the task is finished. Analyze the possible ways to arrive to that goal. Try to assess different solutions before choosing one. Once you have it, split it into steps small enough to be comfortable following them without further analysis. Some people define the task plan whenever they add the task to their task manager. Others prefer to save some time each month to refine the plans of the tasks to be done the next one. The plan is an alive document that changes each Pomodoro cycle and that you'll need to check often. It has to be accessible and it should be easy for you to edit. If you don't know where to start, use the simplest task manager . Try not to overplan though, if at the middle of a task you realize that the rest of the steps don't make sense, all the time invested in their definition will be lost. That's why it's a good idea to have a great detail for the first steps and gradually move to rougher definitions on later ones. Pomodoro \u2691 Pomodoro is a technique used to ensure that for short periods of time, you invest all your mental resources in doing the work needed to finish a task. It's your main unit of work and a good starting point if you have concentration issues. When done well, you'll start moving faster on your tasks, because uninterrupted work is the most efficient. You'll also begin to know if you're drifting from your day's plan , and will have space to adapt it or the task plan to time constrains or unexpected events. If you don't yet have a task plan or day plan , don't worry! Ignore the steps that involve them until you do. The next steps define a Pomodoro cycle: Select the cycle time span. Either 20 minutes or until the next interruption, whichever is shortest. Decide what are you going to do. Analyze yourself to see if you're state of mind is ready to only do that for the chosen time span. If it's not, maybe you need to take a \"Pomodoro break\", take 20 minutes off doing something that replenish your willpower or the personal attribute that is preventing you to be able to work. Start the timer. Work uninterruptedly on what you've decided until the timer goes off. Take 20s to look away from the screen (this is good for your ejes). Update your task and day plans: Tick off the done task steps. Refine the task steps that can be addressed in the next cycle. Check if you can still meet the day's plan. Check the interruption channels that need to be checked each 20 minutes . At the fourth Pomodoro cycle, you'll have finished a Pomodoro iteration. At the end of the iteration: Check if you're going to meet the day plan , if you're not, change change it or the task plan to make the time constrain. Get a small rest, you've earned it! Get off the chair, stretch or give a small walk. What's important is that you take your mind off the task at hand and let your body rest. Remember, this is a marathon, you need to take care of yourself. Start a new Pomodoro iteration. If you're super focused at the end of a Pomodoro cycle, you can skip the task plan update until the end of the iteration. To make it easy to follow the pomodoro plan I use a script that: Uses timer to show the countdown. Uses safeeyes to track the eye rests. Asks me to follow the list of steps I've previously defined. Day plan \u2691 This plan defines at day level which tasks are you going to work on and schedules when are you going to address them. It's the most basic roadmap to address a group of tasks. The goal is to survive the day. It's a good starting point if you forget to do tasks that need to be done in the day or if you miss appointments. It's also the next step of advance awareness, if you have a day plan, on each Pomodoro iteration you'll get the feeling whether you're going to finish what you proposed yourself. You can make your plan at the start of the day, start by getting an idea of: What do you need to do by checking: The last day's plan. Calendar events. The week's plan if you have it, or the prioritized list of tasks to do. How much uninterrupted time you have between calendar events. Your state of mind. Then create the day schedule: Add the calendar events. Add the interruption events . Setup an alert for the closest calendar event. And the day tasks plan: Decide the tasks to be worked on and think when you want to do them. To follow it throughout the day and when it's coming to an end: Update your week or/and task plans to meet the time constrains. Optionally sketch the next day's plan. When doing the plan keep in mind to minimize the number of tasks and calendar events so as not to get overwhelmed, and not to schedule a new task before you finish what you've already started. It's better to eventually fall short on tasks, than never reaching your goal. To make it easy to follow I use a script that: Asks me to check the weather forecast. Uses timer to show the countdown. Uses safeeyes to track the eye rests. Asks me to follow the list of steps I've previously defined. Week plan \u2691 The plan defines at week level which tasks are you going to work on and schedules when are you going to address them. It's the next roadmap level to address a group of tasks. The goal changes from surviving the day to start planning your life. It's a good starting point if you are comfortable working with the pomodoro, task and day plans, and want to start deciding where you're heading to. It's also the next step of advance awareness, if you have a week plan, each day you'll get the feeling whether you're going to finish what you proposed yourself. You can make your plan at the start of the week, similar to the day plan , start by getting an idea of: What do you need to do by: Closing the last week's plan. Checking upcoming calendar events. The month's plan if you have it, or the prioritized list of tasks to do, identifying the task dependencies that may block the task development. How much uninterrupted time you have between calendar events. Your state of mind. To close the last week's plan: Mark the plan items as done Get an idea of what percent of objectives you actually met. With the mindset of seeing how much you can commit on the next one, not to think how bad you performed, you did the best you could, and nothing else could be done. Clean the active tasks. Throughout the week, there will be tasks that you left unfinished. For each of them: Decide if the task still makes sense and if it's actionable Check if you can merge it with other tasks Check if it belongs to an active objective Remove the rest. Then create the week schedule: Arrange or move calendar events to maximize the uninterrupted periods, then add them to the plan. Add the interruption events . Decide the tasks to be worked on and roughly assign them to the week days. Follow it throughout the week, and when it's coming to an end: Update your month or/and task plans to meet the time constrains. Update the people that may depend on you of possible plan drifts. To make it easy to follow I use a script that: Asks me to follow the list of steps I've previously defined. Fortnight, month, trimester plan \u2691 From the week plan you can increasingly think your roadmap, start with a fortnight plan, when you're comfortable go up to month and trimester plans. The idea is similar to the week plan but with less definition. You deal with bigger tasks that help shape your life in the long run. The process of planning and reviewing each of these should be as short as possible, otherwise you'll soon get tired of it. For example, for the month plan you can: Do the week plan review: Where you transfer the non planned tasks to the month plan. Do the fortnight plan review Do the month plan review: Check the objectives you had and how many you meet, adding notes on your progress. Analyze what to do with the new objectives, adding them to the trimester plan Transfer the non planned elements to the semester plan. Do the month's planning: Review the semester plan if you have it. Decide which big tasks you want to work on Do the fortnight plan Do the week plan References \u2691 Pomodoro article .", "title": "Task management workflows"}, {"location": "task_workflows/#hype-flow-versus-a-defined-plan", "text": "I've found two ways to work on my tasks: following a plan and following the hype flow. The first one helps you finish what you started, and directs your efforts towards big goals. The side effect is that it achieves it by setting constrains on what to do, so you sometimes end up in the position of doing tasks that you don't want to at the moment, and suppressing yourself not to do the ones that you want. The second one takes advantage of letting you work on wherever you want at the moment, which boosts your creativity and productivity. This way imposes less constrains on you and is more pleasant because surfing the hype is awesome. The side effect is that if you have many interests, you can move forward very quickly on many directions leaving a lot of projects half done, instead of pushing in the direction of your big goals. The art here is to combine both at need, if you have a good plan, you may be able to start surfing the hype, and when the time constrains start to press you, switch to a stricter plan to be able to deliver value in time. This makes more sense in work environments, at personal level I usually just surf the hype unless I have a clear objective with a due date to reach.", "title": "Hype flow versus a defined plan"}, {"location": "task_workflows/#planning-workflows", "text": "Task management can be done at different levels. All of them help you in different ways to reduce the mental load, each also gives you extra benefits that can't be gained by the others. Going from lowest to highest abstraction level we have: Task plan. Pomodoro. Day plan. Week plan. Fortnight plan. Month plan. Trimester plan. Year plan. If you're starting your task management career, start with the first level. Once you're comfortable, move one step up until you reach the sweet spot between time invested in management and the profit it returns. Each of the plans defined below describe the most complete process, use them as a starting point to define the plan that works for you depending on your needs and how much time you want to invest at that particular moment. Even I don't follow them strictly. As they change over time, it's useful to find a way to be able to follow them without thinking too much on what are the specific steps, for example having a checklist or a script.", "title": "Planning workflows"}, {"location": "task_workflows/#task-plan", "text": "The task plan defines the steps required to finish a task. It's your most basic roadmap to address a task, and a good starting point if you feel overwhelmed when faced with an assignment. When done well, you'll better understand what you need to do, it will prevent you from wasting time at dead ends as you'll think before acting, and you'll develop the invaluable skill of breaking big problems into smaller ones. To define a task plan, follow the next steps: Decide what do you want to achieve when the task is finished. Analyze the possible ways to arrive to that goal. Try to assess different solutions before choosing one. Once you have it, split it into steps small enough to be comfortable following them without further analysis. Some people define the task plan whenever they add the task to their task manager. Others prefer to save some time each month to refine the plans of the tasks to be done the next one. The plan is an alive document that changes each Pomodoro cycle and that you'll need to check often. It has to be accessible and it should be easy for you to edit. If you don't know where to start, use the simplest task manager . Try not to overplan though, if at the middle of a task you realize that the rest of the steps don't make sense, all the time invested in their definition will be lost. That's why it's a good idea to have a great detail for the first steps and gradually move to rougher definitions on later ones.", "title": "Task plan"}, {"location": "task_workflows/#pomodoro", "text": "Pomodoro is a technique used to ensure that for short periods of time, you invest all your mental resources in doing the work needed to finish a task. It's your main unit of work and a good starting point if you have concentration issues. When done well, you'll start moving faster on your tasks, because uninterrupted work is the most efficient. You'll also begin to know if you're drifting from your day's plan , and will have space to adapt it or the task plan to time constrains or unexpected events. If you don't yet have a task plan or day plan , don't worry! Ignore the steps that involve them until you do. The next steps define a Pomodoro cycle: Select the cycle time span. Either 20 minutes or until the next interruption, whichever is shortest. Decide what are you going to do. Analyze yourself to see if you're state of mind is ready to only do that for the chosen time span. If it's not, maybe you need to take a \"Pomodoro break\", take 20 minutes off doing something that replenish your willpower or the personal attribute that is preventing you to be able to work. Start the timer. Work uninterruptedly on what you've decided until the timer goes off. Take 20s to look away from the screen (this is good for your ejes). Update your task and day plans: Tick off the done task steps. Refine the task steps that can be addressed in the next cycle. Check if you can still meet the day's plan. Check the interruption channels that need to be checked each 20 minutes . At the fourth Pomodoro cycle, you'll have finished a Pomodoro iteration. At the end of the iteration: Check if you're going to meet the day plan , if you're not, change change it or the task plan to make the time constrain. Get a small rest, you've earned it! Get off the chair, stretch or give a small walk. What's important is that you take your mind off the task at hand and let your body rest. Remember, this is a marathon, you need to take care of yourself. Start a new Pomodoro iteration. If you're super focused at the end of a Pomodoro cycle, you can skip the task plan update until the end of the iteration. To make it easy to follow the pomodoro plan I use a script that: Uses timer to show the countdown. Uses safeeyes to track the eye rests. Asks me to follow the list of steps I've previously defined.", "title": "Pomodoro"}, {"location": "task_workflows/#day-plan", "text": "This plan defines at day level which tasks are you going to work on and schedules when are you going to address them. It's the most basic roadmap to address a group of tasks. The goal is to survive the day. It's a good starting point if you forget to do tasks that need to be done in the day or if you miss appointments. It's also the next step of advance awareness, if you have a day plan, on each Pomodoro iteration you'll get the feeling whether you're going to finish what you proposed yourself. You can make your plan at the start of the day, start by getting an idea of: What do you need to do by checking: The last day's plan. Calendar events. The week's plan if you have it, or the prioritized list of tasks to do. How much uninterrupted time you have between calendar events. Your state of mind. Then create the day schedule: Add the calendar events. Add the interruption events . Setup an alert for the closest calendar event. And the day tasks plan: Decide the tasks to be worked on and think when you want to do them. To follow it throughout the day and when it's coming to an end: Update your week or/and task plans to meet the time constrains. Optionally sketch the next day's plan. When doing the plan keep in mind to minimize the number of tasks and calendar events so as not to get overwhelmed, and not to schedule a new task before you finish what you've already started. It's better to eventually fall short on tasks, than never reaching your goal. To make it easy to follow I use a script that: Asks me to check the weather forecast. Uses timer to show the countdown. Uses safeeyes to track the eye rests. Asks me to follow the list of steps I've previously defined.", "title": "Day plan"}, {"location": "task_workflows/#week-plan", "text": "The plan defines at week level which tasks are you going to work on and schedules when are you going to address them. It's the next roadmap level to address a group of tasks. The goal changes from surviving the day to start planning your life. It's a good starting point if you are comfortable working with the pomodoro, task and day plans, and want to start deciding where you're heading to. It's also the next step of advance awareness, if you have a week plan, each day you'll get the feeling whether you're going to finish what you proposed yourself. You can make your plan at the start of the week, similar to the day plan , start by getting an idea of: What do you need to do by: Closing the last week's plan. Checking upcoming calendar events. The month's plan if you have it, or the prioritized list of tasks to do, identifying the task dependencies that may block the task development. How much uninterrupted time you have between calendar events. Your state of mind. To close the last week's plan: Mark the plan items as done Get an idea of what percent of objectives you actually met. With the mindset of seeing how much you can commit on the next one, not to think how bad you performed, you did the best you could, and nothing else could be done. Clean the active tasks. Throughout the week, there will be tasks that you left unfinished. For each of them: Decide if the task still makes sense and if it's actionable Check if you can merge it with other tasks Check if it belongs to an active objective Remove the rest. Then create the week schedule: Arrange or move calendar events to maximize the uninterrupted periods, then add them to the plan. Add the interruption events . Decide the tasks to be worked on and roughly assign them to the week days. Follow it throughout the week, and when it's coming to an end: Update your month or/and task plans to meet the time constrains. Update the people that may depend on you of possible plan drifts. To make it easy to follow I use a script that: Asks me to follow the list of steps I've previously defined.", "title": "Week plan"}, {"location": "task_workflows/#fortnight-month-trimester-plan", "text": "From the week plan you can increasingly think your roadmap, start with a fortnight plan, when you're comfortable go up to month and trimester plans. The idea is similar to the week plan but with less definition. You deal with bigger tasks that help shape your life in the long run. The process of planning and reviewing each of these should be as short as possible, otherwise you'll soon get tired of it. For example, for the month plan you can: Do the week plan review: Where you transfer the non planned tasks to the month plan. Do the fortnight plan review Do the month plan review: Check the objectives you had and how many you meet, adding notes on your progress. Analyze what to do with the new objectives, adding them to the trimester plan Transfer the non planned elements to the semester plan. Do the month's planning: Review the semester plan if you have it. Decide which big tasks you want to work on Do the fortnight plan Do the week plan", "title": "Fortnight, month, trimester plan"}, {"location": "task_workflows/#references", "text": "Pomodoro article .", "title": "References"}, {"location": "teeth/", "text": "Taking good care of your teeth can be easier if you remember that each visit to the dentist is both super expensive and painful. So those 10 minutes each day are really worth it. How to take care of your teeth \u2691 TL;DR: Daily actions to keep your teeth healty Brush your teeth after every meal for at least two minutes. Floss each day before the last teeth brush. Use an electric toothbrush . Replace the brush each three months or at first sign of wear and tear. Don't eat or drink anything but water after your nightly brush. Do not rinse after you brush your teeth . Use floss instead of a toothpick . Use mouthwash daily but not after brushing . Oral hygiene is the practice of keeping one's mouth clean and free of disease and other problems (e.g. bad breath) by regular brushing of the teeth (dental hygiene) and cleaning between the teeth. It is important that oral hygiene be carried out on a regular basis to enable prevention of dental disease and bad breath. The most common types of dental disease are tooth decay (cavities, dental caries) and gum diseases, including gingivitis , and periodontitis . General guidelines suggest brushing twice a day: after breakfast and before going to bed, but ideally the mouth would be cleaned after every meal. Cleaning between the teeth is called interdental cleaning and is as important as tooth brushing. This is because a toothbrush cannot reach between the teeth and therefore only removes about 50% of plaque from the surface of the teeth. There are many tools to clean between the teeth, including floss and interdental brushes; it is up to each individual to choose which tool they prefer to use. Over 80% of cavities occur inside fissures in teeth where brushing cannot reach food left trapped after eating and saliva and fluoride have no access to neutralize acid and remineralize demineralized teeth, unlike easy-to-clean parts of the tooth, where fewer cavities occur. Teeth brushing \u2691 Routine tooth brushing is the principal method of preventing many oral diseases, and perhaps the most important activity an individual can practice to reduce dental plaque and tartar . The dental plaque contains a mixture of bacteria, their acids and sticky byproducts and food remnants. It forms naturally on teeth immediately after you've eaten but doesn't get nasty and start to cause damage to the teeth until it reaches a certain stage of maturity. The exact amount of time this takes isn't known but is at least more than 12 hours. Bacteria consume sugar and, as a byproduct, produce acids which dissolve mineral out of the teeth, leaving microscopic holes we can't see. If the process isn't stopped and they aren't repaired, these can become big, visible cavities. So controlling plaque reduces the risk of the individual suffering from plaque-associated diseases such as gingivitis, periodontitis, and caries. Many oral health care professionals agree that tooth brushing should be done for a minimum of two minutes, and be practiced at least twice a day, but ideally after each meal. Toothbrushing can only clean to a depth of about 1.5 mm inside the gingival pockets, but a sustained regime of plaque removal above the gum line can affect the ecology of the microbes below the gums and may reduce the number of pathogens in pockets up to 5 mm in depth. Toothpaste (dentifrice) with fluoride, or alternatives such as nano-hydroxyapatite, is an important tool to readily use when tooth brushing. The fluoride (or alternatives) in the dentifrice is an important protective factor against caries, and an important supplement needed to remineralize already affected enamel . However, in terms of preventing gum disease, the use of toothpaste does not increase the effectiveness of the activity with respect to the amount of plaque removed. People use toothpaste with nano-hydroxyapatite instead of fluoride as it performs the same function, and some people believe fluoride in toothpaste is a neurotoxin. For maximum benefit, use toothpaste with 1350-1500 ppmF (that's concentration of fluoride in parts per million) to prevent tooth decay. At night, you produce less saliva than during the day. Because of this, your teeth have less protection from saliva and are more vulnerable to acid attacks. That's why it's important to remove food from your teeth before bed so plaque bacteria can't feast overnight. Don't eat or drink anything except water after brushing at night. This also gives fluoride the longest opportunity to work. How to brush your teeth \u2691 The procedure I'm using right now is: Wet the brush but don't add any toothpaste. Slowly and systematically guide the bristle of the electric brush from tooth to tooth, following the contour of the gums and their crowns, remembering to massage the gums. For example, start with the upper left part on the outside, reach the other side of your mouth, clean the bottom of your right side teeth, then go back on the inside of each teeth until you arrive to your left side. Try to avoid brushing with too much force as this can damage the surface of your teeth. Rinse with water. Place a pea sized amount of toothpaste on the brush and repeat the cycle. Brush your tongue. Spit the extra toothpaste but don't rinse or drink anything in the next 30 minutes. The whole process should take at least two minutes. Manual versus electric tooth brush \u2691 If you want to use a manual one, Oral health professionals recommend the use of a tooth brush with a small head and soft bristles as they are most effective in removing plaque without damaging the gums. The technique is crucial to the effectiveness of tooth brushing and disease prevention. Back and forth brushing is not effective in removing plaque at the gum line. Tooth brushing should employ a systematic approach, angle the bristles at a 45-degree angle towards the gums, and make small circular motions at that angle. When using an electric one, the bristle head should be guided from tooth to tooth slowly, following the contour of the gums and crowns of the tooth. The motion of the toothbrush head removes the need to manually oscillate the brush or make circles. Another study suggest that the effectiveness of electric toothbrushes at reducing plaque formation and gingivitis is superior to conventional manual toothbrushes. Regardless of the type, you are always best using a soft-bristled toothbrush with a small head and a flexible neck because this will most effectively remove plaque and debris from your teeth, without damaging your teeth and gums and drawing blood. Toothbrush replacement \u2691 Remember to replace your brush at the first sign of wear-and-tear or every three months, whichever comes first. Frayed or broken bristles won't clean your mouth properly. Change your toothbrush once the bristles lose their flexibility. 1 Also, after a couple of months of daily use, bacteria and food particles begin to accumulate on the toothbrush. To rinse or not to rinse \u2691 There is a lot of controversy on the topic on whether you should rinse or not your mouth after brushing your teeth. ( 1 , 2 ) People in favor of rinsing your mouth argue that: You\u2019ll get rid of the excess toothpaste along with any food or bacteria that could have been stuck in your teeth or released by the brushing itself. You\u2019ll also be removing the fluoride from your mouth, which if swallowed, might upset your stomach. People against rinsing your mouth argue that: When you rinse with water, you\u2019re potentially washing away any remnants of toothpaste, including the fluoride that makes it work. That could mean that even though you are brushing your teeth, it might not be as effective as it should be. Whilst there have been studies on the effectiveness of rinsing, the results only indicate that there COULD be an advantage of one over the other. So it's up to you to evaluate the advantages and disadvantages of each method. Some people are prone to cavities, or might have poor dental health. If your teeth chip, crack or break easily, it\u2019s strongly recommended that you don't rinse after you brush. Similarly, if you consume a lot of sugar, you should probably avoid rinsing. If you don't fit into these categories, then it\u2019s really based on your own preference. Keep your brush away from your feces \u2691 As the Mythbusters showed , Fecal coliform were found on toothbrushes stored at the bathroom. And even though none were of a level high enough to be dangerous, and experts confirm that such bacteria are impossible to completely avoid, you can reduce the risk by: Storing the brush in a cupboard or in other room. Putting a lid on your toothbrush Closing the lid on your toilet seat before flushing. How to floss \u2691 Tooth brushing alone will not remove plaque from all surfaces of the tooth as 40% of the surfaces are interdental. One technique that can be used to access these areas is dental floss. When the proper technique is used, flossing can remove plaque and food particles from between the teeth and below the gums. The American Dental Association (ADA) reports that up to 80% of plaque may be removed by this method. The ADA recommends cleaning between the teeth as part of one's daily oral hygiene regime, with a different piece of floss at each flossing session. The correct technique to ensure maximum plaque removal is as follows: ( 1 , 2 ) Floss length: 15\u201325 cm wrapped around middle fingers. For upper teeth grasp the floss with thumb and index finger, for lower teeth with both index fingers. Ensure that a length of roughly 2.5cm is left between the fingers. Ease the floss gently between the teeth using a back and forth motion. Do not snap the floss into the gums. When the floss reaches your gumline, curve it into a C-shape against a tooth until you feel resistance. Hold the floss against the tooth. Gently scrape the side of the tooth, moving the floss away from the gum. Repeat on the other side of the gap, along the side of the next tooth. Do not forget the back of your last tooth. Ensure that the floss is taken below the gum margins using a back and forth up and down motion. You should floss before brushing your teeth because any food, plaque, and bacteria released by flossing are removed by the afterwards brushing. Another tips regarding flossing are : Skip the toothpick: Use floss instead of a toothpick to remove food stuck in between your teeth. Using a toothpick can damage your gums and lead to an infection. Be gentle: Don't be too aggressive when flossing to avoid bleeding gums. When you first start flossing, your gums may be tender and bleed a little. Carry on flossing your teeth and the bleeding should stop as your gums become healthier. If you're still getting regular bleeding after a few days, see your dental team. They can check if you're flossing correctly. Mouth washing \u2691 Using a mouthwash that contains fluoride can help prevent tooth decay, but don't use mouthwash (even a fluoride one) straight after brushing your teeth or it'll wash away the concentrated fluoride in the toothpaste left on your teeth. [ 1 ] Choose a different time to use mouthwash, such as after lunch. And remember not to eat or drink for 30 minutes after using a fluoride mouthwash. Do yearly check ups \u2691 First find a dentist that you trust, until you do, search for second and third options before diving into anything you may regret. Once you have it, yearly go to their dreaded places so they can: Check that everything is alright. Do a regular clean, but beware of unnecessary deep cleaning . References \u2691 Wikipedia oral hygiene article CNN health article on oral hygiene", "title": "Teeth"}, {"location": "teeth/#how-to-take-care-of-your-teeth", "text": "TL;DR: Daily actions to keep your teeth healty Brush your teeth after every meal for at least two minutes. Floss each day before the last teeth brush. Use an electric toothbrush . Replace the brush each three months or at first sign of wear and tear. Don't eat or drink anything but water after your nightly brush. Do not rinse after you brush your teeth . Use floss instead of a toothpick . Use mouthwash daily but not after brushing . Oral hygiene is the practice of keeping one's mouth clean and free of disease and other problems (e.g. bad breath) by regular brushing of the teeth (dental hygiene) and cleaning between the teeth. It is important that oral hygiene be carried out on a regular basis to enable prevention of dental disease and bad breath. The most common types of dental disease are tooth decay (cavities, dental caries) and gum diseases, including gingivitis , and periodontitis . General guidelines suggest brushing twice a day: after breakfast and before going to bed, but ideally the mouth would be cleaned after every meal. Cleaning between the teeth is called interdental cleaning and is as important as tooth brushing. This is because a toothbrush cannot reach between the teeth and therefore only removes about 50% of plaque from the surface of the teeth. There are many tools to clean between the teeth, including floss and interdental brushes; it is up to each individual to choose which tool they prefer to use. Over 80% of cavities occur inside fissures in teeth where brushing cannot reach food left trapped after eating and saliva and fluoride have no access to neutralize acid and remineralize demineralized teeth, unlike easy-to-clean parts of the tooth, where fewer cavities occur.", "title": "How to take care of your teeth"}, {"location": "teeth/#teeth-brushing", "text": "Routine tooth brushing is the principal method of preventing many oral diseases, and perhaps the most important activity an individual can practice to reduce dental plaque and tartar . The dental plaque contains a mixture of bacteria, their acids and sticky byproducts and food remnants. It forms naturally on teeth immediately after you've eaten but doesn't get nasty and start to cause damage to the teeth until it reaches a certain stage of maturity. The exact amount of time this takes isn't known but is at least more than 12 hours. Bacteria consume sugar and, as a byproduct, produce acids which dissolve mineral out of the teeth, leaving microscopic holes we can't see. If the process isn't stopped and they aren't repaired, these can become big, visible cavities. So controlling plaque reduces the risk of the individual suffering from plaque-associated diseases such as gingivitis, periodontitis, and caries. Many oral health care professionals agree that tooth brushing should be done for a minimum of two minutes, and be practiced at least twice a day, but ideally after each meal. Toothbrushing can only clean to a depth of about 1.5 mm inside the gingival pockets, but a sustained regime of plaque removal above the gum line can affect the ecology of the microbes below the gums and may reduce the number of pathogens in pockets up to 5 mm in depth. Toothpaste (dentifrice) with fluoride, or alternatives such as nano-hydroxyapatite, is an important tool to readily use when tooth brushing. The fluoride (or alternatives) in the dentifrice is an important protective factor against caries, and an important supplement needed to remineralize already affected enamel . However, in terms of preventing gum disease, the use of toothpaste does not increase the effectiveness of the activity with respect to the amount of plaque removed. People use toothpaste with nano-hydroxyapatite instead of fluoride as it performs the same function, and some people believe fluoride in toothpaste is a neurotoxin. For maximum benefit, use toothpaste with 1350-1500 ppmF (that's concentration of fluoride in parts per million) to prevent tooth decay. At night, you produce less saliva than during the day. Because of this, your teeth have less protection from saliva and are more vulnerable to acid attacks. That's why it's important to remove food from your teeth before bed so plaque bacteria can't feast overnight. Don't eat or drink anything except water after brushing at night. This also gives fluoride the longest opportunity to work.", "title": "Teeth brushing"}, {"location": "teeth/#how-to-brush-your-teeth", "text": "The procedure I'm using right now is: Wet the brush but don't add any toothpaste. Slowly and systematically guide the bristle of the electric brush from tooth to tooth, following the contour of the gums and their crowns, remembering to massage the gums. For example, start with the upper left part on the outside, reach the other side of your mouth, clean the bottom of your right side teeth, then go back on the inside of each teeth until you arrive to your left side. Try to avoid brushing with too much force as this can damage the surface of your teeth. Rinse with water. Place a pea sized amount of toothpaste on the brush and repeat the cycle. Brush your tongue. Spit the extra toothpaste but don't rinse or drink anything in the next 30 minutes. The whole process should take at least two minutes.", "title": "How to brush your teeth"}, {"location": "teeth/#manual-versus-electric-tooth-brush", "text": "If you want to use a manual one, Oral health professionals recommend the use of a tooth brush with a small head and soft bristles as they are most effective in removing plaque without damaging the gums. The technique is crucial to the effectiveness of tooth brushing and disease prevention. Back and forth brushing is not effective in removing plaque at the gum line. Tooth brushing should employ a systematic approach, angle the bristles at a 45-degree angle towards the gums, and make small circular motions at that angle. When using an electric one, the bristle head should be guided from tooth to tooth slowly, following the contour of the gums and crowns of the tooth. The motion of the toothbrush head removes the need to manually oscillate the brush or make circles. Another study suggest that the effectiveness of electric toothbrushes at reducing plaque formation and gingivitis is superior to conventional manual toothbrushes. Regardless of the type, you are always best using a soft-bristled toothbrush with a small head and a flexible neck because this will most effectively remove plaque and debris from your teeth, without damaging your teeth and gums and drawing blood.", "title": "Manual versus electric tooth brush"}, {"location": "teeth/#toothbrush-replacement", "text": "Remember to replace your brush at the first sign of wear-and-tear or every three months, whichever comes first. Frayed or broken bristles won't clean your mouth properly. Change your toothbrush once the bristles lose their flexibility. 1 Also, after a couple of months of daily use, bacteria and food particles begin to accumulate on the toothbrush.", "title": "Toothbrush replacement"}, {"location": "teeth/#to-rinse-or-not-to-rinse", "text": "There is a lot of controversy on the topic on whether you should rinse or not your mouth after brushing your teeth. ( 1 , 2 ) People in favor of rinsing your mouth argue that: You\u2019ll get rid of the excess toothpaste along with any food or bacteria that could have been stuck in your teeth or released by the brushing itself. You\u2019ll also be removing the fluoride from your mouth, which if swallowed, might upset your stomach. People against rinsing your mouth argue that: When you rinse with water, you\u2019re potentially washing away any remnants of toothpaste, including the fluoride that makes it work. That could mean that even though you are brushing your teeth, it might not be as effective as it should be. Whilst there have been studies on the effectiveness of rinsing, the results only indicate that there COULD be an advantage of one over the other. So it's up to you to evaluate the advantages and disadvantages of each method. Some people are prone to cavities, or might have poor dental health. If your teeth chip, crack or break easily, it\u2019s strongly recommended that you don't rinse after you brush. Similarly, if you consume a lot of sugar, you should probably avoid rinsing. If you don't fit into these categories, then it\u2019s really based on your own preference.", "title": "To rinse or not to rinse"}, {"location": "teeth/#keep-your-brush-away-from-your-feces", "text": "As the Mythbusters showed , Fecal coliform were found on toothbrushes stored at the bathroom. And even though none were of a level high enough to be dangerous, and experts confirm that such bacteria are impossible to completely avoid, you can reduce the risk by: Storing the brush in a cupboard or in other room. Putting a lid on your toothbrush Closing the lid on your toilet seat before flushing.", "title": "Keep your brush away from your feces"}, {"location": "teeth/#how-to-floss", "text": "Tooth brushing alone will not remove plaque from all surfaces of the tooth as 40% of the surfaces are interdental. One technique that can be used to access these areas is dental floss. When the proper technique is used, flossing can remove plaque and food particles from between the teeth and below the gums. The American Dental Association (ADA) reports that up to 80% of plaque may be removed by this method. The ADA recommends cleaning between the teeth as part of one's daily oral hygiene regime, with a different piece of floss at each flossing session. The correct technique to ensure maximum plaque removal is as follows: ( 1 , 2 ) Floss length: 15\u201325 cm wrapped around middle fingers. For upper teeth grasp the floss with thumb and index finger, for lower teeth with both index fingers. Ensure that a length of roughly 2.5cm is left between the fingers. Ease the floss gently between the teeth using a back and forth motion. Do not snap the floss into the gums. When the floss reaches your gumline, curve it into a C-shape against a tooth until you feel resistance. Hold the floss against the tooth. Gently scrape the side of the tooth, moving the floss away from the gum. Repeat on the other side of the gap, along the side of the next tooth. Do not forget the back of your last tooth. Ensure that the floss is taken below the gum margins using a back and forth up and down motion. You should floss before brushing your teeth because any food, plaque, and bacteria released by flossing are removed by the afterwards brushing. Another tips regarding flossing are : Skip the toothpick: Use floss instead of a toothpick to remove food stuck in between your teeth. Using a toothpick can damage your gums and lead to an infection. Be gentle: Don't be too aggressive when flossing to avoid bleeding gums. When you first start flossing, your gums may be tender and bleed a little. Carry on flossing your teeth and the bleeding should stop as your gums become healthier. If you're still getting regular bleeding after a few days, see your dental team. They can check if you're flossing correctly.", "title": "How to floss"}, {"location": "teeth/#mouth-washing", "text": "Using a mouthwash that contains fluoride can help prevent tooth decay, but don't use mouthwash (even a fluoride one) straight after brushing your teeth or it'll wash away the concentrated fluoride in the toothpaste left on your teeth. [ 1 ] Choose a different time to use mouthwash, such as after lunch. And remember not to eat or drink for 30 minutes after using a fluoride mouthwash.", "title": "Mouth washing"}, {"location": "teeth/#do-yearly-check-ups", "text": "First find a dentist that you trust, until you do, search for second and third options before diving into anything you may regret. Once you have it, yearly go to their dreaded places so they can: Check that everything is alright. Do a regular clean, but beware of unnecessary deep cleaning .", "title": "Do yearly check ups"}, {"location": "teeth/#references", "text": "Wikipedia oral hygiene article CNN health article on oral hygiene", "title": "References"}, {"location": "teeth_deep_cleaning/", "text": "TL;DR: Ask the opinion of two or three independent dentists before doing a deep clean. Scaling and root planing , also known as conventional periodontal therapy, non-surgical periodontal therapy or deep cleaning, is a procedure involving removal of dental plaque and calculus (scaling or debridement) and then smoothing, or planing, of the (exposed) surfaces of the roots, removing cementum or dentine that is impregnated with calculus, toxins, or microorganisms, the etiologic agents that cause inflammation. It is a part of non-surgical periodontal therapy. This helps to establish a periodontium that is in remission of periodontal disease. As to the frequency of cleaning, research on this matter is inconclusive. That is, it has neither been shown that more frequent cleaning leads to better outcomes nor that it does not. Thus, any general recommendation for a frequency of routine cleaning (e.g. every six months, every year) has no empirical basis. ( 1 ) Why do we need deep cleaning \u2691 We all have a plethora of bacteria in our mouths. Those bacteria mix with other substances to form sticky plaque on teeth, which is mostly banished by regular brushing and flossing. Plaques that don't get brushed away can harden and form a substance known as tartar, which can only be removed with a dental cleaning. When tartar remains on the teeth, it can cause inflammation of the gums, a condition called gingivitis, characterized by red swollen gums that can bleed easily. A mild form of gum disease, gingivitis can usually be reversed through regular brushing and flossing along with cleanings by a dentist or hygienist. If gingivitis isn't cured, it can advance to a more severe form of gum disease called periodontitis, in which the inflamed tissue begins to pull away from the teeth, forming spaces, or pockets. As the pockets become deeper, more of the tooth below the gum line is exposed to bacteria, which can damage the bone holding teeth in place. Eventually, if the pockets become deep enough, teeth can become loose and may even be lost. Dentists measure the depth of the pockets with a probe that has a tiny ruler on the end. Healthy gums have pockets that measure no more than 3 mm \u2014 or a little less than a tenth of an inch \u2014 deep. More than that and you\u2019re getting into trouble. One way to slow or halt the process is through deep cleaning, which removes the plaque below the gum line and smooths rough spots on the tooth root, making it harder for bacteria to accumulate there. Signs of periodontitis \u2691 Red or swollen gums Tender or bleeding gums Persistent bad breath Your teeth look like they\u2019ve been getting longer as gums recede. Teeth that are sensitive Loose teeth Pain when chewing Evidence-based dentistry \u2691 Several systematic reviews have been made of the effectiveness of scaling and root planing as evidence-based dentistry. A Cochrane review by Worthington et al. in 2013 considered only scaling and polishing of the teeth, but not root planing. After examining 88 papers they found only three studies that met all their requirements, remarking that \"the quality of the evidence was generally low.\" An extensive review that did involve root planing was published by the Canadian Agency for Drugs and Technologies in Health in 2016 . It made a number of findings, including (1) In five randomized controlled trials, scaling and root planing \"was associated with a decrease in plaque from baseline at one month, three months, or six months;\" and (2) Four studies analyzed changes in the gingival index (GI) from the baseline and \"found a significant improvement from baseline in the scaling and root planing group at three months and six months.\" This study also discussed evidence-based guidelines for frequency of scaling with and without root planing for patients both with and without chronic periodontitis. The group that produced one of the main systematic reviews used in the 2016 Canadian review has published guidelines based on its findings. They recommend that scaling and root planing (SRP) should be considered as the initial treatment for patients with chronic periodontitis. They note that \"the strength of the recommendation is limited because SRP is considered the reference standard and thus used as an active control for periodontal trials and there are few studies in which investigators compare SRP with no treatment.\" They add however that \"root planing ... carries the risk of damaging the root surface and potentially causing tooth or root sensitivity. Generally expected post-SRP procedural adverse effects include discomfort.\" Enamel cracks, early caries and resin restorations can be damaged during scaling. Effectiveness of the procedure \u2691 A scaling and root planing procedure is to be considered effective if the patient is subsequently able to maintain their periodontal health without further bone or attachment loss and if it prevents recurrent infection with periodontal pathogens. The long term effectiveness of scaling and root planing depends upon a number of factors. These factors include patient compliance, disease progress at the time of intervention, probing depth, and anatomical factors like grooves in the roots of teeth, concavities, and furcation involvement which may limit visibility of underlying deep calculus and debris. First and foremost, periodontal scaling and root planing is a procedure that must be done thoroughly and with attention to detail in order to ensure complete removal of all calculus and plaque from involved sites. If these causative agents are not removed, the disease will continue to progress and further damage will result. In cases of mild to moderate periodontitis, scaling and root planing can achieve excellent results if the procedure is thorough. As periodontitis increases in severity, a greater amount of supporting bone is destroyed by the infection. This is illustrated clinically by the deepening of the periodontal pockets targeted for cleaning and disinfection during the procedure. Once the periodontal pockets exceed 6 mm in depth, the effectiveness of deposit removal begins to decrease, and the likelihood of complete healing after one procedure begins to decline as well. The more severe the infection prior to intervention, the greater the effort required to arrest its progress and return the patient to health. Diseased pockets over 6 mm can be resolved through periodontal flap surgery. Although healing of the soft tissues will begin immediately following removal of the microbial biofilm and calculus that cause the disease, scaling and root planing is only the first step in arresting the disease process. Following initial cleaning and disinfection of all affected sites, it is necessary to prevent the infection from recurring. Therefore, patient compliance is, by far, the most important factor, having the greatest influence on the success or failure of periodontal intervention. Immediately following treatment, the patient will need to maintain excellent oral care at home. With proper homecare, which includes but is by no means limited to brushing twice daily for 2\u20133 minutes, flossing daily and use of mouthrinse, the potential for effective healing following scaling and root planing increases. Commitment to and diligence in the thorough completion of daily oral hygiene practices are essential to this success. The process which allows for the formation of deep periodontal pockets does not occur overnight. Therefore, it is unrealistic to expect the tissue to heal completely in a similarly short time period. Gains in gingival attachment may occur slowly over time, and ongoing periodontal maintenance visits are usually recommended (by some sources). Side effects \u2691 The process carries it's risks, such as: Pop out a filling Gums damage in an irreversible way. End up with an abscess if a tiny piece of tartar is knocked loose and becomes trapped. Have more sensitivity after the procedure. Conclusion \u2691 Deep cleaning is an invasive procedure There is a lack of scientific studies supporting the frequency of it's application, specially for people that doesn't suffer from periodontitis. It's an expensive procedure. So, ask the opinion of two or three independent dentists before doing a deep clean.", "title": "Teeth deep cleaning"}, {"location": "teeth_deep_cleaning/#why-do-we-need-deep-cleaning", "text": "We all have a plethora of bacteria in our mouths. Those bacteria mix with other substances to form sticky plaque on teeth, which is mostly banished by regular brushing and flossing. Plaques that don't get brushed away can harden and form a substance known as tartar, which can only be removed with a dental cleaning. When tartar remains on the teeth, it can cause inflammation of the gums, a condition called gingivitis, characterized by red swollen gums that can bleed easily. A mild form of gum disease, gingivitis can usually be reversed through regular brushing and flossing along with cleanings by a dentist or hygienist. If gingivitis isn't cured, it can advance to a more severe form of gum disease called periodontitis, in which the inflamed tissue begins to pull away from the teeth, forming spaces, or pockets. As the pockets become deeper, more of the tooth below the gum line is exposed to bacteria, which can damage the bone holding teeth in place. Eventually, if the pockets become deep enough, teeth can become loose and may even be lost. Dentists measure the depth of the pockets with a probe that has a tiny ruler on the end. Healthy gums have pockets that measure no more than 3 mm \u2014 or a little less than a tenth of an inch \u2014 deep. More than that and you\u2019re getting into trouble. One way to slow or halt the process is through deep cleaning, which removes the plaque below the gum line and smooths rough spots on the tooth root, making it harder for bacteria to accumulate there.", "title": "Why do we need deep cleaning"}, {"location": "teeth_deep_cleaning/#signs-of-periodontitis", "text": "Red or swollen gums Tender or bleeding gums Persistent bad breath Your teeth look like they\u2019ve been getting longer as gums recede. Teeth that are sensitive Loose teeth Pain when chewing", "title": "Signs of periodontitis"}, {"location": "teeth_deep_cleaning/#evidence-based-dentistry", "text": "Several systematic reviews have been made of the effectiveness of scaling and root planing as evidence-based dentistry. A Cochrane review by Worthington et al. in 2013 considered only scaling and polishing of the teeth, but not root planing. After examining 88 papers they found only three studies that met all their requirements, remarking that \"the quality of the evidence was generally low.\" An extensive review that did involve root planing was published by the Canadian Agency for Drugs and Technologies in Health in 2016 . It made a number of findings, including (1) In five randomized controlled trials, scaling and root planing \"was associated with a decrease in plaque from baseline at one month, three months, or six months;\" and (2) Four studies analyzed changes in the gingival index (GI) from the baseline and \"found a significant improvement from baseline in the scaling and root planing group at three months and six months.\" This study also discussed evidence-based guidelines for frequency of scaling with and without root planing for patients both with and without chronic periodontitis. The group that produced one of the main systematic reviews used in the 2016 Canadian review has published guidelines based on its findings. They recommend that scaling and root planing (SRP) should be considered as the initial treatment for patients with chronic periodontitis. They note that \"the strength of the recommendation is limited because SRP is considered the reference standard and thus used as an active control for periodontal trials and there are few studies in which investigators compare SRP with no treatment.\" They add however that \"root planing ... carries the risk of damaging the root surface and potentially causing tooth or root sensitivity. Generally expected post-SRP procedural adverse effects include discomfort.\" Enamel cracks, early caries and resin restorations can be damaged during scaling.", "title": "Evidence-based dentistry"}, {"location": "teeth_deep_cleaning/#effectiveness-of-the-procedure", "text": "A scaling and root planing procedure is to be considered effective if the patient is subsequently able to maintain their periodontal health without further bone or attachment loss and if it prevents recurrent infection with periodontal pathogens. The long term effectiveness of scaling and root planing depends upon a number of factors. These factors include patient compliance, disease progress at the time of intervention, probing depth, and anatomical factors like grooves in the roots of teeth, concavities, and furcation involvement which may limit visibility of underlying deep calculus and debris. First and foremost, periodontal scaling and root planing is a procedure that must be done thoroughly and with attention to detail in order to ensure complete removal of all calculus and plaque from involved sites. If these causative agents are not removed, the disease will continue to progress and further damage will result. In cases of mild to moderate periodontitis, scaling and root planing can achieve excellent results if the procedure is thorough. As periodontitis increases in severity, a greater amount of supporting bone is destroyed by the infection. This is illustrated clinically by the deepening of the periodontal pockets targeted for cleaning and disinfection during the procedure. Once the periodontal pockets exceed 6 mm in depth, the effectiveness of deposit removal begins to decrease, and the likelihood of complete healing after one procedure begins to decline as well. The more severe the infection prior to intervention, the greater the effort required to arrest its progress and return the patient to health. Diseased pockets over 6 mm can be resolved through periodontal flap surgery. Although healing of the soft tissues will begin immediately following removal of the microbial biofilm and calculus that cause the disease, scaling and root planing is only the first step in arresting the disease process. Following initial cleaning and disinfection of all affected sites, it is necessary to prevent the infection from recurring. Therefore, patient compliance is, by far, the most important factor, having the greatest influence on the success or failure of periodontal intervention. Immediately following treatment, the patient will need to maintain excellent oral care at home. With proper homecare, which includes but is by no means limited to brushing twice daily for 2\u20133 minutes, flossing daily and use of mouthrinse, the potential for effective healing following scaling and root planing increases. Commitment to and diligence in the thorough completion of daily oral hygiene practices are essential to this success. The process which allows for the formation of deep periodontal pockets does not occur overnight. Therefore, it is unrealistic to expect the tissue to heal completely in a similarly short time period. Gains in gingival attachment may occur slowly over time, and ongoing periodontal maintenance visits are usually recommended (by some sources).", "title": "Effectiveness of the procedure"}, {"location": "teeth_deep_cleaning/#side-effects", "text": "The process carries it's risks, such as: Pop out a filling Gums damage in an irreversible way. End up with an abscess if a tiny piece of tartar is knocked loose and becomes trapped. Have more sensitivity after the procedure.", "title": "Side effects"}, {"location": "teeth_deep_cleaning/#conclusion", "text": "Deep cleaning is an invasive procedure There is a lack of scientific studies supporting the frequency of it's application, specially for people that doesn't suffer from periodontitis. It's an expensive procedure. So, ask the opinion of two or three independent dentists before doing a deep clean.", "title": "Conclusion"}, {"location": "tenacity/", "text": "Tenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything. Installation \u2691 pip install tenacity Usage \u2691 Tenacity isn't api compatible with retrying but adds significant new functionality and fixes a number of longstanding bugs. The simplest use case is retrying a flaky function whenever an Exception occurs until a value is returned. import random from tenacity import retry @retry def do_something_unreliable (): if random . randint ( 0 , 10 ) > 1 : raise IOError ( \"Broken sauce, everything is hosed!!!111one\" ) else : return \"Awesome sauce!\" print ( do_something_unreliable ()) Basic Retry \u2691 As you saw above, the default behavior is to retry forever without waiting when an exception is raised. @retry def never_gonna_give_you_up (): print ( \"Retry forever ignoring Exceptions, don't wait between retries\" ) raise Exception Stopping \u2691 Let\u2019s be a little less persistent and set some boundaries, such as the number of attempts before giving up. @retry ( stop = stop_after_attempt ( 7 )) def stop_after_7_attempts (): print ( \"Stopping after 7 attempts\" ) raise Exception We don\u2019t have all day, so let\u2019s set a boundary for how long we should be retrying stuff. @retry ( stop = stop_after_delay ( 10 )) def stop_after_10_s (): print ( \"Stopping after 10 seconds\" ) raise Exception You can combine several stop conditions by using the | operator: @retry ( stop = ( stop_after_delay ( 10 ) | stop_after_attempt ( 5 ))) def stop_after_10_s_or_5_retries (): print ( \"Stopping after 10 seconds or 5 retries\" ) raise Exception Waiting before retrying \u2691 Most things don\u2019t like to be polled as fast as possible, so let\u2019s just wait 2 seconds between retries. @retry ( wait = wait_fixed ( 2 )) def wait_2_s (): print ( \"Wait 2 second between retries\" ) raise Exception Some things perform best with a bit of randomness injected. @retry ( wait = wait_random ( min = 1 , max = 2 )) def wait_random_1_to_2_s (): print ( \"Randomly wait 1 to 2 seconds between retries\" ) raise Exception Then again, it\u2019s hard to beat exponential backoff when retrying distributed services and other remote endpoints. @retry ( wait = wait_exponential ( multiplier = 1 , min = 4 , max = 10 )) def wait_exponential_1 (): print ( \"Wait 2^x * 1 second between each retry starting with 4 seconds, then up to 10 seconds, then 10 seconds afterwards\" ) raise Exception Then again, it\u2019s also hard to beat combining fixed waits and jitter (to help avoid thundering herds) when retrying distributed services and other remote endpoints. @retry ( wait = wait_fixed ( 3 ) + wait_random ( 0 , 2 )) def wait_fixed_jitter (): print ( \"Wait at least 3 seconds, and add up to 2 seconds of random delay\" ) raise Exception When multiple processes are in contention for a shared resource, exponentially increasing jitter helps minimise collisions. @retry ( wait = wait_random_exponential ( multiplier = 1 , max = 60 )) def wait_exponential_jitter (): print ( \"Randomly wait up to 2^x * 1 seconds between each retry until the range reaches 60 seconds, then randomly up to 60 seconds afterwards\" ) raise Exception Whether to retry \u2691 We have a few options for dealing with retries that raise specific or general exceptions, as in the cases here. @retry ( retry = retry_if_exception_type ( IOError )) def might_io_error (): print ( \"Retry forever with no wait if an IOError occurs, raise any other errors\" ) raise Exception We can also use the result of the function to alter the behavior of retrying. def is_none_p ( value ): \"\"\"Return True if value is None\"\"\" return value is None @retry ( retry = retry_if_result ( is_none_p )) def might_return_none (): print ( \"Retry with no wait if return value is None\" ) We can also combine several conditions: def is_none_p ( value ): \"\"\"Return True if value is None\"\"\" return value is None @retry ( retry = ( retry_if_result ( is_none_p ) | retry_if_exception_type ())) def might_return_none (): print ( \"Retry forever ignoring Exceptions with no wait if return value is None\" ) Any combination of stop, wait, etc. is also supported to give you the freedom to mix and match. It\u2019s also possible to retry explicitly at any time by raising the TryAgain exception: @retry def do_something (): result = something_else () if result == 23 : raise TryAgain References \u2691 Git", "title": "Tenacity"}, {"location": "tenacity/#installation", "text": "pip install tenacity", "title": "Installation"}, {"location": "tenacity/#usage", "text": "Tenacity isn't api compatible with retrying but adds significant new functionality and fixes a number of longstanding bugs. The simplest use case is retrying a flaky function whenever an Exception occurs until a value is returned. import random from tenacity import retry @retry def do_something_unreliable (): if random . randint ( 0 , 10 ) > 1 : raise IOError ( \"Broken sauce, everything is hosed!!!111one\" ) else : return \"Awesome sauce!\" print ( do_something_unreliable ())", "title": "Usage"}, {"location": "tenacity/#basic-retry", "text": "As you saw above, the default behavior is to retry forever without waiting when an exception is raised. @retry def never_gonna_give_you_up (): print ( \"Retry forever ignoring Exceptions, don't wait between retries\" ) raise Exception", "title": "Basic Retry"}, {"location": "tenacity/#stopping", "text": "Let\u2019s be a little less persistent and set some boundaries, such as the number of attempts before giving up. @retry ( stop = stop_after_attempt ( 7 )) def stop_after_7_attempts (): print ( \"Stopping after 7 attempts\" ) raise Exception We don\u2019t have all day, so let\u2019s set a boundary for how long we should be retrying stuff. @retry ( stop = stop_after_delay ( 10 )) def stop_after_10_s (): print ( \"Stopping after 10 seconds\" ) raise Exception You can combine several stop conditions by using the | operator: @retry ( stop = ( stop_after_delay ( 10 ) | stop_after_attempt ( 5 ))) def stop_after_10_s_or_5_retries (): print ( \"Stopping after 10 seconds or 5 retries\" ) raise Exception", "title": "Stopping"}, {"location": "tenacity/#waiting-before-retrying", "text": "Most things don\u2019t like to be polled as fast as possible, so let\u2019s just wait 2 seconds between retries. @retry ( wait = wait_fixed ( 2 )) def wait_2_s (): print ( \"Wait 2 second between retries\" ) raise Exception Some things perform best with a bit of randomness injected. @retry ( wait = wait_random ( min = 1 , max = 2 )) def wait_random_1_to_2_s (): print ( \"Randomly wait 1 to 2 seconds between retries\" ) raise Exception Then again, it\u2019s hard to beat exponential backoff when retrying distributed services and other remote endpoints. @retry ( wait = wait_exponential ( multiplier = 1 , min = 4 , max = 10 )) def wait_exponential_1 (): print ( \"Wait 2^x * 1 second between each retry starting with 4 seconds, then up to 10 seconds, then 10 seconds afterwards\" ) raise Exception Then again, it\u2019s also hard to beat combining fixed waits and jitter (to help avoid thundering herds) when retrying distributed services and other remote endpoints. @retry ( wait = wait_fixed ( 3 ) + wait_random ( 0 , 2 )) def wait_fixed_jitter (): print ( \"Wait at least 3 seconds, and add up to 2 seconds of random delay\" ) raise Exception When multiple processes are in contention for a shared resource, exponentially increasing jitter helps minimise collisions. @retry ( wait = wait_random_exponential ( multiplier = 1 , max = 60 )) def wait_exponential_jitter (): print ( \"Randomly wait up to 2^x * 1 seconds between each retry until the range reaches 60 seconds, then randomly up to 60 seconds afterwards\" ) raise Exception", "title": "Waiting before retrying"}, {"location": "tenacity/#whether-to-retry", "text": "We have a few options for dealing with retries that raise specific or general exceptions, as in the cases here. @retry ( retry = retry_if_exception_type ( IOError )) def might_io_error (): print ( \"Retry forever with no wait if an IOError occurs, raise any other errors\" ) raise Exception We can also use the result of the function to alter the behavior of retrying. def is_none_p ( value ): \"\"\"Return True if value is None\"\"\" return value is None @retry ( retry = retry_if_result ( is_none_p )) def might_return_none (): print ( \"Retry with no wait if return value is None\" ) We can also combine several conditions: def is_none_p ( value ): \"\"\"Return True if value is None\"\"\" return value is None @retry ( retry = ( retry_if_result ( is_none_p ) | retry_if_exception_type ())) def might_return_none (): print ( \"Retry forever ignoring Exceptions with no wait if return value is None\" ) Any combination of stop, wait, etc. is also supported to give you the freedom to mix and match. It\u2019s also possible to retry explicitly at any time by raising the TryAgain exception: @retry def do_something (): result = something_else () if result == 23 : raise TryAgain", "title": "Whether to retry"}, {"location": "tenacity/#references", "text": "Git", "title": "References"}, {"location": "terraform/", "text": "Terraform is an open-source infrastructure as code software tool created by HashiCorp. It enables users to define and provision a datacenter infrastructure using an awful high-level configuration language known as Hashicorp Configuration Language (HCL), or optionally JSON. Terraform supports a number of cloud infrastructure providers such as Amazon Web Services, IBM Cloud , Google Cloud Platform, DigitalOcean, Linode, Microsoft Azure, Oracle Cloud Infrastructure, OVH, or VMware vSphere as well as OpenNebula and OpenStack. Tools \u2691 tfschema : A binary that allows you to see the attributes of the resources of the different providers. There are some times that there are complex attributes that aren't shown on the docs with an example. Here you'll see them clearly. tfschema resource list aws | grep aws_iam_user > aws_iam_user > aws_iam_user_group_membership > aws_iam_user_login_profile > aws_iam_user_policy > aws_iam_user_policy_attachment > aws_iam_user_ssh_key tfschema resource show aws_iam_user +----------------------+-------------+----------+----------+----------+-----------+ | ATTRIBUTE | TYPE | REQUIRED | OPTIONAL | COMPUTED | SENSITIVE | +----------------------+-------------+----------+----------+----------+-----------+ | arn | string | false | false | true | false | | force_destroy | bool | false | true | false | false | | id | string | false | true | true | false | | name | string | true | false | false | false | | path | string | false | true | false | false | | permissions_boundary | string | false | true | false | false | | tags | map ( string ) | false | true | false | false | | unique_id | string | false | false | true | false | +----------------------+-------------+----------+----------+----------+-----------+ # Open the documentation of the resource in the browser tfschema resource browse aws_iam_user terraforming : Tool to export existing resources to terraform terraboard : Web dashboard to visualize and query terraform tfstate, you can search, compare and see the most active ones. There are deployments for k8s. export AWS_ACCESS_KEY_ID = XXXXXXXXXXXXXXXXXXXX export AWS_SECRET_ACCESS_KEY = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX export AWS_DEFAULT_REGION = eu-west-1 export AWS_BUCKET = terraform-tfstate-20180119 export TERRABOARD_LOG_LEVEL = debug docker network create terranet docker run -ti --rm --name db -e POSTGRES_USER = gorm -e POSTGRES_DB = gorm -e POSTGRES_PASSWORD = \"mypassword\" --net terranet postgres docker run -ti --rm -p 8080 :8080 -e AWS_REGION = \" $AWS_DEFAULT_REGION \" -e AWS_ACCESS_KEY_ID = \" ${ AWS_ACCESS_KEY_ID } \" -e AWS_SECRET_ACCESS_KEY = \" ${ AWS_SECRET_ACCESS_KEY } \" -e AWS_BUCKET = \" $AWS_BUCKET \" -e DB_PASSWORD = \"mypassword\" --net terranet camptocamp/terraboard:latest tfenv: Install different versions of terraform git clone https://github.com/tfutils/tfenv.git ~/.tfenv echo 'export PATH=\"$HOME/.tfenv/bin:$PATH\"' >> ~/.bashrc echo 'export PATH=\"$HOME/.tfenv/bin:$PATH\"' >> ~/.zshrc tfenv list-remote tfenv install 0 .12.8 terraform version tfenv install 0 .11.15 terraform version tfenv use 0 .12.8 terraform version https://github.com/eerkunt/terraform-compliance landscape : A program to modify the plan and show a nicer version, really useful when it's shown as json. Right now it only works for terraform 11. terraform plan | landscape k2tf : Program to convert k8s yaml manifestos to HCL. Editor Plugins \u2691 For Vim : vim-terraform : Execute tf from vim and autoformat when saving. vim-terraform-completion : linter and autocomplete. Good practices and maintaining \u2691 fmt : Formats the code following hashicorp best practices. terraform fmt Validate : Tests that the syntax is correct. terraform validate Documentaci\u00f3n : Generates a table in markdown with the inputs and outputs. terraform-docs markdown table *.tf > README.md ## Inputs | Name | Description | Type | Default | Required | | ------ | ------------- | :----: | :-----: | :-----: | | broker_numbers | Number of brokers | number | ` \"3\" ` | no | | broker_size | AWS instance type for the brokers | string | ` \"kafka.m5.large\" ` | no | | ebs_size | Size of the brokers disks | string | ` \"300\" ` | no | | kafka_version | Kafka version | string | ` \"2.1.0\" ` | no | ## Outputs | Name | Description | | ------ | ------------- | | brokers_masked_endpoints | Zookeeper masked endpoints | | brokers_real_endpoints | Zookeeper real endpoints | | zookeeper_masked_endpoints | Zookeeper masked endpoints | | zookeeper_real_endpoints | Zookeeper real endpoints | Terraform lint ( tflint ): Only works with some AWS resources. It allows the validation against a third party API. For example: resource \"aws_instance\" \"foo\" { ami = \"ami-0ff8a91507f77f867\" instance_type = \"t1.2xlarge\" # invalid type! } The code is valid, but in AWS there doesn't exist the type t1.2xlarge . This test avoids this kind of issues. wget https://github.com/wata727/tflint/releases/download/v0.11.1/tflint_darwin_amd64.zip unzip tflint_darwin_amd64.zip sudo install tflint /usr/local/bin/ tflint -v We can automate all the above to be executed before we do a commit using the pre-commit framework. sudo pip install pre-commit cd $proyectoConTerraform echo \"\"\"repos: - repo: git://github.com/antonbabenko/pre-commit-terraform rev: v1.19.0 hooks: - id: terraform_fmt - id: terraform_validate - id: terraform_docs - id: terraform_tflint \"\"\" > .pre-commit-config.yaml pre-commit install pre-commit run terraform_fmt pre-commit run terraform_validate --file dynamo.tf pre-commit run -a Tests \u2691 Motivation Static analysis \u2691 Linters \u2691 conftest tflint terraform validate Dry run \u2691 terraform plan hashicorp sentinel terraform-compliance Unit tests \u2691 There is no real unit testing in infrastructure code as you need to deploy it in a real environment terratest (works for k8s and terraform) Some sample code in: github.com/gruntwork-io/infrastructure-as-code-testing-talk gruntwork.io E2E test \u2691 Too slow and too brittle to be worth it Use incremental e2e testing Variables \u2691 It's a good practice to name the resource before the particularization of the resource, so you can search all the elements of that resource, for example, instead of client_cidr and operations_cidr use cidr_operations and cidr_client variable \"list_example\" { description = \"An example of a list\" type = \"list\" default = [ 1 , 2 , 3 ] } variable \"map_example\" { description = \"An example of a dictionary\" type = \"map\" default = { key1 = \"value1\" key2 = \"value2\" } } For the use of maps inside maps or lists investigate zipmap To access you have to use \"${var.list_example}\" For secret variables we use: variable \"db_password\" { description = \"The password for the database\" } Which has no default value, we save that password in our keystore and pass it as environmental variable export TF_VAR_db_password = \"{{ your password }}\" terragrunt plan As a reminder, Terraform stores all variables in its state file in plain text, including this database password, which is why your terragrunt config should always enable encryption for remote state storage in S3 Interpolation of variables \u2691 You can't interpolate in variables, so instead of variable \"sistemas_gpg\" { description = \"Sistemas public GPG key for Zena\" type = \"string\" default = \"${file(\"sistemas_zena.pub\")}\" } You have to use locals locals { sistemas_gpg = \"${file(\"sistemas_zena.pub\")}\" } \"${local.sistemas_gpg}\" Show information of the resources \u2691 Get information of the infrastructure. Output variables show up in the console after you run terraform apply , you can also use terraform output [{{ output_name }}] to see the value of a specific output without applying any changes output \"public_ip\" { value = \"${aws_instance.example.public_ip}\" } > terraform apply aws_security_group.instance: Refreshing state... ( ID: sg-db91dba1 ) aws_instance.example: Refreshing state... ( ID: i-61744350 ) Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Outputs: public_ip = 54 .174.13.5 Data source \u2691 A data source represents a piece of read-only information that is fetched from the provider every time you run Terraform. It does not create anything new data \"aws_availability_zones\" \"all\" {} And you reference it with \"${data.aws_availability_zones.all.names}\" Read-only state source \u2691 With terraform_remote_state you an fetch the Terraform state file stored by another set of templates in a completely read-only manner. From an app template we can read the info of the ddbb with data \"terraform_remote_state\" \"db\" { backend = \"s3\" config { bucket = \"(YOUR_BUCKET_NAME)\" key = \"stage/data-stores/mysql/terraform.tfstate\" region = \"us-east-1\" } } And you would access the variables inside the database terraform file with data.terraform_remote_state.db.outputs.port To share variables from state, you need to to set them in the outputs.tf file. Template_file source \u2691 It is used to load templates, it has two parameters, template which is a string and vars which is a map of variables. it has one output attribute called rendered , which is the result of rendering template. For example # File: user-data.sh #!/bin/bash cat > index.html <<EOF <h1>Hello, World</h1> <p>DB address: ${db_address}</p> <p>DB port: ${db_port}</p> EOF nohup busybox httpd -f -p \" ${ server_port } \" & data \"template_file\" \"user_data\" { template = \"${file(\"user-data.sh\")}\" vars { server_port = \"${var.server_port}\" db_address = \"${data.terraform_remote_state.db.address}\" db_port = \"${data.terraform_remote_state.db.port}\" } } Resource lifecycle \u2691 The lifecycle parameter is a meta-parameter , it exist on about every resource in Terraform. You can add a lifecycle block to any resource to configure how that resource should be created, updated or destroyed. The available options are: * create_before_destroy : Which if set to true will create a replacement resource before destroying hte original resource * prevent_destroy : If set to true, any attempt to delete that resource ( terraform destroy ), will fail, to delete it you have to first remove the prevent_destroy resource \"aws_launch_configuration\" \"example\" { image_id = \"ami-40d28157\" instance_type = \"t2.micro\" security_groups = [ \"${aws_security_group.instance.id}\" ] user_data = <<- EOF #!/bin/bash echo \"Hello, World\" > index.html nohup busybox httpd -f -p \"${var.server_port}\" & EOF lifecycle { create_before_destroy = true } } If you set the create_before_destroy on a resource, you also have to set it on every resource that X depends on (if you forget, you'll get errors about cyclical dependencies). In the case of the launch configuration, that means you need to set create_before_destroy to true on the security group: resource \"aws_security_group\" \"instance\" { name = \"terraform-example-instance\" ingress { from_port = \"${var.server_port}\" to_port = \"${var.server_port}\" protocol = \"tcp\" cidr_blocks = [ \"0.0.0.0/0\" ] } lifecycle { create_before_destroy = true } } Use collaboratively \u2691 Share state \u2691 The best option is to use S3 as bucket of the config. First create it resource \"aws_s3_bucket\" \"terraform_state\" { bucket = \"terraform-up-and-running-state\" versioning { enabled = true } lifecycle { prevent_destroy = true } } And then configure terraform terraform remote config \\ -backend = s3 \\ -backend-config = \"bucket=(YOUR_BUCKET_NAME)\" \\ -backend-config = \"key=global/s3/terraform.tfstate\" \\ -backend-config = \"region=us-east-1\" \\ -backend-config = \"encrypt=true\" In this way terraform will automatically pull the latest state from this bucked and push the latest state after running a command Lock terraform \u2691 To avoid several people running terraform at the same time, we'd use terragrunt a wrapper for terraform that manages remote state for you automatically and provies locking by using DynamoDB (in the free tier) Inside the terraform_config.tf you create the dynamodb table and then configure your s3 backend to use it resource \"aws_dynamodb_table\" \"terraform_statelock\" { name = \"global-s3\" read_capacity = 20 write_capacity = 20 hash_key = \"LockID\" attribute { name = \"LockID\" type = \"S\" } } terraform { backend \"s3\" { bucket = \"grupo-zena-tfstate\" key = \"global/s3/terraform.tfstate\" region = \"eu-west-1\" encrypt = \"true\" dynamodb_table = \"global-s3\" } } You'll probably need to execute an terraform apply with the dynamodb_table line commented If you want to unforce a lock, execute: terraform force-unlock {{ unlock_id }} You get the unlock_id from an error trying to execute any terraform command Modules \u2691 In terraform you can put code inside of a module and reuse in multiple places throughout your code. The provider resource should be specified by the user and not in the modules Whenever you add a module to your terraform template or modify its source parameter you need to run a get command before you run plan or apply terraform get To extract output variables of a module to the parent tf file you should use ${module.{{module.name}}.{{output_name}}} Basics \u2691 Any set of Terraform templates in a directory is a module. The good practice is to have a directory called modules in your parent project directory. There you git clone the desired modules. and for example inside pro/services/bastion/main.tf you'd call it with: provider \"aws\" { region = \"eu-west-1\" } module \"bastion\" { source = \"../../../modules/services/bastion/\" } Outputs \u2691 Modules encapsulate their resources. A resource in one module cannot directly depend on resources or attributes in other modules, unless those are exported through outputs. These outputs can be referenced in other places in your configuration, for example: resource \"aws_instance\" \"client\" { ami = \"ami-408c7f28\" instance_type = \"t1.micro\" availability_zone = \"${module.consul.server_availability_zone}\" } Import \u2691 You can import the different parts with terraform import {{resource_type}}.{{resource_name}} {{ resource_id }} For examples see the documentation of the desired resource. Bulk import \u2691 But if you want to bulk import sources, I suggest using terraforming . Bad points \u2691 Manually added resources wont be managed by terraform, therefore you can't use it to enforce as shown in this bug . If you modify the LC of an ASG, the instances don't get rolling updated, you have to do it manually. They call the dictionaries map ... (/\uff9f\u0414\uff9f)/ The conditionals are really ugly. You need to use count . You can't split long strings xD Best practices \u2691 Name the resources with _ instead of - so the editor's completion work :) VPC \u2691 Don't use the default vpc Security groups \u2691 Instead of using aws_security_group to define the ingress and egress rules, use it only to create the empty security group and use aws_security_group_rule to add the rules, otherwise you'll get into a cycle loop The sintaxis of an egress security group must be egress_from_{{source}}_to_destination . The sintaxis of an ingress security group must be ingress_to_{{destination}}_from_{{source}} Also set the order of the arguments, so they look like the name. For ingress rule: security_group_id = ... cidr_blocks = ... And in egress should look like: security_group_id = ... cidr_blocks = ... Imagine you want to filter the traffic from A -> B, the egress rule from A to B should go besides the ingress rule from B to A. Default security group \u2691 You can't manage the default security group of an vpc, therefore you have to adopt it and set it to no rules at all with aws_default_security_group resource IAM \u2691 You have to generate an gpg key and export it in base64 gpg --export {{ gpg_id }} | base64 To see the secrets you have to decrypt it terraform output secret | base64 --decode | gpg -d Sensitive information \u2691 There are several approaches here. First rely on the S3 encryption to protect the information in your state file Second use Vault provider to protect the state file. Third (but I won't use it) would be to use terrahelp RDS credentials \u2691 The RDS credentials are saved in plaintext both in the definition and in the state file, see this bug for more information. The value of password is not compared against the value of the password in the cloud, so as long as the string in the code and the state file remains the same, it won't try to change it. As a workaround, you can create the RDS with a fake password changeme , and once the resource is created, run an aws command to change it. That way, the value in your code and the state is not the real one, but it won't try to change it. Inspired in this gist and the local-exec docs, you could do: resource \"aws_db_instance\" \"main\" { username = \"postgres\" password = \"changeme\" ... } resource \"null_resource\" \"master_password\" { triggers { db_host = aws_db_instance.main.address } provisioner \"local-exec\" { command = \"pass generate rds_main_password; aws rds modify-db-instance --db-instance-identifier $INSTANCE --master-user-password $(pass show rds_main_password)\" environment = { INSTANCE = aws_db_instance.main.identifier } } } Where the password is stored in your pass repository that can be shared with the team. If you're wondering why I added such a long line, well it's because of HCL! as you can't split long strings , marvelous isn't it? xD Loops \u2691 You can't use nested lists or dictionaries, see this 2015 bug Loop over a variable \u2691 variable \"vpn_egress_tcp_ports\" { description = \"VPN egress tcp ports \" type = \"list\" default = [50, 51, 500, 4500] } resource \"aws_security_group_rule\" \"ingress_tcp_from_ops_to_vpn_instance\"{ count = \"${length(var.vpn_egress_tcp_ports)}\" type = \"ingress\" from_port = \"${element(var.vpn_egress_tcp_ports, count.index)}\" to_port = \"${element(var.vpn_egress_tcp_ports, count.index)}\" protocol = \"tcp\" cidr_blocks = [ \"${var.cidr}\"] security_group_id = \"${aws_security_group.pro_ins_vpn.id}\" } Refactoring \u2691 Refactoring in terraform is ugly business Refactoring in modules \u2691 If you try to refactor your terraform state into modules it will try to destroy and recreate all the elements of the module... Refactoring the state file \u2691 terraform state mv -state-out = other.tfstate module.web module.web Google cloud integration \u2691 You configure it in the terraform directory // Configure the Google Cloud provider provider \"google\" { credentials = \"${file(\"account.json\")}\" project = \"my-gce-project\" region = \"us-central1\" } To download the json go to the Google Developers Console . Go to Credentials then Create credentials and finally Service account key . Select Compute engine default service account and select JSON as the key type. Ignore the change of an attribute \u2691 Sometimes you don't care whether some attributes of a resource change, if that's the case use the lifecycle statement: resource \"aws_instance\" \"example\" { # ... lifecycle { ignore_changes = [ # Ignore changes to tags, e.g. because a management agent # updates these based on some ruleset managed elsewhere. tags, ] } } Define the default value of an variable that contains an object as empty \u2691 variable \"database\" { type = object({ size = number instance_type = string storage_type = string engine = string engine_version = string parameter_group_name = string multi_az = bool }) default = null Conditionals \u2691 Elif \u2691 locals { test = \"${ condition ? value : (elif-condition ? elif-value : else-value)}\" } Do a conditional if a variable is not null \u2691 resource \"aws_db_instance\" \"instance\" { count = var.database == null ? 0 : 1 ... Debugging \u2691 You can set the TF_LOG environmental variable to one of the log levels TRACE , DEBUG , INFO , WARN or ERROR to change the verbosity of the logs. To remove the debug traces run unset TF_LOG . References \u2691 Docs Modules registry terraform-aws-modules AWS providers AWS examples GCloud examples Good and bad sides of terraform Awesome Terraform", "title": "Terraform"}, {"location": "terraform/#tools", "text": "tfschema : A binary that allows you to see the attributes of the resources of the different providers. There are some times that there are complex attributes that aren't shown on the docs with an example. Here you'll see them clearly. tfschema resource list aws | grep aws_iam_user > aws_iam_user > aws_iam_user_group_membership > aws_iam_user_login_profile > aws_iam_user_policy > aws_iam_user_policy_attachment > aws_iam_user_ssh_key tfschema resource show aws_iam_user +----------------------+-------------+----------+----------+----------+-----------+ | ATTRIBUTE | TYPE | REQUIRED | OPTIONAL | COMPUTED | SENSITIVE | +----------------------+-------------+----------+----------+----------+-----------+ | arn | string | false | false | true | false | | force_destroy | bool | false | true | false | false | | id | string | false | true | true | false | | name | string | true | false | false | false | | path | string | false | true | false | false | | permissions_boundary | string | false | true | false | false | | tags | map ( string ) | false | true | false | false | | unique_id | string | false | false | true | false | +----------------------+-------------+----------+----------+----------+-----------+ # Open the documentation of the resource in the browser tfschema resource browse aws_iam_user terraforming : Tool to export existing resources to terraform terraboard : Web dashboard to visualize and query terraform tfstate, you can search, compare and see the most active ones. There are deployments for k8s. export AWS_ACCESS_KEY_ID = XXXXXXXXXXXXXXXXXXXX export AWS_SECRET_ACCESS_KEY = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX export AWS_DEFAULT_REGION = eu-west-1 export AWS_BUCKET = terraform-tfstate-20180119 export TERRABOARD_LOG_LEVEL = debug docker network create terranet docker run -ti --rm --name db -e POSTGRES_USER = gorm -e POSTGRES_DB = gorm -e POSTGRES_PASSWORD = \"mypassword\" --net terranet postgres docker run -ti --rm -p 8080 :8080 -e AWS_REGION = \" $AWS_DEFAULT_REGION \" -e AWS_ACCESS_KEY_ID = \" ${ AWS_ACCESS_KEY_ID } \" -e AWS_SECRET_ACCESS_KEY = \" ${ AWS_SECRET_ACCESS_KEY } \" -e AWS_BUCKET = \" $AWS_BUCKET \" -e DB_PASSWORD = \"mypassword\" --net terranet camptocamp/terraboard:latest tfenv: Install different versions of terraform git clone https://github.com/tfutils/tfenv.git ~/.tfenv echo 'export PATH=\"$HOME/.tfenv/bin:$PATH\"' >> ~/.bashrc echo 'export PATH=\"$HOME/.tfenv/bin:$PATH\"' >> ~/.zshrc tfenv list-remote tfenv install 0 .12.8 terraform version tfenv install 0 .11.15 terraform version tfenv use 0 .12.8 terraform version https://github.com/eerkunt/terraform-compliance landscape : A program to modify the plan and show a nicer version, really useful when it's shown as json. Right now it only works for terraform 11. terraform plan | landscape k2tf : Program to convert k8s yaml manifestos to HCL.", "title": "Tools"}, {"location": "terraform/#editor-plugins", "text": "For Vim : vim-terraform : Execute tf from vim and autoformat when saving. vim-terraform-completion : linter and autocomplete.", "title": "Editor Plugins"}, {"location": "terraform/#good-practices-and-maintaining", "text": "fmt : Formats the code following hashicorp best practices. terraform fmt Validate : Tests that the syntax is correct. terraform validate Documentaci\u00f3n : Generates a table in markdown with the inputs and outputs. terraform-docs markdown table *.tf > README.md ## Inputs | Name | Description | Type | Default | Required | | ------ | ------------- | :----: | :-----: | :-----: | | broker_numbers | Number of brokers | number | ` \"3\" ` | no | | broker_size | AWS instance type for the brokers | string | ` \"kafka.m5.large\" ` | no | | ebs_size | Size of the brokers disks | string | ` \"300\" ` | no | | kafka_version | Kafka version | string | ` \"2.1.0\" ` | no | ## Outputs | Name | Description | | ------ | ------------- | | brokers_masked_endpoints | Zookeeper masked endpoints | | brokers_real_endpoints | Zookeeper real endpoints | | zookeeper_masked_endpoints | Zookeeper masked endpoints | | zookeeper_real_endpoints | Zookeeper real endpoints | Terraform lint ( tflint ): Only works with some AWS resources. It allows the validation against a third party API. For example: resource \"aws_instance\" \"foo\" { ami = \"ami-0ff8a91507f77f867\" instance_type = \"t1.2xlarge\" # invalid type! } The code is valid, but in AWS there doesn't exist the type t1.2xlarge . This test avoids this kind of issues. wget https://github.com/wata727/tflint/releases/download/v0.11.1/tflint_darwin_amd64.zip unzip tflint_darwin_amd64.zip sudo install tflint /usr/local/bin/ tflint -v We can automate all the above to be executed before we do a commit using the pre-commit framework. sudo pip install pre-commit cd $proyectoConTerraform echo \"\"\"repos: - repo: git://github.com/antonbabenko/pre-commit-terraform rev: v1.19.0 hooks: - id: terraform_fmt - id: terraform_validate - id: terraform_docs - id: terraform_tflint \"\"\" > .pre-commit-config.yaml pre-commit install pre-commit run terraform_fmt pre-commit run terraform_validate --file dynamo.tf pre-commit run -a", "title": "Good practices and maintaining"}, {"location": "terraform/#tests", "text": "Motivation", "title": "Tests"}, {"location": "terraform/#static-analysis", "text": "", "title": "Static analysis"}, {"location": "terraform/#linters", "text": "conftest tflint terraform validate", "title": "Linters"}, {"location": "terraform/#dry-run", "text": "terraform plan hashicorp sentinel terraform-compliance", "title": "Dry run"}, {"location": "terraform/#unit-tests", "text": "There is no real unit testing in infrastructure code as you need to deploy it in a real environment terratest (works for k8s and terraform) Some sample code in: github.com/gruntwork-io/infrastructure-as-code-testing-talk gruntwork.io", "title": "Unit tests"}, {"location": "terraform/#e2e-test", "text": "Too slow and too brittle to be worth it Use incremental e2e testing", "title": "E2E test"}, {"location": "terraform/#variables", "text": "It's a good practice to name the resource before the particularization of the resource, so you can search all the elements of that resource, for example, instead of client_cidr and operations_cidr use cidr_operations and cidr_client variable \"list_example\" { description = \"An example of a list\" type = \"list\" default = [ 1 , 2 , 3 ] } variable \"map_example\" { description = \"An example of a dictionary\" type = \"map\" default = { key1 = \"value1\" key2 = \"value2\" } } For the use of maps inside maps or lists investigate zipmap To access you have to use \"${var.list_example}\" For secret variables we use: variable \"db_password\" { description = \"The password for the database\" } Which has no default value, we save that password in our keystore and pass it as environmental variable export TF_VAR_db_password = \"{{ your password }}\" terragrunt plan As a reminder, Terraform stores all variables in its state file in plain text, including this database password, which is why your terragrunt config should always enable encryption for remote state storage in S3", "title": "Variables"}, {"location": "terraform/#interpolation-of-variables", "text": "You can't interpolate in variables, so instead of variable \"sistemas_gpg\" { description = \"Sistemas public GPG key for Zena\" type = \"string\" default = \"${file(\"sistemas_zena.pub\")}\" } You have to use locals locals { sistemas_gpg = \"${file(\"sistemas_zena.pub\")}\" } \"${local.sistemas_gpg}\"", "title": "Interpolation of variables"}, {"location": "terraform/#show-information-of-the-resources", "text": "Get information of the infrastructure. Output variables show up in the console after you run terraform apply , you can also use terraform output [{{ output_name }}] to see the value of a specific output without applying any changes output \"public_ip\" { value = \"${aws_instance.example.public_ip}\" } > terraform apply aws_security_group.instance: Refreshing state... ( ID: sg-db91dba1 ) aws_instance.example: Refreshing state... ( ID: i-61744350 ) Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Outputs: public_ip = 54 .174.13.5", "title": "Show information of the resources"}, {"location": "terraform/#data-source", "text": "A data source represents a piece of read-only information that is fetched from the provider every time you run Terraform. It does not create anything new data \"aws_availability_zones\" \"all\" {} And you reference it with \"${data.aws_availability_zones.all.names}\"", "title": "Data source"}, {"location": "terraform/#read-only-state-source", "text": "With terraform_remote_state you an fetch the Terraform state file stored by another set of templates in a completely read-only manner. From an app template we can read the info of the ddbb with data \"terraform_remote_state\" \"db\" { backend = \"s3\" config { bucket = \"(YOUR_BUCKET_NAME)\" key = \"stage/data-stores/mysql/terraform.tfstate\" region = \"us-east-1\" } } And you would access the variables inside the database terraform file with data.terraform_remote_state.db.outputs.port To share variables from state, you need to to set them in the outputs.tf file.", "title": "Read-only state source"}, {"location": "terraform/#template_file-source", "text": "It is used to load templates, it has two parameters, template which is a string and vars which is a map of variables. it has one output attribute called rendered , which is the result of rendering template. For example # File: user-data.sh #!/bin/bash cat > index.html <<EOF <h1>Hello, World</h1> <p>DB address: ${db_address}</p> <p>DB port: ${db_port}</p> EOF nohup busybox httpd -f -p \" ${ server_port } \" & data \"template_file\" \"user_data\" { template = \"${file(\"user-data.sh\")}\" vars { server_port = \"${var.server_port}\" db_address = \"${data.terraform_remote_state.db.address}\" db_port = \"${data.terraform_remote_state.db.port}\" } }", "title": "Template_file source"}, {"location": "terraform/#resource-lifecycle", "text": "The lifecycle parameter is a meta-parameter , it exist on about every resource in Terraform. You can add a lifecycle block to any resource to configure how that resource should be created, updated or destroyed. The available options are: * create_before_destroy : Which if set to true will create a replacement resource before destroying hte original resource * prevent_destroy : If set to true, any attempt to delete that resource ( terraform destroy ), will fail, to delete it you have to first remove the prevent_destroy resource \"aws_launch_configuration\" \"example\" { image_id = \"ami-40d28157\" instance_type = \"t2.micro\" security_groups = [ \"${aws_security_group.instance.id}\" ] user_data = <<- EOF #!/bin/bash echo \"Hello, World\" > index.html nohup busybox httpd -f -p \"${var.server_port}\" & EOF lifecycle { create_before_destroy = true } } If you set the create_before_destroy on a resource, you also have to set it on every resource that X depends on (if you forget, you'll get errors about cyclical dependencies). In the case of the launch configuration, that means you need to set create_before_destroy to true on the security group: resource \"aws_security_group\" \"instance\" { name = \"terraform-example-instance\" ingress { from_port = \"${var.server_port}\" to_port = \"${var.server_port}\" protocol = \"tcp\" cidr_blocks = [ \"0.0.0.0/0\" ] } lifecycle { create_before_destroy = true } }", "title": "Resource lifecycle"}, {"location": "terraform/#use-collaboratively", "text": "", "title": "Use collaboratively"}, {"location": "terraform/#share-state", "text": "The best option is to use S3 as bucket of the config. First create it resource \"aws_s3_bucket\" \"terraform_state\" { bucket = \"terraform-up-and-running-state\" versioning { enabled = true } lifecycle { prevent_destroy = true } } And then configure terraform terraform remote config \\ -backend = s3 \\ -backend-config = \"bucket=(YOUR_BUCKET_NAME)\" \\ -backend-config = \"key=global/s3/terraform.tfstate\" \\ -backend-config = \"region=us-east-1\" \\ -backend-config = \"encrypt=true\" In this way terraform will automatically pull the latest state from this bucked and push the latest state after running a command", "title": "Share state"}, {"location": "terraform/#lock-terraform", "text": "To avoid several people running terraform at the same time, we'd use terragrunt a wrapper for terraform that manages remote state for you automatically and provies locking by using DynamoDB (in the free tier) Inside the terraform_config.tf you create the dynamodb table and then configure your s3 backend to use it resource \"aws_dynamodb_table\" \"terraform_statelock\" { name = \"global-s3\" read_capacity = 20 write_capacity = 20 hash_key = \"LockID\" attribute { name = \"LockID\" type = \"S\" } } terraform { backend \"s3\" { bucket = \"grupo-zena-tfstate\" key = \"global/s3/terraform.tfstate\" region = \"eu-west-1\" encrypt = \"true\" dynamodb_table = \"global-s3\" } } You'll probably need to execute an terraform apply with the dynamodb_table line commented If you want to unforce a lock, execute: terraform force-unlock {{ unlock_id }} You get the unlock_id from an error trying to execute any terraform command", "title": "Lock terraform"}, {"location": "terraform/#modules", "text": "In terraform you can put code inside of a module and reuse in multiple places throughout your code. The provider resource should be specified by the user and not in the modules Whenever you add a module to your terraform template or modify its source parameter you need to run a get command before you run plan or apply terraform get To extract output variables of a module to the parent tf file you should use ${module.{{module.name}}.{{output_name}}}", "title": "Modules"}, {"location": "terraform/#basics", "text": "Any set of Terraform templates in a directory is a module. The good practice is to have a directory called modules in your parent project directory. There you git clone the desired modules. and for example inside pro/services/bastion/main.tf you'd call it with: provider \"aws\" { region = \"eu-west-1\" } module \"bastion\" { source = \"../../../modules/services/bastion/\" }", "title": "Basics"}, {"location": "terraform/#outputs", "text": "Modules encapsulate their resources. A resource in one module cannot directly depend on resources or attributes in other modules, unless those are exported through outputs. These outputs can be referenced in other places in your configuration, for example: resource \"aws_instance\" \"client\" { ami = \"ami-408c7f28\" instance_type = \"t1.micro\" availability_zone = \"${module.consul.server_availability_zone}\" }", "title": "Outputs"}, {"location": "terraform/#import", "text": "You can import the different parts with terraform import {{resource_type}}.{{resource_name}} {{ resource_id }} For examples see the documentation of the desired resource.", "title": "Import"}, {"location": "terraform/#bulk-import", "text": "But if you want to bulk import sources, I suggest using terraforming .", "title": "Bulk import"}, {"location": "terraform/#bad-points", "text": "Manually added resources wont be managed by terraform, therefore you can't use it to enforce as shown in this bug . If you modify the LC of an ASG, the instances don't get rolling updated, you have to do it manually. They call the dictionaries map ... (/\uff9f\u0414\uff9f)/ The conditionals are really ugly. You need to use count . You can't split long strings xD", "title": "Bad points"}, {"location": "terraform/#best-practices", "text": "Name the resources with _ instead of - so the editor's completion work :)", "title": "Best practices"}, {"location": "terraform/#vpc", "text": "Don't use the default vpc", "title": "VPC"}, {"location": "terraform/#security-groups", "text": "Instead of using aws_security_group to define the ingress and egress rules, use it only to create the empty security group and use aws_security_group_rule to add the rules, otherwise you'll get into a cycle loop The sintaxis of an egress security group must be egress_from_{{source}}_to_destination . The sintaxis of an ingress security group must be ingress_to_{{destination}}_from_{{source}} Also set the order of the arguments, so they look like the name. For ingress rule: security_group_id = ... cidr_blocks = ... And in egress should look like: security_group_id = ... cidr_blocks = ... Imagine you want to filter the traffic from A -> B, the egress rule from A to B should go besides the ingress rule from B to A.", "title": "Security groups"}, {"location": "terraform/#default-security-group", "text": "You can't manage the default security group of an vpc, therefore you have to adopt it and set it to no rules at all with aws_default_security_group resource", "title": "Default security group"}, {"location": "terraform/#iam", "text": "You have to generate an gpg key and export it in base64 gpg --export {{ gpg_id }} | base64 To see the secrets you have to decrypt it terraform output secret | base64 --decode | gpg -d", "title": "IAM"}, {"location": "terraform/#sensitive-information", "text": "There are several approaches here. First rely on the S3 encryption to protect the information in your state file Second use Vault provider to protect the state file. Third (but I won't use it) would be to use terrahelp", "title": "Sensitive information"}, {"location": "terraform/#rds-credentials", "text": "The RDS credentials are saved in plaintext both in the definition and in the state file, see this bug for more information. The value of password is not compared against the value of the password in the cloud, so as long as the string in the code and the state file remains the same, it won't try to change it. As a workaround, you can create the RDS with a fake password changeme , and once the resource is created, run an aws command to change it. That way, the value in your code and the state is not the real one, but it won't try to change it. Inspired in this gist and the local-exec docs, you could do: resource \"aws_db_instance\" \"main\" { username = \"postgres\" password = \"changeme\" ... } resource \"null_resource\" \"master_password\" { triggers { db_host = aws_db_instance.main.address } provisioner \"local-exec\" { command = \"pass generate rds_main_password; aws rds modify-db-instance --db-instance-identifier $INSTANCE --master-user-password $(pass show rds_main_password)\" environment = { INSTANCE = aws_db_instance.main.identifier } } } Where the password is stored in your pass repository that can be shared with the team. If you're wondering why I added such a long line, well it's because of HCL! as you can't split long strings , marvelous isn't it? xD", "title": "RDS credentials"}, {"location": "terraform/#loops", "text": "You can't use nested lists or dictionaries, see this 2015 bug", "title": "Loops"}, {"location": "terraform/#loop-over-a-variable", "text": "variable \"vpn_egress_tcp_ports\" { description = \"VPN egress tcp ports \" type = \"list\" default = [50, 51, 500, 4500] } resource \"aws_security_group_rule\" \"ingress_tcp_from_ops_to_vpn_instance\"{ count = \"${length(var.vpn_egress_tcp_ports)}\" type = \"ingress\" from_port = \"${element(var.vpn_egress_tcp_ports, count.index)}\" to_port = \"${element(var.vpn_egress_tcp_ports, count.index)}\" protocol = \"tcp\" cidr_blocks = [ \"${var.cidr}\"] security_group_id = \"${aws_security_group.pro_ins_vpn.id}\" }", "title": "Loop over a variable"}, {"location": "terraform/#refactoring", "text": "Refactoring in terraform is ugly business", "title": "Refactoring"}, {"location": "terraform/#refactoring-in-modules", "text": "If you try to refactor your terraform state into modules it will try to destroy and recreate all the elements of the module...", "title": "Refactoring in modules"}, {"location": "terraform/#refactoring-the-state-file", "text": "terraform state mv -state-out = other.tfstate module.web module.web", "title": "Refactoring the state file"}, {"location": "terraform/#google-cloud-integration", "text": "You configure it in the terraform directory // Configure the Google Cloud provider provider \"google\" { credentials = \"${file(\"account.json\")}\" project = \"my-gce-project\" region = \"us-central1\" } To download the json go to the Google Developers Console . Go to Credentials then Create credentials and finally Service account key . Select Compute engine default service account and select JSON as the key type.", "title": "Google cloud integration"}, {"location": "terraform/#ignore-the-change-of-an-attribute", "text": "Sometimes you don't care whether some attributes of a resource change, if that's the case use the lifecycle statement: resource \"aws_instance\" \"example\" { # ... lifecycle { ignore_changes = [ # Ignore changes to tags, e.g. because a management agent # updates these based on some ruleset managed elsewhere. tags, ] } }", "title": "Ignore the change of an attribute"}, {"location": "terraform/#define-the-default-value-of-an-variable-that-contains-an-object-as-empty", "text": "variable \"database\" { type = object({ size = number instance_type = string storage_type = string engine = string engine_version = string parameter_group_name = string multi_az = bool }) default = null", "title": "Define the default value of an variable that contains an object as empty"}, {"location": "terraform/#conditionals", "text": "", "title": "Conditionals"}, {"location": "terraform/#elif", "text": "locals { test = \"${ condition ? value : (elif-condition ? elif-value : else-value)}\" }", "title": "Elif"}, {"location": "terraform/#do-a-conditional-if-a-variable-is-not-null", "text": "resource \"aws_db_instance\" \"instance\" { count = var.database == null ? 0 : 1 ...", "title": "Do a conditional if a variable is not null"}, {"location": "terraform/#debugging", "text": "You can set the TF_LOG environmental variable to one of the log levels TRACE , DEBUG , INFO , WARN or ERROR to change the verbosity of the logs. To remove the debug traces run unset TF_LOG .", "title": "Debugging"}, {"location": "terraform/#references", "text": "Docs Modules registry terraform-aws-modules AWS providers AWS examples GCloud examples Good and bad sides of terraform Awesome Terraform", "title": "References"}, {"location": "time_management/", "text": "Time management is the process of planning and exercising conscious control of time spent on specific activities, especially to increase effectiveness, efficiency, and productivity. It involves a juggling act of various demands upon a person relating to work, social life, family, hobbies, personal interests, and commitments with the finiteness of time. Using time effectively gives the person \"choice\" on spending or managing activities at their own time and expediency. To be able to do time management, you first need to define how do you want to increase your effectiveness, efficiency, and productivity. For me, it means increasing the amount and quality of work per unit of time or effort. Understanding work as any task that gets me closer to a goal. It doesn't necessarily be related with professional work, it can be applied to a personal project, cleaning or hanging out with friends. The rest of the article describes the approaches I use to maximize this idea of efficiency. If you have a different understanding, goal or your brain works in a different way than mine, most of the guidelines may not apply to you, but they could spark some ideas that you can implement on your daily life. To increase the productivity we can: Reduce the time spent doing unproductive tasks . Improve the way you do the tasks . Improve how you manage your tools . Improve your state and environment to be more efficient . Reduce the time spent doing unproductive tasks \u2691 Sadly, the day has only 24 hours you can use. There's nothing to do about it, we can however reduce the amount of wasted time to make a better use of the time that we have. Minimize the context switches \u2691 Each time we switch from one task to another, the brain needs to load all the necessary information to be able to address the new task. Dumping the old task information and loading the new is both time consuming and exhausting, so do it consciously and sparingly. One way of improving this behaviour is by using the Pomodoro technique . Interruption management \u2691 We've come to accept that we need to be available 24/7 and answer immediately, that makes us slaves of the interruptions, it drives our work and personal relations. I feel that out of respect of ourselves and the other's time, we need to change that perspective. Most of the times interruptions can wait 20 or 60 minutes, and many of them can be avoided with better task and time planning. Interruptions are one of the main productivity killers. Not only they unexpectedly break your workflow, they also add undesired mental load as you are waiting for them to happen, and need to check them often. As we've seen previously, to be productive you need to be able to work on a task for 20 minutes without checking the interruption channels. Fill up your own interruption analysis report and define your workflow to manage them. Avoid lost time doing nothing \u2691 Sometimes I catch myself watching at the screen with zero mental activity and drooling. Other times I endlessly switch between browser tabs or the email client and the chat clients for no reason, it's just a reflex act. You probably have similar behaviours that lead you nowhere. Some should be an alert that you need a break (don't drool the keyboard please), but others are bad uncontrolled behaviours that could be identified and got rid of. Fix your environment \u2691 When we loose time, we don't do it consciously, that's why it's difficult for us to stay alert and actively try to change those behaviours. It's much easier to fix your environment so that the reasons that trigger the time loss don't happen at all. For example, if you keep on going back to the email client regularly even though you decided only to check it three times a day, instead of mentally punishing yourself when you check it, close the client or move it to another workspace so it's not where you're used to see it. Don't wait, switch task \u2691 Even though we want to minimize the context switches , staring at the screen for a long process to end makes no sense. If you do task management well, the context switch toll gets smaller enough that whenever you hit a block in the task you're working on, you can switch to another one. A block can be caused by a long running process or waiting for someone to do something. If you find concentrating difficult, don't do this, it's a hard skill to master. When a block comes, I first try to switch back to processes that I was already working on. Try to have as less processes as possible, less than three if possible. If there is only one active process, look at the task plan for the next step that could be done in parallel. As both processes work on the same task, they share most of the context, so the switch is cheap. If there is none, go to the day plan to start the first step of the next task in the plan. Improve the way you do the tasks \u2691 Improve how you manage your tasks to: Reduce your mental load, so you can use those resources doing productive work. Improve your efficiency. Make more realistic estimations, thus meeting the commited deadlines. Finish what you start. Know you're working towards your ultimate goals Stop feeling lost or overburdened. Make context switches cheaper. Improve how you manage your tools \u2691 Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it. Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next. Email management . Instant messages management . Meetings . Meetings \u2691 Calls, video calls, group calls or physical meetings are the best communication channel to transmit non trivial short messages. Even if they are the most efficient, they will break your working workflow, as you'll need to prepare yourself to know what to say and how, go to the meeting location, and then process all the information gathered. That's why if not used wisely, it can be a sink of productivity. Try to minimize and group the meetings, thus having less interruptions. Maximize the continuous uninterrupted time, so schedule them at the start or end of the morning or afternoon. Once you agreed to attend, make each of them count. Define an agenda and a time limit per section. That'll keep the conversation on track, and will give enough information to the attendees to decide if they need to be there. Likewise, whenever you're invited to a meeting, value if you need to go. If you don't, politely decline the offer. Sometimes assigning someone the role to conduct the meeting, or taking turns to talk can help. There are more informal meetings where you don't need all these constrains and formality. For example in a coffee break. You know that they are going to be unproductive but that's ok too. Master your tools and apply them where you think they are needed. Improve your state \u2691 To be able to work efficiently, manage your tasks and change your habits you need to have the appropriate state of mind. This last factor is often overlooked, but one of the most important. To be efficient you need to take care of yourself. Analyze how are you to detect what physical or mental attributes aren't at the optimum level and act accordingly by fixing them and adjusting your plans. This will be difficult to most of us, as we are disconnected from our bodies, and don't know how to study ourselves. If it's your case, you could start by meditating or to quantifying yourself. Some of the vectors you can work on to improve your state are: Sleep better . Work out. Hang out. Isolate your personal life from your work life. Procrastinate mindfully. Don't be a slave of the interruptions . Improve your working environment . Prevent illnesses through hygiene and exercise .", "title": "Time management"}, {"location": "time_management/#reduce-the-time-spent-doing-unproductive-tasks", "text": "Sadly, the day has only 24 hours you can use. There's nothing to do about it, we can however reduce the amount of wasted time to make a better use of the time that we have.", "title": "Reduce the time spent doing unproductive tasks"}, {"location": "time_management/#minimize-the-context-switches", "text": "Each time we switch from one task to another, the brain needs to load all the necessary information to be able to address the new task. Dumping the old task information and loading the new is both time consuming and exhausting, so do it consciously and sparingly. One way of improving this behaviour is by using the Pomodoro technique .", "title": "Minimize the context switches"}, {"location": "time_management/#interruption-management", "text": "We've come to accept that we need to be available 24/7 and answer immediately, that makes us slaves of the interruptions, it drives our work and personal relations. I feel that out of respect of ourselves and the other's time, we need to change that perspective. Most of the times interruptions can wait 20 or 60 minutes, and many of them can be avoided with better task and time planning. Interruptions are one of the main productivity killers. Not only they unexpectedly break your workflow, they also add undesired mental load as you are waiting for them to happen, and need to check them often. As we've seen previously, to be productive you need to be able to work on a task for 20 minutes without checking the interruption channels. Fill up your own interruption analysis report and define your workflow to manage them.", "title": "Interruption management"}, {"location": "time_management/#avoid-lost-time-doing-nothing", "text": "Sometimes I catch myself watching at the screen with zero mental activity and drooling. Other times I endlessly switch between browser tabs or the email client and the chat clients for no reason, it's just a reflex act. You probably have similar behaviours that lead you nowhere. Some should be an alert that you need a break (don't drool the keyboard please), but others are bad uncontrolled behaviours that could be identified and got rid of.", "title": "Avoid lost time doing nothing"}, {"location": "time_management/#fix-your-environment", "text": "When we loose time, we don't do it consciously, that's why it's difficult for us to stay alert and actively try to change those behaviours. It's much easier to fix your environment so that the reasons that trigger the time loss don't happen at all. For example, if you keep on going back to the email client regularly even though you decided only to check it three times a day, instead of mentally punishing yourself when you check it, close the client or move it to another workspace so it's not where you're used to see it.", "title": "Fix your environment"}, {"location": "time_management/#dont-wait-switch-task", "text": "Even though we want to minimize the context switches , staring at the screen for a long process to end makes no sense. If you do task management well, the context switch toll gets smaller enough that whenever you hit a block in the task you're working on, you can switch to another one. A block can be caused by a long running process or waiting for someone to do something. If you find concentrating difficult, don't do this, it's a hard skill to master. When a block comes, I first try to switch back to processes that I was already working on. Try to have as less processes as possible, less than three if possible. If there is only one active process, look at the task plan for the next step that could be done in parallel. As both processes work on the same task, they share most of the context, so the switch is cheap. If there is none, go to the day plan to start the first step of the next task in the plan.", "title": "Don't wait, switch task"}, {"location": "time_management/#improve-the-way-you-do-the-tasks", "text": "Improve how you manage your tasks to: Reduce your mental load, so you can use those resources doing productive work. Improve your efficiency. Make more realistic estimations, thus meeting the commited deadlines. Finish what you start. Know you're working towards your ultimate goals Stop feeling lost or overburdened. Make context switches cheaper.", "title": "Improve the way you do the tasks"}, {"location": "time_management/#improve-how-you-manage-your-tools", "text": "Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it. Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next. Email management . Instant messages management . Meetings .", "title": "Improve how you manage your tools"}, {"location": "time_management/#meetings", "text": "Calls, video calls, group calls or physical meetings are the best communication channel to transmit non trivial short messages. Even if they are the most efficient, they will break your working workflow, as you'll need to prepare yourself to know what to say and how, go to the meeting location, and then process all the information gathered. That's why if not used wisely, it can be a sink of productivity. Try to minimize and group the meetings, thus having less interruptions. Maximize the continuous uninterrupted time, so schedule them at the start or end of the morning or afternoon. Once you agreed to attend, make each of them count. Define an agenda and a time limit per section. That'll keep the conversation on track, and will give enough information to the attendees to decide if they need to be there. Likewise, whenever you're invited to a meeting, value if you need to go. If you don't, politely decline the offer. Sometimes assigning someone the role to conduct the meeting, or taking turns to talk can help. There are more informal meetings where you don't need all these constrains and formality. For example in a coffee break. You know that they are going to be unproductive but that's ok too. Master your tools and apply them where you think they are needed.", "title": "Meetings"}, {"location": "time_management/#improve-your-state", "text": "To be able to work efficiently, manage your tasks and change your habits you need to have the appropriate state of mind. This last factor is often overlooked, but one of the most important. To be efficient you need to take care of yourself. Analyze how are you to detect what physical or mental attributes aren't at the optimum level and act accordingly by fixing them and adjusting your plans. This will be difficult to most of us, as we are disconnected from our bodies, and don't know how to study ourselves. If it's your case, you could start by meditating or to quantifying yourself. Some of the vectors you can work on to improve your state are: Sleep better . Work out. Hang out. Isolate your personal life from your work life. Procrastinate mindfully. Don't be a slave of the interruptions . Improve your working environment . Prevent illnesses through hygiene and exercise .", "title": "Improve your state"}, {"location": "tool_management/", "text": "Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it. Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next.", "title": "Tool management"}, {"location": "typer/", "text": "Typer is a library for building CLI applications that users will love using and developers will love creating. Based on Python 3.6+ type hints. The key features are: Intuitive to write : Great editor support. Completion everywhere. Less time debugging. Designed to be easy to use and learn. Less time reading docs. Easy to use : It's easy to use for the final users. Automatic help, and automatic completion for all shells. Short : Minimize code duplication. Multiple features from each parameter declaration. Fewer bugs. Start simple : The simplest example adds only 2 lines of code to your app: 1 import, 1 function call. Grow large : Grow in complexity as much as you want, create arbitrarily complex trees of commands and groups of subcommands, with options and arguments. Installation \u2691 pip install 'typer[all]' Minimal usage \u2691 import typer def main ( name : str ): print ( f \"Hello { name } \" ) if __name__ == \"__main__\" : typer . run ( main ) Usage \u2691 Create a typer.Typer() app, and create two subcommands with their parameters. import typer app = typer . Typer () @app . command () def hello ( name : str ): print ( f \"Hello { name } \" ) @app . command () def goodbye ( name : str , formal : bool = False ): if formal : print ( f \"Goodbye Ms. { name } . Have a good day.\" ) else : print ( f \"Bye { name } !\" ) if __name__ == \"__main__\" : app () Using subcommands \u2691 In some cases, it's possible that your application code needs to live on a single file. import typer app = typer . Typer () items_app = typer . Typer () app . add_typer ( items_app , name = \"items\" ) users_app = typer . Typer () app . add_typer ( users_app , name = \"users\" ) @items_app . command ( \"create\" ) def items_create ( item : str ): print ( f \"Creating item: { item } \" ) @items_app . command ( \"delete\" ) def items_delete ( item : str ): print ( f \"Deleting item: { item } \" ) @items_app . command ( \"sell\" ) def items_sell ( item : str ): print ( f \"Selling item: { item } \" ) @users_app . command ( \"create\" ) def users_create ( user_name : str ): print ( f \"Creating user: { user_name } \" ) @users_app . command ( \"delete\" ) def users_delete ( user_name : str ): print ( f \"Deleting user: { user_name } \" ) if __name__ == \"__main__\" : app () Then you'll be able to call each subcommand with: python main.py items create For more complex code use nested subcommands Nested Subcommands \u2691 You can split the commands in different files for clarity once the code starts to grow: File: reigns.py : import typer app = typer . Typer () @app . command () def conquer ( name : str ): print ( f \"Conquering reign: { name } \" ) @app . command () def destroy ( name : str ): print ( f \"Destroying reign: { name } \" ) if __name__ == \"__main__\" : app () File: towns.py : import typer app = typer . Typer () @app . command () def found ( name : str ): print ( f \"Founding town: { name } \" ) @app . command () def burn ( name : str ): print ( f \"Burning town: { name } \" ) if __name__ == \"__main__\" : app () File: lands.py : import typer import reigns import towns app = typer . Typer () app . add_typer ( reigns . app , name = \"reigns\" ) app . add_typer ( towns . app , name = \"towns\" ) if __name__ == \"__main__\" : app () File: users.py : import typer app = typer . Typer () @app . command () def create ( user_name : str ): print ( f \"Creating user: { user_name } \" ) @app . command () def delete ( user_name : str ): print ( f \"Deleting user: { user_name } \" ) if __name__ == \"__main__\" : app () File: items.py : import typer app = typer . Typer () @app . command () def create ( item : str ): print ( f \"Creating item: { item } \" ) @app . command () def delete ( item : str ): print ( f \"Deleting item: { item } \" ) @app . command () def sell ( item : str ): print ( f \"Selling item: { item } \" ) if __name__ == \"__main__\" : app () File: main.py : import typer import items import lands import users app = typer . Typer () app . add_typer ( users . app , name = \"users\" ) app . add_typer ( items . app , name = \"items\" ) app . add_typer ( lands . app , name = \"lands\" ) if __name__ == \"__main__\" : app () Using the context \u2691 When you create a Typer application it uses Click underneath. And every Click application has a special object called a \"Context\" that is normally hidden. But you can access the context by declaring a function parameter of type typer.Context . The context is also used to store objects that you may need for all the commands, for example a repository . Tiangolo ( typer s main developer) suggests to use global variables or a function with lru_cache . Using short option names \u2691 import typer def main ( user_name : str = typer . Option ( ... , \"--name\" , \"-n\" )): print ( f \"Hello { user_name } \" ) if __name__ == \"__main__\" : typer . run ( main ) The ... as the first argument is to make the option required Create -vvv \u2691 You can make a CLI option work as a counter with the counter parameter: import typer def main ( verbose : int = typer . Option ( 0 , \"--verbose\" , \"-v\" , count = True )): print ( f \"Verbose level is { verbose } \" ) if __name__ == \"__main__\" : typer . run ( main ) Get the command line application directory \u2691 You can get the application directory where you can, for example, save configuration files with typer.get_app_dir() : from pathlib import Path import typer APP_NAME = \"my-super-cli-app\" def main () -> None : \"\"\"Define the main command line interface.\"\"\" app_dir = typer . get_app_dir ( APP_NAME ) config_path : Path = Path ( app_dir ) / \"config.json\" if not config_path . is_file (): print ( \"Config file doesn't exist yet\" ) if __name__ == \"__main__\" : typer . run ( main ) It will give you a directory for storing configurations appropriate for your CLI program for the current user in each operating system. Exiting with an error code \u2691 typer.Exit() takes an optional code parameter. By default, code is 0 , meaning there was no error. You can pass a code with a number other than 0 to tell the terminal that there was an error in the execution of the program: import typer def main ( username : str ): if username == \"root\" : print ( \"The root user is reserved\" ) raise typer . Exit ( code = 1 ) print ( f \"New user created: { username } \" ) if __name__ == \"__main__\" : typer . run ( main ) Create a --version command \u2691 You could use a callback to implement a --version CLI option. It would show the version of your CLI program and then it would terminate it. Even before any other CLI parameter is processed. from typing import Optional import typer __version__ = \"0.1.0\" def version_callback ( value : bool ) -> None : \"\"\"Print the version of the program.\"\"\" if value : print ( f \"Awesome CLI Version: { __version__ } \" ) raise typer . Exit () def main ( version : Optional [ bool ] = typer . Option ( None , \"--version\" , callback = version_callback , is_eager = True ), ) -> None : ... if __name__ == \"__main__\" : typer . run ( main ) Testing \u2691 Testing is similar to click testing , but you import the CliRunner directly from typer : from typer.testing import CliRunner References \u2691 Docs Source Issues", "title": "Typer"}, {"location": "typer/#installation", "text": "pip install 'typer[all]'", "title": "Installation"}, {"location": "typer/#minimal-usage", "text": "import typer def main ( name : str ): print ( f \"Hello { name } \" ) if __name__ == \"__main__\" : typer . run ( main )", "title": "Minimal usage"}, {"location": "typer/#usage", "text": "Create a typer.Typer() app, and create two subcommands with their parameters. import typer app = typer . Typer () @app . command () def hello ( name : str ): print ( f \"Hello { name } \" ) @app . command () def goodbye ( name : str , formal : bool = False ): if formal : print ( f \"Goodbye Ms. { name } . Have a good day.\" ) else : print ( f \"Bye { name } !\" ) if __name__ == \"__main__\" : app ()", "title": "Usage"}, {"location": "typer/#using-subcommands", "text": "In some cases, it's possible that your application code needs to live on a single file. import typer app = typer . Typer () items_app = typer . Typer () app . add_typer ( items_app , name = \"items\" ) users_app = typer . Typer () app . add_typer ( users_app , name = \"users\" ) @items_app . command ( \"create\" ) def items_create ( item : str ): print ( f \"Creating item: { item } \" ) @items_app . command ( \"delete\" ) def items_delete ( item : str ): print ( f \"Deleting item: { item } \" ) @items_app . command ( \"sell\" ) def items_sell ( item : str ): print ( f \"Selling item: { item } \" ) @users_app . command ( \"create\" ) def users_create ( user_name : str ): print ( f \"Creating user: { user_name } \" ) @users_app . command ( \"delete\" ) def users_delete ( user_name : str ): print ( f \"Deleting user: { user_name } \" ) if __name__ == \"__main__\" : app () Then you'll be able to call each subcommand with: python main.py items create For more complex code use nested subcommands", "title": "Using subcommands"}, {"location": "typer/#nested-subcommands", "text": "You can split the commands in different files for clarity once the code starts to grow: File: reigns.py : import typer app = typer . Typer () @app . command () def conquer ( name : str ): print ( f \"Conquering reign: { name } \" ) @app . command () def destroy ( name : str ): print ( f \"Destroying reign: { name } \" ) if __name__ == \"__main__\" : app () File: towns.py : import typer app = typer . Typer () @app . command () def found ( name : str ): print ( f \"Founding town: { name } \" ) @app . command () def burn ( name : str ): print ( f \"Burning town: { name } \" ) if __name__ == \"__main__\" : app () File: lands.py : import typer import reigns import towns app = typer . Typer () app . add_typer ( reigns . app , name = \"reigns\" ) app . add_typer ( towns . app , name = \"towns\" ) if __name__ == \"__main__\" : app () File: users.py : import typer app = typer . Typer () @app . command () def create ( user_name : str ): print ( f \"Creating user: { user_name } \" ) @app . command () def delete ( user_name : str ): print ( f \"Deleting user: { user_name } \" ) if __name__ == \"__main__\" : app () File: items.py : import typer app = typer . Typer () @app . command () def create ( item : str ): print ( f \"Creating item: { item } \" ) @app . command () def delete ( item : str ): print ( f \"Deleting item: { item } \" ) @app . command () def sell ( item : str ): print ( f \"Selling item: { item } \" ) if __name__ == \"__main__\" : app () File: main.py : import typer import items import lands import users app = typer . Typer () app . add_typer ( users . app , name = \"users\" ) app . add_typer ( items . app , name = \"items\" ) app . add_typer ( lands . app , name = \"lands\" ) if __name__ == \"__main__\" : app ()", "title": "Nested Subcommands"}, {"location": "typer/#using-the-context", "text": "When you create a Typer application it uses Click underneath. And every Click application has a special object called a \"Context\" that is normally hidden. But you can access the context by declaring a function parameter of type typer.Context . The context is also used to store objects that you may need for all the commands, for example a repository . Tiangolo ( typer s main developer) suggests to use global variables or a function with lru_cache .", "title": "Using the context"}, {"location": "typer/#using-short-option-names", "text": "import typer def main ( user_name : str = typer . Option ( ... , \"--name\" , \"-n\" )): print ( f \"Hello { user_name } \" ) if __name__ == \"__main__\" : typer . run ( main ) The ... as the first argument is to make the option required", "title": "Using short option names"}, {"location": "typer/#create-vvv", "text": "You can make a CLI option work as a counter with the counter parameter: import typer def main ( verbose : int = typer . Option ( 0 , \"--verbose\" , \"-v\" , count = True )): print ( f \"Verbose level is { verbose } \" ) if __name__ == \"__main__\" : typer . run ( main )", "title": "Create -vvv"}, {"location": "typer/#get-the-command-line-application-directory", "text": "You can get the application directory where you can, for example, save configuration files with typer.get_app_dir() : from pathlib import Path import typer APP_NAME = \"my-super-cli-app\" def main () -> None : \"\"\"Define the main command line interface.\"\"\" app_dir = typer . get_app_dir ( APP_NAME ) config_path : Path = Path ( app_dir ) / \"config.json\" if not config_path . is_file (): print ( \"Config file doesn't exist yet\" ) if __name__ == \"__main__\" : typer . run ( main ) It will give you a directory for storing configurations appropriate for your CLI program for the current user in each operating system.", "title": "Get the command line application directory"}, {"location": "typer/#exiting-with-an-error-code", "text": "typer.Exit() takes an optional code parameter. By default, code is 0 , meaning there was no error. You can pass a code with a number other than 0 to tell the terminal that there was an error in the execution of the program: import typer def main ( username : str ): if username == \"root\" : print ( \"The root user is reserved\" ) raise typer . Exit ( code = 1 ) print ( f \"New user created: { username } \" ) if __name__ == \"__main__\" : typer . run ( main )", "title": "Exiting with an error code"}, {"location": "typer/#create-a-version-command", "text": "You could use a callback to implement a --version CLI option. It would show the version of your CLI program and then it would terminate it. Even before any other CLI parameter is processed. from typing import Optional import typer __version__ = \"0.1.0\" def version_callback ( value : bool ) -> None : \"\"\"Print the version of the program.\"\"\" if value : print ( f \"Awesome CLI Version: { __version__ } \" ) raise typer . Exit () def main ( version : Optional [ bool ] = typer . Option ( None , \"--version\" , callback = version_callback , is_eager = True ), ) -> None : ... if __name__ == \"__main__\" : typer . run ( main )", "title": "Create a --version command"}, {"location": "typer/#testing", "text": "Testing is similar to click testing , but you import the CliRunner directly from typer : from typer.testing import CliRunner", "title": "Testing"}, {"location": "typer/#references", "text": "Docs Source Issues", "title": "References"}, {"location": "use_warnings/", "text": "Regardless of the versioning system you're using, once you reach your first stable version, the commitment to your end users must be that you give them time to adapt to the changes in your program. So whenever you want to introduce a breaking change release it under a new interface, and in parallel, start emitting DeprecationWarning or UserWarning messages whenever someone invokes the old one. Maintain this state for a defined period (for example six months), and communicate explicitly in the warning message the timeline for when users have to migrate. This gives everyone time to move to the new interface without breaking their system, and then the library may remove the change and get rid of the old design chains forever. As an added benefit, only people using the old interface will ever see the warning, as opposed to affecting everyone (as seen with the semantic versioning major version bump). If you're following semantic versioning you'd do this change in a minor release, and you'll finally remove the functionality in another minor release. As you've given your users enough time to adequate to the new version of the code, it's not understood as a breaking change. This allows too for your users to be less afraid and stop upper-pinning you in their dependencies . Another benefit of using warnings is that if you configure your test runner to capture the warnings (which you should!) you can use your test suite to see the real impact of the deprecation, you may even realize why was that feature there and that you can't deprecate it at all. Using warnings \u2691 Even though there are many warnings, I usually use UserWarning or DeprecationWarning . The full list is : Class Description Warning This is the base class of all warning category classes. UserWarning The default category for warn(). DeprecationWarning Warn other developers about deprecated features. FutureWarning Warn other end users of applications about deprecated features. SyntaxWarning Warn about dubious syntactic features. RuntimeWarning Warn about dubious runtime features. PendingDeprecationWarning Warn about features that will be deprecated in the future (ignored by default). ImportWarning Warn triggered during the process of importing a module (ignored by default). UnicodeWarning Warn related to Unicode. BytesWarning Warn related to bytes and bytearray. ResourceWarning Warn related to resource usage (ignored by default). How to raise a warning \u2691 Warning messages are typically issued in situations where it is useful to alert the user of some condition in a program, where that condition doesn\u2019t warrant raising an exception and terminating the program. import warnings def f (): warnings . warn ( 'Message' , DeprecationWarning ) Suppressing a warning \u2691 To disable in the whole file, add to the top: import warnings warnings . filterwarnings ( \"ignore\" , message = \"divide by zero encountered in divide\" ) If you want this to apply to only one section of code, then use the warnings context manager: import warnings with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \"divide by zero encountered in divide\" ) # .. your divide-by-zero code .. And if you want to disable it for the whole code base configure pytest accordingly . How to evolve your code \u2691 To ensure that the transition is smooth you need to tweak your code so that the user can switch a flag and make sure that their code keeps on working with the new changes. For example imagine that we have a class MyClass with a method my_method . class MyClass : def my_method ( self , argument ): # my_method code goes here You can add an argument deprecate_my_method that defaults to False , or you can take the chance to change the signature of the function, so that if the user is using the old argument, it uses the old behaviour and gets the warning, and if it's using the new argument, it uses the new. The advantage of changing the signature is that you don't need to do another deprecation for the temporal argument flag. !!! note \"Or you can use environmental variables \" class MyClass : def __init__ ( self , deprecate_my_method = False ): self . deprecate_my_method = deprecate_my_method def my_method ( self , argument ): if self . deprecate_my_method : # my_method new functionality else : warnings . warn ( \"Use my_new_method instead\" , UserWarning ) # my_method old code goes here That way when users get the new version of your code, if they are not using my_method they won't get the exception, and if they are, they can change how they initialize their classes with MyClass(deprecate_my_method=True) , run their tests tweaking their code to meet the new functionality and make sure that they are ready for the method to be deprecated. Once removed, another UserWarning will be raised to stop using deprecate_my_method as an argument to initialize the class as it is no longer needed. Until you remove the old code, you need to keep both functionalities and make sure all your test suite works with both cases. To do that, create the warning, run the tests and see what tests are raising the exception. For each of them you need to think if this test will make sense with the new code: If it doesn't, make sure that the warning is raised . If it is, make sure that the warning is raised and create another test with the deprecate_my_method enabled. Once the deprecation date arrives you'll need to search for the date in your code to see where the warning is raised and used, remove the old functionality and update the tests. If you used a temporal argument to let the users try the new behaviour, issue the warning to deprecate it. Use environmental variables \u2691 A cleaner way to handle it is with environmental variables, that way you don't need to change the signature of the function twice. I've learned this from boto where they informed their users this way: If you wish to test the new feature we have created a new environment variable BOTO_DISABLE_COMMONNAME . Setting this to true will suppress the warning and use the new functionality. If you are concerned about this change causing disruptions, you can pin your version of botocore to <1.28.0 until you are ready to migrate. If you are only concerned about silencing the warning in your logs, use warnings.filterwarnings when instantiating a new service client. import warnings warnings . filterwarnings ( 'ignore' , category = FutureWarning , module = 'botocore.client' ) Testing warnings \u2691 To test the function with pytest you can use pytest.warns : import warnings import pytest def test_warning (): with pytest . warns ( UserWarning , match = 'my warning' ): warnings . warn ( \"my warning\" , UserWarning ) For the DeprecationWarnings you can use deprecated_call : Or you can use deprecated : def test_myfunction_deprecated (): with pytest . deprecated_call (): f () @deprecated ( version = '1.2.0' , reason = \"You should use another function\" ) def some_old_function ( x , y ): return x + y But it adds a dependency to your program, although they don't have any downstream dependencies. References \u2691 Bernat post on versioning", "title": "Use warnings to evolve your code"}, {"location": "use_warnings/#using-warnings", "text": "Even though there are many warnings, I usually use UserWarning or DeprecationWarning . The full list is : Class Description Warning This is the base class of all warning category classes. UserWarning The default category for warn(). DeprecationWarning Warn other developers about deprecated features. FutureWarning Warn other end users of applications about deprecated features. SyntaxWarning Warn about dubious syntactic features. RuntimeWarning Warn about dubious runtime features. PendingDeprecationWarning Warn about features that will be deprecated in the future (ignored by default). ImportWarning Warn triggered during the process of importing a module (ignored by default). UnicodeWarning Warn related to Unicode. BytesWarning Warn related to bytes and bytearray. ResourceWarning Warn related to resource usage (ignored by default).", "title": "Using warnings"}, {"location": "use_warnings/#how-to-raise-a-warning", "text": "Warning messages are typically issued in situations where it is useful to alert the user of some condition in a program, where that condition doesn\u2019t warrant raising an exception and terminating the program. import warnings def f (): warnings . warn ( 'Message' , DeprecationWarning )", "title": "How to raise a warning"}, {"location": "use_warnings/#suppressing-a-warning", "text": "To disable in the whole file, add to the top: import warnings warnings . filterwarnings ( \"ignore\" , message = \"divide by zero encountered in divide\" ) If you want this to apply to only one section of code, then use the warnings context manager: import warnings with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \"divide by zero encountered in divide\" ) # .. your divide-by-zero code .. And if you want to disable it for the whole code base configure pytest accordingly .", "title": "Suppressing a warning"}, {"location": "use_warnings/#how-to-evolve-your-code", "text": "To ensure that the transition is smooth you need to tweak your code so that the user can switch a flag and make sure that their code keeps on working with the new changes. For example imagine that we have a class MyClass with a method my_method . class MyClass : def my_method ( self , argument ): # my_method code goes here You can add an argument deprecate_my_method that defaults to False , or you can take the chance to change the signature of the function, so that if the user is using the old argument, it uses the old behaviour and gets the warning, and if it's using the new argument, it uses the new. The advantage of changing the signature is that you don't need to do another deprecation for the temporal argument flag. !!! note \"Or you can use environmental variables \" class MyClass : def __init__ ( self , deprecate_my_method = False ): self . deprecate_my_method = deprecate_my_method def my_method ( self , argument ): if self . deprecate_my_method : # my_method new functionality else : warnings . warn ( \"Use my_new_method instead\" , UserWarning ) # my_method old code goes here That way when users get the new version of your code, if they are not using my_method they won't get the exception, and if they are, they can change how they initialize their classes with MyClass(deprecate_my_method=True) , run their tests tweaking their code to meet the new functionality and make sure that they are ready for the method to be deprecated. Once removed, another UserWarning will be raised to stop using deprecate_my_method as an argument to initialize the class as it is no longer needed. Until you remove the old code, you need to keep both functionalities and make sure all your test suite works with both cases. To do that, create the warning, run the tests and see what tests are raising the exception. For each of them you need to think if this test will make sense with the new code: If it doesn't, make sure that the warning is raised . If it is, make sure that the warning is raised and create another test with the deprecate_my_method enabled. Once the deprecation date arrives you'll need to search for the date in your code to see where the warning is raised and used, remove the old functionality and update the tests. If you used a temporal argument to let the users try the new behaviour, issue the warning to deprecate it.", "title": "How to evolve your code"}, {"location": "use_warnings/#use-environmental-variables", "text": "A cleaner way to handle it is with environmental variables, that way you don't need to change the signature of the function twice. I've learned this from boto where they informed their users this way: If you wish to test the new feature we have created a new environment variable BOTO_DISABLE_COMMONNAME . Setting this to true will suppress the warning and use the new functionality. If you are concerned about this change causing disruptions, you can pin your version of botocore to <1.28.0 until you are ready to migrate. If you are only concerned about silencing the warning in your logs, use warnings.filterwarnings when instantiating a new service client. import warnings warnings . filterwarnings ( 'ignore' , category = FutureWarning , module = 'botocore.client' )", "title": "Use environmental variables"}, {"location": "use_warnings/#testing-warnings", "text": "To test the function with pytest you can use pytest.warns : import warnings import pytest def test_warning (): with pytest . warns ( UserWarning , match = 'my warning' ): warnings . warn ( \"my warning\" , UserWarning ) For the DeprecationWarnings you can use deprecated_call : Or you can use deprecated : def test_myfunction_deprecated (): with pytest . deprecated_call (): f () @deprecated ( version = '1.2.0' , reason = \"You should use another function\" ) def some_old_function ( x , y ): return x + y But it adds a dependency to your program, although they don't have any downstream dependencies.", "title": "Testing warnings"}, {"location": "use_warnings/#references", "text": "Bernat post on versioning", "title": "References"}, {"location": "vdirsyncer/", "text": "vdirsyncer is a Python command-line tool for synchronizing calendars and addressbooks between a variety of servers and the local filesystem. The most popular usecase is to synchronize a server with a local folder and use a set of other programs such as khal to change the local events and contacts. Vdirsyncer can then synchronize those changes back to the server. However, vdirsyncer is not limited to synchronizing between clients and servers. It can also be used to synchronize calendars and/or addressbooks between two servers directly. It aims to be for calendars and contacts what OfflineIMAP is for emails. Installation \u2691 Although it's available in the major package managers, you can get a more bleeding edge version with pip . pipx install vdirsyncer If you don't have pipx you can use pip . You also need to install some dependencies for it to work: sudo apt-get install libxml2 libxslt1.1 zlib1g Configuration \u2691 In this example we set up contacts synchronization, but calendar sync works almost the same. Just swap type = \"carddav\" for type = \"caldav\" and fileext = \".vcf\" for fileext = \".ics\" . By default, vdirsyncer looks for its configuration file in the following locations: The file pointed to by the VDIRSYNCER_CONFIG environment variable. ~/.vdirsyncer/config . $XDG_CONFIG_HOME/vdirsyncer/config , which is normally ~/.config/vdirsyncer/config . You need to create the directory as it's not created by default and the base config file . The config file should start with a general section, where the only required parameter is status_path. The following is a minimal example: [general] status_path = \"~/.vdirsyncer/status/\" After the general section, an arbitrary amount of pair and storage sections might come. In vdirsyncer, synchronization is always done between two storages. Such storages are defined in storage sections, and which pairs of storages should actually be synchronized is defined in pair section. This format is copied from OfflineIMAP, where storages are called repositories and pairs are called accounts. Syncing a calendar \u2691 To sync to a nextcloud calendar: [pair my_calendars] a = \"my_calendars_local\" b = \"my_calendars_remote\" collections = [\"from a\", \"from b\"] metadata = [\"color\"] [storage my_calendars_local] type = \"filesystem\" path = \"~/.calendars/\" fileext = \".ics\" [storage my_calendars_remote] type = \"caldav\" #Can be obtained from nextcloud url = \"https://yournextcloud.example.lcl/remote.php/dav/calendars/USERNAME/personal/\" username = \"<USERNAME>\" #Instead of inserting my plaintext password I fetch it using pass password.fetch = [\"command\", \"pass\", \"nextcloud\"] #SSL certificate fingerprint verify_fingerprint = \"FINGERPRINT\" #Verify ssl certificate. Set to false if it is self signed and not installed on local machine verify = true Read the SSl and certificate validation section to see how to create the verify_fingerprint . Syncing an address book \u2691 The following example synchronizes ownCloud\u2019s addressbooks to ~/.contacts/ : [pair my_contacts] a = \"my_contacts_local\" b = \"my_contacts_remote\" collections = [\"from a\", \"from b\"] [storage my_contacts_local] type = \"filesystem\" path = \"~/.contacts/\" fileext = \".vcf\" [storage my_contacts_remote] type = \"carddav\" # We can simplify this URL here as well. In theory it shouldn't matter. url = \"https://owncloud.example.com/remote.php/carddav/\" username = \"bob\" password = \"asdf\" Note Configuration for other servers can be found at Servers . After running vdirsyncer discover and vdirsyncer sync , ~/.contacts/ will contain subdirectories for each addressbook, which in turn will contain a bunch of .vcf files which all contain a contact in VCARD format each. You can modify their contents, add new ones and delete some, and your changes will be synchronized to the CalDAV server after you run vdirsyncer sync again. Conflict resolution \u2691 If the same item is changed on both sides vdirsyncer can manage the conflict in three ways: Displaying an error message (the default). Choosing one alternative version over the other. Starts a command of your choice that is supposed to merge the two alternative versions. Options 2 and 3 require adding a conflict_resolution parameter to the pair section. Option 2 requires giving either a wins or b wins as value to the parameter: [pair my_contacts] ... conflict_resolution = \"b wins\" Earlier we wrote that b = \"my_contacts_remote\" , so when vdirsyncer encounters the situation where an item changed on both sides, it will simply overwrite the local item with the one from the server. Option 3 requires specifying as value of conflict_resolution an array starting with command and containing paths and arguments to a command. For example: [pair my_contacts] ... conflict_resolution = [\"command\", \"vimdiff\"] In this example, vimdiff <a> <b> will be called with <a> and <b> being two temporary files containing the conflicting files. The files need to be exactly the same when the command returns. More arguments can be passed to the command by adding more elements to the array. SSL and certificate validation \u2691 To pin the certificate by fingerprint: [storage foo] type = \"caldav\" ... verify_fingerprint = \"94:FD:7A:CB:50:75:A4:69:82:0A:F8:23:DF:07:FC:69:3E:CD:90:CA\" #verify = false # Optional: Disable CA validation, useful for self-signed certs SHA1-, SHA256- or MD5-Fingerprints can be used. You can use the following command for obtaining a SHA-1 fingerprint: echo -n | openssl s_client -connect unterwaditzer.net:443 | openssl x509 -noout -fingerprint Note that verify_fingerprint doesn't suffice for vdirsyncer to work with self-signed certificates (or certificates that are not in your trust store). You most likely need to set verify = false as well. This disables verification of the SSL certificate\u2019s expiration time and the existence of it in your trust store, all that\u2019s verified now is the fingerprint. However, please consider using Let\u2019s Encrypt such that you can forget about all of that. It is easier to deploy a free certificate from them than configuring all of your clients to accept the self-signed certificate. Storing passwords \u2691 vdirsyncer can fetch passwords from several sources other than the config file. Say you have the following configuration: [storage foo] type = \"caldav\" url = ... username = \"foo\" password = \"bar\" But it bugs you that the password is stored in cleartext in the config file. You can do this: [storage foo] type = \"caldav\" url = ... username = \"foo\" password.fetch = [\"command\", \"~/get-password.sh\", \"more\", \"args\"] You can fetch the username as well: [storage foo] type = \"caldav\" url = ... username.fetch = [\"command\", \"~/get-username.sh\"] password.fetch = [\"command\", \"~/get-password.sh\"] Or really any kind of parameter in a storage section. With pass for example, you might find yourself writing something like this in your configuration file: password.fetch = [\"command\", \"pass\", \"caldav\"] Google \u2691 vdirsyncer supports synchronization with Google calendars with the restriction that VTODO files are rejected by the server. Synchronization with Google contacts is less reliable due to negligence of Google\u2019s CardDAV API. Google\u2019s CardDAV implementation is allegedly a disaster in terms of data safety. Always back up your data. At first run you will be asked to authorize application for Google account access. To use this storage type, you need to install some additional dependencies: pip install vdirsyncer [ google ] Official steps \u2691 As of 2022-10-13 these didn't work for me, see the next section Furthermore you need to register vdirsyncer as an application yourself to obtain client_id and client_secret , as it is against Google\u2019s Terms of Service to hardcode those into open source software: Go to the Google API Manager and create a new project under any name. Within that project, enable the CalDAV and CardDAV APIs (not the Calendar and Contacts APIs, those are different and won\u2019t work). There should be a searchbox where you can just enter those terms. In the sidebar, select Credentials and create a new OAuth Client ID . The application type is Other . You\u2019ll be prompted to create a OAuth consent screen first. Fill out that form however you like. Finally you should have a Client ID and a Client secret. Provide these in your storage config. The token_file parameter should be a filepath where vdirsyncer can later store authentication-related data. You do not need to create the file itself or write anything to it. [storage example_for_google_calendar] type = \"google_calendar\" token_file = \"...\" client_id = \"...\" client_secret = \"...\" #start_date = null #end_date = null #item_types = [] Use Nekr0z patch solution \u2691 look the previous section if you have doubts on any of the steps If the official steps failed for you, try these ones: Go to the Google API Manager and create a new project under any name. Selected the vdirsyncer project Went to Credentials -> Create Credentials -> OAuth Client ID Select \"Web Application\" Under \"Authorised redirect URIs\" added http://127.0.0.1:8088 pressed \"Create\". Edit your vdirsyncer config [storage google] section to have the new client_id and client_secret (). Find the location of the vdirsyncer/storage/google.py in your environment (mine was in ~/.local/pipx/venvs/vdirsyncer/lib/python3.10/site-packages/vdirsyncer/storage ) and changed line 65 from redirect_uri = \"urn:ietf:wg:oauth:2.0:oob\" , to redirect_uri = \"http://127.0.0.1:8088\" , Run vdirsyncer discover my_calendar . Opened the link in my browser (on my desktop machine). Proceeded with Google authentication until \"Firefox can not connect to 127.0.0.1:8088.\" was displayed. from the browser's address bar that looked like: http://127.0.0.1:8088/?state=SOMETHING&code=HERECOMESTHECODE&scope=https://www.googleapis.com/auth/calendar * Copy the HERECOMESTHECODE part. * Paste the code into the session where vdirsyncer was running See differences between syncs \u2691 If you create a git repository where you have your calendars you can do a git diff and see the files that have changed. If you do a commit after each sync you can have all the history. References \u2691 Docs Git", "title": "vdirsyncer"}, {"location": "vdirsyncer/#installation", "text": "Although it's available in the major package managers, you can get a more bleeding edge version with pip . pipx install vdirsyncer If you don't have pipx you can use pip . You also need to install some dependencies for it to work: sudo apt-get install libxml2 libxslt1.1 zlib1g", "title": "Installation"}, {"location": "vdirsyncer/#configuration", "text": "In this example we set up contacts synchronization, but calendar sync works almost the same. Just swap type = \"carddav\" for type = \"caldav\" and fileext = \".vcf\" for fileext = \".ics\" . By default, vdirsyncer looks for its configuration file in the following locations: The file pointed to by the VDIRSYNCER_CONFIG environment variable. ~/.vdirsyncer/config . $XDG_CONFIG_HOME/vdirsyncer/config , which is normally ~/.config/vdirsyncer/config . You need to create the directory as it's not created by default and the base config file . The config file should start with a general section, where the only required parameter is status_path. The following is a minimal example: [general] status_path = \"~/.vdirsyncer/status/\" After the general section, an arbitrary amount of pair and storage sections might come. In vdirsyncer, synchronization is always done between two storages. Such storages are defined in storage sections, and which pairs of storages should actually be synchronized is defined in pair section. This format is copied from OfflineIMAP, where storages are called repositories and pairs are called accounts.", "title": "Configuration"}, {"location": "vdirsyncer/#syncing-a-calendar", "text": "To sync to a nextcloud calendar: [pair my_calendars] a = \"my_calendars_local\" b = \"my_calendars_remote\" collections = [\"from a\", \"from b\"] metadata = [\"color\"] [storage my_calendars_local] type = \"filesystem\" path = \"~/.calendars/\" fileext = \".ics\" [storage my_calendars_remote] type = \"caldav\" #Can be obtained from nextcloud url = \"https://yournextcloud.example.lcl/remote.php/dav/calendars/USERNAME/personal/\" username = \"<USERNAME>\" #Instead of inserting my plaintext password I fetch it using pass password.fetch = [\"command\", \"pass\", \"nextcloud\"] #SSL certificate fingerprint verify_fingerprint = \"FINGERPRINT\" #Verify ssl certificate. Set to false if it is self signed and not installed on local machine verify = true Read the SSl and certificate validation section to see how to create the verify_fingerprint .", "title": "Syncing a calendar"}, {"location": "vdirsyncer/#syncing-an-address-book", "text": "The following example synchronizes ownCloud\u2019s addressbooks to ~/.contacts/ : [pair my_contacts] a = \"my_contacts_local\" b = \"my_contacts_remote\" collections = [\"from a\", \"from b\"] [storage my_contacts_local] type = \"filesystem\" path = \"~/.contacts/\" fileext = \".vcf\" [storage my_contacts_remote] type = \"carddav\" # We can simplify this URL here as well. In theory it shouldn't matter. url = \"https://owncloud.example.com/remote.php/carddav/\" username = \"bob\" password = \"asdf\" Note Configuration for other servers can be found at Servers . After running vdirsyncer discover and vdirsyncer sync , ~/.contacts/ will contain subdirectories for each addressbook, which in turn will contain a bunch of .vcf files which all contain a contact in VCARD format each. You can modify their contents, add new ones and delete some, and your changes will be synchronized to the CalDAV server after you run vdirsyncer sync again.", "title": "Syncing an address book"}, {"location": "vdirsyncer/#conflict-resolution", "text": "If the same item is changed on both sides vdirsyncer can manage the conflict in three ways: Displaying an error message (the default). Choosing one alternative version over the other. Starts a command of your choice that is supposed to merge the two alternative versions. Options 2 and 3 require adding a conflict_resolution parameter to the pair section. Option 2 requires giving either a wins or b wins as value to the parameter: [pair my_contacts] ... conflict_resolution = \"b wins\" Earlier we wrote that b = \"my_contacts_remote\" , so when vdirsyncer encounters the situation where an item changed on both sides, it will simply overwrite the local item with the one from the server. Option 3 requires specifying as value of conflict_resolution an array starting with command and containing paths and arguments to a command. For example: [pair my_contacts] ... conflict_resolution = [\"command\", \"vimdiff\"] In this example, vimdiff <a> <b> will be called with <a> and <b> being two temporary files containing the conflicting files. The files need to be exactly the same when the command returns. More arguments can be passed to the command by adding more elements to the array.", "title": "Conflict resolution"}, {"location": "vdirsyncer/#ssl-and-certificate-validation", "text": "To pin the certificate by fingerprint: [storage foo] type = \"caldav\" ... verify_fingerprint = \"94:FD:7A:CB:50:75:A4:69:82:0A:F8:23:DF:07:FC:69:3E:CD:90:CA\" #verify = false # Optional: Disable CA validation, useful for self-signed certs SHA1-, SHA256- or MD5-Fingerprints can be used. You can use the following command for obtaining a SHA-1 fingerprint: echo -n | openssl s_client -connect unterwaditzer.net:443 | openssl x509 -noout -fingerprint Note that verify_fingerprint doesn't suffice for vdirsyncer to work with self-signed certificates (or certificates that are not in your trust store). You most likely need to set verify = false as well. This disables verification of the SSL certificate\u2019s expiration time and the existence of it in your trust store, all that\u2019s verified now is the fingerprint. However, please consider using Let\u2019s Encrypt such that you can forget about all of that. It is easier to deploy a free certificate from them than configuring all of your clients to accept the self-signed certificate.", "title": "SSL and certificate validation"}, {"location": "vdirsyncer/#storing-passwords", "text": "vdirsyncer can fetch passwords from several sources other than the config file. Say you have the following configuration: [storage foo] type = \"caldav\" url = ... username = \"foo\" password = \"bar\" But it bugs you that the password is stored in cleartext in the config file. You can do this: [storage foo] type = \"caldav\" url = ... username = \"foo\" password.fetch = [\"command\", \"~/get-password.sh\", \"more\", \"args\"] You can fetch the username as well: [storage foo] type = \"caldav\" url = ... username.fetch = [\"command\", \"~/get-username.sh\"] password.fetch = [\"command\", \"~/get-password.sh\"] Or really any kind of parameter in a storage section. With pass for example, you might find yourself writing something like this in your configuration file: password.fetch = [\"command\", \"pass\", \"caldav\"]", "title": "Storing passwords"}, {"location": "vdirsyncer/#google", "text": "vdirsyncer supports synchronization with Google calendars with the restriction that VTODO files are rejected by the server. Synchronization with Google contacts is less reliable due to negligence of Google\u2019s CardDAV API. Google\u2019s CardDAV implementation is allegedly a disaster in terms of data safety. Always back up your data. At first run you will be asked to authorize application for Google account access. To use this storage type, you need to install some additional dependencies: pip install vdirsyncer [ google ]", "title": "Google"}, {"location": "vdirsyncer/#official-steps", "text": "As of 2022-10-13 these didn't work for me, see the next section Furthermore you need to register vdirsyncer as an application yourself to obtain client_id and client_secret , as it is against Google\u2019s Terms of Service to hardcode those into open source software: Go to the Google API Manager and create a new project under any name. Within that project, enable the CalDAV and CardDAV APIs (not the Calendar and Contacts APIs, those are different and won\u2019t work). There should be a searchbox where you can just enter those terms. In the sidebar, select Credentials and create a new OAuth Client ID . The application type is Other . You\u2019ll be prompted to create a OAuth consent screen first. Fill out that form however you like. Finally you should have a Client ID and a Client secret. Provide these in your storage config. The token_file parameter should be a filepath where vdirsyncer can later store authentication-related data. You do not need to create the file itself or write anything to it. [storage example_for_google_calendar] type = \"google_calendar\" token_file = \"...\" client_id = \"...\" client_secret = \"...\" #start_date = null #end_date = null #item_types = []", "title": "Official steps"}, {"location": "vdirsyncer/#use-nekr0z-patch-solution", "text": "look the previous section if you have doubts on any of the steps If the official steps failed for you, try these ones: Go to the Google API Manager and create a new project under any name. Selected the vdirsyncer project Went to Credentials -> Create Credentials -> OAuth Client ID Select \"Web Application\" Under \"Authorised redirect URIs\" added http://127.0.0.1:8088 pressed \"Create\". Edit your vdirsyncer config [storage google] section to have the new client_id and client_secret (). Find the location of the vdirsyncer/storage/google.py in your environment (mine was in ~/.local/pipx/venvs/vdirsyncer/lib/python3.10/site-packages/vdirsyncer/storage ) and changed line 65 from redirect_uri = \"urn:ietf:wg:oauth:2.0:oob\" , to redirect_uri = \"http://127.0.0.1:8088\" , Run vdirsyncer discover my_calendar . Opened the link in my browser (on my desktop machine). Proceeded with Google authentication until \"Firefox can not connect to 127.0.0.1:8088.\" was displayed. from the browser's address bar that looked like: http://127.0.0.1:8088/?state=SOMETHING&code=HERECOMESTHECODE&scope=https://www.googleapis.com/auth/calendar * Copy the HERECOMESTHECODE part. * Paste the code into the session where vdirsyncer was running", "title": "Use Nekr0z patch solution"}, {"location": "vdirsyncer/#see-differences-between-syncs", "text": "If you create a git repository where you have your calendars you can do a git diff and see the files that have changed. If you do a commit after each sync you can have all the history.", "title": "See differences between syncs"}, {"location": "vdirsyncer/#references", "text": "Docs Git", "title": "References"}, {"location": "versioning/", "text": "The Don't Repeat Yourself principle encourages developers to abstract code into a separate components and reuse them rather than write it over and over again. If this happens across the system, the best practice is to put it inside a package that lives on its own (a library) and then pull it in from the applications when required. !!! note \"This article is heavily based on the posts in the references sections, the main credit goes to them, I've just refactored all together under my personal opinion.\" As most of us can\u2019t think of every feature that the library might offer or what bugs it might contain, these packages tend to evolve. Therefore, we need some mechanism to encode these evolutions of the library, so that downstream users can understand how big the change is. Most commonly, developers use three methods: A version number. A changelog . The git history. The version change is used as a concise way for the project to communicate this evolution, and it's what we're going to analyze in this article. However, encoding all the information of a change into a number switch has proven to be far from perfect . That's why keeping a good and detailed changelog makes a lot of sense, as it will better transmit that intent, and what will be the impact of upgrading. Once again, this falls into the same problem as before, while a change log is more descriptive, it still only tells you what changes (or breakages) a project intended to make, it doesn\u2019t go into any detail about unintended consequences of changes made. Ultimately, a change log\u2019s accuracy is no different than that of the version itself, it's just (hopefully!) more detailed. Fundamentally, any indicator of change that isn\u2019t a full diff is just a lossy encoding of that change. You can't expect though to read all the diffs of the libraries that you use, that's why version numbers and changelogs make a lot of sense. We just need to be aware of the limits of each system. That being said, you'll use version numbers in two ways: As a producer of applications and libraries where you\u2019ll have to decide what versioning system to use. As a consumer of dependencies, you\u2019ll have to express what versions of a given library your application/library is compatible. Deciding what version system to use for your programs \u2691 The two most popular versioning systems are: Semantic Versioning : A way to define your program's version based on the type of changes you've introduced. Calendar Versioning : A versioning convention based on your project's release calendar, instead of arbitrary numbers. Each has it's advantages and disadvantages. From a consumer perspective, I think that projects should generally default to SemVer-ish, following the spirit of the documentation rather than the letter of the specification because: Your version number becomes a means of communicating your changes intents to your end users. If you use the semantic versioning commit message guidelines , you are more likely to have a useful git history and can automatically maintain the project's changelog . There are however, corner cases where CalVer makes more sense: You\u2019re tracking something that is already versioned using dates or for which the version number can only really be described as a point in time release. The pytz is a good example of both of these cases, the Olson TZ database is versioned using a date based scheme and the information that it is providing is best represented as a snapshot of the state of what timezones were like at a particular point in time. Your project is going to be breaking compatibility in every release and you do not want to make any promises of compatibility. You should still document this fact in your README, but if there\u2019s no promise of compatibility between releases, then there\u2019s no information to be communicated in the version number. Your project is never going to intentionally breaking compatibility in a release, and you strive to always maintain compatibility. Projects can always just use the latest version of your software. Your changes will only ever be additive, and if you need to change your API, you\u2019ll do something like leave the old API intact, and add a new API with the new semantics. An example of this case would be the Ubuntu versions. How to evolve your code version \u2691 Assuming you're using Semantic Versioning you can improve your code evolution by: Avoid becoming a ZeroVer package . Use Warnings to avoid major changes Avoid becoming a ZeroVer package \u2691 Once your project reach it's first level of maturity you should release 1.0.0 to avoid falling into ZeroVer . For example you can use one of the next indicators: If you're frequently using it and haven't done any breaking change in 3 months. If 30 users are depending on it. For example counting the project stars. Use Warnings to avoid major changes \u2691 Semantic versioning uses the major version to defend against breaking changes, and at the same offers maintainers the freedom to evolve the library without breaking users. Nevertheless, this does not seem to work that well . So it's better to use Warnings to avoid major changes . Communicate with your users \u2691 You should warn your users not to blindly trust that any version change is not going to break their code and that you assume that they are actively testing the package updates . Keep the Requires-Python metadata updated \u2691 It's important not to upper cap the Python version and to maintain the Requires-Python package metadata updated. Dependency solvers will use this information to fetch the correct versions of the packages for the users. Deciding how to manage the versions of your dependencies \u2691 As a consumer of other dependencies, you need to specify in your package what versions does your code support. The traditional way to do it is by pinning those versions in your package definition. For example in python it lives either in the setup.py or in the pyproject.toml . Lower version pinning \u2691 When you're developing a program that uses a dependency, you usually don't know if a previous version of that dependency is compatible with your code, so in theory it makes sense to specify that you don't support any version smaller than the actual with something like >=1.2 . If you follow this train of thought, each time you update your dependencies, you should update your lower pins, because you're only running your test suite on those versions. If the libraries didn't do upper version pinning , then there would be no problem as you wouldn't be risking to get into version conflicts . A more relaxed approach would be not to update the pins when you update, in that case, you should run your tests both against the oldest possible values and the newest to ensure that everything works as expected. This way you'll be more kind to your users as you'll reduce possible version conflicts, but it'll add work to the maintainers. The most relaxed approach would be not to use pins at all, it will suppress most of the version conflicts but you won't be sure that the dependencies that your users are using are compatible with your code. Think about how much work you want to invest in maintaining your package and how much stability you want to offer before you choose one or the other method. Once you've made your choice, it would be nice if you communicate it to your users through your documentation. Upper version pinning \u2691 Program maintainers often rely on upper version pinning to guarantee that their code is not going to be broken due to a dependency update. We\u2019ll cover the valid use cases for capping after this section. But, just to be clear, if you know you do not support a new release of a library, then absolutely, go ahead and cap it as soon as you know this to be true. If something does not work, you should cap (or maybe restrict a single version if the upstream library has a temporary bug rather than a design direction that\u2019s causing the failure). You should also do as much as you can to quickly remove the cap, as all the downsides of capping in the next sections still apply. The following will assume you are capping before knowing that something does not work, but just out of general principle, like Poetry recommends and defaults to with poetry add . In most cases, the answer will be don\u2019t . For simplicity, I will also assume you are being tempted to cap to major releases ( ^1.0.0 in Poetry or ~=1.0 in all other tooling that follows Python standards via PEP 440) following the false security that only major changes can to break your code. If you cap to minor versions ~=1.0.0 , this is much worse, and the arguments below apply even more strongly. Version limits break code too \u2691 Following this path will effectively opt you out of bug fixes and security updates , as most of the projects only maintain the latest version of their program and what worse, you'll be preventing everyone using your library not to use the latest version of those libraries . All in exchange to defend yourself against a change that in practice will rarely impact you. Sure, you can move on to the next version of each of your pins each time they increase a major via something like Click>=8, <9 . However, this involves manual intervention on their code, and you might not have the time to do this for every one of your projects. If we add the fact that not only major but any other version change may break your code due to unintended changes and the difference in the change categorization , then you can treat all changes equally, so it makes no sense on pinning the major version either. This is specially useless when you add dependencies that follow CalVer . poetry add packaging will still do ^21 for the version it adds. You shouldn\u2019t be capping versions, but you really shouldn\u2019t be capping CalVer. SemVer never promises to break your code \u2691 A really easy but incorrect generalization of the SemVer rules is \u201ca major version will break my code\u201d. Even if the library follows true SemVer perfectly, a major version bump does not promise to break downstream code . It promises that some downstream code may break. If you use pytest to test your code, for example, the next major version will be very unlikely to break. If you write a pytest extension, however, then the chances of something breaking are much higher (but not 100%, maybe not even 50%). Quite ironically, the better a package follows SemVer, the smaller the change will trigger a major version, and therefore the less likely a major version will break a particular downstream code. As a general rule, if you have a reasonably stable dependency, and you only use the documented API, especially if your usage is pretty light/general, then a major update is extremely unlikely to break your code. It\u2019s quite rare for light usage of a library to break on a major update. It can happen, of course, but is unlikely. If you are using something very heavily, if you are working on a framework extension, or if you use internals that are not publicly documented, then your chances of breaking on a major release are much higher. Python has a culture of producing FutureWarnings , DeprecationWarnings , or PendingDeprecationWarnings (make sure they are on in your testing, and turn into errors ), good libraries will use them. Version conflicts \u2691 And then there\u2019s another aspect version pinning will introduce: version conflicts. An application or library will have a set of libraries it depends on directly. These are libraries you\u2019re directly importing within the application/library you\u2019re maintaining, but then the libraries themselves may rely on other libraries. This is known as a transitive dependency. Very soon, you\u2019ll get to a point where two different components use the same library, and both of them might express version constraints on it. For example, consider the case of tenacity : a general-purpose retrying library. Imagine you were using this in your application, and being a religious follower of semantic versioning, you\u2019ve pinned it to the version that was out when you created the app in early 2018: 4.11 . The constraint would specify version 4.11 or later, but less than the next major version 5 . At the same time, you also connect to an HTTP service. This connection is handled by another library, and the maintainer of that decided to also use tenacity to offer automatic retry functionality. They pinned it similarly following the semantic versioning convention. Back in 2018, this caused no issues. But then August comes, and version 5.0 is released. The service and its library maintainers have a lot more time on their hands (perhaps because they are paid to do so), so they quickly move to version 5.0 . Or perhaps they want to use a feature from the new major version. Now they introduce the pin greater than five but less than six on tenacity. Their public interface does not change at all at this point, so they do not bump their major version. It\u2019s just a patch release. Python can only have one version of a library installed at a given time. At this point, there is a version conflict. You\u2019re requesting a version between four and five, while the service library is requesting a version between five and six. Both constraints cannot be satisfied. If you use a version of pip older than 20.2 (the release in which it added a dependency resolver ) it will just install a version matching the first constraint it finds and ignore any subsequent constraints. Versions of pip after 20.2 would fail with an error indicating that the constraint cannot be satisfied. Either way, your application no longer works. The only way to make it work is to either pin the service library down to the last working patch number, or upgrade your version pinning of tenacity . This is generating extra work for you with minimal benefit. Often it might not be even possible to use two conflicting libraries until one of them relaxes their requirements. It also means you must support a wide version range; ironically. If you update to requiring `tenacity 5 , your update can\u2019t be installed with another library still on 4.11 . So you have to support tenacity>=4.11,<6` for a while until most libraries have similarly updated. And for those who might think this doesn\u2019t happen often, let me say that tenacity released another major version a year later in November 2019. Thus, the cycle starts all over again. In both cases, your code most likely did not need to change at all, as just a small part of their public API changed. In my experience, this happens a lot more often than when a major version bump breaks you. I've found myself investing most of my project maintenance time opening issues in third party dependencies to update their pins. It doesn\u2019t scale \u2691 If you have a single library that doesn\u2019t play well, then you probably will get a working solve easily (this is one reason that this practice doesn\u2019t seem so bad at first). If more packages start following this tight capping, however, you end up with a situation where things simply cannot solve. A moderately sized application can have a hundred or more dependencies when expanded, so such issues in my experience start to appear every few months. You need only 5-6 of such cases for every 100 libraries for this issue to pop up every two months on your plate. And potentially for a multiple of your applications. The entire point of packaging is to allow you to get lots of packages that each do some job for you. We should be trying to make it easy to be able to add dependencies, not harder. The implication of this is you should be very careful when you see tight requirements in packages and you have any upper bound caps anywhere in the dependency chain. If something caps dependencies, there\u2019s a very good chance adding two such packages will break your solve, so you should pick just one, or just avoid them altogether, so you can add one in the future. This is a good rule, actually: Never add a library to your dependencies that has excessive upper bound capping . When I have failed to follow this rule for a larger package, I have usually come to regret it. If you are doing the capping and are providing a library, you now have a commitment to quickly release an update, ideally right before any capped dependency comes out with a new version. Though if you cap, how to you install development versions or even know when a major version is released? This makes it harder for downstream packages to update, because they have to wait for all the caps to be moved for all upstream. It conflicts with tight lower bounds \u2691 A tight lower bound is only bad if packages cap upper bounds. If you can avoid upper-cap packages, you can accept tight lower bound packages, which are much better; better features, better security, better compatibility with new hardware and OS\u2019s. A good packaging system should allow you to require modern packages; why develop for really old versions of things if the packaging system can upgrade them? But a upper bound cap breaks this. Hopefully anyone who is writing software and pushing versions will agree that tight lower limits are much better than tight upper limits, so if one has to go, it\u2019s the upper limits. It is also rather rare that packages solve for lower bounds in CI (I would love to see such a solver become an option, by the way!), so setting a tight lower bound is one way to avoid rare errors when old packages are cached that you don\u2019t actually support. CI almost never has a cache of old packages, but users do. Capping dependencies hides incompatibilities \u2691 Another serious side effect of capping dependencies is that you are not notified properly of incoming incompatibilities, and you have to be extra proactive in monitoring your dependencies for updates. If you don\u2019t cap your dependencies, you are immediately notified when a dependency releases a new version, probably by your CI, the first time you build with that new version. If you are running your CI with the --dev flag on your pip install (uncommon, but probably a good idea), then you might even catch and fix the issue before a release is even made. If you don\u2019t do this, however, then you don\u2019t know about the incompatibility until (much) later. If you are not following all of your dependencies, you might not notice that you are out of date until it\u2019s both a serious problem for users and it\u2019s really hard for you to tell what change broke your usage because several versions have been released. While I\u2019m not a huge fan of Google\u2019s live-at-head philosophy (primarily because it has heavy requirements not applicable for most open-source projects), I appreciate and love catching a dependency incompatibility as soon as you possibly can; the smaller the change set, the easier it is to identify and fix the issue. Capping all dependencies hides real incompatibilities \u2691 If you see X>=1.1 , that tells you that the package is using features from 1.1 and do not support 1.0 . If you see X<1.2 , this should tell you that there\u2019s a problem with 1.2 and the current software, specifically something they know the dependency will not fix/revert. Not that you just capped all your dependencies and have no idea if that will or won\u2019t work at all. A cap should be like a TODO; it\u2019s a known issue that needs to be worked on soon. As in yesterday. Pinning the Python version is special \u2691 Another practice pushed by Poetry is adding an upper cap to the Python version. This is misusing a feature designed to help with dropping old Python versions to instead stop new Python versions from being used. \u201cScrolling back\u201d through older releases to find the newest version that does not restrict the version of Python being used is exactly the wrong behavior for an upper cap, and that is what the purpose of this field is. Current versions of pip do seem to fail when this is capped, rather than scrolling back to find an older uncapped version, but I haven\u2019t found many libraries that have \u201cadded\u201d this after releasing to be sure of that. To be clear, this is very different from a library: specifically, you can\u2019t downgrade your Python version if this is capped to something below your current version. You can only fail. So this does not \u201cfix\u201d something by getting an older, working version, it only causes hard failures if it works the way you might hope it does. This means instead of seeing the real failure and possibly helping to fix it, users just see a Python doesn\u2019t match error. And, most of the time, it\u2019s not even a real error; if you support Python 3.x without warnings, you should support Python 3.x+1 (and 3.x+2 , too). Capping to <4 (something like ^3.6 in Poetry) is also directly in conflict with the Python developer\u2019s own statements; they promise the 3->4 transition will be more like the 1->2 transition than the 2->3 transition. When Python 4 does come out, it will be really hard to even run your CI on 4 until all your dependencies uncap. And you won\u2019t actually see the real failures, you\u2019ll just see incompatibility errors, so you won\u2019t even know what to report to those libraries. And this practice makes it hard to test development versions of Python. And, if you use Poetry, as soon as someone caps the Python version, every Poetry project that uses it must also cap, even if you believe it is a detestable practice and confusing to users. It is also wrong unless you fully pin the dependency that forced the cap. If the dependency drops it in a patch release or something else you support, you no longer would need the cap. Applications are slightly different \u2691 If you have a true application (that is, if you are not intending your package to be used as a library), upper version constraints are much less problematic, and some of the reasons above don't apply. This due to two reasons. First, if you are writing a library, your \u201cusers\u201d are specifying your package in their dependencies; if an update breaks them, they can always add the necessary exclusion or cap for you to help end users. It\u2019s a leaky abstraction, they shouldn\u2019t have to care about what your dependencies are, but when capping interferes with what they can use, that\u2019s also a leaky and unfixable abstraction. For an application, the \u201cusers\u201d are more likely to be installing your package directly, where the users are generally other developers adding to requirements for libraries. Second, for an app that is installed from PyPI, you are less likely to have to worry about what else is installed (the other issues are still true). Many (most?) users will not be using pipx or a fresh virtual environment each time, so in practice, you\u2019ll still run into problems with tight constraints, but there is a workaround (use pipx , for example). You still are still affected by most of the arguments above, though, so personally I\u2019d still not recommend adding untested caps. When is it ok to set an upper limit? \u2691 Valid reasons to add an upper limit are: If a dependency is known to be broken, block out the broken version. Try very hard to fix this problem quickly, then remove the block if it\u2019s fixable on your end. If the fix happens upstream, excluding just the broken version is fine (or they can \u201cyank\u201d the bad release to help everyone). If you know upstream is about to make a major change that is very likely to break your usage, you can cap. But try to fix this as quickly as possible so you can remove the cap by the time they release. Possibly add development branch/release testing until this is resolved. If upstream asks users to cap, then I still don\u2019t like it, but it is okay if you want to follow the upstream recommendation. You should ask yourself: do you want to use a library that may intentionally break you and require changes on your part without help via deprecation periods? A one-time major rewrite might be an acceptable reason. Also, if you are upstream, it is very un-Pythonic to break users without deprecation warnings first. Don\u2019t do it if possible. If you are writing an extension for an ecosystem/framework (pytest extension, Sphinx extension, Jupyter extension, etc), then capping on the major version of that library is acceptable. Note this happens once - you have a single library that can be capped. You must release as soon as you possibly can after a new major release, and you should be closely following upstream probably using development releases for testing, etc. But doing this for one library is probably manageable. You are releasing two or more libraries in sync with each other. You control the release cadence for both libraries. This is likely the \u201cbest\u201d reason to cap. Some of the above issues don\u2019t apply in this case - since you control the release cadence and can keep them in sync. You depend on private internal details of a library. You should also rethink your choices - this can be broken in a minor or patch release, and often is. If you cap in these situations, I wouldn\u2019t complain, but I wouldn\u2019t really recommend it either: If you have a heavy dependency on a library, maybe cap. A really large API surface is more likely to be hit by the possible breakage. If a library is very new, say on version 1 or a ZeroVer library, and has very few users, maybe cap if it seems rather unstable. See if the library authors recommend capping - they might plan to make a large change if it\u2019s early in development. This is not blanket permission to cap ZeroVer libraries! If a library looks really unstable, such as having a history of making big changes, then cap. Or use a different library. Even better, contact the authors, and make sure that your usage is safe for the near future. Summary \u2691 No more than 1-2 of your dependencies should fall into the categories of acceptable upper pinning. In every other case, do not cap your dependences, specially if you are writing a library! You could probably summarize it like this: if there\u2019s a high chance (say 75%+ ) that a dependency will break for you when it updates, you can add a cap. But if there\u2019s no reason to believe it will break, do not add the cap; you will cause more severe (unfixable) pain than the breakage would. If you have an app instead of a library, you can be cautiously more relaxed, but not much. Apps do not have to live in shared environments, though they might. Notice many of the above instances are due to very close/special interaction with a small number of libraries (either a plugin for a framework, synchronized releases, or very heavy usage). Most libraries you use do not fall into this category. Remember, library authors don\u2019t want to break users who follow their public API and documentation. If they do, it\u2019s for a special and good reason (or it is a bad library to depend on). They will probably have a deprecation period, produce warnings, etc. If you do version cap anything, you are promising to closely follow that dependency, update the cap as soon as possible, follow beta or RC releases or the development branch, etc. When a new version of a library comes out, end users should be able to start trying it out. If they can\u2019t, your library\u2019s dependencies are a leaky abstraction (users shouldn\u2019t have to care about what dependencies libraries use). Automatically upgrade and test your dependencies \u2691 Now that you have minimized the upper bound pins and defined the lower bound pins you need to ensure that your code works with the latest version of your dependencies. One way to do it is running a periodic cronjob (daily probably) that updates your requirements lock, optionally your lower bounds , and checks that the tests keep on passing. Monitor your dependencies evolution \u2691 You rely on your dependencies to fulfill critical parts of your package, therefore it makes sense to know how they are changing in order to: Change your package to use new features. Be aware of the new possibilities to solve future problems. Get an idea of the dependency stability and future. Depending on how much you rely on the dependency, different levels of monitorization can be used, ordered from least to most you could check: Release messages : Some projects post them in their blogs, you can use their RSS feed to keep updated. If the project uses Github to create the release messages, you can get notifications on just those release messages. If the project uses Semantic Versioning , it can help you dismiss all changes that are micro , review without urgency the minor and prioritize the major ones. If all you're given is a CalVer style version then you're forced to dedicate the same time to each of the changes. Changelog : if you get a notification of a new release, head to the changelog to get a better detail of what has changed. Pull requests : Depending on the project release workflow, it may take some time from a change to be accepted until it's published under a new release, if you monitor the pull requests, you get an early idea of what will be included in the new version. Issues : Most of changes introduced in a project are created from the outcome of a repository issue, where a user expresses their desire to introduce the change. If you monitor them you'll get the idea of how the project will evolve in the future. Summary \u2691 Is semantic versioning irrevocably broken? Should it never be used? I don\u2019t think so. It still makes a lot of sense where there are ample resources to maintain multiple versions in parallel. A great example of this is Django. However, it feels less practical for projects that have just a few maintainers. In this case, it often leads to opting people out of bug fixes and security updates. It also encourages version conflicts in environments that can\u2019t have multiple versions of the same library, as is the case with Python. Furthermore, it makes it a lot harder for developers to learn from their mistakes and evolve the API to a better place. Rotten old design decisions will pull down the library for years to come. A better solution at hand can be using CalVer and a time-window based warning system to evolve the API and remove old interfaces. Does it solve all problems? Absolutely not. One thing it makes harder is library rewrites. For example, consider virtualenv's recent rewrite. Version 20 introduced a completely new API and changed some behaviours to new defaults. For such use cases in a CalVer world, you would likely need to release the rewritten project under a new name, such as virtualenv2. Then again, such complete rewrites are extremely rare (in the case of virtualenv, it involved twelve years passing). No version scheme will allow you to predict with any certainty how compatible your software will be with potential future versions of your dependencies. The only reasonable choices are for libraries to choose minimum versions/excluded versions only, never maximum versions. For applications, do the same thing, but also add in a lock file of known, good versions with exact pins (this is the fundamental difference between install_requires and requirements.txt). This doesn't necessarily apply to other ecosystems \u2691 All of this advice coming from me does not necessarily apply to all other packaging ecosystems. Python's flat dependency management has its pros and cons, hence why some other ecosystems do things differently. References \u2691 Bernat post on versioning Should You Use Upper Bound Version Constraints? by Henry Schreiner Why I don't like SemVer anymore by Snarky Versioning Software by donald stufft", "title": "Program Versioning"}, {"location": "versioning/#deciding-what-version-system-to-use-for-your-programs", "text": "The two most popular versioning systems are: Semantic Versioning : A way to define your program's version based on the type of changes you've introduced. Calendar Versioning : A versioning convention based on your project's release calendar, instead of arbitrary numbers. Each has it's advantages and disadvantages. From a consumer perspective, I think that projects should generally default to SemVer-ish, following the spirit of the documentation rather than the letter of the specification because: Your version number becomes a means of communicating your changes intents to your end users. If you use the semantic versioning commit message guidelines , you are more likely to have a useful git history and can automatically maintain the project's changelog . There are however, corner cases where CalVer makes more sense: You\u2019re tracking something that is already versioned using dates or for which the version number can only really be described as a point in time release. The pytz is a good example of both of these cases, the Olson TZ database is versioned using a date based scheme and the information that it is providing is best represented as a snapshot of the state of what timezones were like at a particular point in time. Your project is going to be breaking compatibility in every release and you do not want to make any promises of compatibility. You should still document this fact in your README, but if there\u2019s no promise of compatibility between releases, then there\u2019s no information to be communicated in the version number. Your project is never going to intentionally breaking compatibility in a release, and you strive to always maintain compatibility. Projects can always just use the latest version of your software. Your changes will only ever be additive, and if you need to change your API, you\u2019ll do something like leave the old API intact, and add a new API with the new semantics. An example of this case would be the Ubuntu versions.", "title": "Deciding what version system to use for your programs"}, {"location": "versioning/#how-to-evolve-your-code-version", "text": "Assuming you're using Semantic Versioning you can improve your code evolution by: Avoid becoming a ZeroVer package . Use Warnings to avoid major changes", "title": "How to evolve your code version"}, {"location": "versioning/#avoid-becoming-a-zerover-package", "text": "Once your project reach it's first level of maturity you should release 1.0.0 to avoid falling into ZeroVer . For example you can use one of the next indicators: If you're frequently using it and haven't done any breaking change in 3 months. If 30 users are depending on it. For example counting the project stars.", "title": "Avoid becoming a ZeroVer package"}, {"location": "versioning/#use-warnings-to-avoid-major-changes", "text": "Semantic versioning uses the major version to defend against breaking changes, and at the same offers maintainers the freedom to evolve the library without breaking users. Nevertheless, this does not seem to work that well . So it's better to use Warnings to avoid major changes .", "title": "Use Warnings to avoid major changes"}, {"location": "versioning/#communicate-with-your-users", "text": "You should warn your users not to blindly trust that any version change is not going to break their code and that you assume that they are actively testing the package updates .", "title": "Communicate with your users"}, {"location": "versioning/#keep-the-requires-python-metadata-updated", "text": "It's important not to upper cap the Python version and to maintain the Requires-Python package metadata updated. Dependency solvers will use this information to fetch the correct versions of the packages for the users.", "title": "Keep the Requires-Python metadata updated"}, {"location": "versioning/#deciding-how-to-manage-the-versions-of-your-dependencies", "text": "As a consumer of other dependencies, you need to specify in your package what versions does your code support. The traditional way to do it is by pinning those versions in your package definition. For example in python it lives either in the setup.py or in the pyproject.toml .", "title": "Deciding how to manage the versions of your dependencies"}, {"location": "versioning/#lower-version-pinning", "text": "When you're developing a program that uses a dependency, you usually don't know if a previous version of that dependency is compatible with your code, so in theory it makes sense to specify that you don't support any version smaller than the actual with something like >=1.2 . If you follow this train of thought, each time you update your dependencies, you should update your lower pins, because you're only running your test suite on those versions. If the libraries didn't do upper version pinning , then there would be no problem as you wouldn't be risking to get into version conflicts . A more relaxed approach would be not to update the pins when you update, in that case, you should run your tests both against the oldest possible values and the newest to ensure that everything works as expected. This way you'll be more kind to your users as you'll reduce possible version conflicts, but it'll add work to the maintainers. The most relaxed approach would be not to use pins at all, it will suppress most of the version conflicts but you won't be sure that the dependencies that your users are using are compatible with your code. Think about how much work you want to invest in maintaining your package and how much stability you want to offer before you choose one or the other method. Once you've made your choice, it would be nice if you communicate it to your users through your documentation.", "title": "Lower version pinning"}, {"location": "versioning/#upper-version-pinning", "text": "Program maintainers often rely on upper version pinning to guarantee that their code is not going to be broken due to a dependency update. We\u2019ll cover the valid use cases for capping after this section. But, just to be clear, if you know you do not support a new release of a library, then absolutely, go ahead and cap it as soon as you know this to be true. If something does not work, you should cap (or maybe restrict a single version if the upstream library has a temporary bug rather than a design direction that\u2019s causing the failure). You should also do as much as you can to quickly remove the cap, as all the downsides of capping in the next sections still apply. The following will assume you are capping before knowing that something does not work, but just out of general principle, like Poetry recommends and defaults to with poetry add . In most cases, the answer will be don\u2019t . For simplicity, I will also assume you are being tempted to cap to major releases ( ^1.0.0 in Poetry or ~=1.0 in all other tooling that follows Python standards via PEP 440) following the false security that only major changes can to break your code. If you cap to minor versions ~=1.0.0 , this is much worse, and the arguments below apply even more strongly.", "title": "Upper version pinning"}, {"location": "versioning/#version-limits-break-code-too", "text": "Following this path will effectively opt you out of bug fixes and security updates , as most of the projects only maintain the latest version of their program and what worse, you'll be preventing everyone using your library not to use the latest version of those libraries . All in exchange to defend yourself against a change that in practice will rarely impact you. Sure, you can move on to the next version of each of your pins each time they increase a major via something like Click>=8, <9 . However, this involves manual intervention on their code, and you might not have the time to do this for every one of your projects. If we add the fact that not only major but any other version change may break your code due to unintended changes and the difference in the change categorization , then you can treat all changes equally, so it makes no sense on pinning the major version either. This is specially useless when you add dependencies that follow CalVer . poetry add packaging will still do ^21 for the version it adds. You shouldn\u2019t be capping versions, but you really shouldn\u2019t be capping CalVer.", "title": "Version limits break code too"}, {"location": "versioning/#semver-never-promises-to-break-your-code", "text": "A really easy but incorrect generalization of the SemVer rules is \u201ca major version will break my code\u201d. Even if the library follows true SemVer perfectly, a major version bump does not promise to break downstream code . It promises that some downstream code may break. If you use pytest to test your code, for example, the next major version will be very unlikely to break. If you write a pytest extension, however, then the chances of something breaking are much higher (but not 100%, maybe not even 50%). Quite ironically, the better a package follows SemVer, the smaller the change will trigger a major version, and therefore the less likely a major version will break a particular downstream code. As a general rule, if you have a reasonably stable dependency, and you only use the documented API, especially if your usage is pretty light/general, then a major update is extremely unlikely to break your code. It\u2019s quite rare for light usage of a library to break on a major update. It can happen, of course, but is unlikely. If you are using something very heavily, if you are working on a framework extension, or if you use internals that are not publicly documented, then your chances of breaking on a major release are much higher. Python has a culture of producing FutureWarnings , DeprecationWarnings , or PendingDeprecationWarnings (make sure they are on in your testing, and turn into errors ), good libraries will use them.", "title": "SemVer never promises to break your code"}, {"location": "versioning/#version-conflicts", "text": "And then there\u2019s another aspect version pinning will introduce: version conflicts. An application or library will have a set of libraries it depends on directly. These are libraries you\u2019re directly importing within the application/library you\u2019re maintaining, but then the libraries themselves may rely on other libraries. This is known as a transitive dependency. Very soon, you\u2019ll get to a point where two different components use the same library, and both of them might express version constraints on it. For example, consider the case of tenacity : a general-purpose retrying library. Imagine you were using this in your application, and being a religious follower of semantic versioning, you\u2019ve pinned it to the version that was out when you created the app in early 2018: 4.11 . The constraint would specify version 4.11 or later, but less than the next major version 5 . At the same time, you also connect to an HTTP service. This connection is handled by another library, and the maintainer of that decided to also use tenacity to offer automatic retry functionality. They pinned it similarly following the semantic versioning convention. Back in 2018, this caused no issues. But then August comes, and version 5.0 is released. The service and its library maintainers have a lot more time on their hands (perhaps because they are paid to do so), so they quickly move to version 5.0 . Or perhaps they want to use a feature from the new major version. Now they introduce the pin greater than five but less than six on tenacity. Their public interface does not change at all at this point, so they do not bump their major version. It\u2019s just a patch release. Python can only have one version of a library installed at a given time. At this point, there is a version conflict. You\u2019re requesting a version between four and five, while the service library is requesting a version between five and six. Both constraints cannot be satisfied. If you use a version of pip older than 20.2 (the release in which it added a dependency resolver ) it will just install a version matching the first constraint it finds and ignore any subsequent constraints. Versions of pip after 20.2 would fail with an error indicating that the constraint cannot be satisfied. Either way, your application no longer works. The only way to make it work is to either pin the service library down to the last working patch number, or upgrade your version pinning of tenacity . This is generating extra work for you with minimal benefit. Often it might not be even possible to use two conflicting libraries until one of them relaxes their requirements. It also means you must support a wide version range; ironically. If you update to requiring `tenacity 5 , your update can\u2019t be installed with another library still on 4.11 . So you have to support tenacity>=4.11,<6` for a while until most libraries have similarly updated. And for those who might think this doesn\u2019t happen often, let me say that tenacity released another major version a year later in November 2019. Thus, the cycle starts all over again. In both cases, your code most likely did not need to change at all, as just a small part of their public API changed. In my experience, this happens a lot more often than when a major version bump breaks you. I've found myself investing most of my project maintenance time opening issues in third party dependencies to update their pins.", "title": "Version conflicts"}, {"location": "versioning/#it-doesnt-scale", "text": "If you have a single library that doesn\u2019t play well, then you probably will get a working solve easily (this is one reason that this practice doesn\u2019t seem so bad at first). If more packages start following this tight capping, however, you end up with a situation where things simply cannot solve. A moderately sized application can have a hundred or more dependencies when expanded, so such issues in my experience start to appear every few months. You need only 5-6 of such cases for every 100 libraries for this issue to pop up every two months on your plate. And potentially for a multiple of your applications. The entire point of packaging is to allow you to get lots of packages that each do some job for you. We should be trying to make it easy to be able to add dependencies, not harder. The implication of this is you should be very careful when you see tight requirements in packages and you have any upper bound caps anywhere in the dependency chain. If something caps dependencies, there\u2019s a very good chance adding two such packages will break your solve, so you should pick just one, or just avoid them altogether, so you can add one in the future. This is a good rule, actually: Never add a library to your dependencies that has excessive upper bound capping . When I have failed to follow this rule for a larger package, I have usually come to regret it. If you are doing the capping and are providing a library, you now have a commitment to quickly release an update, ideally right before any capped dependency comes out with a new version. Though if you cap, how to you install development versions or even know when a major version is released? This makes it harder for downstream packages to update, because they have to wait for all the caps to be moved for all upstream.", "title": "It doesn\u2019t scale"}, {"location": "versioning/#it-conflicts-with-tight-lower-bounds", "text": "A tight lower bound is only bad if packages cap upper bounds. If you can avoid upper-cap packages, you can accept tight lower bound packages, which are much better; better features, better security, better compatibility with new hardware and OS\u2019s. A good packaging system should allow you to require modern packages; why develop for really old versions of things if the packaging system can upgrade them? But a upper bound cap breaks this. Hopefully anyone who is writing software and pushing versions will agree that tight lower limits are much better than tight upper limits, so if one has to go, it\u2019s the upper limits. It is also rather rare that packages solve for lower bounds in CI (I would love to see such a solver become an option, by the way!), so setting a tight lower bound is one way to avoid rare errors when old packages are cached that you don\u2019t actually support. CI almost never has a cache of old packages, but users do.", "title": "It conflicts with tight lower bounds"}, {"location": "versioning/#capping-dependencies-hides-incompatibilities", "text": "Another serious side effect of capping dependencies is that you are not notified properly of incoming incompatibilities, and you have to be extra proactive in monitoring your dependencies for updates. If you don\u2019t cap your dependencies, you are immediately notified when a dependency releases a new version, probably by your CI, the first time you build with that new version. If you are running your CI with the --dev flag on your pip install (uncommon, but probably a good idea), then you might even catch and fix the issue before a release is even made. If you don\u2019t do this, however, then you don\u2019t know about the incompatibility until (much) later. If you are not following all of your dependencies, you might not notice that you are out of date until it\u2019s both a serious problem for users and it\u2019s really hard for you to tell what change broke your usage because several versions have been released. While I\u2019m not a huge fan of Google\u2019s live-at-head philosophy (primarily because it has heavy requirements not applicable for most open-source projects), I appreciate and love catching a dependency incompatibility as soon as you possibly can; the smaller the change set, the easier it is to identify and fix the issue.", "title": "Capping dependencies hides incompatibilities"}, {"location": "versioning/#capping-all-dependencies-hides-real-incompatibilities", "text": "If you see X>=1.1 , that tells you that the package is using features from 1.1 and do not support 1.0 . If you see X<1.2 , this should tell you that there\u2019s a problem with 1.2 and the current software, specifically something they know the dependency will not fix/revert. Not that you just capped all your dependencies and have no idea if that will or won\u2019t work at all. A cap should be like a TODO; it\u2019s a known issue that needs to be worked on soon. As in yesterday.", "title": "Capping all dependencies hides real incompatibilities"}, {"location": "versioning/#pinning-the-python-version-is-special", "text": "Another practice pushed by Poetry is adding an upper cap to the Python version. This is misusing a feature designed to help with dropping old Python versions to instead stop new Python versions from being used. \u201cScrolling back\u201d through older releases to find the newest version that does not restrict the version of Python being used is exactly the wrong behavior for an upper cap, and that is what the purpose of this field is. Current versions of pip do seem to fail when this is capped, rather than scrolling back to find an older uncapped version, but I haven\u2019t found many libraries that have \u201cadded\u201d this after releasing to be sure of that. To be clear, this is very different from a library: specifically, you can\u2019t downgrade your Python version if this is capped to something below your current version. You can only fail. So this does not \u201cfix\u201d something by getting an older, working version, it only causes hard failures if it works the way you might hope it does. This means instead of seeing the real failure and possibly helping to fix it, users just see a Python doesn\u2019t match error. And, most of the time, it\u2019s not even a real error; if you support Python 3.x without warnings, you should support Python 3.x+1 (and 3.x+2 , too). Capping to <4 (something like ^3.6 in Poetry) is also directly in conflict with the Python developer\u2019s own statements; they promise the 3->4 transition will be more like the 1->2 transition than the 2->3 transition. When Python 4 does come out, it will be really hard to even run your CI on 4 until all your dependencies uncap. And you won\u2019t actually see the real failures, you\u2019ll just see incompatibility errors, so you won\u2019t even know what to report to those libraries. And this practice makes it hard to test development versions of Python. And, if you use Poetry, as soon as someone caps the Python version, every Poetry project that uses it must also cap, even if you believe it is a detestable practice and confusing to users. It is also wrong unless you fully pin the dependency that forced the cap. If the dependency drops it in a patch release or something else you support, you no longer would need the cap.", "title": "Pinning the Python version is special"}, {"location": "versioning/#applications-are-slightly-different", "text": "If you have a true application (that is, if you are not intending your package to be used as a library), upper version constraints are much less problematic, and some of the reasons above don't apply. This due to two reasons. First, if you are writing a library, your \u201cusers\u201d are specifying your package in their dependencies; if an update breaks them, they can always add the necessary exclusion or cap for you to help end users. It\u2019s a leaky abstraction, they shouldn\u2019t have to care about what your dependencies are, but when capping interferes with what they can use, that\u2019s also a leaky and unfixable abstraction. For an application, the \u201cusers\u201d are more likely to be installing your package directly, where the users are generally other developers adding to requirements for libraries. Second, for an app that is installed from PyPI, you are less likely to have to worry about what else is installed (the other issues are still true). Many (most?) users will not be using pipx or a fresh virtual environment each time, so in practice, you\u2019ll still run into problems with tight constraints, but there is a workaround (use pipx , for example). You still are still affected by most of the arguments above, though, so personally I\u2019d still not recommend adding untested caps.", "title": "Applications are slightly different"}, {"location": "versioning/#when-is-it-ok-to-set-an-upper-limit", "text": "Valid reasons to add an upper limit are: If a dependency is known to be broken, block out the broken version. Try very hard to fix this problem quickly, then remove the block if it\u2019s fixable on your end. If the fix happens upstream, excluding just the broken version is fine (or they can \u201cyank\u201d the bad release to help everyone). If you know upstream is about to make a major change that is very likely to break your usage, you can cap. But try to fix this as quickly as possible so you can remove the cap by the time they release. Possibly add development branch/release testing until this is resolved. If upstream asks users to cap, then I still don\u2019t like it, but it is okay if you want to follow the upstream recommendation. You should ask yourself: do you want to use a library that may intentionally break you and require changes on your part without help via deprecation periods? A one-time major rewrite might be an acceptable reason. Also, if you are upstream, it is very un-Pythonic to break users without deprecation warnings first. Don\u2019t do it if possible. If you are writing an extension for an ecosystem/framework (pytest extension, Sphinx extension, Jupyter extension, etc), then capping on the major version of that library is acceptable. Note this happens once - you have a single library that can be capped. You must release as soon as you possibly can after a new major release, and you should be closely following upstream probably using development releases for testing, etc. But doing this for one library is probably manageable. You are releasing two or more libraries in sync with each other. You control the release cadence for both libraries. This is likely the \u201cbest\u201d reason to cap. Some of the above issues don\u2019t apply in this case - since you control the release cadence and can keep them in sync. You depend on private internal details of a library. You should also rethink your choices - this can be broken in a minor or patch release, and often is. If you cap in these situations, I wouldn\u2019t complain, but I wouldn\u2019t really recommend it either: If you have a heavy dependency on a library, maybe cap. A really large API surface is more likely to be hit by the possible breakage. If a library is very new, say on version 1 or a ZeroVer library, and has very few users, maybe cap if it seems rather unstable. See if the library authors recommend capping - they might plan to make a large change if it\u2019s early in development. This is not blanket permission to cap ZeroVer libraries! If a library looks really unstable, such as having a history of making big changes, then cap. Or use a different library. Even better, contact the authors, and make sure that your usage is safe for the near future.", "title": "When is it ok to set an upper limit?"}, {"location": "versioning/#summary", "text": "No more than 1-2 of your dependencies should fall into the categories of acceptable upper pinning. In every other case, do not cap your dependences, specially if you are writing a library! You could probably summarize it like this: if there\u2019s a high chance (say 75%+ ) that a dependency will break for you when it updates, you can add a cap. But if there\u2019s no reason to believe it will break, do not add the cap; you will cause more severe (unfixable) pain than the breakage would. If you have an app instead of a library, you can be cautiously more relaxed, but not much. Apps do not have to live in shared environments, though they might. Notice many of the above instances are due to very close/special interaction with a small number of libraries (either a plugin for a framework, synchronized releases, or very heavy usage). Most libraries you use do not fall into this category. Remember, library authors don\u2019t want to break users who follow their public API and documentation. If they do, it\u2019s for a special and good reason (or it is a bad library to depend on). They will probably have a deprecation period, produce warnings, etc. If you do version cap anything, you are promising to closely follow that dependency, update the cap as soon as possible, follow beta or RC releases or the development branch, etc. When a new version of a library comes out, end users should be able to start trying it out. If they can\u2019t, your library\u2019s dependencies are a leaky abstraction (users shouldn\u2019t have to care about what dependencies libraries use).", "title": "Summary"}, {"location": "versioning/#automatically-upgrade-and-test-your-dependencies", "text": "Now that you have minimized the upper bound pins and defined the lower bound pins you need to ensure that your code works with the latest version of your dependencies. One way to do it is running a periodic cronjob (daily probably) that updates your requirements lock, optionally your lower bounds , and checks that the tests keep on passing.", "title": "Automatically upgrade and test your dependencies"}, {"location": "versioning/#monitor-your-dependencies-evolution", "text": "You rely on your dependencies to fulfill critical parts of your package, therefore it makes sense to know how they are changing in order to: Change your package to use new features. Be aware of the new possibilities to solve future problems. Get an idea of the dependency stability and future. Depending on how much you rely on the dependency, different levels of monitorization can be used, ordered from least to most you could check: Release messages : Some projects post them in their blogs, you can use their RSS feed to keep updated. If the project uses Github to create the release messages, you can get notifications on just those release messages. If the project uses Semantic Versioning , it can help you dismiss all changes that are micro , review without urgency the minor and prioritize the major ones. If all you're given is a CalVer style version then you're forced to dedicate the same time to each of the changes. Changelog : if you get a notification of a new release, head to the changelog to get a better detail of what has changed. Pull requests : Depending on the project release workflow, it may take some time from a change to be accepted until it's published under a new release, if you monitor the pull requests, you get an early idea of what will be included in the new version. Issues : Most of changes introduced in a project are created from the outcome of a repository issue, where a user expresses their desire to introduce the change. If you monitor them you'll get the idea of how the project will evolve in the future.", "title": "Monitor your dependencies evolution"}, {"location": "versioning/#summary_1", "text": "Is semantic versioning irrevocably broken? Should it never be used? I don\u2019t think so. It still makes a lot of sense where there are ample resources to maintain multiple versions in parallel. A great example of this is Django. However, it feels less practical for projects that have just a few maintainers. In this case, it often leads to opting people out of bug fixes and security updates. It also encourages version conflicts in environments that can\u2019t have multiple versions of the same library, as is the case with Python. Furthermore, it makes it a lot harder for developers to learn from their mistakes and evolve the API to a better place. Rotten old design decisions will pull down the library for years to come. A better solution at hand can be using CalVer and a time-window based warning system to evolve the API and remove old interfaces. Does it solve all problems? Absolutely not. One thing it makes harder is library rewrites. For example, consider virtualenv's recent rewrite. Version 20 introduced a completely new API and changed some behaviours to new defaults. For such use cases in a CalVer world, you would likely need to release the rewritten project under a new name, such as virtualenv2. Then again, such complete rewrites are extremely rare (in the case of virtualenv, it involved twelve years passing). No version scheme will allow you to predict with any certainty how compatible your software will be with potential future versions of your dependencies. The only reasonable choices are for libraries to choose minimum versions/excluded versions only, never maximum versions. For applications, do the same thing, but also add in a lock file of known, good versions with exact pins (this is the fundamental difference between install_requires and requirements.txt).", "title": "Summary"}, {"location": "versioning/#this-doesnt-necessarily-apply-to-other-ecosystems", "text": "All of this advice coming from me does not necessarily apply to all other packaging ecosystems. Python's flat dependency management has its pros and cons, hence why some other ecosystems do things differently.", "title": "This doesn't necessarily apply to other ecosystems"}, {"location": "versioning/#references", "text": "Bernat post on versioning Should You Use Upper Bound Version Constraints? by Henry Schreiner Why I don't like SemVer anymore by Snarky Versioning Software by donald stufft", "title": "References"}, {"location": "vim/", "text": "Vim is a lightweight keyboard driven editor. It's the road to fly over the keyboard as it increases productivity and usability. If you doubt between learning emacs or vim, go with emacs with spacemacs I am a power vim user for more than 10 years, and seeing what my friends do with emacs, I suggest you to learn it while keeping the vim movement. Spacemacs is a preconfigured Emacs with those bindings and a lot of more stuff, but it's a good way to start. Vi vs Vim vs Neovim \u2691 TL;DR: Use Neovim Small comparison: Vi Follows the Single Unix Specification and POSIX. Original code written by Bill Joy in 1976. BSD license. Doesn't even have a git repository -.- . Vim Written by Bram Moolenaar in 1991. Vim is free and open source software, license is compatible with the GNU General Public License. C: 47.6 %, Vim Script: 44.8%, Roff 1.9%, Makefile 1.7%, C++ 1.2% Commits: 7120, Branch: 1 , Releases: 5639, Contributor: 1 Lines: 1.295.837 Neovim Written by the community from 2014 Published under the Apache 2.0 license Commits: 7994, Branch 1 , Releases: 9, Contributors: 303 Vim script: 46.9%, C:38.9%, Lua 11.3%, Python 0.9%, C++ 0.6% Lines: 937.508 (27.65% less code than vim) Refactor: Simplify maintenance and encourage contributions Easy update, just symlinks Ahead of vim, new features inserted in Vim 8.0 (async) Neovim is a refactor of Vim to make it viable for another 30 years of hacking. Neovim very intentionally builds on the long history of Vim community knowledge and user habits. That means \u201cswitching\u201d from Vim to Neovim is just an \u201cupgrade\u201d. From the start, one of Neovim\u2019s explicit goals has been simplify maintenance and encourage contributions. By building a codebase and community that enables experimentation and low-cost trials of new features.. And there\u2019s evidence of real progress towards that ambition. We\u2019ve successfully executed non-trivial \u201coff-the-roadmap\u201d patches: features which are important to their authors, but not the highest priority for the project. These patches were included because they: Fit into existing conventions/design. Included robust test coverage (enabled by an advanced test framework and CI). Received thoughtful review by other contributors. Abbreviations \u2691 In order to reduce the amount of typing and fix common typos, I use the Vim abbreviations support. Those are split into two files, ~/.vim/abbreviations.vim for abbreviations that can be used in every type of format and ~/.vim/markdown-abbreviations.vim for the ones that can interfere with programming typing. Those files are sourced in my .vimrc \" Abbreviations source ~ /.vim/ abbreviations. vim autocmd BufNewFile , BufReadPost *.md source ~ /.vim/ markdown - abbreviations. vim To avoid getting worse in grammar, I don't add abbreviations for words that I doubt their spelling or that I usually mistake. Instead I use it for common typos such as teh . The process has it's inconveniences: You need different abbreviations for the capitalized versions, so you'd need two abbreviations for iab cant can't and iab Cant Can't It's not user friendly to add new words, as you need to open a file. The Vim Abolish plugin solves that. For example: \" Typing the following: Abolish seperate separate \" Is equivalent to: iabbrev seperate separate iabbrev Seperate Separate iabbrev SEPERATE SEPARATE Or create more complex rules, were each {} gets captured and expanded with different caps : Abolish {despa , sepe}rat{ e , es , ed , ing , ely , ion , ions , or} {despe , sepa}rat{} With a bang ( :Abolish! ) the abbreviation is also appended to the file in g:abolish_save_file . By default after/plugin/abolish.vim which is loaded by default. Typing :Abolish! im I'm will append the following to the end of this file: Abolish im I' m To make it quicker I've added a mapping for <leader>s . nnoremap < leader > s :Abolish !< Space > Check the README for more details. Troubleshooting \u2691 Abbreviations with dashes or if you only want the first letter in capital need to be specified with the first letter in capital letters as stated in this issue . Abolish knobas knowledge - based Abolish w what Will yield KnowledgeBased if invoked with Knobas , and WHAT if invoked with W . Therefore the following definitions are preferred: Abolish Knobas Knowledge - based Abolish W What Auto complete prose text \u2691 Tools like YouCompleteMe allow you to auto complete variables and functions. If you want the same functionality for prose, you need to enable it for markdown and text, as it's disabled by default. let g :ycm_filetype_blacklist = { \\ 'tagbar' : 1 , \\ 'qf' : 1 , \\ 'notes' : 1 , \\ 'unite' : 1 , \\ 'vimwiki' : 1 , \\ 'pandoc' : 1 , \\ 'infolog' : 1 \\} When writing prose you don't need all possible suggestions, as navigating the options is slower than keep on typing. So I'm limiting the results just to one, to avoid unnecessary distractions. \" Limit the results for markdown files to 1 au FileType markdown let g :ycm_max_num_candidates = 1 au FileType markdown let g :ycm_max_num_identifier_candidates = 1 Find synonyms \u2691 Sometimes the prose linters tell you that a word is wordy or too complex, or you may be repeating a word too much. The thesaurus query plugin allows you to search synonyms of the word under the cursor. Assuming you use Vundle, add the following lines to your config. File: ~/.vimrc Plugin 'ron89/thesaurus_query.vim' \" Thesaurus let g :tq_enabled_backends = [ \"mthesaur_txt\" ] let g :tq_mthesaur_file = \"~/.vim/thesaurus\" nnoremap < leader > r :ThesaurusQueryReplaceCurrentWord < CR > inoremap < leader > r < esc > :ThesaurusQueryReplaceCurrentWord < CR > Run :PluginInstall and download the thesaurus text from gutenberg.org Next time you find a word like therefore you can press :ThesaurusQueryReplaceCurrentWord and you'll get a window with the following: In line: ... therefore ... Candidates for therefore, found by backend: mthesaur_txt Synonyms: (0)accordingly (1)according to circumstances (2)and so (3)appropriately (4)as a consequence (5)as a result (6)as it is (7)as matters stand (8)at that rate (9)because of that (10)because of this (11)compliantly (12)conformably (13)consequently (14)equally (15)ergo (16)finally (17)for that (18)for that cause (19)for that reason (20)for this cause (21)for this reason (22)for which reason (23)hence (24)hereat (25)in that case (26)in that event (27)inconsequence (28)inevitably (29)it follows that (30)naturally (31)naturellement (32)necessarily (33)of course (34)of necessity (35)on that account (36)on that ground (37)on this account (38)propter hoc (39)suitably (40)that being so (41)then (42)thence (43)thereat (44)therefor (45)thus (46)thusly (47)thuswise (48)under the circumstances (49)whence (50)wherefore (51)wherefrom Type number and <Enter> (empty cancels; 'n': use next backend; 'p' use previous backend): If for example you type 45 and hit enter, it will change it for thus . Keep foldings \u2691 When running fixers usually the foldings go to hell. To keep the foldings add the following snippet to your vimrc file augroup remember_folds autocmd ! autocmd BufLeave * mkview autocmd BufEnter * silent ! loadview augroup END Python folding done right \u2691 Folding Python in Vim is not easy, the python-mode plugin doesn't do it for me by default and after fighting with it for 2 hours... SimpylFold does the trick just fine. Delete a file inside vim \u2691 : call delete ( expand ( '%' )) | bdelete ! You can make a function so it's easier to remember function ! Rm () call delete ( expand ( '%' )) | bdelete ! endfunction Now you need to run :call Rm() . Resources \u2691 Nvim news spacevim awesome-vim : a list of vim resources maintained by the community Vimrc tweaking \u2691 jessfraz vimrc Learning \u2691 vim golf Vim game tutorial : very funny and challenging, buuuuut at lvl 3 you have to pay :(. PacVim : Pacman like vim game to learn. Vimgenius : Increase your speed and improve your muscle memory with Vim Genius, a timed flashcard-style game designed to make you faster in Vim. It\u2019s free and you don\u2019t need to sign up. What are you waiting for? Openvim : Interactive tutorial for vim.", "title": "Vim"}, {"location": "vim/#vi-vs-vim-vs-neovim", "text": "TL;DR: Use Neovim Small comparison: Vi Follows the Single Unix Specification and POSIX. Original code written by Bill Joy in 1976. BSD license. Doesn't even have a git repository -.- . Vim Written by Bram Moolenaar in 1991. Vim is free and open source software, license is compatible with the GNU General Public License. C: 47.6 %, Vim Script: 44.8%, Roff 1.9%, Makefile 1.7%, C++ 1.2% Commits: 7120, Branch: 1 , Releases: 5639, Contributor: 1 Lines: 1.295.837 Neovim Written by the community from 2014 Published under the Apache 2.0 license Commits: 7994, Branch 1 , Releases: 9, Contributors: 303 Vim script: 46.9%, C:38.9%, Lua 11.3%, Python 0.9%, C++ 0.6% Lines: 937.508 (27.65% less code than vim) Refactor: Simplify maintenance and encourage contributions Easy update, just symlinks Ahead of vim, new features inserted in Vim 8.0 (async) Neovim is a refactor of Vim to make it viable for another 30 years of hacking. Neovim very intentionally builds on the long history of Vim community knowledge and user habits. That means \u201cswitching\u201d from Vim to Neovim is just an \u201cupgrade\u201d. From the start, one of Neovim\u2019s explicit goals has been simplify maintenance and encourage contributions. By building a codebase and community that enables experimentation and low-cost trials of new features.. And there\u2019s evidence of real progress towards that ambition. We\u2019ve successfully executed non-trivial \u201coff-the-roadmap\u201d patches: features which are important to their authors, but not the highest priority for the project. These patches were included because they: Fit into existing conventions/design. Included robust test coverage (enabled by an advanced test framework and CI). Received thoughtful review by other contributors.", "title": "Vi vs Vim vs Neovim"}, {"location": "vim/#abbreviations", "text": "In order to reduce the amount of typing and fix common typos, I use the Vim abbreviations support. Those are split into two files, ~/.vim/abbreviations.vim for abbreviations that can be used in every type of format and ~/.vim/markdown-abbreviations.vim for the ones that can interfere with programming typing. Those files are sourced in my .vimrc \" Abbreviations source ~ /.vim/ abbreviations. vim autocmd BufNewFile , BufReadPost *.md source ~ /.vim/ markdown - abbreviations. vim To avoid getting worse in grammar, I don't add abbreviations for words that I doubt their spelling or that I usually mistake. Instead I use it for common typos such as teh . The process has it's inconveniences: You need different abbreviations for the capitalized versions, so you'd need two abbreviations for iab cant can't and iab Cant Can't It's not user friendly to add new words, as you need to open a file. The Vim Abolish plugin solves that. For example: \" Typing the following: Abolish seperate separate \" Is equivalent to: iabbrev seperate separate iabbrev Seperate Separate iabbrev SEPERATE SEPARATE Or create more complex rules, were each {} gets captured and expanded with different caps : Abolish {despa , sepe}rat{ e , es , ed , ing , ely , ion , ions , or} {despe , sepa}rat{} With a bang ( :Abolish! ) the abbreviation is also appended to the file in g:abolish_save_file . By default after/plugin/abolish.vim which is loaded by default. Typing :Abolish! im I'm will append the following to the end of this file: Abolish im I' m To make it quicker I've added a mapping for <leader>s . nnoremap < leader > s :Abolish !< Space > Check the README for more details.", "title": "Abbreviations"}, {"location": "vim/#troubleshooting", "text": "Abbreviations with dashes or if you only want the first letter in capital need to be specified with the first letter in capital letters as stated in this issue . Abolish knobas knowledge - based Abolish w what Will yield KnowledgeBased if invoked with Knobas , and WHAT if invoked with W . Therefore the following definitions are preferred: Abolish Knobas Knowledge - based Abolish W What", "title": "Troubleshooting"}, {"location": "vim/#auto-complete-prose-text", "text": "Tools like YouCompleteMe allow you to auto complete variables and functions. If you want the same functionality for prose, you need to enable it for markdown and text, as it's disabled by default. let g :ycm_filetype_blacklist = { \\ 'tagbar' : 1 , \\ 'qf' : 1 , \\ 'notes' : 1 , \\ 'unite' : 1 , \\ 'vimwiki' : 1 , \\ 'pandoc' : 1 , \\ 'infolog' : 1 \\} When writing prose you don't need all possible suggestions, as navigating the options is slower than keep on typing. So I'm limiting the results just to one, to avoid unnecessary distractions. \" Limit the results for markdown files to 1 au FileType markdown let g :ycm_max_num_candidates = 1 au FileType markdown let g :ycm_max_num_identifier_candidates = 1", "title": "Auto complete prose text"}, {"location": "vim/#find-synonyms", "text": "Sometimes the prose linters tell you that a word is wordy or too complex, or you may be repeating a word too much. The thesaurus query plugin allows you to search synonyms of the word under the cursor. Assuming you use Vundle, add the following lines to your config. File: ~/.vimrc Plugin 'ron89/thesaurus_query.vim' \" Thesaurus let g :tq_enabled_backends = [ \"mthesaur_txt\" ] let g :tq_mthesaur_file = \"~/.vim/thesaurus\" nnoremap < leader > r :ThesaurusQueryReplaceCurrentWord < CR > inoremap < leader > r < esc > :ThesaurusQueryReplaceCurrentWord < CR > Run :PluginInstall and download the thesaurus text from gutenberg.org Next time you find a word like therefore you can press :ThesaurusQueryReplaceCurrentWord and you'll get a window with the following: In line: ... therefore ... Candidates for therefore, found by backend: mthesaur_txt Synonyms: (0)accordingly (1)according to circumstances (2)and so (3)appropriately (4)as a consequence (5)as a result (6)as it is (7)as matters stand (8)at that rate (9)because of that (10)because of this (11)compliantly (12)conformably (13)consequently (14)equally (15)ergo (16)finally (17)for that (18)for that cause (19)for that reason (20)for this cause (21)for this reason (22)for which reason (23)hence (24)hereat (25)in that case (26)in that event (27)inconsequence (28)inevitably (29)it follows that (30)naturally (31)naturellement (32)necessarily (33)of course (34)of necessity (35)on that account (36)on that ground (37)on this account (38)propter hoc (39)suitably (40)that being so (41)then (42)thence (43)thereat (44)therefor (45)thus (46)thusly (47)thuswise (48)under the circumstances (49)whence (50)wherefore (51)wherefrom Type number and <Enter> (empty cancels; 'n': use next backend; 'p' use previous backend): If for example you type 45 and hit enter, it will change it for thus .", "title": "Find synonyms"}, {"location": "vim/#keep-foldings", "text": "When running fixers usually the foldings go to hell. To keep the foldings add the following snippet to your vimrc file augroup remember_folds autocmd ! autocmd BufLeave * mkview autocmd BufEnter * silent ! loadview augroup END", "title": "Keep foldings"}, {"location": "vim/#python-folding-done-right", "text": "Folding Python in Vim is not easy, the python-mode plugin doesn't do it for me by default and after fighting with it for 2 hours... SimpylFold does the trick just fine.", "title": "Python folding done right"}, {"location": "vim/#delete-a-file-inside-vim", "text": ": call delete ( expand ( '%' )) | bdelete ! You can make a function so it's easier to remember function ! Rm () call delete ( expand ( '%' )) | bdelete ! endfunction Now you need to run :call Rm() .", "title": "Delete a file inside vim"}, {"location": "vim/#resources", "text": "Nvim news spacevim awesome-vim : a list of vim resources maintained by the community", "title": "Resources"}, {"location": "vim/#vimrc-tweaking", "text": "jessfraz vimrc", "title": "Vimrc tweaking"}, {"location": "vim/#learning", "text": "vim golf Vim game tutorial : very funny and challenging, buuuuut at lvl 3 you have to pay :(. PacVim : Pacman like vim game to learn. Vimgenius : Increase your speed and improve your muscle memory with Vim Genius, a timed flashcard-style game designed to make you faster in Vim. It\u2019s free and you don\u2019t need to sign up. What are you waiting for? Openvim : Interactive tutorial for vim.", "title": "Learning"}, {"location": "vim_tabs/", "text": "This article is almost a copy paste of joshldavis post First I have to admit, I was a heavy user of tabs in Vim. I was using tabs in Vim as you\u2019d use tabs in most other programs (Firefox, Terminal, Adium, etc.). I was used to the idea of a tab being the place where a document lives. When you want to edit a document, you open a new tab and edit away! That\u2019s how tabs work so that must be how they work in Vim right? Nope. Stop the Tab Madness \u2691 If you are using tabs like this then you are really limiting yourself and using a feature of Vim that wasn't meant to work like this. Before I explain that, let\u2019s be sure we understand what a buffer is in Vim as well as a few other basic things. After that, I\u2019ll explain the correct way to use tabs within Vim. Buffers \u2691 A buffer is nothing more than text that you are editing. For example, when you open a file, the content of the file is loaded into a buffer. So when you issue this command: vim .vimrc You are actually launching Vim with a single buffer that is filled with the contents of the .vimrc file. Now let\u2019s look at what happens when you try to edit multiple files. Let\u2019s issue this command: vim .vimrc .bashrc In Vim run :bnext Vim does what it did before, but instead of just 1 buffer, it opens another buffer that is filled with .bashrc . So now we have two buffers open. If you want to pause editing .vimrc and move to .bashrc , you could run this command in Vim :bnext which will show the .bashrc buffer. There are various other commands to manipulate buffers which you can see if you type :h buffer-list . Or you can use easymotion . Windows \u2691 A window in Vim is just a way to view a buffer. Whenever you create a new vertical or horizontal split, that is a window. For example, if you were to type in :help window , it would launch a new window that shows the help documentation. The important thing to note is that a window can view any buffer it wishes; it isn't forced to look at the same buffer. When editing a file, if we type :vsplit , we will get a vertical split and in the other window, we will see the current buffer we are editing. That should no longer be confusing because a window lets us look at any buffer. It just so happens that when creating a new split: :split or :vsplit , the buffer that we view is just the current one. By running any of the buffer commands from :h buffer-list , we can modify which buffer a window is viewing. For an example of this, by running the following commands, we will start editing two files in Vim, open a new window by horizontally splitting, and then view the second buffer in the original window. vim .vimrc .bashrc In Vim run: :split and :bnext So a Tab is\u2026? \u2691 So now that we know what a buffer is and what a window is. Here is what Vim says in the Vim documentation regarding a buffer/window/tab: Summary: A buffer is the in-memory text of a file. A window is a viewport on a buffer. A tab page is a collection of windows. According to the documentation, a tab is just a collection of windows. This goes back to our earlier definition in that a tab is really just a layout. A tab is only designed to give you a different layout of windows. The Tab Problem \u2691 Tabs were only designed to let us have different layouts of windows. They aren't intended to be where an open file lives; an open file is instead loaded into a buffer. If you can view the same buffer across all tabs, how is this like a normal tab in most other editors? If you try to force a single tab to point to a single buffer, that is just futile. Vim just wasn't meant to work like this. The Buffer Solution \u2691 To reconcile all of this and learn how to use Vim\u2019s buffers/windows effectively, it might be useful to stop using tabs altogether until you understand how to edit with just using buffers/windows. The first thing I did was install a plugin that allows me to visualize all the buffers open across the top. I use bufferline for this. Instead of replicating tabs across the top like we did in the previous solution, we are instead going to use the power of being able to open many buffers simultaneously without worrying about which ones are open. In my experience, CtrlP gives a powerful fuzzy finder to navigate through the buffers. Instead of worrying about closing buffers and managing your pseudo-tabs that was mentioned in the previous solution, you just open files that you want to edit using CtrlP and don't worry about closing buffers or how many you have opened. When you are done editing a file, you just save it and then open CtrlP and continue onto the next file. CtrlP offers a few different ways to fuzzy find. You can use the following fuzziness: Find in your current directory. Find within all your open buffers. Find within all your open buffers sorted by Most Recently Used (MRU). Find with a mix of all the above. Using Tabs Correctly \u2691 This doesn't mean you should stop using tabs altogether. You should just use them how Vim intended you to use them. Instead you should use them to change the layout among windows. Imagine you are working on a C project. It might be helpful to have one tab dedicated to normal editing, but another tab for using a vertical split for the file.h and file.c files to make editing between them easier. Tabs also work really well to divide up what you are working on. You could be working on one part of the project in one tab and another part of the project in another tab. Just remember though, if you are using a single tab for each file, that isn't how tabs in Vim were designed to be used. Default option when switching \u2691 The default behavior when trying to switch the buffer is to not allow you to change buffer if it's not saved, but we can change it if we set one of the next options: set hidden : allow to switch buffers even though it's changes aren't saved. set autowrite : Auto save when switching buffers. Share buffers and all vim information between vim instances. \u2691 This is not my ideal behavior, nvim should let the user use the window manager to manage the windows... duh, instead of vsplitting buffers or using tabs. But sadly as of Nvim 0.1.7 and Vim 8.0 it's not implemented . You have the --server option but it only sends files to the already opened vim instance. you can't connect two vim instances to the same buffer pool. It's been discussed in neovim 1 , 2 . Currently gVim cannot have separate 'toplevel' windows for the same process/session. There is a TODO item to implement an inter-process communication system between multiple Vim instances to make it behave as though the separate processes are unified. (See :help todo and search for \"top-level\".) There is an interesting hax formalized in here which I will want to have time to test. Another solution would be to try to use neovim remote", "title": "Tabs vs Buffers"}, {"location": "vim_tabs/#stop-the-tab-madness", "text": "If you are using tabs like this then you are really limiting yourself and using a feature of Vim that wasn't meant to work like this. Before I explain that, let\u2019s be sure we understand what a buffer is in Vim as well as a few other basic things. After that, I\u2019ll explain the correct way to use tabs within Vim.", "title": "Stop the Tab Madness"}, {"location": "vim_tabs/#buffers", "text": "A buffer is nothing more than text that you are editing. For example, when you open a file, the content of the file is loaded into a buffer. So when you issue this command: vim .vimrc You are actually launching Vim with a single buffer that is filled with the contents of the .vimrc file. Now let\u2019s look at what happens when you try to edit multiple files. Let\u2019s issue this command: vim .vimrc .bashrc In Vim run :bnext Vim does what it did before, but instead of just 1 buffer, it opens another buffer that is filled with .bashrc . So now we have two buffers open. If you want to pause editing .vimrc and move to .bashrc , you could run this command in Vim :bnext which will show the .bashrc buffer. There are various other commands to manipulate buffers which you can see if you type :h buffer-list . Or you can use easymotion .", "title": "Buffers"}, {"location": "vim_tabs/#windows", "text": "A window in Vim is just a way to view a buffer. Whenever you create a new vertical or horizontal split, that is a window. For example, if you were to type in :help window , it would launch a new window that shows the help documentation. The important thing to note is that a window can view any buffer it wishes; it isn't forced to look at the same buffer. When editing a file, if we type :vsplit , we will get a vertical split and in the other window, we will see the current buffer we are editing. That should no longer be confusing because a window lets us look at any buffer. It just so happens that when creating a new split: :split or :vsplit , the buffer that we view is just the current one. By running any of the buffer commands from :h buffer-list , we can modify which buffer a window is viewing. For an example of this, by running the following commands, we will start editing two files in Vim, open a new window by horizontally splitting, and then view the second buffer in the original window. vim .vimrc .bashrc In Vim run: :split and :bnext", "title": "Windows"}, {"location": "vim_tabs/#so-a-tab-is", "text": "So now that we know what a buffer is and what a window is. Here is what Vim says in the Vim documentation regarding a buffer/window/tab: Summary: A buffer is the in-memory text of a file. A window is a viewport on a buffer. A tab page is a collection of windows. According to the documentation, a tab is just a collection of windows. This goes back to our earlier definition in that a tab is really just a layout. A tab is only designed to give you a different layout of windows.", "title": "So a Tab is\u2026?"}, {"location": "vim_tabs/#the-tab-problem", "text": "Tabs were only designed to let us have different layouts of windows. They aren't intended to be where an open file lives; an open file is instead loaded into a buffer. If you can view the same buffer across all tabs, how is this like a normal tab in most other editors? If you try to force a single tab to point to a single buffer, that is just futile. Vim just wasn't meant to work like this.", "title": "The Tab Problem"}, {"location": "vim_tabs/#the-buffer-solution", "text": "To reconcile all of this and learn how to use Vim\u2019s buffers/windows effectively, it might be useful to stop using tabs altogether until you understand how to edit with just using buffers/windows. The first thing I did was install a plugin that allows me to visualize all the buffers open across the top. I use bufferline for this. Instead of replicating tabs across the top like we did in the previous solution, we are instead going to use the power of being able to open many buffers simultaneously without worrying about which ones are open. In my experience, CtrlP gives a powerful fuzzy finder to navigate through the buffers. Instead of worrying about closing buffers and managing your pseudo-tabs that was mentioned in the previous solution, you just open files that you want to edit using CtrlP and don't worry about closing buffers or how many you have opened. When you are done editing a file, you just save it and then open CtrlP and continue onto the next file. CtrlP offers a few different ways to fuzzy find. You can use the following fuzziness: Find in your current directory. Find within all your open buffers. Find within all your open buffers sorted by Most Recently Used (MRU). Find with a mix of all the above.", "title": "The Buffer Solution"}, {"location": "vim_tabs/#using-tabs-correctly", "text": "This doesn't mean you should stop using tabs altogether. You should just use them how Vim intended you to use them. Instead you should use them to change the layout among windows. Imagine you are working on a C project. It might be helpful to have one tab dedicated to normal editing, but another tab for using a vertical split for the file.h and file.c files to make editing between them easier. Tabs also work really well to divide up what you are working on. You could be working on one part of the project in one tab and another part of the project in another tab. Just remember though, if you are using a single tab for each file, that isn't how tabs in Vim were designed to be used.", "title": "Using Tabs Correctly"}, {"location": "vim_tabs/#default-option-when-switching", "text": "The default behavior when trying to switch the buffer is to not allow you to change buffer if it's not saved, but we can change it if we set one of the next options: set hidden : allow to switch buffers even though it's changes aren't saved. set autowrite : Auto save when switching buffers.", "title": "Default option when switching"}, {"location": "vim_tabs/#share-buffers-and-all-vim-information-between-vim-instances", "text": "This is not my ideal behavior, nvim should let the user use the window manager to manage the windows... duh, instead of vsplitting buffers or using tabs. But sadly as of Nvim 0.1.7 and Vim 8.0 it's not implemented . You have the --server option but it only sends files to the already opened vim instance. you can't connect two vim instances to the same buffer pool. It's been discussed in neovim 1 , 2 . Currently gVim cannot have separate 'toplevel' windows for the same process/session. There is a TODO item to implement an inter-process communication system between multiple Vim instances to make it behave as though the separate processes are unified. (See :help todo and search for \"top-level\".) There is an interesting hax formalized in here which I will want to have time to test. Another solution would be to try to use neovim remote", "title": "Share buffers and all vim information between vim instances."}, {"location": "virtual_assistant/", "text": "Virtual assistant is a software agent that can perform tasks or services for an individual based on commands or questions. Of the open source solutions kalliope is the one I've liked most. I've also looked at mycroft but it seems less oriented to self hosted solutions, although it's possible . Mycroft has a bigger community behind though. To interact with it I may start with the android app , but then I'll probably install a Raspberry pi zero with Pirate Audio and an akaso external mic in the kitchen to speed up the grocy inventory management. STT \u2691 The only self hosted Speech-To-Text (STT) solution available now is CMUSphinx , which is based on pocketsphinx that has 2.8k stars but last update was on 28 th of March of 2020. The CMUSphinx documentation suggest you to use Vosk based on vosk-api with 1.2k stars and last updated 2 days ago. There is an open issue to support it in kalliope, with already a french proposal . That led me to the issue to support DeepSpeech , Mozilla's STT solution , that has 16.5k stars and updated 3 days ago, so it would be the way to go in my opinion if the existent one fails. Right now there is no support, but this would be the place to start. For spanish, based on the mozilla discourse thread I arrived to DeepSpeech-Polyglot that has taken many datasets such as Common Voice one and generated the models .", "title": "Virtual assistant"}, {"location": "virtual_assistant/#stt", "text": "The only self hosted Speech-To-Text (STT) solution available now is CMUSphinx , which is based on pocketsphinx that has 2.8k stars but last update was on 28 th of March of 2020. The CMUSphinx documentation suggest you to use Vosk based on vosk-api with 1.2k stars and last updated 2 days ago. There is an open issue to support it in kalliope, with already a french proposal . That led me to the issue to support DeepSpeech , Mozilla's STT solution , that has 16.5k stars and updated 3 days ago, so it would be the way to go in my opinion if the existent one fails. Right now there is no support, but this would be the place to start. For spanish, based on the mozilla discourse thread I arrived to DeepSpeech-Polyglot that has taken many datasets such as Common Voice one and generated the models .", "title": "STT"}, {"location": "vite/", "text": "Vite is a build tool that aims to provide a faster and leaner development experience for modern web projects. It consists of two major parts: A dev server that provides rich feature enhancements over native ES modules, for example extremely fast Hot Module Replacement (HMR). A build command that bundles your code with Rollup, pre-configured to output highly optimized static assets for production. Vite is opinionated and comes with sensible defaults out of the box, but is also highly extensible via its Plugin API and JavaScript API with full typing support. References \u2691 Docs", "title": "Vite"}, {"location": "vite/#references", "text": "Docs", "title": "References"}, {"location": "vitest/", "text": "Vitest is a blazing fast unit-test framework powered by Vite. Install \u2691 Add it to your project with: npm install -D vitest If you've used Vite, Vitest will read the configuration from the vite.config.js file so add the test property there. /// <reference types=\"vitest\" /> import { defineConfig } from 'vite' export default defineConfig ({ test : { // ... }, }) To run the tests use npx vitest , to see the coverage use npx vitest --coverage . References \u2691 Home", "title": "Vitest"}, {"location": "vitest/#install", "text": "Add it to your project with: npm install -D vitest If you've used Vite, Vitest will read the configuration from the vite.config.js file so add the test property there. /// <reference types=\"vitest\" /> import { defineConfig } from 'vite' export default defineConfig ({ test : { // ... }, }) To run the tests use npx vitest , to see the coverage use npx vitest --coverage .", "title": "Install"}, {"location": "vitest/#references", "text": "Home", "title": "References"}, {"location": "vscodium/", "text": "VSCodium are binary releases of VS Code without MS branding/telemetry/licensing. References \u2691 Home Git", "title": "VSCodium"}, {"location": "vscodium/#references", "text": "Home Git", "title": "References"}, {"location": "vue_snippets/", "text": "Apply a style to a component given a condition \u2691 if you use :class you can write javascript code in the value, for example: < b-form-radio class = \"user-retrieve-language p-2\" :class = \"{'font-weight-bold': selected === language.key}\" v-for = \"language in languages\" v-model = \"selected\" :id = \"language.key\" :checked = \"selected === language.key\" :value = \"language.key\" > Get assets url \u2691 If you're using Vite, you can save the assets such as images or audios in the src/assets directory, and you can get the url with: getImage () { return new URL ( `../assets/pictures/ ${ this . active_id } .jpg` , import . meta . url ). href }, This way it will give you the correct url whether you're in the development environment or in production. Play audio files \u2691 You can get the file and save it into a data element with: getAudio () { this . audio = new Audio ( new URL ( `../assets/audio/ ${ this . active_id } .mp3` , import . meta . url ). href ) }, You can start playing with this.audio.play() , and stop with this.audio.pause() . Run function in background \u2691 To achieve that you need to use the javascript method called setInterval() . It\u2019s a simple function that would repeat the same task over and over again. Here\u2019s an example: function myFunction () { setInterval ( function (){ alert ( \"Hello world\" ); }, 3000 ); } If you add a call to this method for any button and click on it, it will print Hello world every 3 seconds (3000 milliseconds) until you close the page. In Vue you could do something like: <script> export default { data: () => ({ inbox_retry: undefined }), methods: { retryGetInbox() { this.inbox_retry = setInterval(() => { if (this.showError) { console.log('Retrying the fetch of the inbox') // Add your code here. } else { clearInterval(this.inbox_retry) } }, 30000) } }, You can call this.retryGetInbox() whenever you want to start running the function periodically. Once this.showError is false , we stop running the function with clearInterval(this.inbox_retry) . Truncate text given a height \u2691 By default css is able to truncate text with the size of the screen but only on one line, if you want to fill up a portion of the screen (specified in number of lines or height css parameter) and then truncate all the text that overflows, you need to use vue-clamp . They have a nice demo in their page where you can see their features. Installation \u2691 If you're lucky and this issue has been solved, you can simply: npm i --save vue-clamp Else you need to create the Vue component yourself VueClamp.vue <script> import { addListener, removeListener } from \"resize-detector\"; import { defineComponent } from \"vue\"; import { h } from \"vue\"; export default defineComponent({ name: \"vue-clamp\", props: { tag: { type: String, default: \"div\", }, autoresize: { type: Boolean, default: false, }, maxLines: Number, maxHeight: [String, Number], ellipsis: { type: String, default: \"\u2026\", }, location: { type: String, default: \"end\", validator(value) { return [\"start\", \"middle\", \"end\"].indexOf(value) !== -1; }, }, expanded: Boolean, }, data() { return { offset: null, text: this.getText(), localExpanded: !!this.expanded, }; }, computed: { clampedText() { if (this.location === \"start\") { return this.ellipsis + (this.text.slice(0, this.offset) || \"\").trim(); } else if (this.location === \"middle\") { const split = Math.floor(this.offset / 2); return ( (this.text.slice(0, split) || \"\").trim() + this.ellipsis + (this.text.slice(-split) || \"\").trim() ); } return (this.text.slice(0, this.offset) || \"\").trim() + this.ellipsis; }, isClamped() { if (!this.text) { return false; } return this.offset !== this.text.length; }, realText() { return this.isClamped ? this.clampedText : this.text; }, realMaxHeight() { if (this.localExpanded) { return null; } const { maxHeight } = this; if (!maxHeight) { return null; } return typeof maxHeight === \"number\" ? `${maxHeight}px` : maxHeight; }, }, watch: { expanded(val) { this.localExpanded = val; }, localExpanded(val) { if (val) { this.clampAt(this.text.length); } else { this.update(); } if (this.expanded !== val) { this.$emit(\"update:expanded\", val); } }, isClamped: { handler(val) { this.$nextTick(() => this.$emit(\"clampchange\", val)); }, immediate: true, }, }, mounted() { this.init(); this.$watch( (vm) => [vm.maxLines, vm.maxHeight, vm.ellipsis, vm.isClamped].join(), this.update ); this.$watch((vm) => [vm.tag, vm.text, vm.autoresize].join(), this.init); }, updated() { this.text = this.getText(); this.applyChange(); }, beforeUnmount() { this.cleanUp(); }, methods: { init() { const contents = this.$slots.default(); if (!contents) { return; } this.offset = this.text.length; this.cleanUp(); if (this.autoresize) { addListener(this.$el, this.update); this.unregisterResizeCallback = () => { removeListener(this.$el, this.update); }; } this.update(); }, update() { if (this.localExpanded) { return; } this.applyChange(); if (this.isOverflow() || this.isClamped) { this.search(); } }, expand() { this.localExpanded = true; }, collapse() { this.localExpanded = false; }, toggle() { this.localExpanded = !this.localExpanded; }, getLines() { return Object.keys( Array.prototype.slice .call(this.$refs.content.getClientRects()) .reduce((prev, { top, bottom }) => { const key = `${top}/${bottom}`; if (!prev[key]) { prev[key] = true; } return prev; }, {}) ).length; }, isOverflow() { if (!this.maxLines && !this.maxHeight) { return false; } if (this.maxLines) { if (this.getLines() > this.maxLines) { return true; } } if (this.maxHeight) { if (this.$el.scrollHeight > this.$el.offsetHeight) { return true; } } return false; }, getText() { // Look for the first non-empty text node const [content] = (this.$slots.default() || []).filter( (node) => !node.tag && !node.isComment ); return content ? content.children : \"\"; }, moveEdge(steps) { this.clampAt(this.offset + steps); }, clampAt(offset) { this.offset = offset; this.applyChange(); }, applyChange() { this.$refs.text.textContent = this.realText; }, stepToFit() { this.fill(); this.clamp(); }, fill() { while ( (!this.isOverflow() || this.getLines() < 2) && this.offset < this.text.length ) { this.moveEdge(1); } }, clamp() { while (this.isOverflow() && this.getLines() > 1 && this.offset > 0) { this.moveEdge(-1); } }, search(...range) { const [from = 0, to = this.offset] = range; if (to - from <= 3) { this.stepToFit(); return; } const target = Math.floor((to + from) / 2); this.clampAt(target); if (this.isOverflow()) { this.search(from, target); } else { this.search(target, to); } }, cleanUp() { if (this.unregisterResizeCallback) { this.unregisterResizeCallback(); } }, }, render() { const contents = [ h( \"span\", { ref: \"text\", attrs: { \"aria-label\": this.text?.trim(), }, }, this.realText ), ]; const { expand, collapse, toggle } = this; const scope = { expand, collapse, toggle, clamped: this.isClamped, expanded: this.localExpanded, }; const before = this.$slots.before ? this.$slots.before(scope) : this.$slots.before; if (before) { contents.unshift(...(Array.isArray(before) ? before : [before])); } const after = this.$slots.after ? this.$slots.after(scope) : this.$slots.after; if (after) { contents.push(...(Array.isArray(after) ? after : [after])); } const lines = [ h( \"span\", { style: { boxShadow: \"transparent 0 0\", }, ref: \"content\", }, contents ), ]; return h( this.tag, { style: { maxHeight: this.realMaxHeight, overflow: \"hidden\", }, }, lines ); }, }); </script> Usage \u2691 If you were able to install it with npm , use: <template> <v-clamp autoresize :max-lines=\"3\">{{ text }}</v-clamp> </template> <script> import VClamp from 'vue-clamp' export default { components: { VClamp }, data () { return { text: 'Some very very long text content.' } } } </script> Else use: <template> <VueClamp maxHeight=\"30vh\"> {{ text }} </VueClamp> </template> <script> import VueClamp from './VueClamp.vue' export default { components: { VueClamp }, data () { return { text: 'Some very very long text content.' } } } </script> Display time ago from timestamp \u2691 Install with: npm install vue2-timeago@next", "title": "Vue snippets"}, {"location": "vue_snippets/#apply-a-style-to-a-component-given-a-condition", "text": "if you use :class you can write javascript code in the value, for example: < b-form-radio class = \"user-retrieve-language p-2\" :class = \"{'font-weight-bold': selected === language.key}\" v-for = \"language in languages\" v-model = \"selected\" :id = \"language.key\" :checked = \"selected === language.key\" :value = \"language.key\" >", "title": "Apply a style to a component given a condition"}, {"location": "vue_snippets/#get-assets-url", "text": "If you're using Vite, you can save the assets such as images or audios in the src/assets directory, and you can get the url with: getImage () { return new URL ( `../assets/pictures/ ${ this . active_id } .jpg` , import . meta . url ). href }, This way it will give you the correct url whether you're in the development environment or in production.", "title": "Get assets url"}, {"location": "vue_snippets/#play-audio-files", "text": "You can get the file and save it into a data element with: getAudio () { this . audio = new Audio ( new URL ( `../assets/audio/ ${ this . active_id } .mp3` , import . meta . url ). href ) }, You can start playing with this.audio.play() , and stop with this.audio.pause() .", "title": "Play audio files"}, {"location": "vue_snippets/#run-function-in-background", "text": "To achieve that you need to use the javascript method called setInterval() . It\u2019s a simple function that would repeat the same task over and over again. Here\u2019s an example: function myFunction () { setInterval ( function (){ alert ( \"Hello world\" ); }, 3000 ); } If you add a call to this method for any button and click on it, it will print Hello world every 3 seconds (3000 milliseconds) until you close the page. In Vue you could do something like: <script> export default { data: () => ({ inbox_retry: undefined }), methods: { retryGetInbox() { this.inbox_retry = setInterval(() => { if (this.showError) { console.log('Retrying the fetch of the inbox') // Add your code here. } else { clearInterval(this.inbox_retry) } }, 30000) } }, You can call this.retryGetInbox() whenever you want to start running the function periodically. Once this.showError is false , we stop running the function with clearInterval(this.inbox_retry) .", "title": "Run function in background"}, {"location": "vue_snippets/#truncate-text-given-a-height", "text": "By default css is able to truncate text with the size of the screen but only on one line, if you want to fill up a portion of the screen (specified in number of lines or height css parameter) and then truncate all the text that overflows, you need to use vue-clamp . They have a nice demo in their page where you can see their features.", "title": "Truncate text given a height"}, {"location": "vue_snippets/#installation", "text": "If you're lucky and this issue has been solved, you can simply: npm i --save vue-clamp Else you need to create the Vue component yourself VueClamp.vue <script> import { addListener, removeListener } from \"resize-detector\"; import { defineComponent } from \"vue\"; import { h } from \"vue\"; export default defineComponent({ name: \"vue-clamp\", props: { tag: { type: String, default: \"div\", }, autoresize: { type: Boolean, default: false, }, maxLines: Number, maxHeight: [String, Number], ellipsis: { type: String, default: \"\u2026\", }, location: { type: String, default: \"end\", validator(value) { return [\"start\", \"middle\", \"end\"].indexOf(value) !== -1; }, }, expanded: Boolean, }, data() { return { offset: null, text: this.getText(), localExpanded: !!this.expanded, }; }, computed: { clampedText() { if (this.location === \"start\") { return this.ellipsis + (this.text.slice(0, this.offset) || \"\").trim(); } else if (this.location === \"middle\") { const split = Math.floor(this.offset / 2); return ( (this.text.slice(0, split) || \"\").trim() + this.ellipsis + (this.text.slice(-split) || \"\").trim() ); } return (this.text.slice(0, this.offset) || \"\").trim() + this.ellipsis; }, isClamped() { if (!this.text) { return false; } return this.offset !== this.text.length; }, realText() { return this.isClamped ? this.clampedText : this.text; }, realMaxHeight() { if (this.localExpanded) { return null; } const { maxHeight } = this; if (!maxHeight) { return null; } return typeof maxHeight === \"number\" ? `${maxHeight}px` : maxHeight; }, }, watch: { expanded(val) { this.localExpanded = val; }, localExpanded(val) { if (val) { this.clampAt(this.text.length); } else { this.update(); } if (this.expanded !== val) { this.$emit(\"update:expanded\", val); } }, isClamped: { handler(val) { this.$nextTick(() => this.$emit(\"clampchange\", val)); }, immediate: true, }, }, mounted() { this.init(); this.$watch( (vm) => [vm.maxLines, vm.maxHeight, vm.ellipsis, vm.isClamped].join(), this.update ); this.$watch((vm) => [vm.tag, vm.text, vm.autoresize].join(), this.init); }, updated() { this.text = this.getText(); this.applyChange(); }, beforeUnmount() { this.cleanUp(); }, methods: { init() { const contents = this.$slots.default(); if (!contents) { return; } this.offset = this.text.length; this.cleanUp(); if (this.autoresize) { addListener(this.$el, this.update); this.unregisterResizeCallback = () => { removeListener(this.$el, this.update); }; } this.update(); }, update() { if (this.localExpanded) { return; } this.applyChange(); if (this.isOverflow() || this.isClamped) { this.search(); } }, expand() { this.localExpanded = true; }, collapse() { this.localExpanded = false; }, toggle() { this.localExpanded = !this.localExpanded; }, getLines() { return Object.keys( Array.prototype.slice .call(this.$refs.content.getClientRects()) .reduce((prev, { top, bottom }) => { const key = `${top}/${bottom}`; if (!prev[key]) { prev[key] = true; } return prev; }, {}) ).length; }, isOverflow() { if (!this.maxLines && !this.maxHeight) { return false; } if (this.maxLines) { if (this.getLines() > this.maxLines) { return true; } } if (this.maxHeight) { if (this.$el.scrollHeight > this.$el.offsetHeight) { return true; } } return false; }, getText() { // Look for the first non-empty text node const [content] = (this.$slots.default() || []).filter( (node) => !node.tag && !node.isComment ); return content ? content.children : \"\"; }, moveEdge(steps) { this.clampAt(this.offset + steps); }, clampAt(offset) { this.offset = offset; this.applyChange(); }, applyChange() { this.$refs.text.textContent = this.realText; }, stepToFit() { this.fill(); this.clamp(); }, fill() { while ( (!this.isOverflow() || this.getLines() < 2) && this.offset < this.text.length ) { this.moveEdge(1); } }, clamp() { while (this.isOverflow() && this.getLines() > 1 && this.offset > 0) { this.moveEdge(-1); } }, search(...range) { const [from = 0, to = this.offset] = range; if (to - from <= 3) { this.stepToFit(); return; } const target = Math.floor((to + from) / 2); this.clampAt(target); if (this.isOverflow()) { this.search(from, target); } else { this.search(target, to); } }, cleanUp() { if (this.unregisterResizeCallback) { this.unregisterResizeCallback(); } }, }, render() { const contents = [ h( \"span\", { ref: \"text\", attrs: { \"aria-label\": this.text?.trim(), }, }, this.realText ), ]; const { expand, collapse, toggle } = this; const scope = { expand, collapse, toggle, clamped: this.isClamped, expanded: this.localExpanded, }; const before = this.$slots.before ? this.$slots.before(scope) : this.$slots.before; if (before) { contents.unshift(...(Array.isArray(before) ? before : [before])); } const after = this.$slots.after ? this.$slots.after(scope) : this.$slots.after; if (after) { contents.push(...(Array.isArray(after) ? after : [after])); } const lines = [ h( \"span\", { style: { boxShadow: \"transparent 0 0\", }, ref: \"content\", }, contents ), ]; return h( this.tag, { style: { maxHeight: this.realMaxHeight, overflow: \"hidden\", }, }, lines ); }, }); </script>", "title": "Installation"}, {"location": "vue_snippets/#usage", "text": "If you were able to install it with npm , use: <template> <v-clamp autoresize :max-lines=\"3\">{{ text }}</v-clamp> </template> <script> import VClamp from 'vue-clamp' export default { components: { VClamp }, data () { return { text: 'Some very very long text content.' } } } </script> Else use: <template> <VueClamp maxHeight=\"30vh\"> {{ text }} </VueClamp> </template> <script> import VueClamp from './VueClamp.vue' export default { components: { VueClamp }, data () { return { text: 'Some very very long text content.' } } } </script>", "title": "Usage"}, {"location": "vue_snippets/#display-time-ago-from-timestamp", "text": "Install with: npm install vue2-timeago@next", "title": "Display time ago from timestamp"}, {"location": "vuejs/", "text": "Vue.js is a progressive JavaScript framework for building user interfaces. It builds on top of standard HTML, CSS and JavaScript, and provides a declarative and component-based programming model that helps you efficiently develop user interfaces, be it simple or complex. Here is a minimal example: import { createApp } from 'vue' createApp ({ data () { return { count : 0 } } }). mount ( '#app' ) < div id = \"app\" > < button @ click = \"count++\" > Count is: {{ count }} </ button > </ div > The above example demonstrates the two core features of Vue: Declarative Rendering : Vue extends standard HTML with a template syntax that allows us to declaratively describe HTML output based on JavaScript state. Reactivity : Vue automatically tracks JavaScript state changes and efficiently updates the DOM when changes happen. Features \u2691 Single file components \u2691 Single-File Component (also known as *.vue files, abbreviated as SFC) encapsulates the component's logic (JavaScript), template (HTML), and styles (CSS) in a single file. Here's the previous example, written in SFC format: <script> export default { data() { return { count: 0 } } } </script> <template> <button @click=\"count++\">Count is: {{ count }}</button> </template> <style scoped> button { font-weight: bold; } </style> API Styles \u2691 Vue components can be authored in two different API styles: Options API and Composition API. Options API \u2691 With Options API, we define a component's logic using an object of options such as data , methods , and mounted . Properties defined by options are exposed on this inside functions, which points to the component instance: <script> export default { // Properties returned from data() becomes reactive state // and will be exposed on `this`. data() { return { count: 0 } }, // Methods are functions that mutate state and trigger updates. // They can be bound as event listeners in templates. methods: { increment() { this.count++ } }, // Lifecycle hooks are called at different stages // of a component's lifecycle. // This function will be called when the component is mounted. mounted() { console.log(`The initial count is ${this.count}.`) } } </script> <template> <button @click=\"increment\">Count is: {{ count }}</button> </template> The Options API is centered around the concept of a \"component instance\" ( this as seen in the example), which typically aligns better with a class-based mental model for users coming from OOP language backgrounds. It is also more beginner-friendly by abstracting away the reactivity details and enforcing code organisation via option groups. Composition API \u2691 With Composition API, we define a component's logic using imported API functions. In SFCs, Composition API is typically used with <script setup> . The setup attribute is a hint that makes Vue perform compile-time transforms that allow us to use Composition API with less boilerplate. For example, imports and top-level variables / functions declared in <script setup> are directly usable in the template. Here is the same component, with the exact same template, but using Composition API and <script setup> instead: <script setup> import { ref, onMounted } from 'vue' // reactive state const count = ref(0) // functions that mutate state and trigger updates function increment() { count.value++ } // lifecycle hooks onMounted(() => { console.log(`The initial count is ${count.value}.`) }) </script> <template> <button @click=\"increment\">Count is: {{ count }}</button> </template> The Composition API is centered around declaring reactive state variables directly in a function scope, and composing state from multiple functions together to handle complexity. It is more free-form, and requires understanding of how reactivity works in Vue to be used effectively. In return, its flexibility enables more powerful patterns for organizing and reusing logic. Initialize a project \u2691 To create a build-tool-enabled Vue project on your machine, run the following command in your command line. If you don't have npm follow these instructions . npm init vue@latest This command will install and execute create-vue, the official Vue project scaffolding tool. You will be presented with prompts for a number of optional features such as TypeScript and testing support. If you are unsure about an option, simply choose No . Follow their instructions. Once the project is created, follow the instructions to install dependencies and start the dev server: cd <your-project-name> npm install npm run dev When you are ready to ship your app to production, run the following: npm run build The basics \u2691 Creating a Vue Application \u2691 Every Vue application starts by creating a new application instance with the createApp function: import { createApp } from 'vue' const app = createApp({ /* root component options */ }) The object we are passing into createApp is in fact a component. Every app requires a \"root component\" that can contain other components as its children. If you are using Single-File Components, we typically import the root component from another file: import { createApp } from 'vue' // import the root component App from a single-file component. import App from './App.vue' const app = createApp(App) An application instance won't render anything until its .mount() method is called. It expects a \"container\" argument, which can either be an actual DOM element or a selector string: < div id = \"app\" ></ div > app . mount ( '#app' ) The content of the app's root component will be rendered inside the container element. The container element itself is not considered part of the app. The .mount() method should always be called after all app configurations and asset registrations are done. Also note that its return value, unlike the asset registration methods, is the root component instance instead of the application instance. You are not limited to a single application instance on the same page. The createApp API allows multiple Vue applications to co-exist on the same page, each with its own scope for configuration and global assets: const app1 = createApp ({ /* ... */ }) app1 . mount ( '#container-1' ) const app2 = createApp ({ /* ... */ }) app2 . mount ( '#container-2' ) App configurations \u2691 The application instance exposes a .config object that allows us to configure a few app-level options, for example defining an app-level error handler that captures errors from all descendent components: app . config . errorHandler = ( err ) => { /* handle error */ } The application instance also provides a few methods for registering app-scoped assets. For example, registering a component: app . component ( 'TodoDeleteButton' , TodoDeleteButton ) This makes the TodoDeleteButton available for use anywhere in our app. You can use also environment variables Declarative rendering \u2691 The core feature of Vue is declarative rendering: using a template syntax that extends HTML, we can describe how the HTML should look like based on JavaScript state. When the state changes, the HTML updates automatically. State that can trigger updates when changed are considered reactive. In Vue, reactive state is held in components. We can declare reactive state using the data component option, which should be a function that returns an object: export default { data () { return { message : 'Hello World!' } } } The message property will be made available in the template. This is how we can render dynamic text based on the value of message, using mustaches syntax: < h1 > {{ message }} </ h1 > The double mustaches interprets the data as plain text, not HTML. In order to output real HTML, you will need to use the v-html directive, although you should try to avoid it for security reasons. Directives are prefixed with v- to indicate that they are special attributes provided by Vue, they apply special reactive behavior to the rendered DOM. Attribute bindings \u2691 To bind an attribute to a dynamic value, we use the v-bind directive: < div v-bind:id = \"dynamicId\" ></ div > A directive is a special attribute that starts with the v- prefix. They are part of Vue's template syntax. Similar to text interpolations, directive values are JavaScript expressions that have access to the component's state. The part after the colon ( :id ) is the \"argument\" of the directive. Here, the element's id attribute will be synced with the dynamicId property from the component's state. Because v-bind is used so frequently, it has a dedicated shorthand syntax: < div :id = \"dynamicId\" ></ div > Class binding \u2691 For example to turn the h1 in red: <script> export default { data() { return { titleClass: 'title' } } } </script> <template> <h1 :class='titleClass'>Make me red</h1> <!-- add dynamic class binding here --> </template> <style> .title { color: red; } </style> You can have multiple classes toggled by having more fields in the object. In addition, the :class directive can also co-exist with the plain class attribute. So given the following state: data () { return { isActive : true , hasError : false } } And the following template: < div class = \"static\" :class = \"{ active: isActive, 'text-danger': hasError }\" ></ div > It will render: < div class = \"static active\" ></ div > When isActive or hasError changes, the class list will be updated accordingly. For example, if hasError becomes true, the class list will become static active text-danger . The bound object doesn't have to be inline: data () { return { classObject : { active : true , 'text-danger' : false } } } < div :class = \"classObject\" ></ div > This will render the same result. We can also bind to a computed property that returns an object. This is a common and powerful pattern: data () { return { isActive : true , error : null } }, computed : { classObject () { return { active : this . isActive && ! this . error , 'text-danger' : this . error && this . error . type === 'fatal' } } } < div :class = \"classObject\" ></ div > Style binding \u2691 :style supports binding to JavaScript object values. data () { return { activeColor : 'red' , fontSize : 30 } } < div :style = \"{ color: activeColor, fontSize: fontSize + 'px' }\" ></ div > It is often a good idea to bind to a style object directly so that the template is cleaner: data () { return { styleObject : { color : 'red' , fontSize : '13px' } } } < div :style = \"styleObject\" ></ div > Again, object style binding is often used in conjunction with computed properties that return objects. Event listeners \u2691 We can listen to DOM events using the v-on directive: <button v-on:click=\"increment\">{{ count }}</button> Due to its frequent use, v-on also has a shorthand syntax: <button @click=\"increment\">{{ count }}</button> Here, increment references a function declared using the methods option: export default { data () { return { count : 0 } }, methods : { increment () { // update component state this . count ++ } } } Inside a method, we can access the component instance using this . The component instance exposes the data properties declared by data. We can update the component state by mutating these properties. You should avoid using arrow functions when defining methods, as that prevents Vue from binding the appropriate this value. Event modifiers \u2691 It is a very common need to call event.preventDefault() or event.stopPropagation() inside event handlers. Although we can do this easily inside methods, it would be better if the methods can be purely about data logic rather than having to deal with DOM event details. To address this problem, Vue provides event modifiers for v-on . Recall that modifiers are directive postfixes denoted by a dot. .stop .prevent .self .capture .once .passive <!-- the click event's propagation will be stopped --> < a @ click . stop = \"doThis\" ></ a > <!-- the submit event will no longer reload the page --> < form @ submit . prevent = \"onSubmit\" ></ form > <!-- modifiers can be chained --> < a @ click . stop . prevent = \"doThat\" ></ a > <!-- just the modifier --> < form @ submit . prevent ></ form > <!-- only trigger handler if event.target is the element itself --> <!-- i.e. not from a child element --> < div @ click . self = \"doThat\" > ... </ div > <!-- use capture mode when adding the event listener --> <!-- i.e. an event targeting an inner element is handled here before being handled by that element --> < div @ click . capture = \"doThis\" > ... </ div > <!-- the click event will be triggered at most once --> < a @ click . once = \"doThis\" ></ a > <!-- the scroll event's default behavior (scrolling) will happen --> <!-- immediately, instead of waiting for `onScroll` to complete --> <!-- in case it contains `event.preventDefault()` --> < div @ scroll . passive = \"onScroll\" > ... </ div > Key Modifiers \u2691 When listening for keyboard events, we often need to check for specific keys. Vue allows adding key modifiers for v-on or @ when listening for key events: <!-- only call `vm.submit()` when the `key` is `Enter` --> < input @ keyup . enter = \"submit\" /> You can directly use any valid key names exposed via KeyboardEvent.key as modifiers by converting them to kebab-case. < input @ keyup . page-down = \"onPageDown\" /> Vue provides aliases for the most commonly used keys: .enter .tab .delete (captures both \"Delete\" and \"Backspace\" keys) .esc .space .up .down .left .right You can use the following modifiers to trigger mouse or keyboard event listeners only when the corresponding modifier key is pressed: .ctrl .alt .shift .meta For example: <!-- Alt + Enter --> < input @ keyup . alt . enter = \"clear\" /> <!-- Ctrl + Click --> < div @ click . ctrl = \"doSomething\" > Do something </ div > The .exact modifier allows control of the exact combination of system modifiers needed to trigger an event. <!-- this will fire even if Alt or Shift is also pressed --> < button @ click . ctrl = \"onClick\" > A </ button > <!-- this will only fire when Ctrl and no other keys are pressed --> < button @ click . ctrl . exact = \"onCtrlClick\" > A </ button > <!-- this will only fire when no system modifiers are pressed --> < button @ click . exact = \"onClick\" > A </ button > Mouse Button Modifiers \u2691 .left .right .middle These modifiers restrict the handler to events triggered by a specific mouse button. Form bindings \u2691 Basic usage \u2691 Text \u2691 Using v-bind and v-on together, we can create two-way bindings on form input elements: <input :value=\"text\" @input=\"onInput\"> <p>{{ text }}</p> methods : { onInput ( e ) { // a v-on handler receives the native DOM event // as the argument. this . text = e . target . value } } To simplify two-way bindings, Vue provides a directive, v-model , which is essentially a syntax sugar for the above: < input v-model = \"text\" > v-model automatically syncs the <input> 's value with the bound state, so we no longer need to use a event handler for that. v-model works not only on text inputs, but also other input types such as checkboxes, radio buttons, and select dropdowns. Multiline text \u2691 < span > Multiline message is: </ span > < p style = \"white-space: pre-line;\" > {{ message }} </ p > < textarea v-model = \"message\" placeholder = \"add multiple lines\" ></ textarea > Checkbox \u2691 < input type = \"checkbox\" id = \"checkbox\" v-model = \"checked\" /> < label for = \"checkbox\" > {{ checked }} </ label > We can also bind multiple checkboxes to the same array or Set value: export default { data () { return { checkedNames : [] } } } < div > Checked names: {{ checkedNames }} </ div > < input type = \"checkbox\" id = \"jack\" value = \"Jack\" v-model = \"checkedNames\" > < label for = \"jack\" > Jack </ label > < input type = \"checkbox\" id = \"john\" value = \"John\" v-model = \"checkedNames\" > < label for = \"john\" > John </ label > < input type = \"checkbox\" id = \"mike\" value = \"Mike\" v-model = \"checkedNames\" > < label for = \"mike\" > Mike </ label > Radio checkboxes \u2691 < div > Picked: {{ picked }} </ div > < input type = \"radio\" id = \"one\" value = \"One\" v-model = \"picked\" /> < label for = \"one\" > One </ label > < input type = \"radio\" id = \"two\" value = \"Two\" v-model = \"picked\" /> < label for = \"two\" > Two </ label > Select \u2691 Single select: < div > Selected: {{ selected }} </ div > < select v-model = \"selected\" > < option disabled value = \"\" > Please select one </ option > < option > A </ option > < option > B </ option > < option > C </ option > </ select > Multiple select (bound to array): < div > Selected: {{ selected }} </ div > < select v-model = \"selected\" multiple > < option > A </ option > < option > B </ option > < option > C </ option > </ select > Select options can be dynamically rendered with v-for : export default { data () { return { selected : 'A' , options : [ { text : 'One' , value : 'A' }, { text : 'Two' , value : 'B' }, { text : 'Three' , value : 'C' } ] } } } < select v-model = \"selected\" > < option v-for = \"option in options\" :value = \"option.value\" > {{ option.text }} </ option > </ select > < div > Selected: {{ selected }} </ div > Value bindings \u2691 For radio, checkbox and select options, the v-model binding values are usually static strings (or booleans for checkbox):. <!-- `picked` is a string \"a\" when checked --> < input type = \"radio\" v-model = \"picked\" value = \"a\" /> <!-- `toggle` is either true or false --> < input type = \"checkbox\" v-model = \"toggle\" /> <!-- `selected` is a string \"abc\" when the first option is selected --> < select v-model = \"selected\" > < option value = \"abc\" > ABC </ option > </ select > But sometimes we may want to bind the value to a dynamic property on the current active instance. We can use v-bind to achieve that. In addition, using v-bind allows us to bind the input value to non-string values. Checkbox \u2691 < input type = \"checkbox\" v-model = \"toggle\" true-value = \"yes\" false-value = \"no\" /> true-value and false-value are Vue-specific attributes that only work with v-model . Here the toggle property's value will be set to 'yes' when the box is checked, and set to 'no' when unchecked. You can also bind them to dynamic values using v-bind : < input type = \"checkbox\" v-model = \"toggle\" :true-value = \"dynamicTrueValue\" :false-value = \"dynamicFalseValue\" /> Radio \u2691 < input type = \"radio\" v-model = \"pick\" :value = \"first\" /> < input type = \"radio\" v-model = \"pick\" :value = \"second\" /> pick will be set to the value of first when the first radio input is checked, and set to the value of second when the second one is checked. Select Options \u2691 < select v-model = \"selected\" > <!-- inline object literal --> < option :value = \"{ number: 123 }\" > 123 </ option > </ select > v-model supports value bindings of non-string values as well! In the above example, when the option is selected, selected will be set to the object literal value of { number: 123 } . Form modifiers \u2691 .lazy \u2691 By default, v-model syncs the input with the data after each input event. You can add the lazy modifier to instead sync after change events: <!-- synced after \"change\" instead of \"input\" --> < input v-model . lazy = \"msg\" /> .number \u2691 If you want user input to be automatically typecast as a number, you can add the number modifier to your v-model managed inputs: < input v-model . number = \"age\" /> If the value cannot be parsed with parseFloat() , then the original value is used instead. The number modifier is applied automatically if the input has type=\"number\" . .trim \u2691 If you want whitespace from user input to be trimmed automatically, you can add the trim modifier to your v-model managed inputs: < input v-model . trim = \"msg\" /> Conditional rendering \u2691 We can use the v-if directive to conditionally render an element: < h1 v-if = \"awesome\" > Vue is awesome! </ h1 > This <h1> will be rendered only if the value of awesome is truthy. If awesome changes to a falsy value, it will be removed from the DOM. We can also use v-else and v-else-if to denote other branches of the condition: < h1 v-if = \"awesome\" > Vue is awesome! </ h1 > < h1 v-else > Oh no \ud83d\ude22 </ h1 > Because v-if is a directive, it has to be attached to a single element. But what if we want to toggle more than one element? In this case we can use v-if on a <template> element, which serves as an invisible wrapper. The final rendered result will not include the <template> element. < template v-if = \"ok\" > < h1 > Title </ h1 > < p > Paragraph 1 </ p > < p > Paragraph 2 </ p > </ template > Another option for conditionally displaying an element is the v-show directive. The usage is largely the same: < h1 v-show = \"ok\" > Hello! </ h1 > The difference is that an element with v-show will always be rendered and remain in the DOM; v-show only toggles the display CSS property of the element. v-show doesn't support the <template> element, nor does it work with v-else . Generally speaking, v-if has higher toggle costs while v-show has higher initial render costs. So prefer v-show if you need to toggle something very often, and prefer v-if if the condition is unlikely to change at runtime. List rendering \u2691 We can use the v-for directive to render a list of elements based on a source array: < ul > < li v-for = \"todo in todos\" :key = \"todo.id\" > {{ todo.text }} </ li > </ ul > Here todo is a local variable representing the array element currently being iterated on. It's only accessible on or inside the v-for element. Notice how we are also giving each todo object a unique id , and binding it as the special key attribute for each <li> . The key allows Vue to accurately move each <li> to match the position of its corresponding object in the array. There are two ways to update the list: Call mutating methods on the source array: this . todos . push ( newTodo ) Replace the array with a new one: this . todos = this . todos . filter ( /* ... */ ) Example: <script> // give each todo a unique id let id = 0 export default { data() { return { newTodo: '', todos: [ { id: id++, text: 'Learn HTML' }, { id: id++, text: 'Learn JavaScript' }, { id: id++, text: 'Learn Vue' } ] } }, methods: { addTodo() { this.todos.push({ id: id++, text: this.newTodo}) this.newTodo = '' }, removeTodo(todo) { this.todos = this.todos.filter((element) => element.id != todo.id) } } } </script> <template> <form @submit.prevent=\"addTodo\"> <input v-model=\"newTodo\"> <button>Add Todo</button> </form> <ul> <li v-for=\"todo in todos\" :key=\"todo.id\"> {{ todo.text }} <button @click=\"removeTodo(todo)\">X</button> </li> </ul> </template> v-for also supports an optional second alias for the index of the current item: data () { return { parentMessage : 'Parent' , items : [{ message : 'Foo' }, { message : 'Bar' }] } } < li v-for = \"(item, index) in items\" > {{ parentMessage }} - {{ index }} - {{ item.message }} </ li > Similar to template v-if , you can also use a <template> tag with v-for to render a block of multiple elements. For example: < ul > < template v-for = \"item in items\" > < li > {{ item.msg }} </ li > < li class = \"divider\" role = \"presentation\" ></ li > </ template > </ ul > It's not recommended to use v-if and v-for on the same element due to implicit precedence. Instead of: < li v-for = \"todo in todos\" v-if = \"!todo.isComplete\" > {{ todo.name }} </ li > Use: < template v-for = \"todo in todos\" > < li v-if = \"!todo.isComplete\" > {{ todo.name }} </ li > </ template > v-for with an object \u2691 You can also use v-for to iterate through the properties of an object. data () { return { myObject : { title : 'How to do lists in Vue' , author : 'Jane Doe' , publishedAt : '2016-04-10' } } } < ul > < li v-for = \"value in myObject\" > {{ value }} </ li > </ ul > You can also provide a second alias for the property's name: < li v-for = \"(value, key) in myObject\" > {{ key }}: {{ value }} </ li > And another for the index: < li v-for = \"(value, key, index) in myObject\" > {{ index }}. {{ key }}: {{ value }} </ li > v-for with a Range \u2691 v-for can also take an integer. In this case it will repeat the template that many times, based on a range of 1...n . < span v-for = \"n in 10\" > {{ n }} </ span > Note here n starts with an initial value of 1 instead of 0. v-for with a Component \u2691 You can directly use v-for on a component , like any normal element (don't forget to provide a key): < my-component v-for = \"item in items\" :key = \"item.id\" ></ my-component > However, this won't automatically pass any data to the component, because components have isolated scopes of their own. In order to pass the iterated data into the component, we should also use props: < my-component v-for = \"(item, index) in items\" :item = \"item\" :index = \"index\" :key = \"item.id\" ></ my-component > The reason for not automatically injecting item into the component is because that makes the component tightly coupled to how v-for works. Being explicit about where its data comes from makes the component reusable in other situations. Computed Property \u2691 We can declare a property that is reactively computed from other properties using the computed option: export default { // ... computed : { filteredTodos () { if ( this . hideCompleted ) { return this . todos . filter (( t ) => t . done === false ) } else { return this . todos } } } } } A computed property tracks other reactive state used in its computation as dependencies. It caches the result and automatically updates it when its dependencies change. So it's better than defining the function as a method Lifecycle hooks \u2691 Each Vue component instance goes through a series of initialization steps when it's created - for example, it needs to set up data observation, compile the template, mount the instance to the DOM, and update the DOM when data changes. Along the way, it also runs functions called lifecycle hooks, giving users the opportunity to add their own code at specific stages. For example, the mounted hook can be used to run code after the component has finished the initial rendering and created the DOM nodes: export default { mounted () { console . log ( `the component is now mounted.` ) } } There are also other hooks which will be called at different stages of the instance's lifecycle, with the most commonly used being mounted , updated , and unmounted . All lifecycle hooks are called with their this context pointing to the current active instance invoking it. Note this means you should avoid using arrow functions when declaring lifecycle hooks, as you won't be able to access the component instance via this if you do so. Template Refs \u2691 While Vue's declarative rendering model abstracts away most of the direct DOM operations for you, there may still be cases where we need direct access to the underlying DOM elements. To achieve this, we can use the special ref attribute: < input ref = \"input\" > ref allows us to obtain a direct reference to a specific DOM element or child component instance after it's mounted. This may be useful when you want to, for example, programmatically focus an input on component mount, or initialize a 3 rd party library on an element. The resulting ref is exposed on this.$refs : < script > export default { mounted () { this . $refs . input . focus () } } < /script> < template > < input ref = \"input\" /> </ template > Note that you can only access the ref after the component is mounted. If you try to access $refs.input in a template expression, it will be null on the first render. This is because the element doesn't exist until after the first render! Watchers \u2691 Computed properties allow us to declaratively compute derived values. However, there are cases where we need to perform \"side effects\" in reaction to state changes, for example, mutating the DOM, or changing another piece of state based on the result of an async operation. With Options API, we can use the watch option to trigger a function whenever a reactive property changes: export default { data () { return { question : '' , answer : 'Questions usually contain a question mark. ;-)' } }, watch : { // whenever question changes, this function will run question ( newQuestion , oldQuestion ) { if ( newQuestion . indexOf ( '?' ) > - 1 ) { this . getAnswer () } } }, methods : { async getAnswer () { this . answer = 'Thinking...' try { const res = await fetch ( 'https://yesno.wtf/api' ) this . answer = ( await res . json ()). answer } catch ( error ) { this . answer = 'Error! Could not reach the API. ' + error } } } } < p > Ask a yes/no question: < input v-model = \"question\" /> </ p > < p > {{ answer }} </ p > Deep watchers \u2691 watch is shallow by default: the callback will only trigger when the watched property has been assigned a new value - it won't trigger on nested property changes. If you want the callback to fire on all nested mutations, you need to use a deep watcher: export default { watch : { someObject : { handler ( newValue , oldValue ) { // Note: `newValue` will be equal to `oldValue` here // on nested mutations as long as the object itself // hasn't been replaced. }, deep : true } } } Note \"Deep watch requires traversing all nested properties in the watched object, and can be expensive when used on large data structures. Use it only when necessary and beware of the performance implications.\" Eager watchers \u2691 watch is lazy by default: the callback won't be called until the watched source has changed. But in some cases we may want the same callback logic to be run eagerly, for example, we may want to fetch some initial data, and then re-fetch the data whenever relevant state changes. We can force a watcher's callback to be executed immediately by declaring it using an object with a handler function and the immediate: true option: export default { // ... watch : { question : { handler ( newQuestion ) { // this will be run immediately on component creation. }, // force eager callback execution immediate : true } } // ... } Environment variables \u2691 If you're using Vue 3 and Vite you can use the environment variables by defining them in .env files. You can specify environment variables by placing the following files in your project root: .env : Loaded in all cases. .env.local : Loaded in all cases, ignored by git. .env.[mode] : Only loaded in specified mode. .env.[mode].local : Only loaded in specified mode, ignored by git. An env file simply contains key=value pairs of environment variables, by default only variables that start with VITE_ will be exposed.: DB_PASSWORD=foobar VITE_SOME_KEY=123 Only VITE_SOME_KEY will be exposed as import.meta.env.VITE_SOME_KEY to your client source code, but DB_PASSWORD will not. So for example in a component you can use: export default { props: {}, mounted() { console.log(import.meta.env.VITE_SOME_KEY) }, data: () => ({ }), } Make HTTP requests \u2691 There are many ways to do requests to external services: Fetch API Axios Fetch API \u2691 The Fetch API is a standard API for making HTTP requests on the browser. It a great alternative to the old XMLHttpRequestconstructor for making requests. It supports all kinds of requests, including GET, POST, PUT, PATCH, DELETE, and OPTIONS, which is what most people need. To make a request with the Fetch API, we don\u2019t have to do anything. All we have to do is to make the request directly with the fetch object. For instance, you can write: < template > < div id = \"app\" > {{data}} </ div > </ template >< script > export default { name : \"App\" , data () { return { data : {} } }, beforeMount (){ this . getName (); }, methods : { async getName (){ const res = await fetch ( 'https://api.agify.io/?name=michael' ); const data = await res . json (); this . data = data ; } } }; </ script > In the code above, we made a simple GET request from an API and then convert the data from JSON to a JavaScript object with the json() method. Adding headers \u2691 Like most HTTP clients, we can send request headers and bodies with the Fetch API. To send a request with HTTP headers, we can write: < template > < div id = \"app\" > < img :src = \"data.src.tiny\" > </ div > </ template >< script > export default { name : \"App\" , data () { return { data : { src : {} } }; }, beforeMount () { this . getPhoto (); }, methods : { async getPhoto () { const headers = new Headers (); headers . append ( \"Authorization\" , \"api_key\" ); const request = new Request ( \"https://api.pexels.com/v1/curated?per_page=11&page=1\" , { method : \"GET\" , headers , mode : \"cors\" , cache : \"default\" } ); const res = await fetch ( request ); const { photos } = await res . json (); this . data = photos [ 0 ]; } } }; </ script > In the code above, we used the Headers constructor, which is used to add requests headers to Fetch API requests. The append method appends our 'Authorization' header to the request. We\u2019ve to set the mode to 'cors' for a cross-domain request and headers is set to the headers object returned by the Headers constructor. Adding body to a request \u2691 To make a request body, we can write the following: < template > < div id = \"app\" > < form @ submit . prevent = \"createPost\" > < input placeholder = \"name\" v-model = \"post.name\" > < input placeholder = \"title\" v-model = \"post.title\" > < br > < button type = \"submit\" > Create </ button > </ form > {{data}} </ div > </ template >< script > export default { name : \"App\" , data () { return { post : {}, data : {} }; }, methods : { async createPost () { const request = new Request ( \"https://jsonplaceholder.typicode.com/posts\" , { method : \"POST\" , mode : \"cors\" , cache : \"default\" , body : JSON . stringify ( this . post ) } ); const res = await fetch ( request ); const data = await res . json (); this . data = data ; } } }; </ script > In the code above, we made the request by stringifying the this.post object and then sending it with a POST request. Axios \u2691 Axios is a popular HTTP client that works on both browser and Node.js apps. We can install it by running: npm i axios Then we can use it to make requests a simple GET request as follows: < template > < div id = \"app\" > {{data}} </ div > </ template >< script > import axios from 'axios' export default { name : \"App\" , data () { return { data : {} }; }, beforeMount (){ this . getName (); }, methods : { async getName (){ const { data } = await axios . get ( \"https://api.agify.io/?name=michael\" ); this . data = data ; } } }; </ script > In the code above, we call the axios.get method with the URL to make the request. Then we assign the response data to an object. Adding headers \u2691 If we want to make a request with headers, we can write: < template > < div id = \"app\" > < img :src = \"data.src.tiny\" > </ div > </ template >< script > import axios from 'axios' export default { name : \"App\" , data () { return { data : {} }; }, beforeMount () { this . getPhoto (); }, methods : { async getPhoto () { const { data : { photos } } = await axios ({ url : \"https://api.pexels.com/v1/curated?per_page=11&page=1\" , headers : { Authorization : \"api_key\" } }); this . data = photos [ 0 ]; } } }; </ script > In the code above, we made a GET request with our Pexels API key with the axios method, which can be used for making any kind of request. If no request verb is specified, then it\u2019ll be a GET request. As we can see, the code is a bit shorter since we don\u2019t have to create an object with the Headers constructor. If we want to set the same header in multiple requests, we can use a request interceptor to set the header or other config for all requests. For instance, we can rewrite the above example as follows: // main.js: import Vue from \"vue\" ; import App from \"./App.vue\" ; import axios from 'axios' axios . interceptors . request . use ( config => { return { ... config , headers : { Authorization : \"api_key\" } }; }, error => Promise . reject ( error ) ); Vue . config . productionTip = false ; new Vue ({ render : h => h ( App ) }). $mount ( \"#app\" ); < template > < div id = \"app\" > < img :src = \"data.src.tiny\" > </ div > </ template >< script > import axios from 'axios' export default { name : \"App\" , data () { return { data : {} }; }, beforeMount () { this . getPhoto (); }, methods : { async getPhoto () { const { data : { photos } } = await axios ({ url : \"https://api.pexels.com/v1/curated?per_page=11&page=1\" }); this . data = photos [ 0 ]; } } }; </ script > We moved the header to `main.js` inside the code for our interceptor. The first argument that\u2019s passed into `axios.interceptors.request.use` is a function for modifying the request config for all requests. And the 2nd argument is an error handler for handling error of all requests. Likewise, we can configure interceptors for responses as well. #### Adding body to a request To make a POST request with a request body, we can use the `axios.post` method. ```html < template > < div id = \"app\" > < form @ submit . prevent = \"createPost\" > < input placeholder = \"name\" v-model = \"post.name\" > < input placeholder = \"title\" v-model = \"post.title\" > < br > < button type = \"submit\" > Create </ button > </ form > {{data}} </ div > </ template >< script > import axios from 'axios' export default { name : \"App\" , data () { return { post : {}, data : {} }; }, methods : { async createPost () { const { data } = await axios . post ( \"https://jsonplaceholder.typicode.com/posts\" , this . post ); this . data = data ; } } }; </ script > We make the POST request with the axios.post method with the request body in the second argument. Axios also sets the Content-Type header to application/json. This enables web frameworks to automatically parse the data. Then we get back the response data by getting the data property from the resulting response. Shorthand methods for Axios HTTP requests \u2691 Axios also provides a set of shorthand methods for performing different types of requests. The methods are as follows: axios.request(config) axios.get(url[, config]) axios.delete(url[, config]) axios.head(url[, config]) axios.options(url[, config]) axios.post(url[, data[, config]]) axios.put(url[, data[, config]]) axios.patch(url[, data[, config]]) For instance, the following code shows how the previous example could be written using the axios.post() method: axios . post ( '/login' , { firstName : 'Finn' , lastName : 'Williams' }) . then (( response ) => { console . log ( response ); }, ( error ) => { console . log ( error ); }); Once an HTTP POST request is made, Axios returns a promise that is either fulfilled or rejected, depending on the response from the backend service. To handle the result, you can use the then() . method. If the promise is fulfilled, the first argument of then() will be called; if the promise is rejected, the second argument will be called. According to the documentation, the fulfillment value is an object containing the following information: { // `data` is the response that was provided by the server data : {}, // `status` is the HTTP status code from the server response status : 200 , // `statusText` is the HTTP status message from the server response statusText : 'OK' , // `headers` the headers that the server responded with // All header names are lower cased headers : {}, // `config` is the config that was provided to `axios` for the request config : {}, // `request` is the request that generated this response // It is the last ClientRequest instance in node.js (in redirects) // and an XMLHttpRequest instance the browser request : {} } Using interceptors \u2691 One of the key features of Axios is its ability to intercept HTTP requests. HTTP interceptors come in handy when you need to examine or change HTTP requests from your application to the server or vice versa (e.g., logging, authentication, or retrying a failed HTTP request). With interceptors, you won\u2019t have to write separate code for each HTTP request. HTTP interceptors are helpful when you want to set a global strategy for how you handle request and response. axios . interceptors . request . use ( config => { // log a message before any HTTP request is sent console . log ( 'Request was sent' ); return config ; }); // sent a GET request axios . get ( 'https://api.github.com/users/sideshowbarker' ) . then ( response => { console . log ( response . data ); }); In this code, the axios.interceptors.request.use() method is used to define code to be run before an HTTP request is sent. Also, axios.interceptors.response.use() can be used to intercept the response from the server. Let\u2019s say there is a network error; using the response interceptors, you can retry that same request using interceptors. Handling errors \u2691 To catch errors when doing requests you could use: try { let res = await axios . get ( '/my-api-route' ); // Work with the response... } catch ( error ) { if ( error . response ) { // The client was given an error response (5xx, 4xx) console . log ( err . response . data ); console . log ( err . response . status ); console . log ( err . response . headers ); } else if ( error . request ) { // The client never received a response, and the request was never left console . log ( err . request ); } else { // Anything else console . log ( 'Error' , err . message ); } } The differences in the error object, indicate where the request encountered the issue. error.response : If your error object has a response property, it means that your server returned a 4xx/5xx error. This will assist you choose what sort of message to return to users. error.request : This error is caused by a network error, a hanging backend that does not respond instantly to each request, unauthorized or cross-domain requests, and lastly if the backend API returns an error. This occurs when the browser was able to initiate a request but did not receive a valid answer for any reason. Other errors: It's possible that the error object does not have either a response or request object attached to it. In this case it is implied that there was an issue in setting up the request, which eventually triggered an error. For example, this could be the case if you omit the URL parameter from the .get() call, and thus no request was ever made. Sending multiple requests \u2691 One of Axios\u2019 more interesting features is its ability to make multiple requests in parallel by passing an array of arguments to the axios.all() method. This method returns a single promise object that resolves only when all arguments passed as an array have resolved. Here\u2019s a simple example of how to use axios.all to make simultaneous HTTP requests: // execute simultaneous requests axios . all ([ axios . get ( 'https://api.github.com/users/mapbox' ), axios . get ( 'https://api.github.com/users/phantomjs' ) ]) . then ( responseArr => { //this will be executed only when all requests are complete console . log ( 'Date created: ' , responseArr [ 0 ]. data . created_at ); console . log ( 'Date created: ' , responseArr [ 1 ]. data . created_at ); }); // logs: // => Date created: 2011-02-04T19:02:13Z // => Date created: 2017-04-03T17:25:46Z This code makes two requests to the GitHub API and then logs the value of the created_at property of each response to the console. Keep in mind that if any of the arguments rejects then the promise will immediately reject with the reason of the first promise that rejects. For convenience, Axios also provides a method called axios.spread() to assign the properties of the response array to separate variables. Here\u2019s how you could use this method: axios . all ([ axios . get ( 'https://api.github.com/users/mapbox' ), axios . get ( 'https://api.github.com/users/phantomjs' ) ]) . then ( axios . spread (( user1 , user2 ) => { console . log ( 'Date created: ' , user1 . data . created_at ); console . log ( 'Date created: ' , user2 . data . created_at ); })); // logs: // => Date created: 2011-02-04T19:02:13Z // => Date created: 2017-04-03T17:25:46Z The output of this code is the same as the previous example. The only difference is that the axios.spread() method is used to unpack values from the response array. Veredict \u2691 If you\u2019re working on multiple requests, you\u2019ll find that Fetch requires you to write more code than Axios, even when taking into consideration the setup needed for it. Therefore, for simple requests, Fetch API and Axios are quite the same. However, for more complex requests, Axios is better as it allows you to configure multiple requests in one place. If you're making a simple request use the Fetch API, for the other cases use axios because: It allows you to configure multiple requests in one place Code is shorter. It allows you to place all the API calls under services so that these can be reused across components wherever they are needed . It's easy to set a timeout of the request. It supports HTTP interceptors by befault It does automatic JSON data transformation. It's supported by old browsers, although you can bypass the problem with fetch too. It has a progress indicator for large files. Supports simultaneous requests by default. Axios provides an easy-to-use API in a compact package for most of your HTTP communication needs. However, if you prefer to stick with native APIs, nothing stops you from implementing Axios features. For more information read: How To Make API calls in Vue.JS Applications by Bhargav Bachina Axios vs. fetch(): Which is best for making HTTP requests? by Faraz Kelhini Vue Router \u2691 Creating a Single-page Application with Vue + Vue Router feels natural, all we need to do is map our components to the routes and let Vue Router know where to render them. Here's a basic example: < script src = \"https://unpkg.com/vue@3\" ></ script > < script src = \"https://unpkg.com/vue-router@4\" ></ script > < div id = \"app\" > < h1 > Hello App! </ h1 > < p > <!-- use the router-link component for navigation. --> <!-- specify the link by passing the `to` prop. --> <!-- `<router-link>` will render an `<a>` tag with the correct `href` attribute --> < router-link to = \"/\" > Go to Home </ router-link > < router-link to = \"/about\" > Go to About </ router-link > </ p > <!-- route outlet --> <!-- component matched by the route will render here --> < router-view ></ router-view > </ div > Note how instead of using regular a tags, we use a custom component router-link to create links. This allows Vue Router to change the URL without reloading the page, handle URL generation as well as its encoding. router-view will display the component that corresponds to the url. You can put it anywhere to adapt it to your layout. // 1. Define route components. // These can be imported from other files const Home = { template : '<div>Home</div>' } const About = { template : '<div>About</div>' } // 2. Define some routes // Each route should map to a component. // We'll talk about nested routes later. const routes = [ { path : '/' , component : Home }, { path : '/about' , component : About }, ] // 3. Create the router instance and pass the `routes` option // You can pass in additional options here, but let's // keep it simple for now. const router = VueRouter . createRouter ({ // 4. Provide the history implementation to use. We are using the hash history for simplicity here. history : VueRouter . createWebHashHistory (), routes , // short for `routes: routes` }) // 5. Create and mount the root instance. const app = Vue . createApp ({}) // Make sure to _use_ the router instance to make the // whole app router-aware. app . use ( router ) app . mount ( '#app' ) // Now the app has started! By calling app.use(router) , we get access to it as this.$router as well as the current route as this.$route inside of any component: // Home.vue export default { computed: { username() { // We will see what `params` is shortly return this.$route.params.username }, }, methods: { goToDashboard() { if (isAuthenticated) { this.$router.push('/dashboard') } else { this.$router.push('/login') } }, }, } To access the router or the route inside the setup function, call the useRouter or useRoute functions. Dynamic route matching with params \u2691 Very often we will need to map routes with the given pattern to the same component. For example we may have a User component which should be rendered for all users but with different user IDs. In Vue Router we can use a dynamic segment in the path to achieve that, we call that a param : const User = { template : '<div>User</div>' , } // these are passed to `createRouter` const routes = [ // dynamic segments start with a colon { path : '/users/:id' , component : User }, ] Now URLs like /users/johnny and /users/jolyne will both map to the same route. A param is denoted by a colon :. When a route is matched, the value of its params will be exposed as this.$route.params in every component. Therefore, we can render the current user ID by updating User's template to this: const User = { template: ' < div > User {{ $route.params.id }} </ div > ', } You can have multiple params in the same route, and they will map to corresponding fields on $route.params . Examples: pattern matched path $route.params /users/:username /users/eduardo { username: 'eduardo' } /users/:username/posts/:postId /users/eduardo/posts/123 { username: 'eduardo', postId: '123' } In addition to $route.params , the $route object also exposes other useful information such as $route.query (if there is a query in the URL), $route.hash , etc. Reacting to params changes \u2691 One thing to note when using routes with params is that when the user navigates from /users/johnny to /users/jolyne , the same component instance will be reused. Since both routes render the same component, this is more efficient than destroying the old instance and then creating a new one. However, this also means that the lifecycle hooks of the component will not be called. To react to params changes in the same component, you can simply watch anything on the $route object, in this scenario, the $route.params : const User = { template : '...' , created () { this . $watch ( () => this . $route . params , ( toParams , previousParams ) => { // react to route changes... } ) }, } Or, use the beforeRouteUpdate navigation guard, which also allows to cancel the navigation: const User = { template : '...' , async beforeRouteUpdate ( to , from ) { // react to route changes... this . userData = await fetchUser ( to . params . id ) }, } Components \u2691 Components allow us to split the UI into independent and reusable pieces, and think about each piece in isolation. It's common for an app to be organized into a tree of nested components Defining a component \u2691 When using a build step, we typically define each Vue component in a dedicated file using the .vue extension. <script> export default { data() { return { count: 0 } } } </script> <template> <button @click=\"count++\">You clicked me {{ count }} times.</button> </template> Using a component \u2691 To use a child component, we need to import it in the parent component. Assuming we placed our counter component inside a file called ButtonCounter.vue , the component will be exposed as the file's default export: <script> import ButtonCounter from './ButtonCounter.vue' export default { components: { ButtonCounter } } </script> <template> <h1>Here is a child component!</h1> <ButtonCounter /> </template> To expose the imported component to our template, we need to register it with the components option. The component will then be available as a tag using the key it is registered under. Components can be reused as many times as you want: < h1 > Here are many child components! </ h1 > < ButtonCounter /> < ButtonCounter /> < ButtonCounter /> When clicking on the buttons, each one maintains its own, separate count. That's because each time you use a component, a new instance of it is created. Passing props \u2691 Props are custom attributes you can register on a component. Vue components require explicit props declaration so that Vue knows what external props passed to the component should be treated as fallthrough attributes. <!-- BlogPost.vue --> <script> export default { props: ['title'] } </script> <template> <h4>{{ title }}</h4> </template> When a value is passed to a prop attribute, it becomes a property on that component instance. The value of that property is accessible within the template and on the component's this context, just like any other component property. A component can have as many props as you like and, by default, any value can be passed to any prop. Once a prop is registered, you can pass data to it as a custom attribute, like this: < BlogPost title = \"My journey with Vue\" /> < BlogPost title = \"Blogging with Vue\" /> < BlogPost title = \"Why Vue is so fun\" /> In a typical app, however, you'll likely have an array of posts in your parent component: export default { // ... data () { return { posts : [ { id : 1 , title : 'My journey with Vue' }, { id : 2 , title : 'Blogging with Vue' }, { id : 3 , title : 'Why Vue is so fun' } ] } } } Then want to render a component for each one, using v-for : < BlogPost v-for = \"post in posts\" :key = \"post.id\" :title = \"post.title\" /> We declare long prop names using camelCase because this avoids having to use quotes when using them as property keys. export default { props : { greetingMessage : String } } < span > {{ greetingMessage }} </ span > However, the convention is using kebab-case when passing props to a child component. < MyComponent greeting-message = \"hello\" /> Passing different value types on props \u2691 Numbers: <!-- Even though `42` is static, we need v-bind to tell Vue that --> <!-- this is a JavaScript expression rather than a string. --> < BlogPost :likes = \"42\" /> <!-- Dynamically assign to the value of a variable. --> < BlogPost :likes = \"post.likes\" /> Boolean: <!-- Including the prop with no value will imply `true`. --> < BlogPost is-published /> <!-- Even though `false` is static, we need v-bind to tell Vue that --> <!-- this is a JavaScript expression rather than a string. --> < BlogPost :is-published = \"false\" /> <!-- Dynamically assign to the value of a variable. --> < BlogPost :is-published = \"post.isPublished\" /> Array <!-- Even though the array is static, we need v-bind to tell Vue that --> <!-- this is a JavaScript expression rather than a string. --> < BlogPost :comment-ids = \"[234, 266, 273]\" /> <!-- Dynamically assign to the value of a variable. --> < BlogPost :comment-ids = \"post.commentIds\" /> Object <!-- Even though the object is static, we need v-bind to tell Vue that --> <!-- this is a JavaScript expression rather than a string. --> < BlogPost :author = \"{ name: 'Veronica', company: 'Veridian Dynamics' }\" /> <!-- Dynamically assign to the value of a variable. --> < BlogPost :author = \"post.author\" /> If you want to pass all the properties of an object as props, you can use v-bind without an argument. export default { data () { return { post : { id : 1 , title : 'My Journey with Vue' } } } } The following template: < BlogPost v-bind = \"post\" /> Will be equivalent to: < BlogPost :id = \"post.id\" :title = \"post.title\" /> One-way data flow in props \u2691 All props form a one-way-down binding between the child property and the parent one: when the parent property updates, it will flow down to the child, but not the other way around. Every time the parent component is updated, all props in the child component will be refreshed with the latest value. This means you should not attempt to mutate a prop inside a child component. Prop validation \u2691 Components can specify requirements for their props, if a requirement is not met, Vue will warn you in the browser's JavaScript console. export default { props : { // Basic type check // (`null` and `undefined` values will allow any type) propA : Number , // Multiple possible types propB : [ String , Number ], // Required string propC : { type : String , required : true }, // Number with a default value propD : { type : Number , default : 100 }, // Object with a default value propE : { type : Object , // Object or array defaults must be returned from // a factory function. The function receives the raw // props received by the component as the argument. default ( rawProps ) { // default function receives the raw props object as argument return { message : 'hello' } } }, // Custom validator function propF : { validator ( value ) { // The value must match one of these strings return [ 'success' , 'warning' , 'danger' ]. includes ( value ) } }, // Function with a default value propG : { type : Function , // Unlike object or array default, this is not a factory function - this is a function to serve as a default value default () { return 'Default function' } } } } Additional details: All props are optional by default, unless required: true is specified. An absent optional prop will have undefined value. If a default value is specified, it will be used if the resolved prop value is undefined , this includes both when the prop is absent, or an explicit undefined value is passed. Listening to Events \u2691 As we develop our <BlogPost> component, some features may require communicating back up to the parent. For example, we may decide to include an accessibility feature to enlarge the text of blog posts, while leaving the rest of the page at its default size. In the parent, we can support this feature by adding a postFontSize data property: data () { return { posts : [ /* ... */ ], postFontSize : 1 } } Which can be used in the template to control the font size of all blog posts: < div :style = \"{ fontSize: postFontSize + 'em' }\" > < BlogPost v-for = \"post in posts\" :key = \"post.id\" :title = \"post.title\" /> </ div > Now let's add a button to the <BlogPost> component's template: <!-- BlogPost.vue, omitting <script> --> <template> <div class=\"blog-post\"> <h4>{{ title }}</h4> <button>Enlarge text</button> </div> </template> The button currently doesn't do anything yet - we want clicking the button to communicate to the parent that it should enlarge the text of all posts. To solve this problem, component instances provide a custom events system. The parent can choose to listen to any event on the child component instance with v-on or @, just as we would with a native DOM event: < BlogPost ... @ enlarge-text = \"postFontSize += 0.1\" /> Then the child component can emit an event on itself by calling the built-in $emit method, passing the name of the event: <!-- BlogPost.vue, omitting <script> --> < template > < div class = \"blog-post\" > < h4 > {{ title }} </ h4 > < button @ click = \"$emit('enlarge-text')\" > Enlarge text </ button > </ div > </ template > The first argument to this.$emit() is the event name. Any additional arguments are passed on to the event listener. Thanks to the @enlarge-text=\"postFontSize += 0.1\" listener, the parent will receive the event and update the value of postFontSize . We can optionally declare emitted events using the emits option: <!-- BlogPost.vue --> <script> export default { props: ['title'], emits: ['enlarge-text'] } </script> This documents all the events that a component emits and optionally validates them. It also allows Vue to avoid implicitly applying them as native listeners to the child component's root element. Event arguments \u2691 It's sometimes useful to emit a specific value with an event. For example, we may want the <BlogPost> component to be in charge of how much to enlarge the text by. In those cases, we can pass extra arguments to $emit to provide this value: < button @ click = \"$emit('increaseBy', 1)\" > Increase by 1 </ button > Then, when we listen to the event in the parent, we can use an inline arrow function as the listener, which allows us to access the event argument: < MyButton @ increase-by = \"(n) => count += n\" /> Or, if the event handler is a method: Then the value will be passed as the first parameter of that method: methods : { increaseCount ( n ) { this . count += n } } Declaring emitted events \u2691 Emitted events can be explicitly declared on the component via the emits option. export default { emits : [ 'inFocus' , 'submit' ] } The emits option also supports an object syntax, which allows us to perform runtime validation of the payload of the emitted events: export default { emits : { submit ( payload ) { // return `true` or `false` to indicate // validation pass / fail } } } Although optional, it is recommended to define all emitted events in order to better document how a component should work. Content distribution with Slots \u2691 In addition to passing data via props, the parent component can also pass down template fragments to the child via slots: <ChildComp> This is some slot content! </ChildComp> In the child component, it can render the slot content from the parent using the <slot> element as outlet: <!-- in child template --> <slot/> Content inside the <slot> outlet will be treated as \"fallback\" content: it will be displayed if the parent did not pass down any slot content: <slot>Fallback content</slot> Slot content is not just limited to text. It can be any valid template content. For example, we can pass in multiple elements, or even other components: < FancyButton > < span style = \"color:red\" > Click me! </ span > < AwesomeIcon name = \"plus\" /> </ FancyButton > Slot content has access to the data scope of the parent component, because it is defined in the parent. However, slot content does not have access to the child component's data. As a rule, remember that everything in the parent template is compiled in parent scope; everything in the child template is compiled in the child scope. You can however use child content using scoped slots . Named Slots \u2691 There are times when it's useful to have multiple slot outlets in a single component. For these cases, the <slot> element has a special attribute, name , which can be used to assign a unique ID to different slots so you can determine where content should be rendered: < div class = \"container\" > < header > < slot name = \"header\" ></ slot > </ header > < main > < slot ></ slot > </ main > < footer > < slot name = \"footer\" ></ slot > </ footer > </ div > To pass a named slot, we need to use a <template> element with the v-slot directive, and then pass the name of the slot as an argument to v-slot : < BaseLayout > < template # header > < h1 > Here might be a page title </ h1 > </ template > < template # default > < p > A paragraph for the main content. </ p > < p > And another one. </ p > </ template > < template # footer > < p > Here's some contact info </ p > </ template > </ BaseLayout > Where # is the shorthand of v-slot . Dynamic components \u2691 Sometimes, it's useful to dynamically switch between components, like in a tabbed interface, for example in this page . The above is made possible by Vue's <component> element with the special is attribute: <!-- Component changes when currentTab changes --> <component :is=\"currentTab\"></component> In the example above, the value passed to :is can contain either: The name string of a registered component, OR. The actual imported component object. You can also use the is attribute to create regular HTML elements. When switching between multiple components with <component :is=\"...\"> , a component will be unmounted when it is switched away from. We can force the inactive components to stay \"alive\" with the built-in <KeepAlive> component. Async components \u2691 In large applications, we may need to divide the app into smaller chunks and only load a component from the server when it's needed. To make that possible, Vue has a defineAsyncComponent function: import { defineAsyncComponent } from 'vue' const AsyncComp = defineAsyncComponent (() => import ( './components/MyComponent.vue' ) ) Asynchronous operations inevitably involve loading and error states, defineAsyncComponent() supports handling these states via advanced options: const AsyncComp = defineAsyncComponent ({ // the loader function loader : () => import ( './Foo.vue' ), // A component to use while the async component is loading loadingComponent : LoadingComponent , // Delay before showing the loading component. Default: 200ms. delay : 200 , // A component to use if the load fails errorComponent : ErrorComponent , // The error component will be displayed if a timeout is // provided and exceeded. Default: Infinity. timeout : 3000 }) Testing \u2691 When designing your Vue application's testing strategy, you should leverage the following testing types: Unit : Checks that inputs to a given function, class, or composable are producing the expected output or side effects. Component : Checks that your component mounts, renders, can be interacted with, and behaves as expected. These tests import more code than unit tests, are more complex, and require more time to execute. End-to-end : Checks features that span multiple pages and make real network requests against your production-built Vue application. These tests often involve standing up a database or other backend. Unit testing \u2691 Unit tests will catch issues with a function's business logic and logical correctness. Take for example this increment function: // helpers.js export function increment ( current , max = 10 ) { if ( current < max ) { return current + 1 } return current } Because it's very self-contained, it'll be easy to invoke the increment function and assert that it returns what it's supposed to, so we'll write a Unit Test. If any of these assertions fail, it's clear that the issue is contained within the increment function. // helpers.spec.js import { increment } from './helpers' describe ( 'increment' , () => { test ( 'increments the current number by 1' , () => { expect ( increment ( 0 , 10 )). toBe ( 1 ) }) test ( 'does not increment the current number over the max' , () => { expect ( increment ( 10 , 10 )). toBe ( 10 ) }) test ( 'has a default max of 10' , () => { expect ( increment ( 10 )). toBe ( 10 ) }) }) Unit testing is typically applied to self-contained business logic, components, classes, modules, or functions that do not involve UI rendering, network requests, or other environmental concerns. These are typically plain JavaScript / TypeScript modules unrelated to Vue. In general, writing unit tests for business logic in Vue applications does not differ significantly from applications using other frameworks. There are two instances where you DO unit test Vue-specific features: Composables Components Component testing \u2691 In Vue applications, components are the main building blocks of the UI. Components are therefore the natural unit of isolation when it comes to validating your application's behavior. From a granularity perspective, component testing sits somewhere above unit testing and can be considered a form of integration testing. Much of your Vue Application should be covered by a component test and we recommend that each Vue component has its own spec file. Component tests should catch issues relating to your component's props, events, slots that it provides, styles, classes, lifecycle hooks, and more. Component tests should not mock child components, but instead test the interactions between your component and its children by interacting with the components as a user would. For example, a component test should click on an element like a user would instead of programmatically interacting with the component. Component tests should focus on the component's public interfaces rather than internal implementation details. For most components, the public interface is limited to: events emitted, props, and slots. When testing, remember to test what a component does, not how it does it . For example: For Visual logic assert correct render output based on inputted props and slots. For Behavioral logic: assert correct render updates or emitted events in response to user input events. The recommendation is to use Vitest for components or composables that render headlessly, and Cypress Component Testing for components whose expected behavior depends on properly rendering styles or triggering native DOM event. The main differences between Vitest and browser-based runners are speed and execution context. In short, browser-based runners, like Cypress, can catch issues that node-based runners, like Vitest, cannot (e.g. style issues, real native DOM events, cookies, local storage, and network failures), but browser-based runners are orders of magnitude slower than Vitest because they do open a browser, compile your stylesheets, and more. Component testing often involves mounting the component being tested in isolation, triggering simulated user input events, and asserting on the rendered DOM output. There are dedicated utility libraries that make these tasks simpler. @testing-library/vue is a Vue testing library focused on testing components without relying on implementation details. Built with accessibility in mind, its approach also makes refactoring a breeze. Its guiding principle is that the more tests resemble the way software is used, the more confidence they can provide. @vue/test-utils is the official low-level component testing library that was written to provide users access to Vue specific APIs. It's also the lower-level library @testing-library/vue is built on top of. I recommend using cypress so that you can use the same language either you are doing E2E tests or unit tests. If you're using Vuetify don't try to do component testing, I've tried for days and was unable to make it work . E2E Testing \u2691 While unit tests provide developers with some degree of confidence, unit and component tests are limited in their abilities to provide holistic coverage of an application when deployed to production. As a result, end-to-end (E2E) tests provide coverage on what is arguably the most important aspect of an application: what happens when users actually use your applications. End-to-end tests focus on multi-page application behavior that makes network requests against your production-built Vue application. They often involve standing up a database or other backend and may even be run against a live staging environment. End-to-end tests will often catch issues with your router, state management library, top-level components (e.g. an App or Layout), public assets, or any request handling. As stated above, they catch critical issues that may be impossible to catch with unit tests or component tests. End-to-end tests do not import any of your Vue application's code, but instead rely completely on testing your application by navigating through entire pages in a real browser. End-to-end tests validate many of the layers in your application. They can either target your locally built application, or even a live Staging environment. Testing against your Staging environment not only includes your frontend code and static server, but all associated backend services and infrastructure. E2E tests decisions \u2691 When doing E2E tests keep in mind: Cross-browser testing: One of the primary benefits that end-to-end (E2E) testing is known for is its ability to test your application across multiple browsers. While it may seem desirable to have 100% cross-browser coverage, it is important to note that cross browser testing has diminishing returns on a team's resources due the additional time and machine power required to run them consistently. As a result, it is important to be mindful of this trade-off when choosing the amount of cross-browser testing your application needs. Faster feedback loops: One of the primary problems with end-to-end (E2E) tests and development is that running the entire suite takes a long time. Typically, this is only done in continuous integration and deployment (CI/CD) pipelines. Modern E2E testing frameworks have helped to solve this by adding features like parallelization, which allows for CI/CD pipelines to often run magnitudes faster than before. In addition, when developing locally, the ability to selectively run a single test for the page you are working on while also providing hot reloading of tests can help to boost a developer's workflow and productivity. Visibility in headless mode: When end-to-end (E2E) tests are run in continuous integration / deployment pipelines, they are often run in headless browsers (i.e., no visible browser is opened for the user to watch). A critical feature of modern E2E testing frameworks is the ability to see snapshots and/or videos of the application during testing, providing some insight into why errors are happening. Historically, it was tedious to maintain these integrations. Vue developers suggestion is to use Cypress as it provides the most complete E2E solution with features like an informative graphical interface, excellent debuggability, built-in assertions and stubs, flake-resistance, parallelization, and snapshots. It also provides support for Component Testing. However, it only supports Chromium-based browsers and Firefox. Installation \u2691 In a Vite-based Vue project, run: npm install -D vitest happy-dom @testing-library/vue@next Next, update the Vite configuration to add the test option block: // vite.config.js import { defineConfig } from 'vite' export default defineConfig ({ // ... test : { // enable jest-like global test APIs globals : true , // simulate DOM with happy-dom // (requires installing happy-dom as a peer dependency) environment : 'happy-dom' } }) Then create a file ending in *.test.js in your project. You can place all test files in a test directory in project root, or in test directories next to your source files. Vitest will automatically search for them using the naming convention. // MyComponent.test.js import { render } from '@testing-library/vue' import MyComponent from './MyComponent.vue' test ( 'it should work' , () => { const { getByText } = render ( MyComponent , { props : { /* ... */ } }) // assert output getByText ( '...' ) }) Finally, update package.json to add the test script and run it: { // ... \"scripts\" : { \"test\" : \"vitest\" } } npm test Deploying \u2691 It is common these days to run front-end and back-end services inside Docker containers. The front-end service usually talks using a API with the back-end service. FROM node as ui-builder RUN mkdir /usr/src/app WORKDIR /usr/src/app ENV PATH /usr/src/app/node_modules/.bin: $PATH COPY package.json /usr/src/app/package.json RUN npm install RUN npm install -g @vue/cli COPY . /usr/src/app RUN npm run build FROM nginx COPY --from = ui-builder /usr/src/app/dist /usr/share/nginx/html EXPOSE 80 CMD [ \"nginx\" , \"-g\" , \"daemon off;\" ] The above makes use of the multi-stage build feature of Docker. The first half of the Dockerfile build the artifacts and second half use those artifacts and create a new image from them. To build the production image, run: docker build -t myapp . You can run the container by executing the following command: docker run -it -p 80 :80 --rm myapp-prod The application will now be accessible at http://localhost . Configuration through environmental variables \u2691 In production you want to be able to scale up or down the frontend and the backend independently, to be able to do that you usually have one or many docker for each role. Usually there is an SSL Proxy that acts as gate keeper and is the only component exposed to the public. If the user requests for /api it will forward the requests to the backend, if it asks for any other url it will forward it to the frontend. Note \"You probably don't need to configure the backend api url as an environment variable see here why.\" For the frontend, we need to configure the application. This is usually done through environmental variables , such as EXTERNAL_BACKEND_URL . The problem is that these environment variables are set at build time, and can't be changed at runtime by default, so you can't offer a generic fronted Docker and particularize for the different cases. I've literally cried for hours trying to find a solution for this until Jos\u00e9 Silva came to my rescue . The tweak is to use a docker entrypoint to inject the values we want. To do so you need to: Edit the site main index.html (if you use Vite is in /index.html otherwise it might be at public/index.html to add a placeholder that will be replaced by the dynamic configurations. <!DOCTYPE html> < html lang = \"en\" > < head > < script > // CONFIGURATIONS_PLACEHOLDER </ script > ... Create an executable file named entrypoint.sh in the root of the project. #!/bin/sh JSON_STRING = 'window.configs = { \\ \"VITE_APP_VARIABLE_1\":\"' \" ${ VITE_APP_VARIABLE_1 } \" '\", \\ \"VITE_APP_VARIABLE_2\":\"' \" ${ VITE_APP_VARIABLE_2 } \" '\" \\ }' sed -i \"s@// CONFIGURATIONS_PLACEHOLDER@ ${ JSON_STRING } @\" /usr/share/nginx/html/index.html exec \" $@ \" Its function is to replace the placeholder in the index.html by the configurations, injecting them in the browser window. Create a file named src/utils/env.js with the following utility function: export default function getEnv ( name ) { return window ? . configs ? .[ name ] || process . env [ name ] } Which allows us to easily get the value of the configuration. If it exists in window.configs (used in remote environments like staging or production) it will have priority over the process.env (used for development). Replace the content of the App.vue file with the following: < template > < div id = \"app\" > < img alt = \"Vue logo\" src = \"./assets/logo.png\" > < div > {{ variable1 }} </ div > < div > {{ variable2 }} </ div > </ div > </ template > < script > import getEnv from '@/utils/env' export default { name : 'App' , data () { return { variable1 : getEnv ( 'VITE_APP_VARIABLE_1' ), variable2 : getEnv ( 'VITE_APP_VARIABLE_2' ) } } } </ script > At this point, if you create the .env.local file, in the root of the project, with the values for the printed variables: VITE_APP_VARIABLE_1='I am the develoment variable 1' VITE_APP_VARIABLE_2='I am the develoment variable 2' And run the development server npm run dev you should see those values printed in the application ( http://localhost:8080 ). Update the Dockerfile to load the entrypoint.sh . FROM node as ui-builder RUN mkdir /usr/src/app WORKDIR /usr/src/app ENV PATH /usr/src/app/node_modules/.bin: $PATH COPY package.json /usr/src/app/package.json RUN npm install RUN npm install -g @vue/cli COPY . /usr/src/app ARG VUE_APP_API_URL ENV VUE_APP_API_URL $VUE_APP_API_URL RUN npm run build FROM nginx COPY --from = ui-builder /usr/src/app/dist /usr/share/nginx/html COPY entrypoint.sh /usr/share/nginx/ ENTRYPOINT [ \"/usr/share/nginx/entrypoint.sh\" ] EXPOSE 80 CMD [ \"nginx\" , \"-g\" , \"daemon off;\" ] Build the docker docker build -t my-app . Now if you have a .env.production.local file with the next contents: VITE_APP_VARIABLE_1='I am the production variable 1' VITE_APP_VARIABLE_2='I am the production variable 2' And run docker run -it -p 80:80 --env-file=.env.production.local --rm my-app , you'll see the values of the production variables. You can also pass the variables directly with -e VITE_APP_VARIABLE_1=\"Overriden variable\" . Deploy static site on github pages \u2691 Sites in Github pages have the url structure of https://github_user.github.io/repo_name/ we need to tell vite that the base url is /repo_name/ , otherwise the application will try to load the assets in https://github_user.github.io/assets/ instead of https://github_user.github.io/rpeo_name/assets/ . To change it, add in the vite.config.js file: export default defineConfig ({ base : '/repo_name/' }) Now you need to configure the deployment workflow, to do so, create a new file: .github/workflows/deploy.yml and paste the following code: --- name : Deploy on : push : branches : - main workflow_dispatch : jobs : build : name : Build runs-on : ubuntu-latest steps : - name : Checkout repo uses : actions/checkout@v2 - name : Setup Node uses : actions/setup-node@v1 with : node-version : 16 - name : Install dependencies uses : bahmutov/npm-install@v1 - name : Build project run : npm run build - name : Upload production-ready build files uses : actions/upload-artifact@v2 with : name : production-files path : ./dist deploy : name : Deploy needs : build runs-on : ubuntu-latest if : github.ref == 'refs/heads/main' steps : - name : Download artifact uses : actions/download-artifact@v2 with : name : production-files path : ./dist - name : Deploy to GitHub Pages uses : peaceiris/actions-gh-pages@v3 with : github_token : ${{ secrets.GITHUB_TOKEN }} publish_dir : ./dist You'd probably need to change your repository settings under Actions/General and set the Workflow permissions to Read and write permissions . Once the workflow has been successful, in the repository settings under Pages you need to enable Github Pages to use the gh-pages branch as source. Tip Handling Vue Router with a Custom 404 Page \u2691 One thing to keep in mind when setting up the Github Pages site, is that working with Vue Router gets a little tricky. If you\u2019re using history mode in Vue router, you\u2019ll notice that if you try to go directly to a page other than / you\u2019ll get a 404 error. This is because Github Pages does not automatically redirect all requests to serve index.html . Luckily, there is an easy little workaround. All you have to do is duplicate your index.html file and name the copy 404.html . What this does is make your 404 page serve the same content as your index.html , which means your Vue router will be able to display the right page. Testing \u2691 Debug Jest tests \u2691 If you're not developing in Visual code, running a debugger is not easy in the middle of the tests, so to debug one you can use console.log() statements and when you run them with yarn test:unit you'll see the traces. Troubleshooting \u2691 Failed to resolve component: X \u2691 If you've already imported the component with import X from './X.vue you may have forgotten to add the component to the components property of the module: export default { name : 'Inbox' , components : { X } } References \u2691 Docs Homepage Tutorial Examples Awesome Vue Components Axios \u2691 Docs Git Homepage", "title": "Vue.js"}, {"location": "vuejs/#features", "text": "", "title": "Features"}, {"location": "vuejs/#single-file-components", "text": "Single-File Component (also known as *.vue files, abbreviated as SFC) encapsulates the component's logic (JavaScript), template (HTML), and styles (CSS) in a single file. Here's the previous example, written in SFC format: <script> export default { data() { return { count: 0 } } } </script> <template> <button @click=\"count++\">Count is: {{ count }}</button> </template> <style scoped> button { font-weight: bold; } </style>", "title": "Single file components"}, {"location": "vuejs/#api-styles", "text": "Vue components can be authored in two different API styles: Options API and Composition API.", "title": "API Styles"}, {"location": "vuejs/#options-api", "text": "With Options API, we define a component's logic using an object of options such as data , methods , and mounted . Properties defined by options are exposed on this inside functions, which points to the component instance: <script> export default { // Properties returned from data() becomes reactive state // and will be exposed on `this`. data() { return { count: 0 } }, // Methods are functions that mutate state and trigger updates. // They can be bound as event listeners in templates. methods: { increment() { this.count++ } }, // Lifecycle hooks are called at different stages // of a component's lifecycle. // This function will be called when the component is mounted. mounted() { console.log(`The initial count is ${this.count}.`) } } </script> <template> <button @click=\"increment\">Count is: {{ count }}</button> </template> The Options API is centered around the concept of a \"component instance\" ( this as seen in the example), which typically aligns better with a class-based mental model for users coming from OOP language backgrounds. It is also more beginner-friendly by abstracting away the reactivity details and enforcing code organisation via option groups.", "title": "Options API"}, {"location": "vuejs/#composition-api", "text": "With Composition API, we define a component's logic using imported API functions. In SFCs, Composition API is typically used with <script setup> . The setup attribute is a hint that makes Vue perform compile-time transforms that allow us to use Composition API with less boilerplate. For example, imports and top-level variables / functions declared in <script setup> are directly usable in the template. Here is the same component, with the exact same template, but using Composition API and <script setup> instead: <script setup> import { ref, onMounted } from 'vue' // reactive state const count = ref(0) // functions that mutate state and trigger updates function increment() { count.value++ } // lifecycle hooks onMounted(() => { console.log(`The initial count is ${count.value}.`) }) </script> <template> <button @click=\"increment\">Count is: {{ count }}</button> </template> The Composition API is centered around declaring reactive state variables directly in a function scope, and composing state from multiple functions together to handle complexity. It is more free-form, and requires understanding of how reactivity works in Vue to be used effectively. In return, its flexibility enables more powerful patterns for organizing and reusing logic.", "title": "Composition API"}, {"location": "vuejs/#initialize-a-project", "text": "To create a build-tool-enabled Vue project on your machine, run the following command in your command line. If you don't have npm follow these instructions . npm init vue@latest This command will install and execute create-vue, the official Vue project scaffolding tool. You will be presented with prompts for a number of optional features such as TypeScript and testing support. If you are unsure about an option, simply choose No . Follow their instructions. Once the project is created, follow the instructions to install dependencies and start the dev server: cd <your-project-name> npm install npm run dev When you are ready to ship your app to production, run the following: npm run build", "title": "Initialize a project"}, {"location": "vuejs/#the-basics", "text": "", "title": "The basics"}, {"location": "vuejs/#creating-a-vue-application", "text": "Every Vue application starts by creating a new application instance with the createApp function: import { createApp } from 'vue' const app = createApp({ /* root component options */ }) The object we are passing into createApp is in fact a component. Every app requires a \"root component\" that can contain other components as its children. If you are using Single-File Components, we typically import the root component from another file: import { createApp } from 'vue' // import the root component App from a single-file component. import App from './App.vue' const app = createApp(App) An application instance won't render anything until its .mount() method is called. It expects a \"container\" argument, which can either be an actual DOM element or a selector string: < div id = \"app\" ></ div > app . mount ( '#app' ) The content of the app's root component will be rendered inside the container element. The container element itself is not considered part of the app. The .mount() method should always be called after all app configurations and asset registrations are done. Also note that its return value, unlike the asset registration methods, is the root component instance instead of the application instance. You are not limited to a single application instance on the same page. The createApp API allows multiple Vue applications to co-exist on the same page, each with its own scope for configuration and global assets: const app1 = createApp ({ /* ... */ }) app1 . mount ( '#container-1' ) const app2 = createApp ({ /* ... */ }) app2 . mount ( '#container-2' )", "title": "Creating a Vue Application"}, {"location": "vuejs/#app-configurations", "text": "The application instance exposes a .config object that allows us to configure a few app-level options, for example defining an app-level error handler that captures errors from all descendent components: app . config . errorHandler = ( err ) => { /* handle error */ } The application instance also provides a few methods for registering app-scoped assets. For example, registering a component: app . component ( 'TodoDeleteButton' , TodoDeleteButton ) This makes the TodoDeleteButton available for use anywhere in our app. You can use also environment variables", "title": "App configurations"}, {"location": "vuejs/#declarative-rendering", "text": "The core feature of Vue is declarative rendering: using a template syntax that extends HTML, we can describe how the HTML should look like based on JavaScript state. When the state changes, the HTML updates automatically. State that can trigger updates when changed are considered reactive. In Vue, reactive state is held in components. We can declare reactive state using the data component option, which should be a function that returns an object: export default { data () { return { message : 'Hello World!' } } } The message property will be made available in the template. This is how we can render dynamic text based on the value of message, using mustaches syntax: < h1 > {{ message }} </ h1 > The double mustaches interprets the data as plain text, not HTML. In order to output real HTML, you will need to use the v-html directive, although you should try to avoid it for security reasons. Directives are prefixed with v- to indicate that they are special attributes provided by Vue, they apply special reactive behavior to the rendered DOM.", "title": "Declarative rendering"}, {"location": "vuejs/#attribute-bindings", "text": "To bind an attribute to a dynamic value, we use the v-bind directive: < div v-bind:id = \"dynamicId\" ></ div > A directive is a special attribute that starts with the v- prefix. They are part of Vue's template syntax. Similar to text interpolations, directive values are JavaScript expressions that have access to the component's state. The part after the colon ( :id ) is the \"argument\" of the directive. Here, the element's id attribute will be synced with the dynamicId property from the component's state. Because v-bind is used so frequently, it has a dedicated shorthand syntax: < div :id = \"dynamicId\" ></ div >", "title": "Attribute bindings"}, {"location": "vuejs/#class-binding", "text": "For example to turn the h1 in red: <script> export default { data() { return { titleClass: 'title' } } } </script> <template> <h1 :class='titleClass'>Make me red</h1> <!-- add dynamic class binding here --> </template> <style> .title { color: red; } </style> You can have multiple classes toggled by having more fields in the object. In addition, the :class directive can also co-exist with the plain class attribute. So given the following state: data () { return { isActive : true , hasError : false } } And the following template: < div class = \"static\" :class = \"{ active: isActive, 'text-danger': hasError }\" ></ div > It will render: < div class = \"static active\" ></ div > When isActive or hasError changes, the class list will be updated accordingly. For example, if hasError becomes true, the class list will become static active text-danger . The bound object doesn't have to be inline: data () { return { classObject : { active : true , 'text-danger' : false } } } < div :class = \"classObject\" ></ div > This will render the same result. We can also bind to a computed property that returns an object. This is a common and powerful pattern: data () { return { isActive : true , error : null } }, computed : { classObject () { return { active : this . isActive && ! this . error , 'text-danger' : this . error && this . error . type === 'fatal' } } } < div :class = \"classObject\" ></ div >", "title": "Class binding"}, {"location": "vuejs/#style-binding", "text": ":style supports binding to JavaScript object values. data () { return { activeColor : 'red' , fontSize : 30 } } < div :style = \"{ color: activeColor, fontSize: fontSize + 'px' }\" ></ div > It is often a good idea to bind to a style object directly so that the template is cleaner: data () { return { styleObject : { color : 'red' , fontSize : '13px' } } } < div :style = \"styleObject\" ></ div > Again, object style binding is often used in conjunction with computed properties that return objects.", "title": "Style binding"}, {"location": "vuejs/#event-listeners", "text": "We can listen to DOM events using the v-on directive: <button v-on:click=\"increment\">{{ count }}</button> Due to its frequent use, v-on also has a shorthand syntax: <button @click=\"increment\">{{ count }}</button> Here, increment references a function declared using the methods option: export default { data () { return { count : 0 } }, methods : { increment () { // update component state this . count ++ } } } Inside a method, we can access the component instance using this . The component instance exposes the data properties declared by data. We can update the component state by mutating these properties. You should avoid using arrow functions when defining methods, as that prevents Vue from binding the appropriate this value.", "title": "Event listeners"}, {"location": "vuejs/#event-modifiers", "text": "It is a very common need to call event.preventDefault() or event.stopPropagation() inside event handlers. Although we can do this easily inside methods, it would be better if the methods can be purely about data logic rather than having to deal with DOM event details. To address this problem, Vue provides event modifiers for v-on . Recall that modifiers are directive postfixes denoted by a dot. .stop .prevent .self .capture .once .passive <!-- the click event's propagation will be stopped --> < a @ click . stop = \"doThis\" ></ a > <!-- the submit event will no longer reload the page --> < form @ submit . prevent = \"onSubmit\" ></ form > <!-- modifiers can be chained --> < a @ click . stop . prevent = \"doThat\" ></ a > <!-- just the modifier --> < form @ submit . prevent ></ form > <!-- only trigger handler if event.target is the element itself --> <!-- i.e. not from a child element --> < div @ click . self = \"doThat\" > ... </ div > <!-- use capture mode when adding the event listener --> <!-- i.e. an event targeting an inner element is handled here before being handled by that element --> < div @ click . capture = \"doThis\" > ... </ div > <!-- the click event will be triggered at most once --> < a @ click . once = \"doThis\" ></ a > <!-- the scroll event's default behavior (scrolling) will happen --> <!-- immediately, instead of waiting for `onScroll` to complete --> <!-- in case it contains `event.preventDefault()` --> < div @ scroll . passive = \"onScroll\" > ... </ div >", "title": "Event modifiers"}, {"location": "vuejs/#key-modifiers", "text": "When listening for keyboard events, we often need to check for specific keys. Vue allows adding key modifiers for v-on or @ when listening for key events: <!-- only call `vm.submit()` when the `key` is `Enter` --> < input @ keyup . enter = \"submit\" /> You can directly use any valid key names exposed via KeyboardEvent.key as modifiers by converting them to kebab-case. < input @ keyup . page-down = \"onPageDown\" /> Vue provides aliases for the most commonly used keys: .enter .tab .delete (captures both \"Delete\" and \"Backspace\" keys) .esc .space .up .down .left .right You can use the following modifiers to trigger mouse or keyboard event listeners only when the corresponding modifier key is pressed: .ctrl .alt .shift .meta For example: <!-- Alt + Enter --> < input @ keyup . alt . enter = \"clear\" /> <!-- Ctrl + Click --> < div @ click . ctrl = \"doSomething\" > Do something </ div > The .exact modifier allows control of the exact combination of system modifiers needed to trigger an event. <!-- this will fire even if Alt or Shift is also pressed --> < button @ click . ctrl = \"onClick\" > A </ button > <!-- this will only fire when Ctrl and no other keys are pressed --> < button @ click . ctrl . exact = \"onCtrlClick\" > A </ button > <!-- this will only fire when no system modifiers are pressed --> < button @ click . exact = \"onClick\" > A </ button >", "title": "Key Modifiers"}, {"location": "vuejs/#mouse-button-modifiers", "text": ".left .right .middle These modifiers restrict the handler to events triggered by a specific mouse button.", "title": "Mouse Button Modifiers"}, {"location": "vuejs/#form-bindings", "text": "", "title": "Form bindings"}, {"location": "vuejs/#basic-usage", "text": "", "title": "Basic usage"}, {"location": "vuejs/#text", "text": "Using v-bind and v-on together, we can create two-way bindings on form input elements: <input :value=\"text\" @input=\"onInput\"> <p>{{ text }}</p> methods : { onInput ( e ) { // a v-on handler receives the native DOM event // as the argument. this . text = e . target . value } } To simplify two-way bindings, Vue provides a directive, v-model , which is essentially a syntax sugar for the above: < input v-model = \"text\" > v-model automatically syncs the <input> 's value with the bound state, so we no longer need to use a event handler for that. v-model works not only on text inputs, but also other input types such as checkboxes, radio buttons, and select dropdowns.", "title": "Text"}, {"location": "vuejs/#multiline-text", "text": "< span > Multiline message is: </ span > < p style = \"white-space: pre-line;\" > {{ message }} </ p > < textarea v-model = \"message\" placeholder = \"add multiple lines\" ></ textarea >", "title": "Multiline text"}, {"location": "vuejs/#checkbox", "text": "< input type = \"checkbox\" id = \"checkbox\" v-model = \"checked\" /> < label for = \"checkbox\" > {{ checked }} </ label > We can also bind multiple checkboxes to the same array or Set value: export default { data () { return { checkedNames : [] } } } < div > Checked names: {{ checkedNames }} </ div > < input type = \"checkbox\" id = \"jack\" value = \"Jack\" v-model = \"checkedNames\" > < label for = \"jack\" > Jack </ label > < input type = \"checkbox\" id = \"john\" value = \"John\" v-model = \"checkedNames\" > < label for = \"john\" > John </ label > < input type = \"checkbox\" id = \"mike\" value = \"Mike\" v-model = \"checkedNames\" > < label for = \"mike\" > Mike </ label >", "title": "Checkbox"}, {"location": "vuejs/#radio-checkboxes", "text": "< div > Picked: {{ picked }} </ div > < input type = \"radio\" id = \"one\" value = \"One\" v-model = \"picked\" /> < label for = \"one\" > One </ label > < input type = \"radio\" id = \"two\" value = \"Two\" v-model = \"picked\" /> < label for = \"two\" > Two </ label >", "title": "Radio checkboxes"}, {"location": "vuejs/#select", "text": "Single select: < div > Selected: {{ selected }} </ div > < select v-model = \"selected\" > < option disabled value = \"\" > Please select one </ option > < option > A </ option > < option > B </ option > < option > C </ option > </ select > Multiple select (bound to array): < div > Selected: {{ selected }} </ div > < select v-model = \"selected\" multiple > < option > A </ option > < option > B </ option > < option > C </ option > </ select > Select options can be dynamically rendered with v-for : export default { data () { return { selected : 'A' , options : [ { text : 'One' , value : 'A' }, { text : 'Two' , value : 'B' }, { text : 'Three' , value : 'C' } ] } } } < select v-model = \"selected\" > < option v-for = \"option in options\" :value = \"option.value\" > {{ option.text }} </ option > </ select > < div > Selected: {{ selected }} </ div >", "title": "Select"}, {"location": "vuejs/#value-bindings", "text": "For radio, checkbox and select options, the v-model binding values are usually static strings (or booleans for checkbox):. <!-- `picked` is a string \"a\" when checked --> < input type = \"radio\" v-model = \"picked\" value = \"a\" /> <!-- `toggle` is either true or false --> < input type = \"checkbox\" v-model = \"toggle\" /> <!-- `selected` is a string \"abc\" when the first option is selected --> < select v-model = \"selected\" > < option value = \"abc\" > ABC </ option > </ select > But sometimes we may want to bind the value to a dynamic property on the current active instance. We can use v-bind to achieve that. In addition, using v-bind allows us to bind the input value to non-string values.", "title": "Value bindings"}, {"location": "vuejs/#checkbox_1", "text": "< input type = \"checkbox\" v-model = \"toggle\" true-value = \"yes\" false-value = \"no\" /> true-value and false-value are Vue-specific attributes that only work with v-model . Here the toggle property's value will be set to 'yes' when the box is checked, and set to 'no' when unchecked. You can also bind them to dynamic values using v-bind : < input type = \"checkbox\" v-model = \"toggle\" :true-value = \"dynamicTrueValue\" :false-value = \"dynamicFalseValue\" />", "title": "Checkbox"}, {"location": "vuejs/#radio", "text": "< input type = \"radio\" v-model = \"pick\" :value = \"first\" /> < input type = \"radio\" v-model = \"pick\" :value = \"second\" /> pick will be set to the value of first when the first radio input is checked, and set to the value of second when the second one is checked.", "title": "Radio"}, {"location": "vuejs/#select-options", "text": "< select v-model = \"selected\" > <!-- inline object literal --> < option :value = \"{ number: 123 }\" > 123 </ option > </ select > v-model supports value bindings of non-string values as well! In the above example, when the option is selected, selected will be set to the object literal value of { number: 123 } .", "title": "Select Options"}, {"location": "vuejs/#form-modifiers", "text": "", "title": "Form modifiers"}, {"location": "vuejs/#lazy", "text": "By default, v-model syncs the input with the data after each input event. You can add the lazy modifier to instead sync after change events: <!-- synced after \"change\" instead of \"input\" --> < input v-model . lazy = \"msg\" />", "title": ".lazy"}, {"location": "vuejs/#number", "text": "If you want user input to be automatically typecast as a number, you can add the number modifier to your v-model managed inputs: < input v-model . number = \"age\" /> If the value cannot be parsed with parseFloat() , then the original value is used instead. The number modifier is applied automatically if the input has type=\"number\" .", "title": ".number"}, {"location": "vuejs/#trim", "text": "If you want whitespace from user input to be trimmed automatically, you can add the trim modifier to your v-model managed inputs: < input v-model . trim = \"msg\" />", "title": ".trim"}, {"location": "vuejs/#conditional-rendering", "text": "We can use the v-if directive to conditionally render an element: < h1 v-if = \"awesome\" > Vue is awesome! </ h1 > This <h1> will be rendered only if the value of awesome is truthy. If awesome changes to a falsy value, it will be removed from the DOM. We can also use v-else and v-else-if to denote other branches of the condition: < h1 v-if = \"awesome\" > Vue is awesome! </ h1 > < h1 v-else > Oh no \ud83d\ude22 </ h1 > Because v-if is a directive, it has to be attached to a single element. But what if we want to toggle more than one element? In this case we can use v-if on a <template> element, which serves as an invisible wrapper. The final rendered result will not include the <template> element. < template v-if = \"ok\" > < h1 > Title </ h1 > < p > Paragraph 1 </ p > < p > Paragraph 2 </ p > </ template > Another option for conditionally displaying an element is the v-show directive. The usage is largely the same: < h1 v-show = \"ok\" > Hello! </ h1 > The difference is that an element with v-show will always be rendered and remain in the DOM; v-show only toggles the display CSS property of the element. v-show doesn't support the <template> element, nor does it work with v-else . Generally speaking, v-if has higher toggle costs while v-show has higher initial render costs. So prefer v-show if you need to toggle something very often, and prefer v-if if the condition is unlikely to change at runtime.", "title": "Conditional rendering"}, {"location": "vuejs/#list-rendering", "text": "We can use the v-for directive to render a list of elements based on a source array: < ul > < li v-for = \"todo in todos\" :key = \"todo.id\" > {{ todo.text }} </ li > </ ul > Here todo is a local variable representing the array element currently being iterated on. It's only accessible on or inside the v-for element. Notice how we are also giving each todo object a unique id , and binding it as the special key attribute for each <li> . The key allows Vue to accurately move each <li> to match the position of its corresponding object in the array. There are two ways to update the list: Call mutating methods on the source array: this . todos . push ( newTodo ) Replace the array with a new one: this . todos = this . todos . filter ( /* ... */ ) Example: <script> // give each todo a unique id let id = 0 export default { data() { return { newTodo: '', todos: [ { id: id++, text: 'Learn HTML' }, { id: id++, text: 'Learn JavaScript' }, { id: id++, text: 'Learn Vue' } ] } }, methods: { addTodo() { this.todos.push({ id: id++, text: this.newTodo}) this.newTodo = '' }, removeTodo(todo) { this.todos = this.todos.filter((element) => element.id != todo.id) } } } </script> <template> <form @submit.prevent=\"addTodo\"> <input v-model=\"newTodo\"> <button>Add Todo</button> </form> <ul> <li v-for=\"todo in todos\" :key=\"todo.id\"> {{ todo.text }} <button @click=\"removeTodo(todo)\">X</button> </li> </ul> </template> v-for also supports an optional second alias for the index of the current item: data () { return { parentMessage : 'Parent' , items : [{ message : 'Foo' }, { message : 'Bar' }] } } < li v-for = \"(item, index) in items\" > {{ parentMessage }} - {{ index }} - {{ item.message }} </ li > Similar to template v-if , you can also use a <template> tag with v-for to render a block of multiple elements. For example: < ul > < template v-for = \"item in items\" > < li > {{ item.msg }} </ li > < li class = \"divider\" role = \"presentation\" ></ li > </ template > </ ul > It's not recommended to use v-if and v-for on the same element due to implicit precedence. Instead of: < li v-for = \"todo in todos\" v-if = \"!todo.isComplete\" > {{ todo.name }} </ li > Use: < template v-for = \"todo in todos\" > < li v-if = \"!todo.isComplete\" > {{ todo.name }} </ li > </ template >", "title": "List rendering"}, {"location": "vuejs/#v-for-with-an-object", "text": "You can also use v-for to iterate through the properties of an object. data () { return { myObject : { title : 'How to do lists in Vue' , author : 'Jane Doe' , publishedAt : '2016-04-10' } } } < ul > < li v-for = \"value in myObject\" > {{ value }} </ li > </ ul > You can also provide a second alias for the property's name: < li v-for = \"(value, key) in myObject\" > {{ key }}: {{ value }} </ li > And another for the index: < li v-for = \"(value, key, index) in myObject\" > {{ index }}. {{ key }}: {{ value }} </ li >", "title": "v-for with an object"}, {"location": "vuejs/#v-for-with-a-range", "text": "v-for can also take an integer. In this case it will repeat the template that many times, based on a range of 1...n . < span v-for = \"n in 10\" > {{ n }} </ span > Note here n starts with an initial value of 1 instead of 0.", "title": "v-for with a Range"}, {"location": "vuejs/#v-for-with-a-component", "text": "You can directly use v-for on a component , like any normal element (don't forget to provide a key): < my-component v-for = \"item in items\" :key = \"item.id\" ></ my-component > However, this won't automatically pass any data to the component, because components have isolated scopes of their own. In order to pass the iterated data into the component, we should also use props: < my-component v-for = \"(item, index) in items\" :item = \"item\" :index = \"index\" :key = \"item.id\" ></ my-component > The reason for not automatically injecting item into the component is because that makes the component tightly coupled to how v-for works. Being explicit about where its data comes from makes the component reusable in other situations.", "title": "v-for with a Component"}, {"location": "vuejs/#computed-property", "text": "We can declare a property that is reactively computed from other properties using the computed option: export default { // ... computed : { filteredTodos () { if ( this . hideCompleted ) { return this . todos . filter (( t ) => t . done === false ) } else { return this . todos } } } } } A computed property tracks other reactive state used in its computation as dependencies. It caches the result and automatically updates it when its dependencies change. So it's better than defining the function as a method", "title": "Computed Property"}, {"location": "vuejs/#lifecycle-hooks", "text": "Each Vue component instance goes through a series of initialization steps when it's created - for example, it needs to set up data observation, compile the template, mount the instance to the DOM, and update the DOM when data changes. Along the way, it also runs functions called lifecycle hooks, giving users the opportunity to add their own code at specific stages. For example, the mounted hook can be used to run code after the component has finished the initial rendering and created the DOM nodes: export default { mounted () { console . log ( `the component is now mounted.` ) } } There are also other hooks which will be called at different stages of the instance's lifecycle, with the most commonly used being mounted , updated , and unmounted . All lifecycle hooks are called with their this context pointing to the current active instance invoking it. Note this means you should avoid using arrow functions when declaring lifecycle hooks, as you won't be able to access the component instance via this if you do so.", "title": "Lifecycle hooks"}, {"location": "vuejs/#template-refs", "text": "While Vue's declarative rendering model abstracts away most of the direct DOM operations for you, there may still be cases where we need direct access to the underlying DOM elements. To achieve this, we can use the special ref attribute: < input ref = \"input\" > ref allows us to obtain a direct reference to a specific DOM element or child component instance after it's mounted. This may be useful when you want to, for example, programmatically focus an input on component mount, or initialize a 3 rd party library on an element. The resulting ref is exposed on this.$refs : < script > export default { mounted () { this . $refs . input . focus () } } < /script> < template > < input ref = \"input\" /> </ template > Note that you can only access the ref after the component is mounted. If you try to access $refs.input in a template expression, it will be null on the first render. This is because the element doesn't exist until after the first render!", "title": "Template Refs"}, {"location": "vuejs/#watchers", "text": "Computed properties allow us to declaratively compute derived values. However, there are cases where we need to perform \"side effects\" in reaction to state changes, for example, mutating the DOM, or changing another piece of state based on the result of an async operation. With Options API, we can use the watch option to trigger a function whenever a reactive property changes: export default { data () { return { question : '' , answer : 'Questions usually contain a question mark. ;-)' } }, watch : { // whenever question changes, this function will run question ( newQuestion , oldQuestion ) { if ( newQuestion . indexOf ( '?' ) > - 1 ) { this . getAnswer () } } }, methods : { async getAnswer () { this . answer = 'Thinking...' try { const res = await fetch ( 'https://yesno.wtf/api' ) this . answer = ( await res . json ()). answer } catch ( error ) { this . answer = 'Error! Could not reach the API. ' + error } } } } < p > Ask a yes/no question: < input v-model = \"question\" /> </ p > < p > {{ answer }} </ p >", "title": "Watchers"}, {"location": "vuejs/#deep-watchers", "text": "watch is shallow by default: the callback will only trigger when the watched property has been assigned a new value - it won't trigger on nested property changes. If you want the callback to fire on all nested mutations, you need to use a deep watcher: export default { watch : { someObject : { handler ( newValue , oldValue ) { // Note: `newValue` will be equal to `oldValue` here // on nested mutations as long as the object itself // hasn't been replaced. }, deep : true } } } Note \"Deep watch requires traversing all nested properties in the watched object, and can be expensive when used on large data structures. Use it only when necessary and beware of the performance implications.\"", "title": "Deep watchers"}, {"location": "vuejs/#eager-watchers", "text": "watch is lazy by default: the callback won't be called until the watched source has changed. But in some cases we may want the same callback logic to be run eagerly, for example, we may want to fetch some initial data, and then re-fetch the data whenever relevant state changes. We can force a watcher's callback to be executed immediately by declaring it using an object with a handler function and the immediate: true option: export default { // ... watch : { question : { handler ( newQuestion ) { // this will be run immediately on component creation. }, // force eager callback execution immediate : true } } // ... }", "title": "Eager watchers"}, {"location": "vuejs/#environment-variables", "text": "If you're using Vue 3 and Vite you can use the environment variables by defining them in .env files. You can specify environment variables by placing the following files in your project root: .env : Loaded in all cases. .env.local : Loaded in all cases, ignored by git. .env.[mode] : Only loaded in specified mode. .env.[mode].local : Only loaded in specified mode, ignored by git. An env file simply contains key=value pairs of environment variables, by default only variables that start with VITE_ will be exposed.: DB_PASSWORD=foobar VITE_SOME_KEY=123 Only VITE_SOME_KEY will be exposed as import.meta.env.VITE_SOME_KEY to your client source code, but DB_PASSWORD will not. So for example in a component you can use: export default { props: {}, mounted() { console.log(import.meta.env.VITE_SOME_KEY) }, data: () => ({ }), }", "title": "Environment variables"}, {"location": "vuejs/#make-http-requests", "text": "There are many ways to do requests to external services: Fetch API Axios", "title": "Make HTTP requests"}, {"location": "vuejs/#fetch-api", "text": "The Fetch API is a standard API for making HTTP requests on the browser. It a great alternative to the old XMLHttpRequestconstructor for making requests. It supports all kinds of requests, including GET, POST, PUT, PATCH, DELETE, and OPTIONS, which is what most people need. To make a request with the Fetch API, we don\u2019t have to do anything. All we have to do is to make the request directly with the fetch object. For instance, you can write: < template > < div id = \"app\" > {{data}} </ div > </ template >< script > export default { name : \"App\" , data () { return { data : {} } }, beforeMount (){ this . getName (); }, methods : { async getName (){ const res = await fetch ( 'https://api.agify.io/?name=michael' ); const data = await res . json (); this . data = data ; } } }; </ script > In the code above, we made a simple GET request from an API and then convert the data from JSON to a JavaScript object with the json() method.", "title": "Fetch API"}, {"location": "vuejs/#adding-headers", "text": "Like most HTTP clients, we can send request headers and bodies with the Fetch API. To send a request with HTTP headers, we can write: < template > < div id = \"app\" > < img :src = \"data.src.tiny\" > </ div > </ template >< script > export default { name : \"App\" , data () { return { data : { src : {} } }; }, beforeMount () { this . getPhoto (); }, methods : { async getPhoto () { const headers = new Headers (); headers . append ( \"Authorization\" , \"api_key\" ); const request = new Request ( \"https://api.pexels.com/v1/curated?per_page=11&page=1\" , { method : \"GET\" , headers , mode : \"cors\" , cache : \"default\" } ); const res = await fetch ( request ); const { photos } = await res . json (); this . data = photos [ 0 ]; } } }; </ script > In the code above, we used the Headers constructor, which is used to add requests headers to Fetch API requests. The append method appends our 'Authorization' header to the request. We\u2019ve to set the mode to 'cors' for a cross-domain request and headers is set to the headers object returned by the Headers constructor.", "title": "Adding headers"}, {"location": "vuejs/#adding-body-to-a-request", "text": "To make a request body, we can write the following: < template > < div id = \"app\" > < form @ submit . prevent = \"createPost\" > < input placeholder = \"name\" v-model = \"post.name\" > < input placeholder = \"title\" v-model = \"post.title\" > < br > < button type = \"submit\" > Create </ button > </ form > {{data}} </ div > </ template >< script > export default { name : \"App\" , data () { return { post : {}, data : {} }; }, methods : { async createPost () { const request = new Request ( \"https://jsonplaceholder.typicode.com/posts\" , { method : \"POST\" , mode : \"cors\" , cache : \"default\" , body : JSON . stringify ( this . post ) } ); const res = await fetch ( request ); const data = await res . json (); this . data = data ; } } }; </ script > In the code above, we made the request by stringifying the this.post object and then sending it with a POST request.", "title": "Adding body to a request"}, {"location": "vuejs/#axios", "text": "Axios is a popular HTTP client that works on both browser and Node.js apps. We can install it by running: npm i axios Then we can use it to make requests a simple GET request as follows: < template > < div id = \"app\" > {{data}} </ div > </ template >< script > import axios from 'axios' export default { name : \"App\" , data () { return { data : {} }; }, beforeMount (){ this . getName (); }, methods : { async getName (){ const { data } = await axios . get ( \"https://api.agify.io/?name=michael\" ); this . data = data ; } } }; </ script > In the code above, we call the axios.get method with the URL to make the request. Then we assign the response data to an object.", "title": "Axios"}, {"location": "vuejs/#adding-headers_1", "text": "If we want to make a request with headers, we can write: < template > < div id = \"app\" > < img :src = \"data.src.tiny\" > </ div > </ template >< script > import axios from 'axios' export default { name : \"App\" , data () { return { data : {} }; }, beforeMount () { this . getPhoto (); }, methods : { async getPhoto () { const { data : { photos } } = await axios ({ url : \"https://api.pexels.com/v1/curated?per_page=11&page=1\" , headers : { Authorization : \"api_key\" } }); this . data = photos [ 0 ]; } } }; </ script > In the code above, we made a GET request with our Pexels API key with the axios method, which can be used for making any kind of request. If no request verb is specified, then it\u2019ll be a GET request. As we can see, the code is a bit shorter since we don\u2019t have to create an object with the Headers constructor. If we want to set the same header in multiple requests, we can use a request interceptor to set the header or other config for all requests. For instance, we can rewrite the above example as follows: // main.js: import Vue from \"vue\" ; import App from \"./App.vue\" ; import axios from 'axios' axios . interceptors . request . use ( config => { return { ... config , headers : { Authorization : \"api_key\" } }; }, error => Promise . reject ( error ) ); Vue . config . productionTip = false ; new Vue ({ render : h => h ( App ) }). $mount ( \"#app\" ); < template > < div id = \"app\" > < img :src = \"data.src.tiny\" > </ div > </ template >< script > import axios from 'axios' export default { name : \"App\" , data () { return { data : {} }; }, beforeMount () { this . getPhoto (); }, methods : { async getPhoto () { const { data : { photos } } = await axios ({ url : \"https://api.pexels.com/v1/curated?per_page=11&page=1\" }); this . data = photos [ 0 ]; } } }; </ script > We moved the header to `main.js` inside the code for our interceptor. The first argument that\u2019s passed into `axios.interceptors.request.use` is a function for modifying the request config for all requests. And the 2nd argument is an error handler for handling error of all requests. Likewise, we can configure interceptors for responses as well. #### Adding body to a request To make a POST request with a request body, we can use the `axios.post` method. ```html < template > < div id = \"app\" > < form @ submit . prevent = \"createPost\" > < input placeholder = \"name\" v-model = \"post.name\" > < input placeholder = \"title\" v-model = \"post.title\" > < br > < button type = \"submit\" > Create </ button > </ form > {{data}} </ div > </ template >< script > import axios from 'axios' export default { name : \"App\" , data () { return { post : {}, data : {} }; }, methods : { async createPost () { const { data } = await axios . post ( \"https://jsonplaceholder.typicode.com/posts\" , this . post ); this . data = data ; } } }; </ script > We make the POST request with the axios.post method with the request body in the second argument. Axios also sets the Content-Type header to application/json. This enables web frameworks to automatically parse the data. Then we get back the response data by getting the data property from the resulting response.", "title": "Adding headers"}, {"location": "vuejs/#shorthand-methods-for-axios-http-requests", "text": "Axios also provides a set of shorthand methods for performing different types of requests. The methods are as follows: axios.request(config) axios.get(url[, config]) axios.delete(url[, config]) axios.head(url[, config]) axios.options(url[, config]) axios.post(url[, data[, config]]) axios.put(url[, data[, config]]) axios.patch(url[, data[, config]]) For instance, the following code shows how the previous example could be written using the axios.post() method: axios . post ( '/login' , { firstName : 'Finn' , lastName : 'Williams' }) . then (( response ) => { console . log ( response ); }, ( error ) => { console . log ( error ); }); Once an HTTP POST request is made, Axios returns a promise that is either fulfilled or rejected, depending on the response from the backend service. To handle the result, you can use the then() . method. If the promise is fulfilled, the first argument of then() will be called; if the promise is rejected, the second argument will be called. According to the documentation, the fulfillment value is an object containing the following information: { // `data` is the response that was provided by the server data : {}, // `status` is the HTTP status code from the server response status : 200 , // `statusText` is the HTTP status message from the server response statusText : 'OK' , // `headers` the headers that the server responded with // All header names are lower cased headers : {}, // `config` is the config that was provided to `axios` for the request config : {}, // `request` is the request that generated this response // It is the last ClientRequest instance in node.js (in redirects) // and an XMLHttpRequest instance the browser request : {} }", "title": "Shorthand methods for Axios HTTP requests"}, {"location": "vuejs/#using-interceptors", "text": "One of the key features of Axios is its ability to intercept HTTP requests. HTTP interceptors come in handy when you need to examine or change HTTP requests from your application to the server or vice versa (e.g., logging, authentication, or retrying a failed HTTP request). With interceptors, you won\u2019t have to write separate code for each HTTP request. HTTP interceptors are helpful when you want to set a global strategy for how you handle request and response. axios . interceptors . request . use ( config => { // log a message before any HTTP request is sent console . log ( 'Request was sent' ); return config ; }); // sent a GET request axios . get ( 'https://api.github.com/users/sideshowbarker' ) . then ( response => { console . log ( response . data ); }); In this code, the axios.interceptors.request.use() method is used to define code to be run before an HTTP request is sent. Also, axios.interceptors.response.use() can be used to intercept the response from the server. Let\u2019s say there is a network error; using the response interceptors, you can retry that same request using interceptors.", "title": "Using interceptors"}, {"location": "vuejs/#handling-errors", "text": "To catch errors when doing requests you could use: try { let res = await axios . get ( '/my-api-route' ); // Work with the response... } catch ( error ) { if ( error . response ) { // The client was given an error response (5xx, 4xx) console . log ( err . response . data ); console . log ( err . response . status ); console . log ( err . response . headers ); } else if ( error . request ) { // The client never received a response, and the request was never left console . log ( err . request ); } else { // Anything else console . log ( 'Error' , err . message ); } } The differences in the error object, indicate where the request encountered the issue. error.response : If your error object has a response property, it means that your server returned a 4xx/5xx error. This will assist you choose what sort of message to return to users. error.request : This error is caused by a network error, a hanging backend that does not respond instantly to each request, unauthorized or cross-domain requests, and lastly if the backend API returns an error. This occurs when the browser was able to initiate a request but did not receive a valid answer for any reason. Other errors: It's possible that the error object does not have either a response or request object attached to it. In this case it is implied that there was an issue in setting up the request, which eventually triggered an error. For example, this could be the case if you omit the URL parameter from the .get() call, and thus no request was ever made.", "title": "Handling errors"}, {"location": "vuejs/#sending-multiple-requests", "text": "One of Axios\u2019 more interesting features is its ability to make multiple requests in parallel by passing an array of arguments to the axios.all() method. This method returns a single promise object that resolves only when all arguments passed as an array have resolved. Here\u2019s a simple example of how to use axios.all to make simultaneous HTTP requests: // execute simultaneous requests axios . all ([ axios . get ( 'https://api.github.com/users/mapbox' ), axios . get ( 'https://api.github.com/users/phantomjs' ) ]) . then ( responseArr => { //this will be executed only when all requests are complete console . log ( 'Date created: ' , responseArr [ 0 ]. data . created_at ); console . log ( 'Date created: ' , responseArr [ 1 ]. data . created_at ); }); // logs: // => Date created: 2011-02-04T19:02:13Z // => Date created: 2017-04-03T17:25:46Z This code makes two requests to the GitHub API and then logs the value of the created_at property of each response to the console. Keep in mind that if any of the arguments rejects then the promise will immediately reject with the reason of the first promise that rejects. For convenience, Axios also provides a method called axios.spread() to assign the properties of the response array to separate variables. Here\u2019s how you could use this method: axios . all ([ axios . get ( 'https://api.github.com/users/mapbox' ), axios . get ( 'https://api.github.com/users/phantomjs' ) ]) . then ( axios . spread (( user1 , user2 ) => { console . log ( 'Date created: ' , user1 . data . created_at ); console . log ( 'Date created: ' , user2 . data . created_at ); })); // logs: // => Date created: 2011-02-04T19:02:13Z // => Date created: 2017-04-03T17:25:46Z The output of this code is the same as the previous example. The only difference is that the axios.spread() method is used to unpack values from the response array.", "title": "Sending multiple requests"}, {"location": "vuejs/#veredict", "text": "If you\u2019re working on multiple requests, you\u2019ll find that Fetch requires you to write more code than Axios, even when taking into consideration the setup needed for it. Therefore, for simple requests, Fetch API and Axios are quite the same. However, for more complex requests, Axios is better as it allows you to configure multiple requests in one place. If you're making a simple request use the Fetch API, for the other cases use axios because: It allows you to configure multiple requests in one place Code is shorter. It allows you to place all the API calls under services so that these can be reused across components wherever they are needed . It's easy to set a timeout of the request. It supports HTTP interceptors by befault It does automatic JSON data transformation. It's supported by old browsers, although you can bypass the problem with fetch too. It has a progress indicator for large files. Supports simultaneous requests by default. Axios provides an easy-to-use API in a compact package for most of your HTTP communication needs. However, if you prefer to stick with native APIs, nothing stops you from implementing Axios features. For more information read: How To Make API calls in Vue.JS Applications by Bhargav Bachina Axios vs. fetch(): Which is best for making HTTP requests? by Faraz Kelhini", "title": "Veredict"}, {"location": "vuejs/#vue-router", "text": "Creating a Single-page Application with Vue + Vue Router feels natural, all we need to do is map our components to the routes and let Vue Router know where to render them. Here's a basic example: < script src = \"https://unpkg.com/vue@3\" ></ script > < script src = \"https://unpkg.com/vue-router@4\" ></ script > < div id = \"app\" > < h1 > Hello App! </ h1 > < p > <!-- use the router-link component for navigation. --> <!-- specify the link by passing the `to` prop. --> <!-- `<router-link>` will render an `<a>` tag with the correct `href` attribute --> < router-link to = \"/\" > Go to Home </ router-link > < router-link to = \"/about\" > Go to About </ router-link > </ p > <!-- route outlet --> <!-- component matched by the route will render here --> < router-view ></ router-view > </ div > Note how instead of using regular a tags, we use a custom component router-link to create links. This allows Vue Router to change the URL without reloading the page, handle URL generation as well as its encoding. router-view will display the component that corresponds to the url. You can put it anywhere to adapt it to your layout. // 1. Define route components. // These can be imported from other files const Home = { template : '<div>Home</div>' } const About = { template : '<div>About</div>' } // 2. Define some routes // Each route should map to a component. // We'll talk about nested routes later. const routes = [ { path : '/' , component : Home }, { path : '/about' , component : About }, ] // 3. Create the router instance and pass the `routes` option // You can pass in additional options here, but let's // keep it simple for now. const router = VueRouter . createRouter ({ // 4. Provide the history implementation to use. We are using the hash history for simplicity here. history : VueRouter . createWebHashHistory (), routes , // short for `routes: routes` }) // 5. Create and mount the root instance. const app = Vue . createApp ({}) // Make sure to _use_ the router instance to make the // whole app router-aware. app . use ( router ) app . mount ( '#app' ) // Now the app has started! By calling app.use(router) , we get access to it as this.$router as well as the current route as this.$route inside of any component: // Home.vue export default { computed: { username() { // We will see what `params` is shortly return this.$route.params.username }, }, methods: { goToDashboard() { if (isAuthenticated) { this.$router.push('/dashboard') } else { this.$router.push('/login') } }, }, } To access the router or the route inside the setup function, call the useRouter or useRoute functions.", "title": "Vue Router"}, {"location": "vuejs/#dynamic-route-matching-with-params", "text": "Very often we will need to map routes with the given pattern to the same component. For example we may have a User component which should be rendered for all users but with different user IDs. In Vue Router we can use a dynamic segment in the path to achieve that, we call that a param : const User = { template : '<div>User</div>' , } // these are passed to `createRouter` const routes = [ // dynamic segments start with a colon { path : '/users/:id' , component : User }, ] Now URLs like /users/johnny and /users/jolyne will both map to the same route. A param is denoted by a colon :. When a route is matched, the value of its params will be exposed as this.$route.params in every component. Therefore, we can render the current user ID by updating User's template to this: const User = { template: ' < div > User {{ $route.params.id }} </ div > ', } You can have multiple params in the same route, and they will map to corresponding fields on $route.params . Examples: pattern matched path $route.params /users/:username /users/eduardo { username: 'eduardo' } /users/:username/posts/:postId /users/eduardo/posts/123 { username: 'eduardo', postId: '123' } In addition to $route.params , the $route object also exposes other useful information such as $route.query (if there is a query in the URL), $route.hash , etc.", "title": "Dynamic route matching with params"}, {"location": "vuejs/#reacting-to-params-changes", "text": "One thing to note when using routes with params is that when the user navigates from /users/johnny to /users/jolyne , the same component instance will be reused. Since both routes render the same component, this is more efficient than destroying the old instance and then creating a new one. However, this also means that the lifecycle hooks of the component will not be called. To react to params changes in the same component, you can simply watch anything on the $route object, in this scenario, the $route.params : const User = { template : '...' , created () { this . $watch ( () => this . $route . params , ( toParams , previousParams ) => { // react to route changes... } ) }, } Or, use the beforeRouteUpdate navigation guard, which also allows to cancel the navigation: const User = { template : '...' , async beforeRouteUpdate ( to , from ) { // react to route changes... this . userData = await fetchUser ( to . params . id ) }, }", "title": "Reacting to params changes"}, {"location": "vuejs/#components", "text": "Components allow us to split the UI into independent and reusable pieces, and think about each piece in isolation. It's common for an app to be organized into a tree of nested components", "title": "Components"}, {"location": "vuejs/#defining-a-component", "text": "When using a build step, we typically define each Vue component in a dedicated file using the .vue extension. <script> export default { data() { return { count: 0 } } } </script> <template> <button @click=\"count++\">You clicked me {{ count }} times.</button> </template>", "title": "Defining a component"}, {"location": "vuejs/#using-a-component", "text": "To use a child component, we need to import it in the parent component. Assuming we placed our counter component inside a file called ButtonCounter.vue , the component will be exposed as the file's default export: <script> import ButtonCounter from './ButtonCounter.vue' export default { components: { ButtonCounter } } </script> <template> <h1>Here is a child component!</h1> <ButtonCounter /> </template> To expose the imported component to our template, we need to register it with the components option. The component will then be available as a tag using the key it is registered under. Components can be reused as many times as you want: < h1 > Here are many child components! </ h1 > < ButtonCounter /> < ButtonCounter /> < ButtonCounter /> When clicking on the buttons, each one maintains its own, separate count. That's because each time you use a component, a new instance of it is created.", "title": "Using a component"}, {"location": "vuejs/#passing-props", "text": "Props are custom attributes you can register on a component. Vue components require explicit props declaration so that Vue knows what external props passed to the component should be treated as fallthrough attributes. <!-- BlogPost.vue --> <script> export default { props: ['title'] } </script> <template> <h4>{{ title }}</h4> </template> When a value is passed to a prop attribute, it becomes a property on that component instance. The value of that property is accessible within the template and on the component's this context, just like any other component property. A component can have as many props as you like and, by default, any value can be passed to any prop. Once a prop is registered, you can pass data to it as a custom attribute, like this: < BlogPost title = \"My journey with Vue\" /> < BlogPost title = \"Blogging with Vue\" /> < BlogPost title = \"Why Vue is so fun\" /> In a typical app, however, you'll likely have an array of posts in your parent component: export default { // ... data () { return { posts : [ { id : 1 , title : 'My journey with Vue' }, { id : 2 , title : 'Blogging with Vue' }, { id : 3 , title : 'Why Vue is so fun' } ] } } } Then want to render a component for each one, using v-for : < BlogPost v-for = \"post in posts\" :key = \"post.id\" :title = \"post.title\" /> We declare long prop names using camelCase because this avoids having to use quotes when using them as property keys. export default { props : { greetingMessage : String } } < span > {{ greetingMessage }} </ span > However, the convention is using kebab-case when passing props to a child component. < MyComponent greeting-message = \"hello\" />", "title": "Passing props"}, {"location": "vuejs/#passing-different-value-types-on-props", "text": "Numbers: <!-- Even though `42` is static, we need v-bind to tell Vue that --> <!-- this is a JavaScript expression rather than a string. --> < BlogPost :likes = \"42\" /> <!-- Dynamically assign to the value of a variable. --> < BlogPost :likes = \"post.likes\" /> Boolean: <!-- Including the prop with no value will imply `true`. --> < BlogPost is-published /> <!-- Even though `false` is static, we need v-bind to tell Vue that --> <!-- this is a JavaScript expression rather than a string. --> < BlogPost :is-published = \"false\" /> <!-- Dynamically assign to the value of a variable. --> < BlogPost :is-published = \"post.isPublished\" /> Array <!-- Even though the array is static, we need v-bind to tell Vue that --> <!-- this is a JavaScript expression rather than a string. --> < BlogPost :comment-ids = \"[234, 266, 273]\" /> <!-- Dynamically assign to the value of a variable. --> < BlogPost :comment-ids = \"post.commentIds\" /> Object <!-- Even though the object is static, we need v-bind to tell Vue that --> <!-- this is a JavaScript expression rather than a string. --> < BlogPost :author = \"{ name: 'Veronica', company: 'Veridian Dynamics' }\" /> <!-- Dynamically assign to the value of a variable. --> < BlogPost :author = \"post.author\" /> If you want to pass all the properties of an object as props, you can use v-bind without an argument. export default { data () { return { post : { id : 1 , title : 'My Journey with Vue' } } } } The following template: < BlogPost v-bind = \"post\" /> Will be equivalent to: < BlogPost :id = \"post.id\" :title = \"post.title\" />", "title": "Passing different value types on props"}, {"location": "vuejs/#one-way-data-flow-in-props", "text": "All props form a one-way-down binding between the child property and the parent one: when the parent property updates, it will flow down to the child, but not the other way around. Every time the parent component is updated, all props in the child component will be refreshed with the latest value. This means you should not attempt to mutate a prop inside a child component.", "title": "One-way data flow in props"}, {"location": "vuejs/#prop-validation", "text": "Components can specify requirements for their props, if a requirement is not met, Vue will warn you in the browser's JavaScript console. export default { props : { // Basic type check // (`null` and `undefined` values will allow any type) propA : Number , // Multiple possible types propB : [ String , Number ], // Required string propC : { type : String , required : true }, // Number with a default value propD : { type : Number , default : 100 }, // Object with a default value propE : { type : Object , // Object or array defaults must be returned from // a factory function. The function receives the raw // props received by the component as the argument. default ( rawProps ) { // default function receives the raw props object as argument return { message : 'hello' } } }, // Custom validator function propF : { validator ( value ) { // The value must match one of these strings return [ 'success' , 'warning' , 'danger' ]. includes ( value ) } }, // Function with a default value propG : { type : Function , // Unlike object or array default, this is not a factory function - this is a function to serve as a default value default () { return 'Default function' } } } } Additional details: All props are optional by default, unless required: true is specified. An absent optional prop will have undefined value. If a default value is specified, it will be used if the resolved prop value is undefined , this includes both when the prop is absent, or an explicit undefined value is passed.", "title": "Prop validation"}, {"location": "vuejs/#listening-to-events", "text": "As we develop our <BlogPost> component, some features may require communicating back up to the parent. For example, we may decide to include an accessibility feature to enlarge the text of blog posts, while leaving the rest of the page at its default size. In the parent, we can support this feature by adding a postFontSize data property: data () { return { posts : [ /* ... */ ], postFontSize : 1 } } Which can be used in the template to control the font size of all blog posts: < div :style = \"{ fontSize: postFontSize + 'em' }\" > < BlogPost v-for = \"post in posts\" :key = \"post.id\" :title = \"post.title\" /> </ div > Now let's add a button to the <BlogPost> component's template: <!-- BlogPost.vue, omitting <script> --> <template> <div class=\"blog-post\"> <h4>{{ title }}</h4> <button>Enlarge text</button> </div> </template> The button currently doesn't do anything yet - we want clicking the button to communicate to the parent that it should enlarge the text of all posts. To solve this problem, component instances provide a custom events system. The parent can choose to listen to any event on the child component instance with v-on or @, just as we would with a native DOM event: < BlogPost ... @ enlarge-text = \"postFontSize += 0.1\" /> Then the child component can emit an event on itself by calling the built-in $emit method, passing the name of the event: <!-- BlogPost.vue, omitting <script> --> < template > < div class = \"blog-post\" > < h4 > {{ title }} </ h4 > < button @ click = \"$emit('enlarge-text')\" > Enlarge text </ button > </ div > </ template > The first argument to this.$emit() is the event name. Any additional arguments are passed on to the event listener. Thanks to the @enlarge-text=\"postFontSize += 0.1\" listener, the parent will receive the event and update the value of postFontSize . We can optionally declare emitted events using the emits option: <!-- BlogPost.vue --> <script> export default { props: ['title'], emits: ['enlarge-text'] } </script> This documents all the events that a component emits and optionally validates them. It also allows Vue to avoid implicitly applying them as native listeners to the child component's root element.", "title": "Listening to Events"}, {"location": "vuejs/#event-arguments", "text": "It's sometimes useful to emit a specific value with an event. For example, we may want the <BlogPost> component to be in charge of how much to enlarge the text by. In those cases, we can pass extra arguments to $emit to provide this value: < button @ click = \"$emit('increaseBy', 1)\" > Increase by 1 </ button > Then, when we listen to the event in the parent, we can use an inline arrow function as the listener, which allows us to access the event argument: < MyButton @ increase-by = \"(n) => count += n\" /> Or, if the event handler is a method: Then the value will be passed as the first parameter of that method: methods : { increaseCount ( n ) { this . count += n } }", "title": "Event arguments"}, {"location": "vuejs/#declaring-emitted-events", "text": "Emitted events can be explicitly declared on the component via the emits option. export default { emits : [ 'inFocus' , 'submit' ] } The emits option also supports an object syntax, which allows us to perform runtime validation of the payload of the emitted events: export default { emits : { submit ( payload ) { // return `true` or `false` to indicate // validation pass / fail } } } Although optional, it is recommended to define all emitted events in order to better document how a component should work.", "title": "Declaring emitted events"}, {"location": "vuejs/#content-distribution-with-slots", "text": "In addition to passing data via props, the parent component can also pass down template fragments to the child via slots: <ChildComp> This is some slot content! </ChildComp> In the child component, it can render the slot content from the parent using the <slot> element as outlet: <!-- in child template --> <slot/> Content inside the <slot> outlet will be treated as \"fallback\" content: it will be displayed if the parent did not pass down any slot content: <slot>Fallback content</slot> Slot content is not just limited to text. It can be any valid template content. For example, we can pass in multiple elements, or even other components: < FancyButton > < span style = \"color:red\" > Click me! </ span > < AwesomeIcon name = \"plus\" /> </ FancyButton > Slot content has access to the data scope of the parent component, because it is defined in the parent. However, slot content does not have access to the child component's data. As a rule, remember that everything in the parent template is compiled in parent scope; everything in the child template is compiled in the child scope. You can however use child content using scoped slots .", "title": "Content distribution with Slots"}, {"location": "vuejs/#named-slots", "text": "There are times when it's useful to have multiple slot outlets in a single component. For these cases, the <slot> element has a special attribute, name , which can be used to assign a unique ID to different slots so you can determine where content should be rendered: < div class = \"container\" > < header > < slot name = \"header\" ></ slot > </ header > < main > < slot ></ slot > </ main > < footer > < slot name = \"footer\" ></ slot > </ footer > </ div > To pass a named slot, we need to use a <template> element with the v-slot directive, and then pass the name of the slot as an argument to v-slot : < BaseLayout > < template # header > < h1 > Here might be a page title </ h1 > </ template > < template # default > < p > A paragraph for the main content. </ p > < p > And another one. </ p > </ template > < template # footer > < p > Here's some contact info </ p > </ template > </ BaseLayout > Where # is the shorthand of v-slot .", "title": "Named Slots"}, {"location": "vuejs/#dynamic-components", "text": "Sometimes, it's useful to dynamically switch between components, like in a tabbed interface, for example in this page . The above is made possible by Vue's <component> element with the special is attribute: <!-- Component changes when currentTab changes --> <component :is=\"currentTab\"></component> In the example above, the value passed to :is can contain either: The name string of a registered component, OR. The actual imported component object. You can also use the is attribute to create regular HTML elements. When switching between multiple components with <component :is=\"...\"> , a component will be unmounted when it is switched away from. We can force the inactive components to stay \"alive\" with the built-in <KeepAlive> component.", "title": "Dynamic components"}, {"location": "vuejs/#async-components", "text": "In large applications, we may need to divide the app into smaller chunks and only load a component from the server when it's needed. To make that possible, Vue has a defineAsyncComponent function: import { defineAsyncComponent } from 'vue' const AsyncComp = defineAsyncComponent (() => import ( './components/MyComponent.vue' ) ) Asynchronous operations inevitably involve loading and error states, defineAsyncComponent() supports handling these states via advanced options: const AsyncComp = defineAsyncComponent ({ // the loader function loader : () => import ( './Foo.vue' ), // A component to use while the async component is loading loadingComponent : LoadingComponent , // Delay before showing the loading component. Default: 200ms. delay : 200 , // A component to use if the load fails errorComponent : ErrorComponent , // The error component will be displayed if a timeout is // provided and exceeded. Default: Infinity. timeout : 3000 })", "title": "Async components"}, {"location": "vuejs/#testing", "text": "When designing your Vue application's testing strategy, you should leverage the following testing types: Unit : Checks that inputs to a given function, class, or composable are producing the expected output or side effects. Component : Checks that your component mounts, renders, can be interacted with, and behaves as expected. These tests import more code than unit tests, are more complex, and require more time to execute. End-to-end : Checks features that span multiple pages and make real network requests against your production-built Vue application. These tests often involve standing up a database or other backend.", "title": "Testing"}, {"location": "vuejs/#unit-testing", "text": "Unit tests will catch issues with a function's business logic and logical correctness. Take for example this increment function: // helpers.js export function increment ( current , max = 10 ) { if ( current < max ) { return current + 1 } return current } Because it's very self-contained, it'll be easy to invoke the increment function and assert that it returns what it's supposed to, so we'll write a Unit Test. If any of these assertions fail, it's clear that the issue is contained within the increment function. // helpers.spec.js import { increment } from './helpers' describe ( 'increment' , () => { test ( 'increments the current number by 1' , () => { expect ( increment ( 0 , 10 )). toBe ( 1 ) }) test ( 'does not increment the current number over the max' , () => { expect ( increment ( 10 , 10 )). toBe ( 10 ) }) test ( 'has a default max of 10' , () => { expect ( increment ( 10 )). toBe ( 10 ) }) }) Unit testing is typically applied to self-contained business logic, components, classes, modules, or functions that do not involve UI rendering, network requests, or other environmental concerns. These are typically plain JavaScript / TypeScript modules unrelated to Vue. In general, writing unit tests for business logic in Vue applications does not differ significantly from applications using other frameworks. There are two instances where you DO unit test Vue-specific features: Composables Components", "title": "Unit testing"}, {"location": "vuejs/#component-testing", "text": "In Vue applications, components are the main building blocks of the UI. Components are therefore the natural unit of isolation when it comes to validating your application's behavior. From a granularity perspective, component testing sits somewhere above unit testing and can be considered a form of integration testing. Much of your Vue Application should be covered by a component test and we recommend that each Vue component has its own spec file. Component tests should catch issues relating to your component's props, events, slots that it provides, styles, classes, lifecycle hooks, and more. Component tests should not mock child components, but instead test the interactions between your component and its children by interacting with the components as a user would. For example, a component test should click on an element like a user would instead of programmatically interacting with the component. Component tests should focus on the component's public interfaces rather than internal implementation details. For most components, the public interface is limited to: events emitted, props, and slots. When testing, remember to test what a component does, not how it does it . For example: For Visual logic assert correct render output based on inputted props and slots. For Behavioral logic: assert correct render updates or emitted events in response to user input events. The recommendation is to use Vitest for components or composables that render headlessly, and Cypress Component Testing for components whose expected behavior depends on properly rendering styles or triggering native DOM event. The main differences between Vitest and browser-based runners are speed and execution context. In short, browser-based runners, like Cypress, can catch issues that node-based runners, like Vitest, cannot (e.g. style issues, real native DOM events, cookies, local storage, and network failures), but browser-based runners are orders of magnitude slower than Vitest because they do open a browser, compile your stylesheets, and more. Component testing often involves mounting the component being tested in isolation, triggering simulated user input events, and asserting on the rendered DOM output. There are dedicated utility libraries that make these tasks simpler. @testing-library/vue is a Vue testing library focused on testing components without relying on implementation details. Built with accessibility in mind, its approach also makes refactoring a breeze. Its guiding principle is that the more tests resemble the way software is used, the more confidence they can provide. @vue/test-utils is the official low-level component testing library that was written to provide users access to Vue specific APIs. It's also the lower-level library @testing-library/vue is built on top of. I recommend using cypress so that you can use the same language either you are doing E2E tests or unit tests. If you're using Vuetify don't try to do component testing, I've tried for days and was unable to make it work .", "title": "Component testing"}, {"location": "vuejs/#e2e-testing", "text": "While unit tests provide developers with some degree of confidence, unit and component tests are limited in their abilities to provide holistic coverage of an application when deployed to production. As a result, end-to-end (E2E) tests provide coverage on what is arguably the most important aspect of an application: what happens when users actually use your applications. End-to-end tests focus on multi-page application behavior that makes network requests against your production-built Vue application. They often involve standing up a database or other backend and may even be run against a live staging environment. End-to-end tests will often catch issues with your router, state management library, top-level components (e.g. an App or Layout), public assets, or any request handling. As stated above, they catch critical issues that may be impossible to catch with unit tests or component tests. End-to-end tests do not import any of your Vue application's code, but instead rely completely on testing your application by navigating through entire pages in a real browser. End-to-end tests validate many of the layers in your application. They can either target your locally built application, or even a live Staging environment. Testing against your Staging environment not only includes your frontend code and static server, but all associated backend services and infrastructure.", "title": "E2E Testing"}, {"location": "vuejs/#e2e-tests-decisions", "text": "When doing E2E tests keep in mind: Cross-browser testing: One of the primary benefits that end-to-end (E2E) testing is known for is its ability to test your application across multiple browsers. While it may seem desirable to have 100% cross-browser coverage, it is important to note that cross browser testing has diminishing returns on a team's resources due the additional time and machine power required to run them consistently. As a result, it is important to be mindful of this trade-off when choosing the amount of cross-browser testing your application needs. Faster feedback loops: One of the primary problems with end-to-end (E2E) tests and development is that running the entire suite takes a long time. Typically, this is only done in continuous integration and deployment (CI/CD) pipelines. Modern E2E testing frameworks have helped to solve this by adding features like parallelization, which allows for CI/CD pipelines to often run magnitudes faster than before. In addition, when developing locally, the ability to selectively run a single test for the page you are working on while also providing hot reloading of tests can help to boost a developer's workflow and productivity. Visibility in headless mode: When end-to-end (E2E) tests are run in continuous integration / deployment pipelines, they are often run in headless browsers (i.e., no visible browser is opened for the user to watch). A critical feature of modern E2E testing frameworks is the ability to see snapshots and/or videos of the application during testing, providing some insight into why errors are happening. Historically, it was tedious to maintain these integrations. Vue developers suggestion is to use Cypress as it provides the most complete E2E solution with features like an informative graphical interface, excellent debuggability, built-in assertions and stubs, flake-resistance, parallelization, and snapshots. It also provides support for Component Testing. However, it only supports Chromium-based browsers and Firefox.", "title": "E2E tests decisions"}, {"location": "vuejs/#installation", "text": "In a Vite-based Vue project, run: npm install -D vitest happy-dom @testing-library/vue@next Next, update the Vite configuration to add the test option block: // vite.config.js import { defineConfig } from 'vite' export default defineConfig ({ // ... test : { // enable jest-like global test APIs globals : true , // simulate DOM with happy-dom // (requires installing happy-dom as a peer dependency) environment : 'happy-dom' } }) Then create a file ending in *.test.js in your project. You can place all test files in a test directory in project root, or in test directories next to your source files. Vitest will automatically search for them using the naming convention. // MyComponent.test.js import { render } from '@testing-library/vue' import MyComponent from './MyComponent.vue' test ( 'it should work' , () => { const { getByText } = render ( MyComponent , { props : { /* ... */ } }) // assert output getByText ( '...' ) }) Finally, update package.json to add the test script and run it: { // ... \"scripts\" : { \"test\" : \"vitest\" } } npm test", "title": "Installation"}, {"location": "vuejs/#deploying", "text": "It is common these days to run front-end and back-end services inside Docker containers. The front-end service usually talks using a API with the back-end service. FROM node as ui-builder RUN mkdir /usr/src/app WORKDIR /usr/src/app ENV PATH /usr/src/app/node_modules/.bin: $PATH COPY package.json /usr/src/app/package.json RUN npm install RUN npm install -g @vue/cli COPY . /usr/src/app RUN npm run build FROM nginx COPY --from = ui-builder /usr/src/app/dist /usr/share/nginx/html EXPOSE 80 CMD [ \"nginx\" , \"-g\" , \"daemon off;\" ] The above makes use of the multi-stage build feature of Docker. The first half of the Dockerfile build the artifacts and second half use those artifacts and create a new image from them. To build the production image, run: docker build -t myapp . You can run the container by executing the following command: docker run -it -p 80 :80 --rm myapp-prod The application will now be accessible at http://localhost .", "title": "Deploying"}, {"location": "vuejs/#configuration-through-environmental-variables", "text": "In production you want to be able to scale up or down the frontend and the backend independently, to be able to do that you usually have one or many docker for each role. Usually there is an SSL Proxy that acts as gate keeper and is the only component exposed to the public. If the user requests for /api it will forward the requests to the backend, if it asks for any other url it will forward it to the frontend. Note \"You probably don't need to configure the backend api url as an environment variable see here why.\" For the frontend, we need to configure the application. This is usually done through environmental variables , such as EXTERNAL_BACKEND_URL . The problem is that these environment variables are set at build time, and can't be changed at runtime by default, so you can't offer a generic fronted Docker and particularize for the different cases. I've literally cried for hours trying to find a solution for this until Jos\u00e9 Silva came to my rescue . The tweak is to use a docker entrypoint to inject the values we want. To do so you need to: Edit the site main index.html (if you use Vite is in /index.html otherwise it might be at public/index.html to add a placeholder that will be replaced by the dynamic configurations. <!DOCTYPE html> < html lang = \"en\" > < head > < script > // CONFIGURATIONS_PLACEHOLDER </ script > ... Create an executable file named entrypoint.sh in the root of the project. #!/bin/sh JSON_STRING = 'window.configs = { \\ \"VITE_APP_VARIABLE_1\":\"' \" ${ VITE_APP_VARIABLE_1 } \" '\", \\ \"VITE_APP_VARIABLE_2\":\"' \" ${ VITE_APP_VARIABLE_2 } \" '\" \\ }' sed -i \"s@// CONFIGURATIONS_PLACEHOLDER@ ${ JSON_STRING } @\" /usr/share/nginx/html/index.html exec \" $@ \" Its function is to replace the placeholder in the index.html by the configurations, injecting them in the browser window. Create a file named src/utils/env.js with the following utility function: export default function getEnv ( name ) { return window ? . configs ? .[ name ] || process . env [ name ] } Which allows us to easily get the value of the configuration. If it exists in window.configs (used in remote environments like staging or production) it will have priority over the process.env (used for development). Replace the content of the App.vue file with the following: < template > < div id = \"app\" > < img alt = \"Vue logo\" src = \"./assets/logo.png\" > < div > {{ variable1 }} </ div > < div > {{ variable2 }} </ div > </ div > </ template > < script > import getEnv from '@/utils/env' export default { name : 'App' , data () { return { variable1 : getEnv ( 'VITE_APP_VARIABLE_1' ), variable2 : getEnv ( 'VITE_APP_VARIABLE_2' ) } } } </ script > At this point, if you create the .env.local file, in the root of the project, with the values for the printed variables: VITE_APP_VARIABLE_1='I am the develoment variable 1' VITE_APP_VARIABLE_2='I am the develoment variable 2' And run the development server npm run dev you should see those values printed in the application ( http://localhost:8080 ). Update the Dockerfile to load the entrypoint.sh . FROM node as ui-builder RUN mkdir /usr/src/app WORKDIR /usr/src/app ENV PATH /usr/src/app/node_modules/.bin: $PATH COPY package.json /usr/src/app/package.json RUN npm install RUN npm install -g @vue/cli COPY . /usr/src/app ARG VUE_APP_API_URL ENV VUE_APP_API_URL $VUE_APP_API_URL RUN npm run build FROM nginx COPY --from = ui-builder /usr/src/app/dist /usr/share/nginx/html COPY entrypoint.sh /usr/share/nginx/ ENTRYPOINT [ \"/usr/share/nginx/entrypoint.sh\" ] EXPOSE 80 CMD [ \"nginx\" , \"-g\" , \"daemon off;\" ] Build the docker docker build -t my-app . Now if you have a .env.production.local file with the next contents: VITE_APP_VARIABLE_1='I am the production variable 1' VITE_APP_VARIABLE_2='I am the production variable 2' And run docker run -it -p 80:80 --env-file=.env.production.local --rm my-app , you'll see the values of the production variables. You can also pass the variables directly with -e VITE_APP_VARIABLE_1=\"Overriden variable\" .", "title": "Configuration through environmental variables"}, {"location": "vuejs/#deploy-static-site-on-github-pages", "text": "Sites in Github pages have the url structure of https://github_user.github.io/repo_name/ we need to tell vite that the base url is /repo_name/ , otherwise the application will try to load the assets in https://github_user.github.io/assets/ instead of https://github_user.github.io/rpeo_name/assets/ . To change it, add in the vite.config.js file: export default defineConfig ({ base : '/repo_name/' }) Now you need to configure the deployment workflow, to do so, create a new file: .github/workflows/deploy.yml and paste the following code: --- name : Deploy on : push : branches : - main workflow_dispatch : jobs : build : name : Build runs-on : ubuntu-latest steps : - name : Checkout repo uses : actions/checkout@v2 - name : Setup Node uses : actions/setup-node@v1 with : node-version : 16 - name : Install dependencies uses : bahmutov/npm-install@v1 - name : Build project run : npm run build - name : Upload production-ready build files uses : actions/upload-artifact@v2 with : name : production-files path : ./dist deploy : name : Deploy needs : build runs-on : ubuntu-latest if : github.ref == 'refs/heads/main' steps : - name : Download artifact uses : actions/download-artifact@v2 with : name : production-files path : ./dist - name : Deploy to GitHub Pages uses : peaceiris/actions-gh-pages@v3 with : github_token : ${{ secrets.GITHUB_TOKEN }} publish_dir : ./dist You'd probably need to change your repository settings under Actions/General and set the Workflow permissions to Read and write permissions . Once the workflow has been successful, in the repository settings under Pages you need to enable Github Pages to use the gh-pages branch as source.", "title": "Deploy static site on github pages"}, {"location": "vuejs/#tip-handling-vue-router-with-a-custom-404-page", "text": "One thing to keep in mind when setting up the Github Pages site, is that working with Vue Router gets a little tricky. If you\u2019re using history mode in Vue router, you\u2019ll notice that if you try to go directly to a page other than / you\u2019ll get a 404 error. This is because Github Pages does not automatically redirect all requests to serve index.html . Luckily, there is an easy little workaround. All you have to do is duplicate your index.html file and name the copy 404.html . What this does is make your 404 page serve the same content as your index.html , which means your Vue router will be able to display the right page.", "title": "Tip Handling Vue Router with a Custom 404 Page"}, {"location": "vuejs/#testing_1", "text": "", "title": "Testing"}, {"location": "vuejs/#debug-jest-tests", "text": "If you're not developing in Visual code, running a debugger is not easy in the middle of the tests, so to debug one you can use console.log() statements and when you run them with yarn test:unit you'll see the traces.", "title": "Debug Jest tests"}, {"location": "vuejs/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "vuejs/#failed-to-resolve-component-x", "text": "If you've already imported the component with import X from './X.vue you may have forgotten to add the component to the components property of the module: export default { name : 'Inbox' , components : { X } }", "title": "Failed to resolve component: X"}, {"location": "vuejs/#references", "text": "Docs Homepage Tutorial Examples Awesome Vue Components", "title": "References"}, {"location": "vuejs/#axios_1", "text": "Docs Git Homepage", "title": "Axios"}, {"location": "vuetify/", "text": "Vuetify is a Vue UI Library with beautifully handcrafted Material Components. Install \u2691 First you need vue-cli , install it with: sudo npm install -g @vue/cli Then run: vue add vuetify If you're using Vite select Vite Preview (Vuetify 3 + Vite) . Usage \u2691 Flex \u2691 Control the layout of flex containers with alignment, justification and more with responsive flexbox utilities. Note \"I suggest you use this page only as a reference, if it's the first time you see this content, it's better to see it at the source as you can see Flex in action at the same time you read, which makes it much more easy to understand.\" Using display utilities you can turn any element into a flexbox container transforming direct children elements into flex items. Using additional flex property utilities, you can customize their interaction even further. You can also customize flex utilities to apply based upon various breakpoints. .d-flex .d-inline-flex .d-sm-flex .d-sm-inline-flex .d-md-flex .d-md-inline-flex .d-lg-flex .d-lg-inline-flex .d-xl-flex .d-xl-inline-flex You define the attributes inside the class of the Vuetify object. For example: < v-card class = \"d-flex flex-row mb-6\" /> Display breakpoints \u2691 With Vuetify you can control various aspects of your application based upon the window size. Device Code Type Range Extra small xs Small to large phone < 600px Small sm Small to medium tablet 600px > < 960px Medium md Large tablet to laptop 960px > < 1264px* Large lg Desktop 1264px > < 1904px* Extra large xl 4k and ultra-wide > 1904px* The breakpoint service is a programmatic way of accessing viewport information within components. It exposes a number of properties on the $vuetify object that can be used to control aspects of your application based upon the viewport size. The name property correlates to the currently active breakpoint; e.g. xs, sm, md, lg, xl. In the following snippet, we use a switch statement and the current breakpoint name to modify the height property of the v-card component: < template > < v-card :height = \"height\" > ... </ v-card > </ template > < script > export default { computed : { height () { switch ( this . $vuetify . breakpoint . name ) { case 'xs' : return 220 case 'sm' : return 400 case 'md' : return 500 case 'lg' : return 600 case 'xl' : return 800 } }, }, } </ script > The following is the public signature for the breakpoint service: { // Breakpoints xs : boolean sm : boolean md : boolean lg : boolean xl : boolean // Conditionals xsOnly : boolean smOnly : boolean smAndDown : boolean smAndUp : boolean mdOnly : boolean mdAndDown : boolean mdAndUp : boolean lgOnly : boolean lgAndDown : boolean lgAndUp : boolean xlOnly : boolean // true if screen width < mobileBreakpoint mobile : boolean mobileBreakpoint : number // Current breakpoint name (e.g. 'md') name : string // Dimensions height : number width : number // Thresholds // Configurable through options { xs : number sm : number md : number lg : number } // Scrollbar scrollBarWidth : number } Access these properties within Vue files by referencing $vuetify.breakpoint.<property> For example to log the current viewport width to the console once the component fires the mounted lifecycle hook you can use: <!-- Vue Component --> < script > export default { mounted () { console . log ( this . $vuetify . breakpoint . width ) } } </ script > Flex direction \u2691 By default, d-flex applies flex-direction: row and can generally be omitted. The flex-column and flex-column-reverse utility classes can be used to change the orientation of the flexbox container. There are also responsive variations for flex-direction. .flex-row .flex-row-reverse .flex-column .flex-column-reverse .flex-sm-row .flex-sm-row-reverse .flex-sm-column .flex-sm-column-reverse .flex-md-row .flex-md-row-reverse .flex-md-column .flex-md-column-reverse .flex-lg-row .flex-lg-row-reverse .flex-lg-column .flex-lg-column-reverse .flex-xl-row .flex-xl-row-reverse .flex-xl-column .flex-xl-column-reverse Flex justify \u2691 The justify-content flex setting can be changed using the flex justify classes. This by default will modify the flexbox items on the x-axis but is reversed when using flex-direction: column , modifying the y-axis . Choose from: start (browser default): Everything together on the left. end : Everything together on the right. center : Everything together on the center. space-between : First item on the top left, second on the center, third at the end, with space between the items. space-around : Like space-between but with space on the top left and right too. For example: < v-card class = \"d-flex justify-center mb-6\" /> There are also responsive variations for justify-content . .justify-start .justify-end .justify-center .justify-space-between .justify-space-around .justify-sm-start .justify-sm-end .justify-sm-center .justify-sm-space-between .justify-sm-space-around .justify-md-start .justify-md-end .justify-md-center .justify-md-space-between .justify-md-space-around .justify-lg-start .justify-lg-end .justify-lg-center .justify-lg-space-between .justify-lg-space-around .justify-xl-start .justify-xl-end .justify-xl-center .justify-xl-space-between .justify-xl-space-around Flex align \u2691 The align-items flex setting can be changed using the flex align classes. This by default will modify the flexbox items on the y-axis but is reversed when using flex-direction: column , modifying the x-axis . Choose from: start : Everything together on the top. end : Everything together on the bottom. center : Everything together on the center. baseline : (I don't understand this one). align-stretch : Align content to the top but extend the container to the bottom. For example: < v-card class = \"d-flex align-center mb-6\" /> There are also responsive variations for align-items . .align-start .align-end .align-center .align-baseline .align-stretch .align-sm-start .align-sm-end .align-sm-center .align-sm-baseline .align-sm-stretch .align-md-start .align-md-end .align-md-center .align-md-baseline .align-md-stretch .align-lg-start .align-lg-end .align-lg-center .align-lg-baseline .align-lg-stretch .align-xl-start .align-xl-end .align-xl-center .align-xl-baseline .align-xl-stretch The align-self attribute works like align but for a single element instead of all the children. Margins \u2691 You can define the margins you want with: ma-2 : 2 points in all directions. mb-2 : 2 points of margin on bottom. mt-2 : 2 points of margin on top. mr-2 : 2 points of margin on right. ml-2 : 2 points of margin on left. If instead of a number you use auto it will fill it till the end of the container. To center things around, you can use mx-auto to center in the X axis and my-auto for the Y axis. If you are using a flex-column and you want to put an element to the bottom, you'll use mt-auto so that the space filled on top of the element is filled automatically. Flex grow and shrink \u2691 Vuetify has helper classes for applying grow and shrink manually. These can be applied by adding the helper class in the format flex-{condition}-{value} , where condition can be either grow or shrink and value can be either 0 or 1 . The condition grow will permit an element to grow to fill available space, whereas shrink will permit an element to shrink down to only the space needs for its contents. However, this will only happen if the element must shrink to fit their container such as a container resize or being effected by a flex-grow-1 . The value 0 will prevent the condition from occurring whereas 1 will permit the condition. The following classes are available: flex-grow-0 flex-grow-1 flex-shrink-0 flex-shrink-1 For example: < template > < v-container > < v-row no-gutters style = \"flex-wrap: nowrap;\" > < v-col cols = \"2\" class = \"flex-grow-0 flex-shrink-0\" > < v-card > I'm 2 column wide </ v-card > </ v-col > < v-col cols = \"1\" style = \"min-width: 100px; max-width: 100%;\" class = \"flex-grow-1 flex-shrink-0\" > < v-card > I'm 1 column wide and I grow to take all the space </ v-card > </ v-col > < v-col cols = \"5\" style = \"min-width: 100px;\" class = \"flex-grow-0 flex-shrink-1\" > < v-card > I'm 5 column wide and I shrink if there's not enough space </ v-card > </ v-col > </ v-row > </ v-container > </ template > Position elements with Flex \u2691 If the properties above don't give you the control you need you can use rows and columns directly. Vuetify comes with a 12 point grid system built using Flexbox . The grid is used to create specific layouts within an application\u2019s content. Using v-row (as a flex-container) and v-col (as a flex-item). < v-container > < v-row > < v-col > < v-card class = \"pa-2\" outlined tile > One of three columns </ v-card > </ v-col > < v-col > < v-card class = \"pa-2\" outlined tile > One of three columns </ v-card > </ v-col > < v-col > < v-card class = \"pa-2\" outlined tile > One of three columns </ v-card > </ v-col > </ v-row > </ v-container > v-row has the next properties: align : set the vertical alignment of flex items (one of start , center and end ). It also has one property for each device size ( align-md , align-xl , ...). The align-content variation is also available. justify : set the horizontal alignment of the flex items (one of start , center , end , space-around , space-between ). It also has one property for each device size ( justify-md , justify-xl , ...). no-gutters : Removes the spaces between items. dense : Reduces the spaces between items. v-col has the next properties: cols : Sets the default number of columns the component extends. Available options are 1 -> 12 and auto . you can use lg , md , ... to define the number of columns for the other sizes. offset : Sets the default offset for the column. You can also use offset-lg and the other sizes. Keep the structure even if some components are hidden \u2691 If you want the components to remain in their position even if the items around disappear, you need to use <v-row> and <v-col> . For example: < v-row align = end justify = center class = \"mt-auto\" > < v-col align = center > < v-btn v-show = isNotFirstElement ... > Button </ v-btn > </ v-col > < v-col align = center > < v-rating v-show = \"isNotLastElement\" ... ></ v-rating > </ v-col > < v-col align = center > < v-btn v-show = \"isNotLastVisitedElement && isNotLastElement\" ... > Button </ v-btn > </ v-col > </ v-row > If instead you had use the next snippet, whenever one of the elements got hidden, the rest would move around to fill up the remaining space. < v-row align = end justify = center class = \"mt-auto\" > < v-btn v-show = isNotFirstElement ... > Button </ v-btn > < v-rating v-show = \"isNotLastElement\" ... ></ v-rating > < v-btn v-show = \"isNotLastVisitedElement && isNotLastElement\" ... > Button </ v-btn > </ v-row > Themes \u2691 Vuetify comes with two themes pre-installed, light and dark. To set the default theme of your application, use the defaultTheme option. File: src/plugins/vuetify.js import { createApp } from 'vue' import { createVuetify } from 'vuetify' export default createVuetify ({ theme : { defaultTheme : 'dark' } }) Adding new themes is as easy as defining a new property in the theme.themes object. A theme is a collection of colors and options that change the overall look and feel of your application. One of these options designates the theme as being either a light or dark variation. This makes it possible for Vuetify to implement Material Design concepts such as elevated surfaces having a lighter overlay color the higher up they are. File: src/plugins/vuetify.js import { createApp } from 'vue' import { createVuetify , ThemeDefinition } from 'vuetify' export default createVuetify ({ theme : { defaultTheme : 'myCustomLightTheme' , themes : { myCustomLightTheme : { dark : false , colors : { background : '#FFFFFF' , surface : '#FFFFFF' , primary : '#510560' , 'primary-darken-1' : '#3700B3' , secondary : '#03DAC6' , 'secondary-darken-1' : '#018786' , error : '#B00020' , info : '#2196F3' , success : '#4CAF50' , warning : '#FB8C00' , } } } } }) To dynamically change theme during runtime. < template > < v - app > < v - btn @ click = \"toggleTheme\" > toggle theme < /v-btn> ... < /v-app> < /template> < script > import { useTheme } from 'vuetify' export default { setup () { const theme = useTheme () return { theme , toggleTheme : () => theme . global . name . value = theme . global . current . value . dark ? 'light' : 'dark' } } } < /script> Most components support the theme prop. When used, a new context is created for that specific component and all of its children. In the following example, the v-btn uses the dark theme applied by its parent v-card . < template > < v - app > < v - card theme = \"dark\" > <!-- button uses dark theme --> < v - btn > foo < /v-btn> < /v-card> < /v-app> < /template> Elements \u2691 Cards \u2691 The v-card can be used to place any kind of text on your site, in this case use the variant=text . Buttons \u2691 The sizes can be: x-small , small , default , large , x-large . Illustrations \u2691 You can get nice illustrations for your web on Drawkit , for example I like to use the Classic kit . Icons \u2691 The v-icon component provides a large set of glyphs to provide context to various aspects of your application. < v-icon > fas fa-user </ v-icon > If you have the FontAwesome icons installed, browse them here Install font awesome icons \u2691 npm install @fortawesome/fontawesome-free -D // src/plugins/vuetify.js import '@fortawesome/fontawesome-free/css/all.css' // Ensure your project is capable of handling css files import { createVuetify } from 'vuetify' import { aliases , fa } from 'vuetify/lib/iconsets/fa' export default createVuetify ({ icons : { defaultSet : 'fa' , aliases , sets : { fa , }, }, }) < template > < v-icon icon = \"fas fa-home\" /> </ template > Fonts \u2691 By default it uses the webfontload plugin which slows down a lot the page load, instead you can install the fonts directly. For example for the Roboto font: Install the font npm install --save typeface-roboto Uninstall the webfontload plugin npm remove webfontloader Remove the loading of the webfontload in /main.js the lines: import { loadFonts } from './plugins/webfontloader' loadFonts () * Add the font in the App.vue file: < style lang = \"sass\" > @ import '../node_modules/typeface-roboto/index.css' </ style > Carousels \u2691 Vuetify has their own carousel component, here's it's API . In the Awesome Vue.js compilation there are other suggestions. As some users say, it looks like Vuetify's doesn't have the best responsive behaviour . The best looking alternatives I've seen are: vue-agile : Demo . vue-picture-swipe vue-slick-carousel : Demo . It doesn't yet support Vue3 swiper : Demo vue-splide : Demo Vuetify component \u2691 I tried binding the model with v-model but when I click on the arrows, the image doesn't change and the binded property doesn't change. If I change the property with other component, the image does change vue-agile \u2691 If you encounter the modules have no default error, add this to your vite.config.js : export default defineConfig ({ ... optimizeDeps : { include : [ 'lodash.throttle' , 'lodash.orderby' ] }, ... }) Small vertical carousel \u2691 If you want to do a vertical carousel for example the one shown in the video playlists, you can't yet use v-slide-group . vue-agile doesn't either yet have vertical option . Audio \u2691 vuejs-sound-player vue-audio-visual : Demo vue3-audio-player : Demo vuetify-audio : Demo Testing \u2691 I tried doing component tests with Jest, Vitest and Cypress and found no way of making component tests, they all fail one way or the other. E2E tests worked with Cypress however, that's going to be my way of action till this is solved. References \u2691 Docs Home Git Discord", "title": "Vuetify"}, {"location": "vuetify/#install", "text": "First you need vue-cli , install it with: sudo npm install -g @vue/cli Then run: vue add vuetify If you're using Vite select Vite Preview (Vuetify 3 + Vite) .", "title": "Install"}, {"location": "vuetify/#usage", "text": "", "title": "Usage"}, {"location": "vuetify/#flex", "text": "Control the layout of flex containers with alignment, justification and more with responsive flexbox utilities. Note \"I suggest you use this page only as a reference, if it's the first time you see this content, it's better to see it at the source as you can see Flex in action at the same time you read, which makes it much more easy to understand.\" Using display utilities you can turn any element into a flexbox container transforming direct children elements into flex items. Using additional flex property utilities, you can customize their interaction even further. You can also customize flex utilities to apply based upon various breakpoints. .d-flex .d-inline-flex .d-sm-flex .d-sm-inline-flex .d-md-flex .d-md-inline-flex .d-lg-flex .d-lg-inline-flex .d-xl-flex .d-xl-inline-flex You define the attributes inside the class of the Vuetify object. For example: < v-card class = \"d-flex flex-row mb-6\" />", "title": "Flex"}, {"location": "vuetify/#display-breakpoints", "text": "With Vuetify you can control various aspects of your application based upon the window size. Device Code Type Range Extra small xs Small to large phone < 600px Small sm Small to medium tablet 600px > < 960px Medium md Large tablet to laptop 960px > < 1264px* Large lg Desktop 1264px > < 1904px* Extra large xl 4k and ultra-wide > 1904px* The breakpoint service is a programmatic way of accessing viewport information within components. It exposes a number of properties on the $vuetify object that can be used to control aspects of your application based upon the viewport size. The name property correlates to the currently active breakpoint; e.g. xs, sm, md, lg, xl. In the following snippet, we use a switch statement and the current breakpoint name to modify the height property of the v-card component: < template > < v-card :height = \"height\" > ... </ v-card > </ template > < script > export default { computed : { height () { switch ( this . $vuetify . breakpoint . name ) { case 'xs' : return 220 case 'sm' : return 400 case 'md' : return 500 case 'lg' : return 600 case 'xl' : return 800 } }, }, } </ script > The following is the public signature for the breakpoint service: { // Breakpoints xs : boolean sm : boolean md : boolean lg : boolean xl : boolean // Conditionals xsOnly : boolean smOnly : boolean smAndDown : boolean smAndUp : boolean mdOnly : boolean mdAndDown : boolean mdAndUp : boolean lgOnly : boolean lgAndDown : boolean lgAndUp : boolean xlOnly : boolean // true if screen width < mobileBreakpoint mobile : boolean mobileBreakpoint : number // Current breakpoint name (e.g. 'md') name : string // Dimensions height : number width : number // Thresholds // Configurable through options { xs : number sm : number md : number lg : number } // Scrollbar scrollBarWidth : number } Access these properties within Vue files by referencing $vuetify.breakpoint.<property> For example to log the current viewport width to the console once the component fires the mounted lifecycle hook you can use: <!-- Vue Component --> < script > export default { mounted () { console . log ( this . $vuetify . breakpoint . width ) } } </ script >", "title": "Display breakpoints"}, {"location": "vuetify/#flex-direction", "text": "By default, d-flex applies flex-direction: row and can generally be omitted. The flex-column and flex-column-reverse utility classes can be used to change the orientation of the flexbox container. There are also responsive variations for flex-direction. .flex-row .flex-row-reverse .flex-column .flex-column-reverse .flex-sm-row .flex-sm-row-reverse .flex-sm-column .flex-sm-column-reverse .flex-md-row .flex-md-row-reverse .flex-md-column .flex-md-column-reverse .flex-lg-row .flex-lg-row-reverse .flex-lg-column .flex-lg-column-reverse .flex-xl-row .flex-xl-row-reverse .flex-xl-column .flex-xl-column-reverse", "title": "Flex direction"}, {"location": "vuetify/#flex-justify", "text": "The justify-content flex setting can be changed using the flex justify classes. This by default will modify the flexbox items on the x-axis but is reversed when using flex-direction: column , modifying the y-axis . Choose from: start (browser default): Everything together on the left. end : Everything together on the right. center : Everything together on the center. space-between : First item on the top left, second on the center, third at the end, with space between the items. space-around : Like space-between but with space on the top left and right too. For example: < v-card class = \"d-flex justify-center mb-6\" /> There are also responsive variations for justify-content . .justify-start .justify-end .justify-center .justify-space-between .justify-space-around .justify-sm-start .justify-sm-end .justify-sm-center .justify-sm-space-between .justify-sm-space-around .justify-md-start .justify-md-end .justify-md-center .justify-md-space-between .justify-md-space-around .justify-lg-start .justify-lg-end .justify-lg-center .justify-lg-space-between .justify-lg-space-around .justify-xl-start .justify-xl-end .justify-xl-center .justify-xl-space-between .justify-xl-space-around", "title": "Flex justify"}, {"location": "vuetify/#flex-align", "text": "The align-items flex setting can be changed using the flex align classes. This by default will modify the flexbox items on the y-axis but is reversed when using flex-direction: column , modifying the x-axis . Choose from: start : Everything together on the top. end : Everything together on the bottom. center : Everything together on the center. baseline : (I don't understand this one). align-stretch : Align content to the top but extend the container to the bottom. For example: < v-card class = \"d-flex align-center mb-6\" /> There are also responsive variations for align-items . .align-start .align-end .align-center .align-baseline .align-stretch .align-sm-start .align-sm-end .align-sm-center .align-sm-baseline .align-sm-stretch .align-md-start .align-md-end .align-md-center .align-md-baseline .align-md-stretch .align-lg-start .align-lg-end .align-lg-center .align-lg-baseline .align-lg-stretch .align-xl-start .align-xl-end .align-xl-center .align-xl-baseline .align-xl-stretch The align-self attribute works like align but for a single element instead of all the children.", "title": "Flex align"}, {"location": "vuetify/#margins", "text": "You can define the margins you want with: ma-2 : 2 points in all directions. mb-2 : 2 points of margin on bottom. mt-2 : 2 points of margin on top. mr-2 : 2 points of margin on right. ml-2 : 2 points of margin on left. If instead of a number you use auto it will fill it till the end of the container. To center things around, you can use mx-auto to center in the X axis and my-auto for the Y axis. If you are using a flex-column and you want to put an element to the bottom, you'll use mt-auto so that the space filled on top of the element is filled automatically.", "title": "Margins"}, {"location": "vuetify/#flex-grow-and-shrink", "text": "Vuetify has helper classes for applying grow and shrink manually. These can be applied by adding the helper class in the format flex-{condition}-{value} , where condition can be either grow or shrink and value can be either 0 or 1 . The condition grow will permit an element to grow to fill available space, whereas shrink will permit an element to shrink down to only the space needs for its contents. However, this will only happen if the element must shrink to fit their container such as a container resize or being effected by a flex-grow-1 . The value 0 will prevent the condition from occurring whereas 1 will permit the condition. The following classes are available: flex-grow-0 flex-grow-1 flex-shrink-0 flex-shrink-1 For example: < template > < v-container > < v-row no-gutters style = \"flex-wrap: nowrap;\" > < v-col cols = \"2\" class = \"flex-grow-0 flex-shrink-0\" > < v-card > I'm 2 column wide </ v-card > </ v-col > < v-col cols = \"1\" style = \"min-width: 100px; max-width: 100%;\" class = \"flex-grow-1 flex-shrink-0\" > < v-card > I'm 1 column wide and I grow to take all the space </ v-card > </ v-col > < v-col cols = \"5\" style = \"min-width: 100px;\" class = \"flex-grow-0 flex-shrink-1\" > < v-card > I'm 5 column wide and I shrink if there's not enough space </ v-card > </ v-col > </ v-row > </ v-container > </ template >", "title": "Flex grow and shrink"}, {"location": "vuetify/#position-elements-with-flex", "text": "If the properties above don't give you the control you need you can use rows and columns directly. Vuetify comes with a 12 point grid system built using Flexbox . The grid is used to create specific layouts within an application\u2019s content. Using v-row (as a flex-container) and v-col (as a flex-item). < v-container > < v-row > < v-col > < v-card class = \"pa-2\" outlined tile > One of three columns </ v-card > </ v-col > < v-col > < v-card class = \"pa-2\" outlined tile > One of three columns </ v-card > </ v-col > < v-col > < v-card class = \"pa-2\" outlined tile > One of three columns </ v-card > </ v-col > </ v-row > </ v-container > v-row has the next properties: align : set the vertical alignment of flex items (one of start , center and end ). It also has one property for each device size ( align-md , align-xl , ...). The align-content variation is also available. justify : set the horizontal alignment of the flex items (one of start , center , end , space-around , space-between ). It also has one property for each device size ( justify-md , justify-xl , ...). no-gutters : Removes the spaces between items. dense : Reduces the spaces between items. v-col has the next properties: cols : Sets the default number of columns the component extends. Available options are 1 -> 12 and auto . you can use lg , md , ... to define the number of columns for the other sizes. offset : Sets the default offset for the column. You can also use offset-lg and the other sizes.", "title": "Position elements with Flex"}, {"location": "vuetify/#keep-the-structure-even-if-some-components-are-hidden", "text": "If you want the components to remain in their position even if the items around disappear, you need to use <v-row> and <v-col> . For example: < v-row align = end justify = center class = \"mt-auto\" > < v-col align = center > < v-btn v-show = isNotFirstElement ... > Button </ v-btn > </ v-col > < v-col align = center > < v-rating v-show = \"isNotLastElement\" ... ></ v-rating > </ v-col > < v-col align = center > < v-btn v-show = \"isNotLastVisitedElement && isNotLastElement\" ... > Button </ v-btn > </ v-col > </ v-row > If instead you had use the next snippet, whenever one of the elements got hidden, the rest would move around to fill up the remaining space. < v-row align = end justify = center class = \"mt-auto\" > < v-btn v-show = isNotFirstElement ... > Button </ v-btn > < v-rating v-show = \"isNotLastElement\" ... ></ v-rating > < v-btn v-show = \"isNotLastVisitedElement && isNotLastElement\" ... > Button </ v-btn > </ v-row >", "title": "Keep the structure even if some components are hidden"}, {"location": "vuetify/#themes", "text": "Vuetify comes with two themes pre-installed, light and dark. To set the default theme of your application, use the defaultTheme option. File: src/plugins/vuetify.js import { createApp } from 'vue' import { createVuetify } from 'vuetify' export default createVuetify ({ theme : { defaultTheme : 'dark' } }) Adding new themes is as easy as defining a new property in the theme.themes object. A theme is a collection of colors and options that change the overall look and feel of your application. One of these options designates the theme as being either a light or dark variation. This makes it possible for Vuetify to implement Material Design concepts such as elevated surfaces having a lighter overlay color the higher up they are. File: src/plugins/vuetify.js import { createApp } from 'vue' import { createVuetify , ThemeDefinition } from 'vuetify' export default createVuetify ({ theme : { defaultTheme : 'myCustomLightTheme' , themes : { myCustomLightTheme : { dark : false , colors : { background : '#FFFFFF' , surface : '#FFFFFF' , primary : '#510560' , 'primary-darken-1' : '#3700B3' , secondary : '#03DAC6' , 'secondary-darken-1' : '#018786' , error : '#B00020' , info : '#2196F3' , success : '#4CAF50' , warning : '#FB8C00' , } } } } }) To dynamically change theme during runtime. < template > < v - app > < v - btn @ click = \"toggleTheme\" > toggle theme < /v-btn> ... < /v-app> < /template> < script > import { useTheme } from 'vuetify' export default { setup () { const theme = useTheme () return { theme , toggleTheme : () => theme . global . name . value = theme . global . current . value . dark ? 'light' : 'dark' } } } < /script> Most components support the theme prop. When used, a new context is created for that specific component and all of its children. In the following example, the v-btn uses the dark theme applied by its parent v-card . < template > < v - app > < v - card theme = \"dark\" > <!-- button uses dark theme --> < v - btn > foo < /v-btn> < /v-card> < /v-app> < /template>", "title": "Themes"}, {"location": "vuetify/#elements", "text": "", "title": "Elements"}, {"location": "vuetify/#cards", "text": "The v-card can be used to place any kind of text on your site, in this case use the variant=text .", "title": "Cards"}, {"location": "vuetify/#buttons", "text": "The sizes can be: x-small , small , default , large , x-large .", "title": "Buttons"}, {"location": "vuetify/#illustrations", "text": "You can get nice illustrations for your web on Drawkit , for example I like to use the Classic kit .", "title": "Illustrations"}, {"location": "vuetify/#icons", "text": "The v-icon component provides a large set of glyphs to provide context to various aspects of your application. < v-icon > fas fa-user </ v-icon > If you have the FontAwesome icons installed, browse them here", "title": "Icons"}, {"location": "vuetify/#install-font-awesome-icons", "text": "npm install @fortawesome/fontawesome-free -D // src/plugins/vuetify.js import '@fortawesome/fontawesome-free/css/all.css' // Ensure your project is capable of handling css files import { createVuetify } from 'vuetify' import { aliases , fa } from 'vuetify/lib/iconsets/fa' export default createVuetify ({ icons : { defaultSet : 'fa' , aliases , sets : { fa , }, }, }) < template > < v-icon icon = \"fas fa-home\" /> </ template >", "title": "Install font awesome icons"}, {"location": "vuetify/#fonts", "text": "By default it uses the webfontload plugin which slows down a lot the page load, instead you can install the fonts directly. For example for the Roboto font: Install the font npm install --save typeface-roboto Uninstall the webfontload plugin npm remove webfontloader Remove the loading of the webfontload in /main.js the lines: import { loadFonts } from './plugins/webfontloader' loadFonts () * Add the font in the App.vue file: < style lang = \"sass\" > @ import '../node_modules/typeface-roboto/index.css' </ style >", "title": "Fonts"}, {"location": "vuetify/#carousels", "text": "Vuetify has their own carousel component, here's it's API . In the Awesome Vue.js compilation there are other suggestions. As some users say, it looks like Vuetify's doesn't have the best responsive behaviour . The best looking alternatives I've seen are: vue-agile : Demo . vue-picture-swipe vue-slick-carousel : Demo . It doesn't yet support Vue3 swiper : Demo vue-splide : Demo", "title": "Carousels"}, {"location": "vuetify/#vuetify-component", "text": "I tried binding the model with v-model but when I click on the arrows, the image doesn't change and the binded property doesn't change. If I change the property with other component, the image does change", "title": "Vuetify component"}, {"location": "vuetify/#vue-agile", "text": "If you encounter the modules have no default error, add this to your vite.config.js : export default defineConfig ({ ... optimizeDeps : { include : [ 'lodash.throttle' , 'lodash.orderby' ] }, ... })", "title": "vue-agile"}, {"location": "vuetify/#small-vertical-carousel", "text": "If you want to do a vertical carousel for example the one shown in the video playlists, you can't yet use v-slide-group . vue-agile doesn't either yet have vertical option .", "title": "Small vertical carousel"}, {"location": "vuetify/#audio", "text": "vuejs-sound-player vue-audio-visual : Demo vue3-audio-player : Demo vuetify-audio : Demo", "title": "Audio"}, {"location": "vuetify/#testing", "text": "I tried doing component tests with Jest, Vitest and Cypress and found no way of making component tests, they all fail one way or the other. E2E tests worked with Cypress however, that's going to be my way of action till this is solved.", "title": "Testing"}, {"location": "vuetify/#references", "text": "Docs Home Git Discord", "title": "References"}, {"location": "wake_on_lan/", "text": "Wake on LAN (WoL) is a feature to switch on a computer via the network. Usage \u2691 Host configuration \u2691 On the host you want to activate the wake on lan execute: $: ethtool *interface* | grep Wake-on Supports Wake-on: pumbag Wake-on: d The Wake-on values define what activity triggers wake up: d (disabled), p (PHY activity), u (unicast activity), m (multicast activity), b (broadcast activity), a (ARP activity), and g (magic packet activity). The value g is required for WoL to work, if not, the following command enables the WoL feature in the driver: $: ethtool -s interface wol g If it was not enabled check in the Arch wiki how to make the change persistent. To trigger WoL on a target machine, its MAC address must be known. To obtain it, execute the following command from the machine: $: ip link 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 2 : enp1s0: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc fq_codel master br0 state UP group default qlen 1000 link/ether 48 :05:ca:09:0e:6a brd ff:ff:ff:ff:ff:ff Here the MAC address is 48:05:ca:09:0e:6a . In its simplest form, Wake-on-LAN broadcasts the magic packet as an ethernet frame, containing the MAC address within the current network subnet, below the IP protocol layer. The knowledge of an IP address for the target computer is not necessary, as it operates on layer 2 (Data Link). If used to wake up a computer over the internet or in a different subnet, it typically relies on the router to relay the packet and broadcast it. In this scenario, the external IP address of the router must be known. Keep in mind that most routers by default will not relay subnet directed broadcasts as a safety precaution and need to be explicitly told to do so. Client trigger \u2691 If you are connected directly to another computer through a network cable, or the traffic within a LAN is not firewalled, then using Wake-on-LAN should be straightforward since there is no need to worry about port redirects. If it's firewalled you need to configure the client firewall to allow the outgoing UDP traffic to the port 9. In the simplest case the default broadcast address 255.255.255.255 is used: $ wakeonlan *target_MAC_address* To broadcast the magic packet only to a specific subnet or host, use the -i switch: $ wakeonlan -i *target_IP* *target_MAC_address* References \u2691 Arch wiki post", "title": "Wake on Lan"}, {"location": "wake_on_lan/#usage", "text": "", "title": "Usage"}, {"location": "wake_on_lan/#host-configuration", "text": "On the host you want to activate the wake on lan execute: $: ethtool *interface* | grep Wake-on Supports Wake-on: pumbag Wake-on: d The Wake-on values define what activity triggers wake up: d (disabled), p (PHY activity), u (unicast activity), m (multicast activity), b (broadcast activity), a (ARP activity), and g (magic packet activity). The value g is required for WoL to work, if not, the following command enables the WoL feature in the driver: $: ethtool -s interface wol g If it was not enabled check in the Arch wiki how to make the change persistent. To trigger WoL on a target machine, its MAC address must be known. To obtain it, execute the following command from the machine: $: ip link 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 2 : enp1s0: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc fq_codel master br0 state UP group default qlen 1000 link/ether 48 :05:ca:09:0e:6a brd ff:ff:ff:ff:ff:ff Here the MAC address is 48:05:ca:09:0e:6a . In its simplest form, Wake-on-LAN broadcasts the magic packet as an ethernet frame, containing the MAC address within the current network subnet, below the IP protocol layer. The knowledge of an IP address for the target computer is not necessary, as it operates on layer 2 (Data Link). If used to wake up a computer over the internet or in a different subnet, it typically relies on the router to relay the packet and broadcast it. In this scenario, the external IP address of the router must be known. Keep in mind that most routers by default will not relay subnet directed broadcasts as a safety precaution and need to be explicitly told to do so.", "title": "Host configuration"}, {"location": "wake_on_lan/#client-trigger", "text": "If you are connected directly to another computer through a network cable, or the traffic within a LAN is not firewalled, then using Wake-on-LAN should be straightforward since there is no need to worry about port redirects. If it's firewalled you need to configure the client firewall to allow the outgoing UDP traffic to the port 9. In the simplest case the default broadcast address 255.255.255.255 is used: $ wakeonlan *target_MAC_address* To broadcast the magic packet only to a specific subnet or host, use the -i switch: $ wakeonlan -i *target_IP* *target_MAC_address*", "title": "Client trigger"}, {"location": "wake_on_lan/#references", "text": "Arch wiki post", "title": "References"}, {"location": "wallabag/", "text": "Wallabag is a self-hosted read-it-later application: it saves a web page by keeping content only. Elements like navigation or ads are deleted. Installation \u2691 They provide a working docker-compose version : '3' services : wallabag : image : wallabag/wallabag environment : - MYSQL_ROOT_PASSWORD=wallaroot - SYMFONY__ENV__DATABASE_DRIVER=pdo_mysql - SYMFONY__ENV__DATABASE_HOST=db - SYMFONY__ENV__DATABASE_PORT=3306 - SYMFONY__ENV__DATABASE_NAME=wallabag - SYMFONY__ENV__DATABASE_USER=wallabag - SYMFONY__ENV__DATABASE_PASSWORD=wallapass - SYMFONY__ENV__DATABASE_CHARSET=utf8mb4 - SYMFONY__ENV__SECRET=supersecretenv - SYMFONY__ENV__MAILER_HOST=127.0.0.1 - SYMFONY__ENV__MAILER_USER=~ - SYMFONY__ENV__MAILER_PASSWORD=~ - SYMFONY__ENV__FROM_EMAIL=wallabag@example.com - SYMFONY__ENV__DOMAIN_NAME=https://your-wallabag-url-instance.com - SYMFONY__ENV__SERVER_NAME=\"Your wallabag instance\" ports : - \"80\" volumes : - /opt/wallabag/images:/var/www/wallabag/web/assets/images healthcheck : test : [ \"CMD\" , \"wget\" , \"--no-verbose\" , \"--tries=1\" , \"--spider\" , \"http://localhost\" ] interval : 1m timeout : 3s depends_on : - db - redis db : image : mariadb environment : - MYSQL_ROOT_PASSWORD=wallaroot volumes : - /opt/wallabag/data:/var/lib/mysql healthcheck : test : [ \"CMD\" , \"mysqladmin\" , \"ping\" , \"-h\" , \"localhost\" ] interval : 20s timeout : 3s redis : image : redis:alpine healthcheck : test : [ \"CMD\" , \"redis-cli\" , \"ping\" ] interval : 20s timeout : 3s If you don't want to enable the public registration use the SYMFONY__ENV__FOSUSER_REGISTRATION=false flag. The emailer configuration is only used when a user creates an account, so if you're only going to use it for yourself, it's safe to disable. Remember to change all passwords to a random value. If you create RSS feeds for a user, all articles are shared by default, if you only want to share the starred articles, add to your nginx config: location ~* /feed/.*/.*/(?!starred){ deny all; return 404; } References \u2691 Docs", "title": "Wallabag"}, {"location": "wallabag/#installation", "text": "They provide a working docker-compose version : '3' services : wallabag : image : wallabag/wallabag environment : - MYSQL_ROOT_PASSWORD=wallaroot - SYMFONY__ENV__DATABASE_DRIVER=pdo_mysql - SYMFONY__ENV__DATABASE_HOST=db - SYMFONY__ENV__DATABASE_PORT=3306 - SYMFONY__ENV__DATABASE_NAME=wallabag - SYMFONY__ENV__DATABASE_USER=wallabag - SYMFONY__ENV__DATABASE_PASSWORD=wallapass - SYMFONY__ENV__DATABASE_CHARSET=utf8mb4 - SYMFONY__ENV__SECRET=supersecretenv - SYMFONY__ENV__MAILER_HOST=127.0.0.1 - SYMFONY__ENV__MAILER_USER=~ - SYMFONY__ENV__MAILER_PASSWORD=~ - SYMFONY__ENV__FROM_EMAIL=wallabag@example.com - SYMFONY__ENV__DOMAIN_NAME=https://your-wallabag-url-instance.com - SYMFONY__ENV__SERVER_NAME=\"Your wallabag instance\" ports : - \"80\" volumes : - /opt/wallabag/images:/var/www/wallabag/web/assets/images healthcheck : test : [ \"CMD\" , \"wget\" , \"--no-verbose\" , \"--tries=1\" , \"--spider\" , \"http://localhost\" ] interval : 1m timeout : 3s depends_on : - db - redis db : image : mariadb environment : - MYSQL_ROOT_PASSWORD=wallaroot volumes : - /opt/wallabag/data:/var/lib/mysql healthcheck : test : [ \"CMD\" , \"mysqladmin\" , \"ping\" , \"-h\" , \"localhost\" ] interval : 20s timeout : 3s redis : image : redis:alpine healthcheck : test : [ \"CMD\" , \"redis-cli\" , \"ping\" ] interval : 20s timeout : 3s If you don't want to enable the public registration use the SYMFONY__ENV__FOSUSER_REGISTRATION=false flag. The emailer configuration is only used when a user creates an account, so if you're only going to use it for yourself, it's safe to disable. Remember to change all passwords to a random value. If you create RSS feeds for a user, all articles are shared by default, if you only want to share the starred articles, add to your nginx config: location ~* /feed/.*/.*/(?!starred){ deny all; return 404; }", "title": "Installation"}, {"location": "wallabag/#references", "text": "Docs", "title": "References"}, {"location": "week_management/", "text": "I've been polishing a week reviewing and planning method that suits my needs. I usually follow it on Wednesdays, as I'm too busy on Mondays and Tuesdays and it gives enough time to plan the weekend. Until I've got pydo ready to natively incorporate all this processes, I heavily use taskwarrior to manage my tasks and logs. To make the process faster and reproducible, I've written small python scripts using tasklib. Week review \u2691 Life logging is the only purpose of my weekly review. I've made diw a small python script that for each overdue task allows me to: Review: Opens vim to write a diary entry related with the task. The text is saved as an annotation of the task and another form is filled to record whom I've shared it with. The last information is used to help me take care of people around me. Skip: Don't interact with this task. Done: Complete the task. Delete: Remove the task. Reschedule: Opens a form to specify the new due date. Week planning \u2691 The purpose of the planning is to make sure that I know what I need to do and arrange all tasks in a way that allows me not to explode. First I empty the INBOX file, refactoring all the information in the other knowledge sinks. It's the place to go to quickly gather information, such as movie/book/serie recommendations, human arrangements, miscellaneous thoughts or tasks. This file lives in my mobile. I edit it with Markor and transfer it to my computer with Share via HTTP . Taking different actions to each INBOX element type: Tasks or human arrangements: Do it if it can be completed in less than 3 minutes. Otherwise, create a taskwarrior task. Behavior: Add it to taskwarrior. Movie/Serie recommendation: Introduce it into my media monitorization system. Book recommendation: Introduce into my library management system. Miscellaneous thoughts: Refactor into the blue-book, project documentation or Anki. Then I split my workspace in two terminals, in the first I run task due.before:7d diary where diary is a taskwarrior report that shows pending tasks that are not in the backlog sorted by due date. On the other I: Execute gcal . to show the calendar of the previous, current and next month. Check the weather for the whole week to decide which plans are suitable. Analyze the tasks that need to be done answering the following questions: Do I need to do this task this week? If not, reschedule or delete it. Does it need a due date? If not, remove the due attribute. Having the minimum number of tasks with a fixed date reduces wasted rescheduling time and allows better prioritizing. Can I do the task on the selected date? As most humans, I tend to underestimate both the required time to complete a task and to switch contexts. To avoid it, If the day is full it's better to reschedule. Check that every day has at least one task. Particularly tasks that will help with life logging. If there aren't enough things to fill up all days, check the things that I want to do list and try to do one.", "title": "Week Management"}, {"location": "week_management/#week-review", "text": "Life logging is the only purpose of my weekly review. I've made diw a small python script that for each overdue task allows me to: Review: Opens vim to write a diary entry related with the task. The text is saved as an annotation of the task and another form is filled to record whom I've shared it with. The last information is used to help me take care of people around me. Skip: Don't interact with this task. Done: Complete the task. Delete: Remove the task. Reschedule: Opens a form to specify the new due date.", "title": "Week review"}, {"location": "week_management/#week-planning", "text": "The purpose of the planning is to make sure that I know what I need to do and arrange all tasks in a way that allows me not to explode. First I empty the INBOX file, refactoring all the information in the other knowledge sinks. It's the place to go to quickly gather information, such as movie/book/serie recommendations, human arrangements, miscellaneous thoughts or tasks. This file lives in my mobile. I edit it with Markor and transfer it to my computer with Share via HTTP . Taking different actions to each INBOX element type: Tasks or human arrangements: Do it if it can be completed in less than 3 minutes. Otherwise, create a taskwarrior task. Behavior: Add it to taskwarrior. Movie/Serie recommendation: Introduce it into my media monitorization system. Book recommendation: Introduce into my library management system. Miscellaneous thoughts: Refactor into the blue-book, project documentation or Anki. Then I split my workspace in two terminals, in the first I run task due.before:7d diary where diary is a taskwarrior report that shows pending tasks that are not in the backlog sorted by due date. On the other I: Execute gcal . to show the calendar of the previous, current and next month. Check the weather for the whole week to decide which plans are suitable. Analyze the tasks that need to be done answering the following questions: Do I need to do this task this week? If not, reschedule or delete it. Does it need a due date? If not, remove the due attribute. Having the minimum number of tasks with a fixed date reduces wasted rescheduling time and allows better prioritizing. Can I do the task on the selected date? As most humans, I tend to underestimate both the required time to complete a task and to switch contexts. To avoid it, If the day is full it's better to reschedule. Check that every day has at least one task. Particularly tasks that will help with life logging. If there aren't enough things to fill up all days, check the things that I want to do list and try to do one.", "title": "Week planning"}, {"location": "wesnoth/", "text": "The Battle for Wesnoth is an open source, turn-based strategy game with a high fantasy theme. It features both singleplayer and online/hotseat multiplayer combat. Explore the world of Wesnoth and take part in its different adventures! Embark on a desperate quest to reclaim your rightful throne\u2026 Flee the Lich Lords to a new home across the sea\u2026 Delve into the darkest depths of the earth to craft a jewel of fire itself\u2026 Defend your kingdom against the ravaging hordes of a foul necromancer\u2026 Or lead a straggly band of survivors across the blazing sands to confront an unseen evil. References \u2691 Home Wiki Game manual Mainline campaigns", "title": "The Battle for Wesnoth"}, {"location": "wesnoth/#references", "text": "Home Wiki Game manual Mainline campaigns", "title": "References"}, {"location": "wesnoth_loyalist/", "text": "The Loyalist are a faction of Humans who are loyal to the throne of Wesnoth. The race of men is an extremely diverse one. Although they originally came from the Old Continent, men have spread all over the world and split into many different cultures and races. Although they are not imbued with magic like other creatures, humans can learn to wield it and able to learn more types than most others. They have no extra special abilities or aptitudes except their versatility and drive. While often at odds with all races, they can occasionally form alliances with the less aggressive races such as elves and dwarves. The less scrupulous among them do not shrink back from hiring orcish mercenaries, either. They have no natural enemies, although the majority of men, like most people of all races, have an instinctive dislike of the undead. Men are shorter than the elves, but taller still than dwarves. Their skin color can vary, from almost white to dark brown. Humans are a versatile race who specialize in many different areas. Similarly, the Loyalist faction is a very versatile melee oriented faction with important ranged support from bowmen and mages . How to play loyalists \u2691 Loyalists are considered to be the most versatile faction in the game. They have most available unit types to recruit (8), more than any other faction. Loyalists are mostly melee oriented faction, with only two ranged unit types, the mage and the bowman , but their ranged units play a significant role in the loyalists strategy and have to be used effectively. About the melee troops the loyalist player gets to choose among: Heavy Infantrymen : Few strikes, high damage per strike, slow, higher hit-points, good resistances, low defenses, deals impact damage which is good against undead. Spearmen : Average strikes, average damage per strike, average movement, medium hit-points, normal resistances, average defenses, deals pierce damage which is good against drakes, has a weak ranged attack. Fencer : High number of strikes, low damage per strike, quick, low hit-points, bad resistances, good defenses, deals blade damage which is good against elusive foots or wose, deals less total damage than the other two, is a skirmisher. The loyalists unit price ranges from the cheap spearmen and mermen (14 gold) to the expensive horsemen (23 gold). Loyalist units generally have good hit-points and they deal good damage, even the cheap spearmen and bowmen . Even their scouts have high hit-points and better melee attack compared to most of the scouts of the other factions. But on the other hand, the loyalist scouts have lower defenses than most of the other scouts and they do not have a ranged attack. The loyalists have some units with good resistances, like Heavy Infantry or Cavalryman , which can be very good for defending if you notice that your opponent doesn't have the proper units to counter them. Other units, like the bowman , who is more vulnerable than the previous two, but is also good for defense because it has better defense, is cheaper and it deals solid damage back in both melee and range. Similar goes for the spearman . When attacking , Loyalists have units which can be pretty devastating, like the horseman for attacking enemy mages and other ranged units, mages against the entrenched melee units, fencers for killing enemy injured and valuable units trying to heal behind their lines. But the mentioned units are also very vulnerable and they can die very easily if not properly supported. The loyalists as a faction generally have average defenses (they do not get 70% defense anywhere, with the exception of the fencer, which is a special case) and they are not a terrain dependent faction (like the Elves or Knalgans are). Therefore, even if it good to put spearmen and bowmen on mountains, villages and castles where they get 60% defense, you should also take the faction you are facing into account when deciding where to place your units. For example, if you are fighting Knalgans, you should try not to move your units next to hills and mountains and/or take the hills and mountains with your units and leave the forest to the dwarves. All the loyalist units are lawful, which obviously leads to the conclusion that the loyalists should be more aggressive and attack at day (or even start at dawn, because your opponent in most of the cases will be reluctant to attack too aggressively and leave units on bad ground when the morning arrives) while at night the loyalists should at least play conservatively, or in some cases even run away. Of course, that is also dependent upon the faction of your opponent. The greatest weakness of the loyalists is their limited mobility. The loyalists' units have normal movement, some of their units like the horseman , cavalryman , merman fighter and the fencer are pretty quick when they are in their favorable environment, but all their units get slowed over rough terrain a lot compared to the other factions' units . Half of the loyalists units can not even move on/over mountains, and their scouts get significantly slowed in forest, for example. This lack of mobility will often give your opponent an opportunity to outmaneuver you in the game.To prevent that, sometimes it is good to attack hard on a certain point and defend only with minimal amount of units elsewhere, rather than uniformly spreading your units around the front. When you don't know which faction you are facing, it is good to get a mixture of units. On most of the default 1v1 maps, this initial recruitment list would be good : spearman , bowman , fencer , mage , merman fighter , cavalryman . As always, you should carefully consider about the exact placement of each individual unit from the initial recruits on the castle to come up with the most optimal combination that will allow you the quickest village grabbing in the first turns of the game. Loyalists vs. Undead \u2691 When fighting undead, your recruiting pattern will depend on whether your opponent spams skeleton archers or dark adepts for ranged attacks. If you see lots of skeletons, you'll have to focus heavily on heavy infrantrymen and mages . If you see lots of dark adepts , you'll want some cavalry , horsemen and maybe some spearmen . Bowman : ( D- ) Almost entirely useless. You might use them to poke at walking corpses , ghouls , vampire bats and ghosts , but mages are much better at all of these things. I would only buy a bowman to counter a large zombie horde. Cavalryman : ( B+ ) You'll want some cavalry for scouting and dealing initial damage to dark adepts so that a horseman can finish them. They are also decent at fighting Skeletons in the daytime, because cavalry are blade-resistant. However, the cheap skeleton archers will really ruin cavalry quickly at night, so if there are any skeleton archers on the other side you won't be able to use cavalry to hold territory. If your opponent over-recruits dark adepts , however, you can use cold-resistant cavalry to hold territory against them. They are also decent for holding territory against ghouls because they can run away and heal (unlike your heavy infantrymen ). Fencer : ( C- ) Fencers are a bad recruit in this match up because they are vulnerable to the blade and pierce damage of skeletons and cannot damage them much in return. They also are incapable of holding territory against dark adepts , who cut right through the high defense of fencers . However, you may want to have a fencer or two around for trapping dark adepts or getting in that last hit. With luck, they may also be able to frustrate non-magical attacks from skeletons and the like. Heavy Infantryman : ( B+ ) You need heavy infantry to hold territory against skeletons and skeleton archers , and they will be your unit of choice for dealing melee damage, especially to the cheap skeleton archers . Any heavy infantrymen in the daytime can kill a full health skeleton archer in two hits, while a strong heavy infantrymen in the daytime can kill a full health skeleton in two hits, dealing 18 damage per strike (even a mage in daytime cannot kill a skeleton in one round, unless supported by a lieutenant ). A fearless heavy infantryman may be dangerous even at night. If you don't have enough heavy infantryman to go around, you can get your initial hits in on a skeleton archer with them and finish him with a mage . Just keep heavy infantrymen away from dark adepts , and only let ghouls hit heavy infantrymen if they are on a village (or has a white mage or other healer next to him), since they can't easily run away to heal. Also beware walking corpses , which deal a surprising amount of damage at all times of day since heavy infantrymen can't dodge and impact damage goes around their resistances, and the ranged cold attack of ghosts . The biggest problem with heavy infantrymen is they are slow, which means they're hard to retreat at night and hard to advance in day. Without shielding units they'll get trapped and killed, and if you have to shield a unit maybe it should be a mage instead. Horseman : ( B ) - Because they deal pierce damage, horsemen may not be very useful when faced with skeletons . However, if your opponent over-recruits dark adepts , horsemen can be extremely useful, as dark adepts deal no return damage to the normally risky charge attack. horsemen can even be used to finish skeleton archers , their nemesis, in the daytime. However, if your opponent recruits enough skeleton archers you will have a hard time shielding your horsemen from their devastating pierce attacks, and skeleton archers are dirt cheap. horsemen can also one-shot bats and zombies , which can be useful if you need to clear out a lot of level 0 units quickly. I would want to have at least one horseman around to keep my opponent from getting too bold with dark adepts , if not more. Your opponent will be forced to recruit dark adepts if you have heavy infantrymen in the field. Mage : ( A+ ) mages are an absolute necessity against Undead. If you do not have mages it will be almost impossible for you to kill ghosts , but with mages it's a piece of cake. mages are the best unit for killing almost everything Undead can throw at you, and can even be used to finish dark adepts in the daytime. Your main problem is that dark adepts are cheaper and deal almost as much damage, so your opponent can spam dark adepts while you cannot afford to spam mages . You will also have the difficult task of shielding fragile, expensive mages against lots of cheap Undead units. Your opponent will use skeletons and ghouls to attack your mages when he can, but bats , zombies or just about any other unit will do for killing your mages in a pinch. Shield your mages well, surround them with damage soakers and if you can deliver them safely to their targets you'll be able to clear out the Undead quickly. Merman Fighter : ( C- ) Mermen make a decent navy against Undead, since bats and ghosts will have a hard time killing them with their high defense. Even dark adepts will find Mermen annoying because of their 20% cold resistance. However, Mermen will have a hard time hurting anything the Undead have with their lame pierce weapon. Generally Mermen are only good for holding water hexes and scouting, but don't underestimate how useful that can be. Some well-placed Mermen on a water map can prevent bats from sneaking behind your lines and capturing villages or killing injured units. Even on mostly land maps, a Merman in a river can help hold a defensive line, or a quick Merman can use a river to slip behind the enemy to trap dark adepts or other units that are trying to escape at daytime. Spearman : ( C- ) Spearmen are mostly useful as cheap units for holding villages and occupying space when faced with dark adepts or skeleton archers . (You'll want to avoid letting dark adepts hit your heavy infantrymen because of their vulnerability to cold.) However, you don't really want spearmen to take hits from dark adepts , it would be better to let the cold-resistant cavalry absorb the damage. The only units spearmen are good for attacking are dark adepts and walking corpses . spearmen are completely useless against skeletons unless you level one into a swordsman , and even then they're pretty mediocre. However, if there are lots of skeleton archers you won't be able to use much cavalry or horsemen , so a spearman or two may be necessary as defenders and damage soakers even if they are lousy at dealing damage to Undead. Loyalists vs. Rebels \u2691 Rebels are a very versatile faction like the Loyalists. So you need to counter their versatility with yours. Their weakest trait is low hit-points. That is compensated somewhat by relatively higher defenses and mobility, so you'll need a combination of hard-hitters and speed. mages are also nice for their magical attacks. Anyways, the key for a Loyalist general is to effectively use his hard-hitting units and kill as many units as he can at day. And at night, to defend well as elves ignore the time of day. The smartest thing you can do upon discovering that your opponent is playing Rebels, in multiplayer, is to assume that your opponent has woses and recruit accordingly. woses are a necessary counter to Loyalists' daytime attacks and as soon as your opponent realizes that you are Loyalists he will almost certainly recruit woses . Remember that just because you don't see any woses doesn't mean there aren't a couple hiding in the forest! To fight woses you will need cavalry as meat shields, mages to deal damage, and maybe fencers to get the last hit in. Sadly, cavalry are very vulnerable to elvish archers , so you'll need to make sure that the archers die, which can be difficult if they are in forest. Get horsemen which can one-shot archers (despite their high dodge) and keep up with your cavalry . If one horseman misses, follow up with another one, it's a lucky archer that can dodge 4/4 one-shot attempts. heavy infantrymen are also great against everything but woses and mages , and will reduce the effectiveness of archer spam. spearmen are useless against woses , recruit them sparingly and keep them away from unscouted forest. An alternative way of playing this match up is to maximize the mobility offered by the Loyalists' fastest units, namely the two horse units. By using them to outrun the Rebel units you can achieve some devastating rushes. Some matches require use of both styles of playing. Again, their versatility must be countered with yours. Bowman : ( C- ) Rebels in general are effective in both melee and ranged attacks, so recruiting a ranged unit is less beneficial than most factions because most of the Rebels' units can retaliate against you. They can be useful for taking out an Scout or two, but otherwise, this is not really a smart buy. Cavalryman : ( B+ ) A good scout and an effective counter to woses . Watch out for archers, though, they can really tear horses to shreds. And as always, they are effective against ranged oriented units like the mage . ( A ) If you're going for some faster action, cavalrymen are vital in the attack. They do great damage during the day, and combined with their dangerous mobility, they can be a fearsome unit indeed. Just be careful of archers ZoCing you and tearing the unit to shreds. Fencer : ( B+ ) The fencer shares the 70% defense in the forest like most of the elves, but it has negative resistances. Theoretically, they should be effective against woses , but more often than not woses will crush them easily. In spite of this, Fencers still have skirmsher, which means that they can sneak behind an injured unit and finish them off. Do not over-recruit them, as enemy mages will tear though their high defense. Heavy Infantryman : ( C- ) Usefulness similar to an archer. It's too slow to deal with most of the Rebel units, and it is owned by Mages , woses , and even archers. Even though it has a high damage potential and good resistances, because of its inherently low defense shamans can easily get hits on it and turn it into a piece of metal for a turn. Horseman : ( B ) One horseman may be nice, but no more than that. Enemy archers will tear them apart and shamans totally mess them up. They're expensive too. Their greatest use is probably killing mages or an archer that has gone slightly off-track. ( A- ) Again, when mobility is required, these units need to come and do serious damage. Since Rebels are mostly comprised of low-hp units, horsemen at day can usually rip them apart. Again be careful of archers, for this unit is worth 23 gold. Shield him properly if he is damaged. Mage : ( A- ) You'll need these guys to tear though those high elvish 60-70% evasion in the forest; but be careful, archers and shamans will retaliate and some pretty nasty things may come from that. One purely positive thing though, they just absolutely destroy woses . 13-3 at day. Ouch. mages are expensive and fragile though, so keep them protected. Merman Fighter : ( B+ ) If the map has a lot of water, maybe recruit a few to prevent the Rebel's Mermen Hunters from taking over the waters. Otherwise they don't really contribute much else. Spearman - A - A necessity. To defend at night, to kill pretty much anything except for woses , and to be cheap and cost only 14 gold. These guys pretty much tear though most of the Rebel units, if it were not only for the high-defense archer and shaman . Get a bunch, and move them like a wall against the enemy units. At the start of the game, recruit 1-2 cavalrymen , depending on the map size, 1 mage , (1 merman fighter if you need some water coverage), maybe 1 fencer , and the rest spearman . Later on you maybe can recruit some horseman if you opponent recruits mass mages , or more cavalrymen and mages if he masses woses . Otherwise, spearmen and mages should help you get through most of the match. If you're going for speed, recruit 2-3 cavalrymen , depending on the map, 1 horseman , maybe 1 fencer , and the rest spearmen . Use your spearmen to fill the holes and consolidate the territory that the horses took over. Use the horses to outrun the archers and attack when they can. Loyalists vs Northerners \u2691 The main problem you will have when facing northerners are two things: poison and numbers. If northerners didn't have assassins , it wouldn't be much of a big deal; you could out manoeuvre them with your superior scouting units and skirmishers to grab their villages and then finish them off one by one with spearmen / bowmen . Thing is, assassins poison makes defending at night extremely difficult, and by poisoning your units, they often force you to retreat and heal, or die due to retaliation. The key to winning this match up is to use your superior versatility; the northerners greatest weakness is that ALL of their units are very average and don't often have many special abilities; you have masses. Use your first strike in spearmen to defend against grunts , trolls and wolfriders (even at night), use your magic to finish off wounded assassins , and you can charge any of their wounded units and kill them with one hit. Since all of your units are lawful and theirs are chaotic, you'll want to press your advantage at day and kill as much as possible (but don't attack assassins with bowmen ; you don't want to be poisoned and be forced to retreat in your daytime assault). Another problem the northerners have is that all though their units are many in number, they do not have a very high damage per hex ratio. Use this to your advantage; travel in packs and use ZoC lines to prevent your units from being swarmed too easily, and keep your vulnerable units like mages behind damage soakers like spearmen or heavy infantry . If you can, try and make night time defenses near rivers; northerners will hurt themselves when they attack you, as they have low defense in water and are very annoyed by deep water, because it prevents them from easily swarming you. Spearmen : ( A- ) Pretty much your best unit against northerners. Their first strike ability is great in defense and their attack power to cost ratio is quite high. They also have good health and can retaliate in ranged. As in many matchups, the bulk of your army. Mage : ( B+ ) The mage is fragile and expensive and has a weak melee attack, which is the exact opposite of northerners. Not a great defender unless your opponent goes mad with assassins. However, they have lots of attack power at day and are very useful for nocking trolls of their perches and finishing off those damn assassins . You'll want a few of these, but make sure you keep them protected. Bowmen : ( B ) Good unit to have. They can attack grunts and trolls (although you'd want mages to attack trolls ) without retaliation, and can defend against assassins in the night. They can also take out any wolves that stray too far from the main orchish army. They're not as good as mages , but they're cheaper and tougher. Cavalryman : ( B ) cavalryman are superior to their wolves and can hold ground against grunts and trolls reasonably well. It's also a good idea if your opponent likes to spam lots of assassins , to let them be the target of their poison; cavalryman and can run away and heal. Your heavy infantrymen ? Not so much. Beware though, of the cheap orchish archer, as cavalrymen are weak to pierce. Heavy Infantryman : ( B- ) Heavy infantryman are heavily resistant to physical attacks like blade, pierce and to a lesser degree, impact. You'd think this would make them great units against northerners, if the orchish archer didn't have a ranged fire attack. heavy infantrymen are also much too slow too effectively deal with poison, and that's a 19 gold unit that can't fight. However, they are useful in defense if you opponent hasn't spammed many archers, and they can even be useful in attack to crush up injured grunts . Horseman : ( B ) The horseman is a little controversial in this matchup. northerners are cheap and melee orientated, which is exactly what horseman do badly against. They're also quite tough, which means one hit kills are rare. However, they have one very important advantage over the northerners- high damage per hex. A horseman is very useful for softening up units or outright killing them (particularly when paired with a mage) which will be important in breaking ZoC lines, which can be a real pain with all the units northerners can field. Get one or two, but no more, else it quickly produces diminishing returns. Fencer : ( C+ ) The fencer is a melee based unit that is fragile against blade attacks of grunts , which means they don't have an awful lot of fighting effectiveness with them. On the other hand, the fencer's skirmisher ability is really valuable with the many northener units, and can finish off injured archers or go on sneaky village grabbing incursions. One or two might be useful, but no more, this unit is not a serious fighter! Make sure you keep them on 70% terrain if you can as well. Two hitter grunts can have trouble taking them out. Merman Fighter : ( C+ ) Water based melee unit that doesn't do well against the orc nagas , recruit only if there's lots of water and keep them in defendible terrain, else they'll quickly be killed. As usual, recruit cavalrymen and mermen depending on map (3-4 cavalryman for larger maps, 2-3 for medium sized ones and probably no more than one or two for small maps). Recruit plenty of spearmen and attack at day. Speaking of that, make sure you travel in large groups and do not attack with small numbers even at day. Northerners will swarm you too easily. Also, make sure you don't overextend yourself at day, as northerners do a lot more damage at night. Pay careful attention to your mages , they will be the ones the northerners will be going after, so retreat them quickly. When defending, try to avoid attacking units that can retaliate, unless you have a reasonably high chance of killing them that turn. Avoid attacking trolls if you can't heavily damage them or outright kill them, they're regenaration will quickly mean they can heal from any scratches your bowmen might have dealt them.", "title": "Loyalist"}, {"location": "wesnoth_loyalist/#how-to-play-loyalists", "text": "Loyalists are considered to be the most versatile faction in the game. They have most available unit types to recruit (8), more than any other faction. Loyalists are mostly melee oriented faction, with only two ranged unit types, the mage and the bowman , but their ranged units play a significant role in the loyalists strategy and have to be used effectively. About the melee troops the loyalist player gets to choose among: Heavy Infantrymen : Few strikes, high damage per strike, slow, higher hit-points, good resistances, low defenses, deals impact damage which is good against undead. Spearmen : Average strikes, average damage per strike, average movement, medium hit-points, normal resistances, average defenses, deals pierce damage which is good against drakes, has a weak ranged attack. Fencer : High number of strikes, low damage per strike, quick, low hit-points, bad resistances, good defenses, deals blade damage which is good against elusive foots or wose, deals less total damage than the other two, is a skirmisher. The loyalists unit price ranges from the cheap spearmen and mermen (14 gold) to the expensive horsemen (23 gold). Loyalist units generally have good hit-points and they deal good damage, even the cheap spearmen and bowmen . Even their scouts have high hit-points and better melee attack compared to most of the scouts of the other factions. But on the other hand, the loyalist scouts have lower defenses than most of the other scouts and they do not have a ranged attack. The loyalists have some units with good resistances, like Heavy Infantry or Cavalryman , which can be very good for defending if you notice that your opponent doesn't have the proper units to counter them. Other units, like the bowman , who is more vulnerable than the previous two, but is also good for defense because it has better defense, is cheaper and it deals solid damage back in both melee and range. Similar goes for the spearman . When attacking , Loyalists have units which can be pretty devastating, like the horseman for attacking enemy mages and other ranged units, mages against the entrenched melee units, fencers for killing enemy injured and valuable units trying to heal behind their lines. But the mentioned units are also very vulnerable and they can die very easily if not properly supported. The loyalists as a faction generally have average defenses (they do not get 70% defense anywhere, with the exception of the fencer, which is a special case) and they are not a terrain dependent faction (like the Elves or Knalgans are). Therefore, even if it good to put spearmen and bowmen on mountains, villages and castles where they get 60% defense, you should also take the faction you are facing into account when deciding where to place your units. For example, if you are fighting Knalgans, you should try not to move your units next to hills and mountains and/or take the hills and mountains with your units and leave the forest to the dwarves. All the loyalist units are lawful, which obviously leads to the conclusion that the loyalists should be more aggressive and attack at day (or even start at dawn, because your opponent in most of the cases will be reluctant to attack too aggressively and leave units on bad ground when the morning arrives) while at night the loyalists should at least play conservatively, or in some cases even run away. Of course, that is also dependent upon the faction of your opponent. The greatest weakness of the loyalists is their limited mobility. The loyalists' units have normal movement, some of their units like the horseman , cavalryman , merman fighter and the fencer are pretty quick when they are in their favorable environment, but all their units get slowed over rough terrain a lot compared to the other factions' units . Half of the loyalists units can not even move on/over mountains, and their scouts get significantly slowed in forest, for example. This lack of mobility will often give your opponent an opportunity to outmaneuver you in the game.To prevent that, sometimes it is good to attack hard on a certain point and defend only with minimal amount of units elsewhere, rather than uniformly spreading your units around the front. When you don't know which faction you are facing, it is good to get a mixture of units. On most of the default 1v1 maps, this initial recruitment list would be good : spearman , bowman , fencer , mage , merman fighter , cavalryman . As always, you should carefully consider about the exact placement of each individual unit from the initial recruits on the castle to come up with the most optimal combination that will allow you the quickest village grabbing in the first turns of the game.", "title": "How to play loyalists"}, {"location": "wesnoth_loyalist/#loyalists-vs-undead", "text": "When fighting undead, your recruiting pattern will depend on whether your opponent spams skeleton archers or dark adepts for ranged attacks. If you see lots of skeletons, you'll have to focus heavily on heavy infrantrymen and mages . If you see lots of dark adepts , you'll want some cavalry , horsemen and maybe some spearmen . Bowman : ( D- ) Almost entirely useless. You might use them to poke at walking corpses , ghouls , vampire bats and ghosts , but mages are much better at all of these things. I would only buy a bowman to counter a large zombie horde. Cavalryman : ( B+ ) You'll want some cavalry for scouting and dealing initial damage to dark adepts so that a horseman can finish them. They are also decent at fighting Skeletons in the daytime, because cavalry are blade-resistant. However, the cheap skeleton archers will really ruin cavalry quickly at night, so if there are any skeleton archers on the other side you won't be able to use cavalry to hold territory. If your opponent over-recruits dark adepts , however, you can use cold-resistant cavalry to hold territory against them. They are also decent for holding territory against ghouls because they can run away and heal (unlike your heavy infantrymen ). Fencer : ( C- ) Fencers are a bad recruit in this match up because they are vulnerable to the blade and pierce damage of skeletons and cannot damage them much in return. They also are incapable of holding territory against dark adepts , who cut right through the high defense of fencers . However, you may want to have a fencer or two around for trapping dark adepts or getting in that last hit. With luck, they may also be able to frustrate non-magical attacks from skeletons and the like. Heavy Infantryman : ( B+ ) You need heavy infantry to hold territory against skeletons and skeleton archers , and they will be your unit of choice for dealing melee damage, especially to the cheap skeleton archers . Any heavy infantrymen in the daytime can kill a full health skeleton archer in two hits, while a strong heavy infantrymen in the daytime can kill a full health skeleton in two hits, dealing 18 damage per strike (even a mage in daytime cannot kill a skeleton in one round, unless supported by a lieutenant ). A fearless heavy infantryman may be dangerous even at night. If you don't have enough heavy infantryman to go around, you can get your initial hits in on a skeleton archer with them and finish him with a mage . Just keep heavy infantrymen away from dark adepts , and only let ghouls hit heavy infantrymen if they are on a village (or has a white mage or other healer next to him), since they can't easily run away to heal. Also beware walking corpses , which deal a surprising amount of damage at all times of day since heavy infantrymen can't dodge and impact damage goes around their resistances, and the ranged cold attack of ghosts . The biggest problem with heavy infantrymen is they are slow, which means they're hard to retreat at night and hard to advance in day. Without shielding units they'll get trapped and killed, and if you have to shield a unit maybe it should be a mage instead. Horseman : ( B ) - Because they deal pierce damage, horsemen may not be very useful when faced with skeletons . However, if your opponent over-recruits dark adepts , horsemen can be extremely useful, as dark adepts deal no return damage to the normally risky charge attack. horsemen can even be used to finish skeleton archers , their nemesis, in the daytime. However, if your opponent recruits enough skeleton archers you will have a hard time shielding your horsemen from their devastating pierce attacks, and skeleton archers are dirt cheap. horsemen can also one-shot bats and zombies , which can be useful if you need to clear out a lot of level 0 units quickly. I would want to have at least one horseman around to keep my opponent from getting too bold with dark adepts , if not more. Your opponent will be forced to recruit dark adepts if you have heavy infantrymen in the field. Mage : ( A+ ) mages are an absolute necessity against Undead. If you do not have mages it will be almost impossible for you to kill ghosts , but with mages it's a piece of cake. mages are the best unit for killing almost everything Undead can throw at you, and can even be used to finish dark adepts in the daytime. Your main problem is that dark adepts are cheaper and deal almost as much damage, so your opponent can spam dark adepts while you cannot afford to spam mages . You will also have the difficult task of shielding fragile, expensive mages against lots of cheap Undead units. Your opponent will use skeletons and ghouls to attack your mages when he can, but bats , zombies or just about any other unit will do for killing your mages in a pinch. Shield your mages well, surround them with damage soakers and if you can deliver them safely to their targets you'll be able to clear out the Undead quickly. Merman Fighter : ( C- ) Mermen make a decent navy against Undead, since bats and ghosts will have a hard time killing them with their high defense. Even dark adepts will find Mermen annoying because of their 20% cold resistance. However, Mermen will have a hard time hurting anything the Undead have with their lame pierce weapon. Generally Mermen are only good for holding water hexes and scouting, but don't underestimate how useful that can be. Some well-placed Mermen on a water map can prevent bats from sneaking behind your lines and capturing villages or killing injured units. Even on mostly land maps, a Merman in a river can help hold a defensive line, or a quick Merman can use a river to slip behind the enemy to trap dark adepts or other units that are trying to escape at daytime. Spearman : ( C- ) Spearmen are mostly useful as cheap units for holding villages and occupying space when faced with dark adepts or skeleton archers . (You'll want to avoid letting dark adepts hit your heavy infantrymen because of their vulnerability to cold.) However, you don't really want spearmen to take hits from dark adepts , it would be better to let the cold-resistant cavalry absorb the damage. The only units spearmen are good for attacking are dark adepts and walking corpses . spearmen are completely useless against skeletons unless you level one into a swordsman , and even then they're pretty mediocre. However, if there are lots of skeleton archers you won't be able to use much cavalry or horsemen , so a spearman or two may be necessary as defenders and damage soakers even if they are lousy at dealing damage to Undead.", "title": "Loyalists vs. Undead"}, {"location": "wesnoth_loyalist/#loyalists-vs-rebels", "text": "Rebels are a very versatile faction like the Loyalists. So you need to counter their versatility with yours. Their weakest trait is low hit-points. That is compensated somewhat by relatively higher defenses and mobility, so you'll need a combination of hard-hitters and speed. mages are also nice for their magical attacks. Anyways, the key for a Loyalist general is to effectively use his hard-hitting units and kill as many units as he can at day. And at night, to defend well as elves ignore the time of day. The smartest thing you can do upon discovering that your opponent is playing Rebels, in multiplayer, is to assume that your opponent has woses and recruit accordingly. woses are a necessary counter to Loyalists' daytime attacks and as soon as your opponent realizes that you are Loyalists he will almost certainly recruit woses . Remember that just because you don't see any woses doesn't mean there aren't a couple hiding in the forest! To fight woses you will need cavalry as meat shields, mages to deal damage, and maybe fencers to get the last hit in. Sadly, cavalry are very vulnerable to elvish archers , so you'll need to make sure that the archers die, which can be difficult if they are in forest. Get horsemen which can one-shot archers (despite their high dodge) and keep up with your cavalry . If one horseman misses, follow up with another one, it's a lucky archer that can dodge 4/4 one-shot attempts. heavy infantrymen are also great against everything but woses and mages , and will reduce the effectiveness of archer spam. spearmen are useless against woses , recruit them sparingly and keep them away from unscouted forest. An alternative way of playing this match up is to maximize the mobility offered by the Loyalists' fastest units, namely the two horse units. By using them to outrun the Rebel units you can achieve some devastating rushes. Some matches require use of both styles of playing. Again, their versatility must be countered with yours. Bowman : ( C- ) Rebels in general are effective in both melee and ranged attacks, so recruiting a ranged unit is less beneficial than most factions because most of the Rebels' units can retaliate against you. They can be useful for taking out an Scout or two, but otherwise, this is not really a smart buy. Cavalryman : ( B+ ) A good scout and an effective counter to woses . Watch out for archers, though, they can really tear horses to shreds. And as always, they are effective against ranged oriented units like the mage . ( A ) If you're going for some faster action, cavalrymen are vital in the attack. They do great damage during the day, and combined with their dangerous mobility, they can be a fearsome unit indeed. Just be careful of archers ZoCing you and tearing the unit to shreds. Fencer : ( B+ ) The fencer shares the 70% defense in the forest like most of the elves, but it has negative resistances. Theoretically, they should be effective against woses , but more often than not woses will crush them easily. In spite of this, Fencers still have skirmsher, which means that they can sneak behind an injured unit and finish them off. Do not over-recruit them, as enemy mages will tear though their high defense. Heavy Infantryman : ( C- ) Usefulness similar to an archer. It's too slow to deal with most of the Rebel units, and it is owned by Mages , woses , and even archers. Even though it has a high damage potential and good resistances, because of its inherently low defense shamans can easily get hits on it and turn it into a piece of metal for a turn. Horseman : ( B ) One horseman may be nice, but no more than that. Enemy archers will tear them apart and shamans totally mess them up. They're expensive too. Their greatest use is probably killing mages or an archer that has gone slightly off-track. ( A- ) Again, when mobility is required, these units need to come and do serious damage. Since Rebels are mostly comprised of low-hp units, horsemen at day can usually rip them apart. Again be careful of archers, for this unit is worth 23 gold. Shield him properly if he is damaged. Mage : ( A- ) You'll need these guys to tear though those high elvish 60-70% evasion in the forest; but be careful, archers and shamans will retaliate and some pretty nasty things may come from that. One purely positive thing though, they just absolutely destroy woses . 13-3 at day. Ouch. mages are expensive and fragile though, so keep them protected. Merman Fighter : ( B+ ) If the map has a lot of water, maybe recruit a few to prevent the Rebel's Mermen Hunters from taking over the waters. Otherwise they don't really contribute much else. Spearman - A - A necessity. To defend at night, to kill pretty much anything except for woses , and to be cheap and cost only 14 gold. These guys pretty much tear though most of the Rebel units, if it were not only for the high-defense archer and shaman . Get a bunch, and move them like a wall against the enemy units. At the start of the game, recruit 1-2 cavalrymen , depending on the map size, 1 mage , (1 merman fighter if you need some water coverage), maybe 1 fencer , and the rest spearman . Later on you maybe can recruit some horseman if you opponent recruits mass mages , or more cavalrymen and mages if he masses woses . Otherwise, spearmen and mages should help you get through most of the match. If you're going for speed, recruit 2-3 cavalrymen , depending on the map, 1 horseman , maybe 1 fencer , and the rest spearmen . Use your spearmen to fill the holes and consolidate the territory that the horses took over. Use the horses to outrun the archers and attack when they can.", "title": "Loyalists vs. Rebels"}, {"location": "wesnoth_loyalist/#loyalists-vs-northerners", "text": "The main problem you will have when facing northerners are two things: poison and numbers. If northerners didn't have assassins , it wouldn't be much of a big deal; you could out manoeuvre them with your superior scouting units and skirmishers to grab their villages and then finish them off one by one with spearmen / bowmen . Thing is, assassins poison makes defending at night extremely difficult, and by poisoning your units, they often force you to retreat and heal, or die due to retaliation. The key to winning this match up is to use your superior versatility; the northerners greatest weakness is that ALL of their units are very average and don't often have many special abilities; you have masses. Use your first strike in spearmen to defend against grunts , trolls and wolfriders (even at night), use your magic to finish off wounded assassins , and you can charge any of their wounded units and kill them with one hit. Since all of your units are lawful and theirs are chaotic, you'll want to press your advantage at day and kill as much as possible (but don't attack assassins with bowmen ; you don't want to be poisoned and be forced to retreat in your daytime assault). Another problem the northerners have is that all though their units are many in number, they do not have a very high damage per hex ratio. Use this to your advantage; travel in packs and use ZoC lines to prevent your units from being swarmed too easily, and keep your vulnerable units like mages behind damage soakers like spearmen or heavy infantry . If you can, try and make night time defenses near rivers; northerners will hurt themselves when they attack you, as they have low defense in water and are very annoyed by deep water, because it prevents them from easily swarming you. Spearmen : ( A- ) Pretty much your best unit against northerners. Their first strike ability is great in defense and their attack power to cost ratio is quite high. They also have good health and can retaliate in ranged. As in many matchups, the bulk of your army. Mage : ( B+ ) The mage is fragile and expensive and has a weak melee attack, which is the exact opposite of northerners. Not a great defender unless your opponent goes mad with assassins. However, they have lots of attack power at day and are very useful for nocking trolls of their perches and finishing off those damn assassins . You'll want a few of these, but make sure you keep them protected. Bowmen : ( B ) Good unit to have. They can attack grunts and trolls (although you'd want mages to attack trolls ) without retaliation, and can defend against assassins in the night. They can also take out any wolves that stray too far from the main orchish army. They're not as good as mages , but they're cheaper and tougher. Cavalryman : ( B ) cavalryman are superior to their wolves and can hold ground against grunts and trolls reasonably well. It's also a good idea if your opponent likes to spam lots of assassins , to let them be the target of their poison; cavalryman and can run away and heal. Your heavy infantrymen ? Not so much. Beware though, of the cheap orchish archer, as cavalrymen are weak to pierce. Heavy Infantryman : ( B- ) Heavy infantryman are heavily resistant to physical attacks like blade, pierce and to a lesser degree, impact. You'd think this would make them great units against northerners, if the orchish archer didn't have a ranged fire attack. heavy infantrymen are also much too slow too effectively deal with poison, and that's a 19 gold unit that can't fight. However, they are useful in defense if you opponent hasn't spammed many archers, and they can even be useful in attack to crush up injured grunts . Horseman : ( B ) The horseman is a little controversial in this matchup. northerners are cheap and melee orientated, which is exactly what horseman do badly against. They're also quite tough, which means one hit kills are rare. However, they have one very important advantage over the northerners- high damage per hex. A horseman is very useful for softening up units or outright killing them (particularly when paired with a mage) which will be important in breaking ZoC lines, which can be a real pain with all the units northerners can field. Get one or two, but no more, else it quickly produces diminishing returns. Fencer : ( C+ ) The fencer is a melee based unit that is fragile against blade attacks of grunts , which means they don't have an awful lot of fighting effectiveness with them. On the other hand, the fencer's skirmisher ability is really valuable with the many northener units, and can finish off injured archers or go on sneaky village grabbing incursions. One or two might be useful, but no more, this unit is not a serious fighter! Make sure you keep them on 70% terrain if you can as well. Two hitter grunts can have trouble taking them out. Merman Fighter : ( C+ ) Water based melee unit that doesn't do well against the orc nagas , recruit only if there's lots of water and keep them in defendible terrain, else they'll quickly be killed. As usual, recruit cavalrymen and mermen depending on map (3-4 cavalryman for larger maps, 2-3 for medium sized ones and probably no more than one or two for small maps). Recruit plenty of spearmen and attack at day. Speaking of that, make sure you travel in large groups and do not attack with small numbers even at day. Northerners will swarm you too easily. Also, make sure you don't overextend yourself at day, as northerners do a lot more damage at night. Pay careful attention to your mages , they will be the ones the northerners will be going after, so retreat them quickly. When defending, try to avoid attacking units that can retaliate, unless you have a reasonably high chance of killing them that turn. Avoid attacking trolls if you can't heavily damage them or outright kill them, they're regenaration will quickly mean they can heal from any scratches your bowmen might have dealt them.", "title": "Loyalists vs Northerners"}, {"location": "wesnoth_northerners/", "text": "Northerners are a faction of Orcs and their allies who live in the north of the Great Continent, thus their name. Northerners consist of the warrior orcs race, the enslaved goblins, trolls who are tricked into combat by the orcs, and the serpentine naga. The Northerners play best by taking advantage of having many low-cost and high HP soldiers.", "title": "Northerners"}, {"location": "wesnoth_rebels/", "text": "Rebels are a faction of Elves and their various forest-dwelling allies. They get their human name, Rebels, from the time of Heir to the Throne, when they started the rebellion against the evil Queen Asheviere. Elves are a magical race that are masters of the bow and are capable of living many years longer than humans. In harmony with nature, the elves find allies with the human mages, certain merfolk, and tree creatures called woses. Rebels are best played taking advantage of their high forest defense, mastery of ranged attacks, and the elves' neutral alignment.", "title": "Rebels"}, {"location": "wireshark/", "text": "Wireshark is the world\u2019s foremost and widely-used network protocol analyzer. It lets you see what\u2019s happening on your network at a microscopic level and is the de facto (and often de jure) standard across many commercial and non-profit enterprises, government agencies, and educational institutions. Installation \u2691 apt-get install wireshark If the version delivered by your distribution is not high enough, use Jezz's Docker docker run -d \\ -v /etc/localtime:/etc/localtime:ro \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = unix $DISPLAY \\ -v /tmp/wireshark:/data \\ jess/wireshark Usage \u2691 Filter \u2691 You can filter by traffic type with tcp and tcp.port == 80 , http or ftp or not ftp . It's also possible to nest many operators with (http or ftp) and ip.addr == 192.168.1.14 The most common filters are: Item Description ip.addr IP address (check both source and destination) tcp.port TCP Layer 4 port (check both source and destination) udp.port UDP Layer 4 port (check both source and destination) ip.src IP source address ip.dst IP destination address tcp.srcport TCP source port tcp.dstport TCP destination port udp.srcport UDP source port udp.dstport UDP destination port icmp.type ICMP numeric type ip.tos.precedence IP precedence eth.addr MAC address ip.ttl IP Time to Live (TTL) References \u2691 Home", "title": "Wireshark"}, {"location": "wireshark/#installation", "text": "apt-get install wireshark If the version delivered by your distribution is not high enough, use Jezz's Docker docker run -d \\ -v /etc/localtime:/etc/localtime:ro \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY = unix $DISPLAY \\ -v /tmp/wireshark:/data \\ jess/wireshark", "title": "Installation"}, {"location": "wireshark/#usage", "text": "", "title": "Usage"}, {"location": "wireshark/#filter", "text": "You can filter by traffic type with tcp and tcp.port == 80 , http or ftp or not ftp . It's also possible to nest many operators with (http or ftp) and ip.addr == 192.168.1.14 The most common filters are: Item Description ip.addr IP address (check both source and destination) tcp.port TCP Layer 4 port (check both source and destination) udp.port UDP Layer 4 port (check both source and destination) ip.src IP source address ip.dst IP destination address tcp.srcport TCP source port tcp.dstport TCP destination port udp.srcport UDP source port udp.dstport UDP destination port icmp.type ICMP numeric type ip.tos.precedence IP precedence eth.addr MAC address ip.ttl IP Time to Live (TTL)", "title": "Filter"}, {"location": "wireshark/#references", "text": "Home", "title": "References"}, {"location": "work_interruption_analysis/", "text": "This is the interruption analysis report applied to my everyday work. I've identified the next interruption sources: Physical interruptions . Emails . Calls . Instant message applications . Calendar events . Physical interruptions \u2691 Physical interactions are when someone comes to your desk and expect you to attend them immediately. These interruptions can be categorized as: Asking for help . Social interactions . The obvious solution is to remote work as much as possible. It's less easy for people to interrupt through digital channels than physically. It goes the other way around too. Be respectful to your colleagues and try to use asynchronous communications as much as possible, so they can manage when they attend you. Asking for help \u2691 These interruptions are the most difficult to delay, as it's hard to tell a person to wait when it's already in front of you. If you don't take care of them you may end up in the situation where you can receive 5 o 6 interruptions per minute which can drive you crazy. By definition all these events require an immediate action. The priority and delay may depend on many factors, such as the person or moment. The first thing I'd do is make a mental prioritization of the people that interrupt you, to decide which ones do you accept and which ones you need to regulate. Once you have it, work on how to assertively tell them that they need to reduce their interruptions. You can agree with them a non interruption time where they can aggregate and prepare all the questions so you can work through them efficiently. Often they are able to answer most of them themselves. The length of the period needs to be picked wisely as you want to be interrupted the minimum number of times while you don't make them loose their time trying to solve something you could work out quickly. Other times it's easier to forward them to the team's interruption manager . Social interactions \u2691 Depending how popular you are, you'll have more or less of these interactions. The way I've found to be able to be in control of them is by scheduling social events in my calendar and introducing them in my task management workflow. For example, we agree to go to have lunch all together at the same hour every day, or I arrange a coffee break with someone every Monday at a defined hour. Emails \u2691 Email can be used as one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification of things that don't need to be acted upon immediately or when more powerful mechanisms are not available. In my case emails can be categorized as: General information: They don't usually require any direct action, so they can wait more than 24 hours. Support to internal agents: At work, we have decided that email is not to be used as the internal main communication channel, so I don't receive many and their priority is low. Support to external agents: I'm lucky to not have many of these and they have less priority than internal people so they can wait 4 or more hours. Infrastructure notifications: For example LetsEncrypt renewals or cloud provider notification or support cases. The related actions can wait 4 hours or more. Calendar events: Someone creates a new meeting, changes an existing one or confirms/declines its assistance. We have defined a policy that we don't create or change events with less than 24 hours notice, and in the special cases that we need to, they will be addressed in the chat rooms. So these mails can be read once per day. Monitorization notifications: We've configured Prometheus's alertmanager to send the notifications to the email as a fallback channel, but it's to be checked only if the main channel is down. Source code manager notifications: The web where we host our source code sends us emails when there are new pull requests or when there are comments on existent ones. I automatically mark them as read and move them to a mail directory as I manage these interruptions with other workflow. The CI sends notifications when some job fails. Unless it's a new pipeline or I'm actively working on it, a failed job can wait four hours broken before I interact with it. The issue tracker notifications: It sends them on new or changed issues. At work, I filter them out as I delegate it's management to the Scrum Master. In conclusion, I can check the work email only when I start working, on the lunch break and when I'm about to leave. So its safe to disable the notifications. I'm eager to start the email automation project so I can spend even less time and willpower managing the email. Calls \u2691 We've agreed that the calls are the communication channel used only for critical situations, similar to the physical interruptions , they are synchronous so they're more difficult to manage. As calls are very rare and of high priority, I have my phone configured to ring on incoming calls. Have a work phone independent of your personal Nowadays you can have phone contracts of 0$/month used only to receive calls. Remember to give it to the fewer people as possible. Instant messages \u2691 It's the main internal communication channel, so it has a great volume of events with a wide range of priorities. They can be categorized as: Asking for help through direct messages: We don't have many as we've agreed to use groups as much as possible . So they have high priority and I have the notifications enabled. Social interaction through direct messages: I don't have many as I try to arrange one on one meetings instead , so they have a low priority. As notifications are defined for all direct messages, I inherit the notifications from the last category. Team group or support rooms: We've defined the interruption role so I check them whenever an chosen interruption event comes. If I'm assuming the role I enable the notifications on the channel, if not I'll check them whenever I check the application. Information rooms: They have no priority and can be checked each 4 hours. In conclusion, I can check the work chat applications each pomodoro cycle or when I receive a direct notification until the improve the notification management in Linux project is ready. Calendar events \u2691 Often with a wide range of priorities. decide if you have to go Define an agenda with times", "title": "Work Interruption Analysis"}, {"location": "work_interruption_analysis/#physical-interruptions", "text": "Physical interactions are when someone comes to your desk and expect you to attend them immediately. These interruptions can be categorized as: Asking for help . Social interactions . The obvious solution is to remote work as much as possible. It's less easy for people to interrupt through digital channels than physically. It goes the other way around too. Be respectful to your colleagues and try to use asynchronous communications as much as possible, so they can manage when they attend you.", "title": "Physical interruptions"}, {"location": "work_interruption_analysis/#asking-for-help", "text": "These interruptions are the most difficult to delay, as it's hard to tell a person to wait when it's already in front of you. If you don't take care of them you may end up in the situation where you can receive 5 o 6 interruptions per minute which can drive you crazy. By definition all these events require an immediate action. The priority and delay may depend on many factors, such as the person or moment. The first thing I'd do is make a mental prioritization of the people that interrupt you, to decide which ones do you accept and which ones you need to regulate. Once you have it, work on how to assertively tell them that they need to reduce their interruptions. You can agree with them a non interruption time where they can aggregate and prepare all the questions so you can work through them efficiently. Often they are able to answer most of them themselves. The length of the period needs to be picked wisely as you want to be interrupted the minimum number of times while you don't make them loose their time trying to solve something you could work out quickly. Other times it's easier to forward them to the team's interruption manager .", "title": "Asking for help"}, {"location": "work_interruption_analysis/#social-interactions", "text": "Depending how popular you are, you'll have more or less of these interactions. The way I've found to be able to be in control of them is by scheduling social events in my calendar and introducing them in my task management workflow. For example, we agree to go to have lunch all together at the same hour every day, or I arrange a coffee break with someone every Monday at a defined hour.", "title": "Social interactions"}, {"location": "work_interruption_analysis/#emails", "text": "Email can be used as one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification of things that don't need to be acted upon immediately or when more powerful mechanisms are not available. In my case emails can be categorized as: General information: They don't usually require any direct action, so they can wait more than 24 hours. Support to internal agents: At work, we have decided that email is not to be used as the internal main communication channel, so I don't receive many and their priority is low. Support to external agents: I'm lucky to not have many of these and they have less priority than internal people so they can wait 4 or more hours. Infrastructure notifications: For example LetsEncrypt renewals or cloud provider notification or support cases. The related actions can wait 4 hours or more. Calendar events: Someone creates a new meeting, changes an existing one or confirms/declines its assistance. We have defined a policy that we don't create or change events with less than 24 hours notice, and in the special cases that we need to, they will be addressed in the chat rooms. So these mails can be read once per day. Monitorization notifications: We've configured Prometheus's alertmanager to send the notifications to the email as a fallback channel, but it's to be checked only if the main channel is down. Source code manager notifications: The web where we host our source code sends us emails when there are new pull requests or when there are comments on existent ones. I automatically mark them as read and move them to a mail directory as I manage these interruptions with other workflow. The CI sends notifications when some job fails. Unless it's a new pipeline or I'm actively working on it, a failed job can wait four hours broken before I interact with it. The issue tracker notifications: It sends them on new or changed issues. At work, I filter them out as I delegate it's management to the Scrum Master. In conclusion, I can check the work email only when I start working, on the lunch break and when I'm about to leave. So its safe to disable the notifications. I'm eager to start the email automation project so I can spend even less time and willpower managing the email.", "title": "Emails"}, {"location": "work_interruption_analysis/#calls", "text": "We've agreed that the calls are the communication channel used only for critical situations, similar to the physical interruptions , they are synchronous so they're more difficult to manage. As calls are very rare and of high priority, I have my phone configured to ring on incoming calls. Have a work phone independent of your personal Nowadays you can have phone contracts of 0$/month used only to receive calls. Remember to give it to the fewer people as possible.", "title": "Calls"}, {"location": "work_interruption_analysis/#instant-messages", "text": "It's the main internal communication channel, so it has a great volume of events with a wide range of priorities. They can be categorized as: Asking for help through direct messages: We don't have many as we've agreed to use groups as much as possible . So they have high priority and I have the notifications enabled. Social interaction through direct messages: I don't have many as I try to arrange one on one meetings instead , so they have a low priority. As notifications are defined for all direct messages, I inherit the notifications from the last category. Team group or support rooms: We've defined the interruption role so I check them whenever an chosen interruption event comes. If I'm assuming the role I enable the notifications on the channel, if not I'll check them whenever I check the application. Information rooms: They have no priority and can be checked each 4 hours. In conclusion, I can check the work chat applications each pomodoro cycle or when I receive a direct notification until the improve the notification management in Linux project is ready.", "title": "Instant messages"}, {"location": "work_interruption_analysis/#calendar-events", "text": "Often with a wide range of priorities. decide if you have to go Define an agenda with times", "title": "Calendar events"}, {"location": "write_neovim_plugins/", "text": "plugin example plugin repo The plugin repo has some examples in the tests directory Control an existing nvim instance \u2691 A number of different transports are supported, but the simplest way to get started is with the python REPL. First, start Nvim with a known address (or use the $NVIM_LISTEN_ADDRESS of a running instance): $ NVIM_LISTEN_ADDRESS = /tmp/nvim nvim In another terminal, connect a python REPL to Nvim (note that the API is similar to the one exposed by the python-vim bridge : >>> from neovim import attach # Create a python API session attached to unix domain socket created above: >>> nvim = attach ( 'socket' , path = '/tmp/nvim' ) # Now do some work. >>> buffer = nvim . current . buffer # Get the current buffer >>> buffer [ 0 ] = 'replace first line' >>> buffer [:] = [ 'replace whole buffer' ] >>> nvim . command ( 'vsplit' ) >>> nvim . windows [ 1 ] . width = 10 >>> nvim . vars [ 'global_var' ] = [ 1 , 2 , 3 ] >>> nvim . eval ( 'g:global_var' ) [ 1 , 2 , 3 ] Load buffer \u2691 buffer = nvim . current . buffer # Get the current buffer buffer [ 0 ] = 'replace first line' buffer [:] = [ 'replace whole buffer' ] Get cursor position \u2691 nvim . current . window . cursor", "title": "Write Neovim Plugins"}, {"location": "write_neovim_plugins/#control-an-existing-nvim-instance", "text": "A number of different transports are supported, but the simplest way to get started is with the python REPL. First, start Nvim with a known address (or use the $NVIM_LISTEN_ADDRESS of a running instance): $ NVIM_LISTEN_ADDRESS = /tmp/nvim nvim In another terminal, connect a python REPL to Nvim (note that the API is similar to the one exposed by the python-vim bridge : >>> from neovim import attach # Create a python API session attached to unix domain socket created above: >>> nvim = attach ( 'socket' , path = '/tmp/nvim' ) # Now do some work. >>> buffer = nvim . current . buffer # Get the current buffer >>> buffer [ 0 ] = 'replace first line' >>> buffer [:] = [ 'replace whole buffer' ] >>> nvim . command ( 'vsplit' ) >>> nvim . windows [ 1 ] . width = 10 >>> nvim . vars [ 'global_var' ] = [ 1 , 2 , 3 ] >>> nvim . eval ( 'g:global_var' ) [ 1 , 2 , 3 ]", "title": "Control an existing nvim instance"}, {"location": "write_neovim_plugins/#load-buffer", "text": "buffer = nvim . current . buffer # Get the current buffer buffer [ 0 ] = 'replace first line' buffer [:] = [ 'replace whole buffer' ]", "title": "Load buffer"}, {"location": "write_neovim_plugins/#get-cursor-position", "text": "nvim . current . window . cursor", "title": "Get cursor position"}, {"location": "writing_style/", "text": "Writing style is the manner of expressing thought in language characteristic of an individual, period, school, or nation. It's defined by the grammatical choices writers make, the importance of adhering to norms in certain contexts and deviating from them in others, the expression of social identity, and the emotional effects of particular devices on audiences. Beyond the essential elements of spelling, grammar, and punctuation, writing style is the choice of words, sentence structure, and paragraph structure, used to convey the meaning effectively. The point of good writing style is to: Express the message to the reader simply, clearly, and convincingly. Keep the reader attentive, engaged, and interested. Not to Display the writer's personality. Demonstrate the writer's skills, knowledge, or abilities. General writing principles \u2691 Make it pleasant to the reader \u2691 Writing is a medium of communication, so avoid introducing elements that push away the reader, such as: Spelling mistakes. Gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing. Ugly environment: Present your texts through a pleasant medium such as a mkdocs webpage. Write like you talk: Ask yourself, is this the way I'd say this if I were talking to a friend? . If it isn't, imagine what you would say, and use that instead. Format errors: If you're writing in markdown, make sure that the result has no display bugs. Write short articles: Even though I love Gwern site , I find it daunting most of times. Instead of a big post, I'd rather use multiple well connected articles. Saying more with less \u2691 Never use a long word where a short one will do. Replace words like really like with love or other more appropriate words that save space writing and are more meaningful. Don't use filler words like really . Be aware of pacing \u2691 Be aware of pacing between words and sentences. The sentences ideally should flow into one another. Breaks in form of commas and full steps are important, as they allow for the reader to take a break and absorb the point that you tried to deliver. Try to use less tan 30 words per sentence. For example, change Due to the fact that to because . One purpose \u2691 A good piece of writing has a single, sharp, overriding purpose. Every part of the writing, even the digressions, should serve that purpose. Put another way, clarity of the general purpose is an absolute requirement in a good piece of writing. This observation matters because it's often tempting to let your purpose expand and become vague. Writing a piece about gardens? Hey, why not include that important related thought you had about rainforests? Now you have a piece that's sort of about gardens and sort of about rainforests, and not really about anything. The reader can no longer bond to it. A complicating factor is that sometimes you need to explore beyond the boundaries of your current purpose. You're writing for purpose A, but your instinct says that you need to explore subject B. Unfortunately, you're not yet sure how subject B fits in. If that's the case then you must take time to explore, and to understand how, if at all, subject B fits in, and whether you need to revise your purpose. This is emotionally difficult. It creates uncertainty, and you may feel as though your work on subject B is wasted effort. These doubts must be resisted. Avoid using clich\u00e9s \u2691 Clich\u00e9s prevent readers from visualization , making them an obstacle to creating memorable writing. Citing the sources \u2691 If it's a small phrase or a refactor, link the source inside the phrase or at the header of the section. If it's a big refactor, add it to a references section. If it's a big block without editing use admonition quotes Take all the guidelines as suggestions \u2691 All the sections above are guidelines, not rules to follow blindly, I try to adhere to them as much as possible, but if I feel it doesn't apply I ignore them. Unconnected thoughts \u2691 Replace adjectives with data. Nearly all of -> 84% of . Remove weasel words . Most adverbs are superfluous. When you say \"generally\" or \"usually\" you're probably undermining your point and the use of \"very\" or \"extremely\" are hyperbolic and breathless and make it easier to regard what you're writing as not serious. Examine every word: a surprising number don't serve any purpose. While wrapping your content into a story you may find yourself talking about your achievements more than giving actionable advice. If that happens, try to get to the bottom of how you achieved these achievements and break this process down, then focus on the process more than on your personal achievement. Set up a system that prompts people to review the material. Don't be egocentric, limit the use of I , use the implied subject instead: It's where I go to -> It's the place to go. I take different actions -> Taking different actions . Don't be possessive, use the instead of my . If you don't know how to express something use services like deepl . Use synonyms instead of repeating the same word over and over. Think who are you writing to. Use active voice : Active voice ensures that the actors are identified and it generally leaves less open questions. The exception is if you want to emphasize the object of the sentence. How to end a letter \u2691 How you end a letter is important. It\u2019s your last chance to leave the reader with positive feelings about you and the letter you have written. To make the matter more difficult, each different closing phrase has subtle connotations attached to them that you need to know to use them well. Most formal letter closing options are reserved, but note that there are degrees of warmth and familiarity among the options. Your relationship with the person to whom you\u2019re writing will shape which closing you choose: If you don\u2019t know the individual to whom you\u2019re writing, stick with a professional formal closing. If you\u2019re writing to a colleague, business connection, or someone else you know well, it\u2019s fine to close your letter less formally. Above all, your closing should be appropriate. Ideally, your message will resonate instead of your word choice. TL;DR: You can select from: Simplest, most useful : Sincerely Regards Yours truly Yours sincerely Slightly more personal : Best regards Cordially Yours respectfully More personal : Only use when appropriate to the letter's content. Warm regards Best wishes With appreciation Letter closings to avoid : Always Cheers Love Take Care XOXO The following are letter closings that are appropriate for business and employment related letters. Sincerely, Regards, Yours truly, and Yours sincerely : These are the simplest and most useful letter closings to use in a formal business setting. These are appropriate in almost all instances and are excellent ways to close a cover letter or an inquiry. Best regards, Cordially, and Yours respectfully : These letter closings fill the need for something slightly more personal. They are appropriate once you have some knowledge of the person to whom you are writing. You may have corresponded via email a few times, had a face-to-face or phone interview, or met at a networking event. Warm regards, Best wishes, and With appreciation : These letter closings are also appropriate once you have some knowledge or connection to the person to whom you are writing. Because they can relate back to the content of the letter, they can give closure to the point of the letter. Only use these if they make sense with the content of your letter. Letter closings to avoid \u2691 There are certain closings that you want to avoid in any business letter. Most of these are simply too informal. Some examples of closings to avoid are listed below: Always, Cheers, Love, Take care, XOXO, Talk soon, See ya, Hugs Some closings (such as \u201cLove\u201d and \u201cXOXO\u201d) imply a level of closeness that is not appropriate for a business letter. Rule of thumb : if you would use the closing in a note to a close friend, it\u2019s probably not suitable for business correspondence. Punctuating Farewell Phrases \u2691 When writing your sign-off, it's important to remember to use proper capitalization and punctuation. Only the first word should be capitalized (e.g., Yours truly), and the sign-off should be followed by a comma (or an exclamation mark in some informal settings), not a period. Postscripts \u2691 A P.S. (or postscript) comes after your sign-off and name. It is meant to include material that is supplementary, subordinated, or not vital to your letter. It is best to avoid postscripts in formal writing, as the information may go unnoticed or ignored; in those cases, try to include all information in the body text of the letter. n casual and personal correspondences, a postscript is generally acceptable. However, try to limit it to include only humorous or unnecessary material. Letter closings in detail \u2691 Sincerely \u2691 Sincerely (or sincerely yours) is often the go-to sign off for formal letters, and with good reason. This ending restates the sincerity of your letter's intent; it is a safe choice if you are not overly familiar with the letter's recipient, as it's preferable to use a sign-off that is both common and formal in such a situation. Best \u2691 Ending your letter with best, all the best, all best, or best wishes indicates that you hope the recipient experiences only good things in the future. Although it is not quite as formal as sincerely, it is still acceptable as a polite, formal/semi-formal letter ending, proper for business contacts as well as friends. Best regards \u2691 Quite like the previous sign-off, best regards expresses that you are thinking of the recipient with the best of feelings and intentions. Despite its similarity to best, this sign-off is a little more formal, meant for business letters and unfamiliar contacts. A semi-formal variation is warm regards, and an even more formal variation is simply regards. Speak to you soon \u2691 Variations to this farewell phrase include see you soon, talk to you later, and looking forward to speaking with you soon. These sign-offs indicate that you are expecting to continue the conversation with your contact. It can be an effective ending to a letter or email when confirming or planning a specific date for a face-to-face meeting. Although these endings can be used in either formal or casual settings, they typically carry a more formal tone. The exception here is talk to you later, which errs on the more casual side. Thanks \u2691 This is an effective ending to a letter when you are sincerely expressing gratitude. If you are using it as your standard letter ending, however, it can fall flat; the reader will be confused if there is no reason for you to be thanking them. Try to use thanks (or variations such as thanks so much, thank you, or thanks!) and its variations only when you think you haven't expressed your gratitude enough; otherwise, it can come across as excessive. Furthermore, when you're issuing an order, thanks might not be the best sign-off because it can seem presumptuous to offer thanks before the task has even been accepted or begun. [No sign-off] \u2691 Having no sign-off for your letter is a little unusual, but it is acceptable in some cases. Omitting the sign-off is most appropriately used in cases where you are replying to an email chain. However, in a first email, including neither a sign-off nor your name will make your letter seem to end abruptly. It should be avoided in those situations or when you are not very familiar with the receiver. Yours truly \u2691 This is where the line between formal and informal begins to blur. Yours truly implies the integrity of the message that precedes your name, but it also implies that you are related to the recipient in some way. This ending can be used in various situations, when writing letters to people both familiar and unfamiliar to you; however, yours truly carries a more casual and familiar tone, making it most appropriate for your friends and family. It's best used when you want to emphasize that you mean the contents of your letter. Take care \u2691 Take care is also a semi-formal way to end your letter. Like the sign-off all the best, this ending wishes that no harm come to the reader; however, like ending your letter with yours truly, the word choice is less formal and implies that the writer is at least somewhat familiar with the reader. Cheers \u2691 Cheers is a lighthearted ending that expresses your best wishes for the reader. Due to its association with drinking alcohol, it's best to save this sign-off for cases where you are familiar with the reader and when the tone is optimistic and casual. Also note that because cheers is associated with British English, it may seem odd to readers who speak other styles of English and are not very familiar with the term. Style issues \u2691 Avoid there is at the start of the sentence \u2691 Almost never begin a sentence with \u201cIt is...\u201d or \u201cThere is/are...\u201d. These are examples of unnecessary verbiage that shift the focus from the sentence point. Writing style books \u2691 After you start writing every day professionally, you will see that you will face some hard problems that will haunt you every time you sit down to write. The simplest way to overcome these issues and adopt a philosophy of writing that will make you a more professional, resilient, and wiser writer is to read the books about writing that masters of the craft have published. After reviewing 1 , 2 and 3 I've come to the following list of books I'd like to read. The elements of style \u2691 A classic book on grammar, style, and punctuation. If you feel like you need to improve any of those three aspects of your writing, then this book is a great start. With only 85 pages it covers both the grammar basics, rules that affect the style composition, writing toolbox description, and styling recommendations. On writing well \u2691 They say this book is specially useful to find one's style, develop it, polish it and learn how to write with it. The author doesn't get too philosophical or cutesy in his concepts, neither he gets too technical. In a way, it provides the right balance between The Elements of Style and Bird by Bird . Reading the book feels like you\u2019re being mentored by a wise, highly experienced writer. Bird by bird \u2691 Supposedly the most touching, poetic, and psychological book of the collection. The first part of the book lays around the life of Anne Lamott, a relatively popular fiction writer, who happens to have had a quite interesting life. Just like On Writing (the first book mentioned in here), the author manages to share enough of her life to enlighten the story and thesis of the book. The author explains what it takes to be a writer, what it means to be one, and how you can develop a narrative for a fiction book or story. It looks like it's a real pleasure to read it at the same time as it\u2019s still a wonderful experience that will help you understand how you can overcome your own fears, doubts, and pains of writing. Whether you want to write fiction or nonfiction, Bird by Bird provides a beautiful reading experience that will teach you what it takes to be a writer and how to find your demons. On writing \u2691 It's a book written by Stephen King that even though I haven't read any of his books I know he is known for being a specialist in capturing the reader. I don't know if it's going to be too much oriented to writing novels, but it looks promising. I'll leave it there for now, but keep on reading on Ivan Kreimer's article for more suggestions. References \u2691 Ivan Kreimer's article", "title": "Writing style"}, {"location": "writing_style/#general-writing-principles", "text": "", "title": "General writing principles"}, {"location": "writing_style/#make-it-pleasant-to-the-reader", "text": "Writing is a medium of communication, so avoid introducing elements that push away the reader, such as: Spelling mistakes. Gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing. Ugly environment: Present your texts through a pleasant medium such as a mkdocs webpage. Write like you talk: Ask yourself, is this the way I'd say this if I were talking to a friend? . If it isn't, imagine what you would say, and use that instead. Format errors: If you're writing in markdown, make sure that the result has no display bugs. Write short articles: Even though I love Gwern site , I find it daunting most of times. Instead of a big post, I'd rather use multiple well connected articles.", "title": "Make it pleasant to the reader"}, {"location": "writing_style/#saying-more-with-less", "text": "Never use a long word where a short one will do. Replace words like really like with love or other more appropriate words that save space writing and are more meaningful. Don't use filler words like really .", "title": "Saying more with less"}, {"location": "writing_style/#be-aware-of-pacing", "text": "Be aware of pacing between words and sentences. The sentences ideally should flow into one another. Breaks in form of commas and full steps are important, as they allow for the reader to take a break and absorb the point that you tried to deliver. Try to use less tan 30 words per sentence. For example, change Due to the fact that to because .", "title": "Be aware of pacing"}, {"location": "writing_style/#one-purpose", "text": "A good piece of writing has a single, sharp, overriding purpose. Every part of the writing, even the digressions, should serve that purpose. Put another way, clarity of the general purpose is an absolute requirement in a good piece of writing. This observation matters because it's often tempting to let your purpose expand and become vague. Writing a piece about gardens? Hey, why not include that important related thought you had about rainforests? Now you have a piece that's sort of about gardens and sort of about rainforests, and not really about anything. The reader can no longer bond to it. A complicating factor is that sometimes you need to explore beyond the boundaries of your current purpose. You're writing for purpose A, but your instinct says that you need to explore subject B. Unfortunately, you're not yet sure how subject B fits in. If that's the case then you must take time to explore, and to understand how, if at all, subject B fits in, and whether you need to revise your purpose. This is emotionally difficult. It creates uncertainty, and you may feel as though your work on subject B is wasted effort. These doubts must be resisted.", "title": "One purpose"}, {"location": "writing_style/#avoid-using-cliches", "text": "Clich\u00e9s prevent readers from visualization , making them an obstacle to creating memorable writing.", "title": "Avoid using clich\u00e9s"}, {"location": "writing_style/#citing-the-sources", "text": "If it's a small phrase or a refactor, link the source inside the phrase or at the header of the section. If it's a big refactor, add it to a references section. If it's a big block without editing use admonition quotes", "title": "Citing the sources"}, {"location": "writing_style/#take-all-the-guidelines-as-suggestions", "text": "All the sections above are guidelines, not rules to follow blindly, I try to adhere to them as much as possible, but if I feel it doesn't apply I ignore them.", "title": "Take all the guidelines as suggestions"}, {"location": "writing_style/#unconnected-thoughts", "text": "Replace adjectives with data. Nearly all of -> 84% of . Remove weasel words . Most adverbs are superfluous. When you say \"generally\" or \"usually\" you're probably undermining your point and the use of \"very\" or \"extremely\" are hyperbolic and breathless and make it easier to regard what you're writing as not serious. Examine every word: a surprising number don't serve any purpose. While wrapping your content into a story you may find yourself talking about your achievements more than giving actionable advice. If that happens, try to get to the bottom of how you achieved these achievements and break this process down, then focus on the process more than on your personal achievement. Set up a system that prompts people to review the material. Don't be egocentric, limit the use of I , use the implied subject instead: It's where I go to -> It's the place to go. I take different actions -> Taking different actions . Don't be possessive, use the instead of my . If you don't know how to express something use services like deepl . Use synonyms instead of repeating the same word over and over. Think who are you writing to. Use active voice : Active voice ensures that the actors are identified and it generally leaves less open questions. The exception is if you want to emphasize the object of the sentence.", "title": "Unconnected thoughts"}, {"location": "writing_style/#how-to-end-a-letter", "text": "How you end a letter is important. It\u2019s your last chance to leave the reader with positive feelings about you and the letter you have written. To make the matter more difficult, each different closing phrase has subtle connotations attached to them that you need to know to use them well. Most formal letter closing options are reserved, but note that there are degrees of warmth and familiarity among the options. Your relationship with the person to whom you\u2019re writing will shape which closing you choose: If you don\u2019t know the individual to whom you\u2019re writing, stick with a professional formal closing. If you\u2019re writing to a colleague, business connection, or someone else you know well, it\u2019s fine to close your letter less formally. Above all, your closing should be appropriate. Ideally, your message will resonate instead of your word choice. TL;DR: You can select from: Simplest, most useful : Sincerely Regards Yours truly Yours sincerely Slightly more personal : Best regards Cordially Yours respectfully More personal : Only use when appropriate to the letter's content. Warm regards Best wishes With appreciation Letter closings to avoid : Always Cheers Love Take Care XOXO The following are letter closings that are appropriate for business and employment related letters. Sincerely, Regards, Yours truly, and Yours sincerely : These are the simplest and most useful letter closings to use in a formal business setting. These are appropriate in almost all instances and are excellent ways to close a cover letter or an inquiry. Best regards, Cordially, and Yours respectfully : These letter closings fill the need for something slightly more personal. They are appropriate once you have some knowledge of the person to whom you are writing. You may have corresponded via email a few times, had a face-to-face or phone interview, or met at a networking event. Warm regards, Best wishes, and With appreciation : These letter closings are also appropriate once you have some knowledge or connection to the person to whom you are writing. Because they can relate back to the content of the letter, they can give closure to the point of the letter. Only use these if they make sense with the content of your letter.", "title": "How to end a letter"}, {"location": "writing_style/#letter-closings-to-avoid", "text": "There are certain closings that you want to avoid in any business letter. Most of these are simply too informal. Some examples of closings to avoid are listed below: Always, Cheers, Love, Take care, XOXO, Talk soon, See ya, Hugs Some closings (such as \u201cLove\u201d and \u201cXOXO\u201d) imply a level of closeness that is not appropriate for a business letter. Rule of thumb : if you would use the closing in a note to a close friend, it\u2019s probably not suitable for business correspondence.", "title": "Letter closings to avoid"}, {"location": "writing_style/#punctuating-farewell-phrases", "text": "When writing your sign-off, it's important to remember to use proper capitalization and punctuation. Only the first word should be capitalized (e.g., Yours truly), and the sign-off should be followed by a comma (or an exclamation mark in some informal settings), not a period.", "title": "Punctuating Farewell Phrases"}, {"location": "writing_style/#postscripts", "text": "A P.S. (or postscript) comes after your sign-off and name. It is meant to include material that is supplementary, subordinated, or not vital to your letter. It is best to avoid postscripts in formal writing, as the information may go unnoticed or ignored; in those cases, try to include all information in the body text of the letter. n casual and personal correspondences, a postscript is generally acceptable. However, try to limit it to include only humorous or unnecessary material.", "title": "Postscripts"}, {"location": "writing_style/#letter-closings-in-detail", "text": "", "title": "Letter closings in detail"}, {"location": "writing_style/#sincerely", "text": "Sincerely (or sincerely yours) is often the go-to sign off for formal letters, and with good reason. This ending restates the sincerity of your letter's intent; it is a safe choice if you are not overly familiar with the letter's recipient, as it's preferable to use a sign-off that is both common and formal in such a situation.", "title": "Sincerely"}, {"location": "writing_style/#best", "text": "Ending your letter with best, all the best, all best, or best wishes indicates that you hope the recipient experiences only good things in the future. Although it is not quite as formal as sincerely, it is still acceptable as a polite, formal/semi-formal letter ending, proper for business contacts as well as friends.", "title": "Best"}, {"location": "writing_style/#best-regards", "text": "Quite like the previous sign-off, best regards expresses that you are thinking of the recipient with the best of feelings and intentions. Despite its similarity to best, this sign-off is a little more formal, meant for business letters and unfamiliar contacts. A semi-formal variation is warm regards, and an even more formal variation is simply regards.", "title": "Best regards"}, {"location": "writing_style/#speak-to-you-soon", "text": "Variations to this farewell phrase include see you soon, talk to you later, and looking forward to speaking with you soon. These sign-offs indicate that you are expecting to continue the conversation with your contact. It can be an effective ending to a letter or email when confirming or planning a specific date for a face-to-face meeting. Although these endings can be used in either formal or casual settings, they typically carry a more formal tone. The exception here is talk to you later, which errs on the more casual side.", "title": "Speak to you soon"}, {"location": "writing_style/#thanks", "text": "This is an effective ending to a letter when you are sincerely expressing gratitude. If you are using it as your standard letter ending, however, it can fall flat; the reader will be confused if there is no reason for you to be thanking them. Try to use thanks (or variations such as thanks so much, thank you, or thanks!) and its variations only when you think you haven't expressed your gratitude enough; otherwise, it can come across as excessive. Furthermore, when you're issuing an order, thanks might not be the best sign-off because it can seem presumptuous to offer thanks before the task has even been accepted or begun.", "title": "Thanks"}, {"location": "writing_style/#no-sign-off", "text": "Having no sign-off for your letter is a little unusual, but it is acceptable in some cases. Omitting the sign-off is most appropriately used in cases where you are replying to an email chain. However, in a first email, including neither a sign-off nor your name will make your letter seem to end abruptly. It should be avoided in those situations or when you are not very familiar with the receiver.", "title": "[No sign-off]"}, {"location": "writing_style/#yours-truly", "text": "This is where the line between formal and informal begins to blur. Yours truly implies the integrity of the message that precedes your name, but it also implies that you are related to the recipient in some way. This ending can be used in various situations, when writing letters to people both familiar and unfamiliar to you; however, yours truly carries a more casual and familiar tone, making it most appropriate for your friends and family. It's best used when you want to emphasize that you mean the contents of your letter.", "title": "Yours truly"}, {"location": "writing_style/#take-care", "text": "Take care is also a semi-formal way to end your letter. Like the sign-off all the best, this ending wishes that no harm come to the reader; however, like ending your letter with yours truly, the word choice is less formal and implies that the writer is at least somewhat familiar with the reader.", "title": "Take care"}, {"location": "writing_style/#cheers", "text": "Cheers is a lighthearted ending that expresses your best wishes for the reader. Due to its association with drinking alcohol, it's best to save this sign-off for cases where you are familiar with the reader and when the tone is optimistic and casual. Also note that because cheers is associated with British English, it may seem odd to readers who speak other styles of English and are not very familiar with the term.", "title": "Cheers"}, {"location": "writing_style/#style-issues", "text": "", "title": "Style issues"}, {"location": "writing_style/#avoid-there-is-at-the-start-of-the-sentence", "text": "Almost never begin a sentence with \u201cIt is...\u201d or \u201cThere is/are...\u201d. These are examples of unnecessary verbiage that shift the focus from the sentence point.", "title": "Avoid there is at the start of the sentence"}, {"location": "writing_style/#writing-style-books", "text": "After you start writing every day professionally, you will see that you will face some hard problems that will haunt you every time you sit down to write. The simplest way to overcome these issues and adopt a philosophy of writing that will make you a more professional, resilient, and wiser writer is to read the books about writing that masters of the craft have published. After reviewing 1 , 2 and 3 I've come to the following list of books I'd like to read.", "title": "Writing style books"}, {"location": "writing_style/#the-elements-of-style", "text": "A classic book on grammar, style, and punctuation. If you feel like you need to improve any of those three aspects of your writing, then this book is a great start. With only 85 pages it covers both the grammar basics, rules that affect the style composition, writing toolbox description, and styling recommendations.", "title": "The elements of style"}, {"location": "writing_style/#on-writing-well", "text": "They say this book is specially useful to find one's style, develop it, polish it and learn how to write with it. The author doesn't get too philosophical or cutesy in his concepts, neither he gets too technical. In a way, it provides the right balance between The Elements of Style and Bird by Bird . Reading the book feels like you\u2019re being mentored by a wise, highly experienced writer.", "title": "On writing well"}, {"location": "writing_style/#bird-by-bird", "text": "Supposedly the most touching, poetic, and psychological book of the collection. The first part of the book lays around the life of Anne Lamott, a relatively popular fiction writer, who happens to have had a quite interesting life. Just like On Writing (the first book mentioned in here), the author manages to share enough of her life to enlighten the story and thesis of the book. The author explains what it takes to be a writer, what it means to be one, and how you can develop a narrative for a fiction book or story. It looks like it's a real pleasure to read it at the same time as it\u2019s still a wonderful experience that will help you understand how you can overcome your own fears, doubts, and pains of writing. Whether you want to write fiction or nonfiction, Bird by Bird provides a beautiful reading experience that will teach you what it takes to be a writer and how to find your demons.", "title": "Bird by bird"}, {"location": "writing_style/#on-writing", "text": "It's a book written by Stephen King that even though I haven't read any of his books I know he is known for being a specialist in capturing the reader. I don't know if it's going to be too much oriented to writing novels, but it looks promising. I'll leave it there for now, but keep on reading on Ivan Kreimer's article for more suggestions.", "title": "On writing"}, {"location": "writing_style/#references", "text": "Ivan Kreimer's article", "title": "References"}, {"location": "yamlfix/", "text": "Yamlfix is a simple opinionated yaml formatter that keeps your comments. Install \u2691 pip install yamlfix Usage \u2691 Imagine we've got the following source code: book_library : - title : Why we sleep author : Matthew Walker - title : Harry Potter and the Methods of Rationality author : Eliezer Yudkowsky It has the following errors: There is no --- at the top. The indentation is all wrong. After running yamlfix the resulting source code will be: --- book_library : - title : Why we sleep author : Matthew Walker - title : Harry Potter and the Methods of Rationality author : Eliezer Yudkowsky yamlfix can be used both as command line tool and as a library. As a command line tool: $: yamlfix file.yaml As a library: from yamlfix import fix_files fix_files ([ 'file.py' ]) References \u2691 Git Docs", "title": "Yamlfix"}, {"location": "yamlfix/#install", "text": "pip install yamlfix", "title": "Install"}, {"location": "yamlfix/#usage", "text": "Imagine we've got the following source code: book_library : - title : Why we sleep author : Matthew Walker - title : Harry Potter and the Methods of Rationality author : Eliezer Yudkowsky It has the following errors: There is no --- at the top. The indentation is all wrong. After running yamlfix the resulting source code will be: --- book_library : - title : Why we sleep author : Matthew Walker - title : Harry Potter and the Methods of Rationality author : Eliezer Yudkowsky yamlfix can be used both as command line tool and as a library. As a command line tool: $: yamlfix file.yaml As a library: from yamlfix import fix_files fix_files ([ 'file.py' ])", "title": "Usage"}, {"location": "yamlfix/#references", "text": "Git Docs", "title": "References"}, {"location": "architecture/database_architecture/", "text": "Design a table to keep historical changes in database \u2691 The post suggests two ways of storing a history of changed data in a database table. This might be useful for example for undo functionality or to store the evolution of the attributes. Audit table : Record every single change in every field in an audit table. History table : Each time a change is recorded, the whole line is stored in a History table. Using an Audit table \u2691 The Audit table has the following schema. Column Name Data Type ID int Table varchar(50) Field varchar(50) RecordId int OldValue varchar(255) NewValue varchar(255) AddBy int AddDate date For example, there is a transaction looks like this: Id Description TransactionDate DeliveryDate Status 100 A short text 2019-09-15 2019-09-28 Shipping And now, another user with id 20 modifies the description to A not long text and DeliveryDate to 2019-10-01 . Id Description TransactionDate DeliveryDate Status 100 A not long text 2019-09-15 2019-10-01 Shipping The Audit table entries will look: Id Table Field RecordId OldValue NewValue AddBy AddDate 1 Transaction Description 100 A short text A not long text 20 2019-09-17 2 Transaction DeliveryDate 100 2019-09-28 2019-10-01 20 2019-09-17 And we'll update the original record in the Transaction table. Pros: It's easy to query for field changes. No redundant information is stored. Cons: Possible huge increase of records. Since every change in different fields is one record in the Audit table, it may grow drastically fast. In this case, table indexing plays a vital role for enhancing the querying performance. Suitable for tables with many fields where often only a few change. Using a history table \u2691 The History table has the same schema as the table we are saving the history from. Imagine a Transaction table with the following schema. Column Name Data Type ID int Description text TransactionDate date DeliveryDate date Status varchar(50) AddDate date When doing the same changes as the previous example, we'll introduce the old record into the History table, and update the record in the Transaction table. Pros: Simple query to get the complete history. Cons: Redundant information is stored. Suitable for: A lot of fields are changed in one time. Generating a change report with full record history is needed References \u2691 Decoupling database migrations from server startup", "title": "Database Architecture"}, {"location": "architecture/database_architecture/#design-a-table-to-keep-historical-changes-in-database", "text": "The post suggests two ways of storing a history of changed data in a database table. This might be useful for example for undo functionality or to store the evolution of the attributes. Audit table : Record every single change in every field in an audit table. History table : Each time a change is recorded, the whole line is stored in a History table.", "title": "Design a table to keep historical changes in database"}, {"location": "architecture/database_architecture/#using-an-audit-table", "text": "The Audit table has the following schema. Column Name Data Type ID int Table varchar(50) Field varchar(50) RecordId int OldValue varchar(255) NewValue varchar(255) AddBy int AddDate date For example, there is a transaction looks like this: Id Description TransactionDate DeliveryDate Status 100 A short text 2019-09-15 2019-09-28 Shipping And now, another user with id 20 modifies the description to A not long text and DeliveryDate to 2019-10-01 . Id Description TransactionDate DeliveryDate Status 100 A not long text 2019-09-15 2019-10-01 Shipping The Audit table entries will look: Id Table Field RecordId OldValue NewValue AddBy AddDate 1 Transaction Description 100 A short text A not long text 20 2019-09-17 2 Transaction DeliveryDate 100 2019-09-28 2019-10-01 20 2019-09-17 And we'll update the original record in the Transaction table. Pros: It's easy to query for field changes. No redundant information is stored. Cons: Possible huge increase of records. Since every change in different fields is one record in the Audit table, it may grow drastically fast. In this case, table indexing plays a vital role for enhancing the querying performance. Suitable for tables with many fields where often only a few change.", "title": "Using an Audit table"}, {"location": "architecture/database_architecture/#using-a-history-table", "text": "The History table has the same schema as the table we are saving the history from. Imagine a Transaction table with the following schema. Column Name Data Type ID int Description text TransactionDate date DeliveryDate date Status varchar(50) AddDate date When doing the same changes as the previous example, we'll introduce the old record into the History table, and update the record in the Transaction table. Pros: Simple query to get the complete history. Cons: Redundant information is stored. Suitable for: A lot of fields are changed in one time. Generating a change report with full record history is needed", "title": "Using a history table"}, {"location": "architecture/database_architecture/#references", "text": "Decoupling database migrations from server startup", "title": "References"}, {"location": "architecture/domain_driven_design/", "text": "Domain-driven Design (DDD) is the concept that the structure and language of your code (class names, class methods, class variables) should match the business domain. Domain-driven design is predicated on the following goals: Placing the project's primary focus on the core domain and domain logic. Basing complex designs on a model of the domain. Initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems. It aims to fix these common pitfalls: When asked to design a new system, most developers will start to build a database schema, with the object model treated as an afterthought. Instead, behaviour should come first and drive our storage requirements . Business logic comes spread throughout the layers of our application, making it hard to identify, understand and change. The feared big ball of mud . They are avoided through: Encapsulation an abstraction : understanding behavior encapsulation as identifying a task that needs to be done in our code and giving that task to an abstraction, a well defined object or function. Encapsulating behavior with abstractions is a powerful decoupling tool by hiding details and protecting the consistency of our data, making code more expressive, more testable and easier to maintain. Layering : When one function, module or object uses another, we say that one depends on the other creating a dependency graph. In the big ball of mud the dependencies are out of control, so changing one node becomes difficult because it has the potential to affect many other parts of the system. Layered architectures are one way of tackling this problem by dividing our code into discrete categories or roles, and introducing rules about which categories of code can call each other. By following the Dependency Inversion Principle (the D in SOLID ), we must ensure that our business code doesn't depend on technical details, instead, both should use abstractions. We don't want high-level modules ,which respond to business needs, to be slowed down because they are closely coupled to low-level infrastructure details, which are often harder to change. Similarly, it is important to be able to change the infrastructure details when we need to without needing to make changes to the business layer. Refactoring old code is expensive You may be tempted to migrate all your old code to this architecture once you fall in love with it. Truth being told, it's the best way to learn how to use it, but it's time expensive too! The last refactor I did required a change of 60% of the code. The upside is that I reduced the total lines of code a 25%. Domain modeling \u2691 Keeping in mind that Domain is the problem you are trying to solve, and Model A system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain. The domain model is the mental map that business owners have of their businesses. It's set in a context and it's defined through ubiquitous language . A language structured around the domain model and used by all team members to connect all the activities of the team with the software. To successfully build a domain model we need to: Explore the domain language : Have an initial conversation with the business expert and agree on a glossary and some rules for the first minimal version of the domain model. Wherever possible, ask for concrete examples to illustrate each rule. Testing the domain models : Translate each of the rules gathered in the exploration phase into tests. Keeping in mind: The name of our tests should describe the behaviour that we want to see from the system. The test level in the testing pyramid should be chosen following the high and low gear metaphor . Code the domain modeling object : Choose the objects that match the behavior you are testing keeping in mind: The names of the classes, methods, functions and variables should be taken from the business jargon. Domain modeling objects \u2691 Value object : Any domain object that is uniquely identified by the data it holds, so it has no conceptual identity. They should be treated as immutable. We can still have complex behaviour in value objects. In fact, it's common to support operations, for example, mathematical operators. dataclasses are great for value objects because: They follow the value equality property (two objects with the same attributes are treated as equal). Can be defined as immutable with the frozen=True decorator argument. They define the __hash__ magic method based on the attribute values. __hash__ is used by Python to control the behaviour of objects when you add them to sets or use them as dict keys. @dataclass ( frozen = True ) class Name : first_name : str surname : str assert Name ( 'Harry' , 'Percival' ) == Name ( 'Harry' , 'Percival' ) assert Name ( 'Harry' , 'Percival' ) != Name ( 'Bob' , 'Gregory' ) Entity : An object that is not defined by it's attributes, but rather by a thread of continuity and it's identity. Unlike values, they have identity equality . We can change their values, and they are still recognizably the same thing. class Person : def __init__ ( self , name : Name ): self . name = name def test_barry_is_harry (): harry = Person ( Name ( \"Harry\" , \"Percival\" )) barry = harry barry . name = Namew ( \"Barry\" , \"Percival\" ) assert harry is barry and barry is harry We usually make this explicit in code by implementing equality operators on entities: class Person : ... def __eq__ ( self , other ): if not isinstance ( other , Person ): return False return other . identifier == self . identifier def __hash__ ( self ): return hash ( self . identifier ) Python's __eq__ magic method defines the behavior of the class for the == operator. For entities, the simplest option is to say that the hash is None , meaning that the object is not hashable so it can't be used as dictionary keys. If for some reason you need that, the hash should be based on the attribute that identifies the object over the time. You should also try to somehow make that attribute read-only. Beware, editing __hash__ without modifying __eq__ is tricky business . Service : Functions that hold operations that don't conceptually belong to any object. We take advantage of the fact that Python is a multiparadigm language. Exceptions : Hold constrains imposed over the objects by the business. Domain modeling patterns \u2691 To build a rich robust object model that is decoupled from technical concerns we need to build persistence-ignorant code that uses stable APIs around our domain so we can refactor aggressively. This is achieved through these design patterns: Repository pattern : An abstraction over the idea of persistent storage. Service Layer pattern : Clearly define where our use case begins and ends. Unit of work pattern : Provides atomic operations. Aggregate pattern : Enforces integrity of our data. Unconnected thoughts \u2691 Domain model refactor \u2691 Refactoring an existing project into the domain driven design architecture is not a nice task, These are the steps I've followed: If the domain models are coupled with the ORM, build a basic repository that makes the ORM dependent on the model. For the first version, ignore the relations between the models, just implement the .add and .get methods to persist and read the models from the persistent storage solution. Create a FakeRepository with similar functionality to start building the Service Layer. Inspect the entrypoints of your program and for each orchestration action create a service (always tests first). Building blocks \u2691 Aggregate : A collection of objects that are bound together by a root entity, otherwise known as an aggregate root. The aggregate root guarantees the consistency of changes being made within the aggregate by forbidding external objects from holding references to it's members. Domain Event : A domain object that defines an event. Repository : Methods for retrieving domain objects should delegate to a specialized Repository object such that alternative storage implementations may be easily interchanged. Factory : Methods for creating domain objects should delegate to a specialized Factory object such that alternative implementations may be easily interchanged. Injection of fakes in edge to edge tests \u2691 If you are developing your program with this design pattern, you'll have fake versions of your adapters. When testing the edge to edge tests, you're going to use the fakes when there is no easy way to do a correct end to end test (if for example you need to bring up a service that is complex to configure). I've been banging my head against the keyboard until I've figured how to do it for click command line tests . References \u2691 Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory. Wikipedia article Further reading \u2691 awesome domain driven design Books \u2691 Domain-Driven Design by Eric Evans. Implementing Domain-Driven Design by Vaughn Vermon.", "title": "Domain Driven Design"}, {"location": "architecture/domain_driven_design/#domain-modeling", "text": "Keeping in mind that Domain is the problem you are trying to solve, and Model A system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain. The domain model is the mental map that business owners have of their businesses. It's set in a context and it's defined through ubiquitous language . A language structured around the domain model and used by all team members to connect all the activities of the team with the software. To successfully build a domain model we need to: Explore the domain language : Have an initial conversation with the business expert and agree on a glossary and some rules for the first minimal version of the domain model. Wherever possible, ask for concrete examples to illustrate each rule. Testing the domain models : Translate each of the rules gathered in the exploration phase into tests. Keeping in mind: The name of our tests should describe the behaviour that we want to see from the system. The test level in the testing pyramid should be chosen following the high and low gear metaphor . Code the domain modeling object : Choose the objects that match the behavior you are testing keeping in mind: The names of the classes, methods, functions and variables should be taken from the business jargon.", "title": "Domain modeling"}, {"location": "architecture/domain_driven_design/#domain-modeling-objects", "text": "Value object : Any domain object that is uniquely identified by the data it holds, so it has no conceptual identity. They should be treated as immutable. We can still have complex behaviour in value objects. In fact, it's common to support operations, for example, mathematical operators. dataclasses are great for value objects because: They follow the value equality property (two objects with the same attributes are treated as equal). Can be defined as immutable with the frozen=True decorator argument. They define the __hash__ magic method based on the attribute values. __hash__ is used by Python to control the behaviour of objects when you add them to sets or use them as dict keys. @dataclass ( frozen = True ) class Name : first_name : str surname : str assert Name ( 'Harry' , 'Percival' ) == Name ( 'Harry' , 'Percival' ) assert Name ( 'Harry' , 'Percival' ) != Name ( 'Bob' , 'Gregory' ) Entity : An object that is not defined by it's attributes, but rather by a thread of continuity and it's identity. Unlike values, they have identity equality . We can change their values, and they are still recognizably the same thing. class Person : def __init__ ( self , name : Name ): self . name = name def test_barry_is_harry (): harry = Person ( Name ( \"Harry\" , \"Percival\" )) barry = harry barry . name = Namew ( \"Barry\" , \"Percival\" ) assert harry is barry and barry is harry We usually make this explicit in code by implementing equality operators on entities: class Person : ... def __eq__ ( self , other ): if not isinstance ( other , Person ): return False return other . identifier == self . identifier def __hash__ ( self ): return hash ( self . identifier ) Python's __eq__ magic method defines the behavior of the class for the == operator. For entities, the simplest option is to say that the hash is None , meaning that the object is not hashable so it can't be used as dictionary keys. If for some reason you need that, the hash should be based on the attribute that identifies the object over the time. You should also try to somehow make that attribute read-only. Beware, editing __hash__ without modifying __eq__ is tricky business . Service : Functions that hold operations that don't conceptually belong to any object. We take advantage of the fact that Python is a multiparadigm language. Exceptions : Hold constrains imposed over the objects by the business.", "title": "Domain modeling objects"}, {"location": "architecture/domain_driven_design/#domain-modeling-patterns", "text": "To build a rich robust object model that is decoupled from technical concerns we need to build persistence-ignorant code that uses stable APIs around our domain so we can refactor aggressively. This is achieved through these design patterns: Repository pattern : An abstraction over the idea of persistent storage. Service Layer pattern : Clearly define where our use case begins and ends. Unit of work pattern : Provides atomic operations. Aggregate pattern : Enforces integrity of our data.", "title": "Domain modeling patterns"}, {"location": "architecture/domain_driven_design/#unconnected-thoughts", "text": "", "title": "Unconnected thoughts"}, {"location": "architecture/domain_driven_design/#domain-model-refactor", "text": "Refactoring an existing project into the domain driven design architecture is not a nice task, These are the steps I've followed: If the domain models are coupled with the ORM, build a basic repository that makes the ORM dependent on the model. For the first version, ignore the relations between the models, just implement the .add and .get methods to persist and read the models from the persistent storage solution. Create a FakeRepository with similar functionality to start building the Service Layer. Inspect the entrypoints of your program and for each orchestration action create a service (always tests first).", "title": "Domain model refactor"}, {"location": "architecture/domain_driven_design/#building-blocks", "text": "Aggregate : A collection of objects that are bound together by a root entity, otherwise known as an aggregate root. The aggregate root guarantees the consistency of changes being made within the aggregate by forbidding external objects from holding references to it's members. Domain Event : A domain object that defines an event. Repository : Methods for retrieving domain objects should delegate to a specialized Repository object such that alternative storage implementations may be easily interchanged. Factory : Methods for creating domain objects should delegate to a specialized Factory object such that alternative implementations may be easily interchanged.", "title": "Building blocks"}, {"location": "architecture/domain_driven_design/#injection-of-fakes-in-edge-to-edge-tests", "text": "If you are developing your program with this design pattern, you'll have fake versions of your adapters. When testing the edge to edge tests, you're going to use the fakes when there is no easy way to do a correct end to end test (if for example you need to bring up a service that is complex to configure). I've been banging my head against the keyboard until I've figured how to do it for click command line tests .", "title": "Injection of fakes in edge to edge tests"}, {"location": "architecture/domain_driven_design/#references", "text": "Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory. Wikipedia article", "title": "References"}, {"location": "architecture/domain_driven_design/#further-reading", "text": "awesome domain driven design", "title": "Further reading"}, {"location": "architecture/domain_driven_design/#books", "text": "Domain-Driven Design by Eric Evans. Implementing Domain-Driven Design by Vaughn Vermon.", "title": "Books"}, {"location": "architecture/microservices/", "text": "Microservices are an application architecture style where independent, self-contained programs with a single purpose each can communicate with each other over a network. Typically, these microservices are able to be deployed independently because they have strong separation of responsibilities via a well-defined specification with significant backwards compatibility to avoid sudden dependency breakage. References \u2691 Fullstackpython introduction to microservices Books \u2691 Hand-On Docker for Microservices with Python by Jaime Buelta : Does a good job defining the whole process of building a microservices python application, from the microservice concept to the definition of the CI, integration testing, deployment in Kubernetes, definition of logging and metrics. But it doesn't help much with the project layout definition or if you want to build your application while following it. Hands-On RESTful Python Web Services by Gaston C.Hillar : I didn't like it at all.", "title": "Microservices"}, {"location": "architecture/microservices/#references", "text": "Fullstackpython introduction to microservices", "title": "References"}, {"location": "architecture/microservices/#books", "text": "Hand-On Docker for Microservices with Python by Jaime Buelta : Does a good job defining the whole process of building a microservices python application, from the microservice concept to the definition of the CI, integration testing, deployment in Kubernetes, definition of logging and metrics. But it doesn't help much with the project layout definition or if you want to build your application while following it. Hands-On RESTful Python Web Services by Gaston C.Hillar : I didn't like it at all.", "title": "Books"}, {"location": "architecture/orm_builder_query_or_raw_sql/", "text": "Databases are the core of storing state for almost all web applications. There are three ways for a programming application to interact with the database. After reading this article, you'll know which are the advantages and disadvantages of using the different solutions. Raw SQL \u2691 Raw SQL, sometimes also called native SQL, is the most basic, most low-level form of database interaction. You tell the database what to do in the language of the database. Most developers should know basics of SQL. This means how to CREATE tables and views, how to SELECT and JOIN data, how to UPDATE and DELETE data. Excels: Flexibility : As you are writing raw SQL code, you are not constrained by higher level abstractions. Performance : You can use engine specific tricks to increase the performance and your queries will probably be simpler than the higher abstraction ones. Magic free : It's easier to understand what your code does, as you scale up in the abstraction level, magic starts to appear which is nice if everything goes well, but it backfires when you encounter problems. No logic coupling : As your models are not linked to the way you interact with the storage solution, it's easier to define a clean software architecture that follows the SOLID principles, which also allows to switch between different storage approaches. Cons: SQL Injections : As you are manually writing the queries, it's easier to fall into these vulnerabilities. Change management : Databases change over time. With raw SQL, you typically don't get any support for that. You have to migrate the schema and all queries yourself. Query Extension : If you have an analytical query, it's nice if you can apply slight modifications to it. It\u2019s possible to extend a query when you have raw SQL, but it\u2019s cumbersome. You need to touch the original query and add placeholders. Editor support : As it's interpreted as a string in the editor, your editor is not able to detect typos, syntax highlight or auto complete the SQL code. SQL knowledge : You need to know SQL to interact with the database. Database Locking : You might use features which are specific to that database, which makes a future database switch harder. Query builder \u2691 Query builders are libraries which are written in the programming language you use and use native classes and functions to build SQL queries. Query builders typically have a fluent interface , so the queries are built by an object-oriented interface which uses method chaining. query = Query . from_ ( books ) \\ . select ( \"*\" ) \\ . where ( books . author_id == aid ) Pypika is an example for a Query Builder in Python. Note that the resulting query is still the same as in the raw code, built in another way, so abstraction level over using raw SQL is small. Excels: Performance : Same performance as using raw SQL. Magic free : Same comprehension as using raw SQL. No logic coupling : Same coupling as using raw SQL. Query Extension : Given the fluent interface, it's easier to build, extend and reuse queries. Mitigates: Flexibility : You depend on the builder implementation of the language you are trying to use, but if the functionality you are trying to use is not there, you can always fall back to raw SQL. SQL Injections : Query builders have mechanism to insert parameters into the queries in a safe way. Editor support : The query builder prevents typos in the offered parts \u2014 .select , .from_ , .where , and as it's object oriented you have better syntax highlight and auto completion. Database Locking : Query builders support different databases make database switch easier. Cons: Change management : Databases change over time. With raw SQL, you typically don't get any support for that. You have to migrate the schema and all queries yourself. SQL knowledge : You need to know SQL to interact with the database. Query builder knowledge : You need to know the library to interact with the database. ORM \u2691 ORMs create an object for each database table and allows you to interact between related objects, in a way that you can use your object oriented programming to interact with the database even without knowing SQL. SQLAlchemy is an example for an ORM in Python. This way, there is a language-native representation and thus the languages ecosystem features such as autocomplete and syntax-highlighting work. Excels: Change management : ORM come with helper programs like Alembic which can automatically detect when your models changed compared to the last known state of the database, thus it's able to create schema migration files for you. Query Extension : They have a fluent interface used and developed by a lot of people, so it may have better support than query builders. SQL Injections : As the ORM builds the queries by itself and it maintained by a large community, you're less prone to suffer from this vulnerabilities. Editor support : As you are interacting with Python objects, you have full editor support for highlighting and auto-formatting, which reduces the maintenance by making the queries easier to read Database Locking : ORM fully support different databases, so it's easy to switch between different database solutions. Mitigates: SQL knowledge : In theory you don't need to know SQL, in reality, you need to have some basic knowledge to build the tables and relationships, as well as while debugging. Cons: Flexibility : Being the highest level of abstraction, you are constrained by what the ORM solution offers, allowing you to write raw SQL and try to give enough features, so you don't notice it unless you're writing complex queries. Performance : When you run queries with ORMs, you tend to get more than you need. This is translated in fetching more information and executing more queries than the other solutions. You can try to tweak it but it can be tricky, making it easy to create queries which are wrong in a subtle way. They also encounter the N+1 problem, where you potentially run more queries than you need to fetch the same result. It's all magic : ORMs are complex high level abstractions, so when you encounter errors or want to change the default behaviour, you're going to have a bad time. Big coupling : ORM models already contain all the data you need, so you will be tempted to use it outside of database related code, which introduces a tight coupling between your business model and the storage solution, which decreases flexibility when changing storage drivers, makes testing harder, leads to software architectures that induce the big ball of mud by getting further from the SOLID principles. SQLAlchemy still supports the use of classical mappings between object models and ORM definitions, but I've read that it's a feature that it's not going to be maintained, as it's not the intended way of using it. Even with them I've had issues when trying to integrate the models with pydantic( 1 , 2 . Learn the ORM : ORMs are complex to learn, they have lots of features and different ways to achieve the same result, so it's hard to learn how to use them well, and usually there is no way to fulfill all your needs. Configure the ORM : I've had a hard time understanding the correct way to configure database connection inside a packaged python program, both for the normal use and to define the test environment. I've first learned using the declarative way, and then I had to learn all over again for the classical mapping required by the use of the repository pattern . Conclusion \u2691 Query Builders live in the sweet spot in the abstraction plane. They give enough abstraction to ease the interaction with the database and mitigating security vulnerabilities while retaining the flexibility, performance and architecture cleanness of using raw SQL. Although they require you to learn SQL and the query builder library, it will pay off as you develop your programs. In the end, if you don't expect to use this knowledge in the future, you may better use pandas to your small project than a SQL solution. Raw SQL should be used when: You don't mind spending some time learning SQL. You plan to develop and maintain complex or different projects that use SQL to store data. Query builders should be used when: You don't want to learn SQL and need to create a small script that needs to perform a specific task. ORMs should be used when: Small projects where the developers are already familiar with the ORM. Maintaining existing ORM code, although migrating to query builders should be evaluated. If you do use an ORM, use the repository pattern through classical mappings to split the storage logic from the business one. References \u2691 Raw SQL vs Query Builder vs ORM by Martin Thoma ORMs vs Plain SQL in Python by Koby Bass", "title": "ORM, Query Builder or Raw SQL"}, {"location": "architecture/orm_builder_query_or_raw_sql/#raw-sql", "text": "Raw SQL, sometimes also called native SQL, is the most basic, most low-level form of database interaction. You tell the database what to do in the language of the database. Most developers should know basics of SQL. This means how to CREATE tables and views, how to SELECT and JOIN data, how to UPDATE and DELETE data. Excels: Flexibility : As you are writing raw SQL code, you are not constrained by higher level abstractions. Performance : You can use engine specific tricks to increase the performance and your queries will probably be simpler than the higher abstraction ones. Magic free : It's easier to understand what your code does, as you scale up in the abstraction level, magic starts to appear which is nice if everything goes well, but it backfires when you encounter problems. No logic coupling : As your models are not linked to the way you interact with the storage solution, it's easier to define a clean software architecture that follows the SOLID principles, which also allows to switch between different storage approaches. Cons: SQL Injections : As you are manually writing the queries, it's easier to fall into these vulnerabilities. Change management : Databases change over time. With raw SQL, you typically don't get any support for that. You have to migrate the schema and all queries yourself. Query Extension : If you have an analytical query, it's nice if you can apply slight modifications to it. It\u2019s possible to extend a query when you have raw SQL, but it\u2019s cumbersome. You need to touch the original query and add placeholders. Editor support : As it's interpreted as a string in the editor, your editor is not able to detect typos, syntax highlight or auto complete the SQL code. SQL knowledge : You need to know SQL to interact with the database. Database Locking : You might use features which are specific to that database, which makes a future database switch harder.", "title": "Raw SQL"}, {"location": "architecture/orm_builder_query_or_raw_sql/#query-builder", "text": "Query builders are libraries which are written in the programming language you use and use native classes and functions to build SQL queries. Query builders typically have a fluent interface , so the queries are built by an object-oriented interface which uses method chaining. query = Query . from_ ( books ) \\ . select ( \"*\" ) \\ . where ( books . author_id == aid ) Pypika is an example for a Query Builder in Python. Note that the resulting query is still the same as in the raw code, built in another way, so abstraction level over using raw SQL is small. Excels: Performance : Same performance as using raw SQL. Magic free : Same comprehension as using raw SQL. No logic coupling : Same coupling as using raw SQL. Query Extension : Given the fluent interface, it's easier to build, extend and reuse queries. Mitigates: Flexibility : You depend on the builder implementation of the language you are trying to use, but if the functionality you are trying to use is not there, you can always fall back to raw SQL. SQL Injections : Query builders have mechanism to insert parameters into the queries in a safe way. Editor support : The query builder prevents typos in the offered parts \u2014 .select , .from_ , .where , and as it's object oriented you have better syntax highlight and auto completion. Database Locking : Query builders support different databases make database switch easier. Cons: Change management : Databases change over time. With raw SQL, you typically don't get any support for that. You have to migrate the schema and all queries yourself. SQL knowledge : You need to know SQL to interact with the database. Query builder knowledge : You need to know the library to interact with the database.", "title": "Query builder"}, {"location": "architecture/orm_builder_query_or_raw_sql/#orm", "text": "ORMs create an object for each database table and allows you to interact between related objects, in a way that you can use your object oriented programming to interact with the database even without knowing SQL. SQLAlchemy is an example for an ORM in Python. This way, there is a language-native representation and thus the languages ecosystem features such as autocomplete and syntax-highlighting work. Excels: Change management : ORM come with helper programs like Alembic which can automatically detect when your models changed compared to the last known state of the database, thus it's able to create schema migration files for you. Query Extension : They have a fluent interface used and developed by a lot of people, so it may have better support than query builders. SQL Injections : As the ORM builds the queries by itself and it maintained by a large community, you're less prone to suffer from this vulnerabilities. Editor support : As you are interacting with Python objects, you have full editor support for highlighting and auto-formatting, which reduces the maintenance by making the queries easier to read Database Locking : ORM fully support different databases, so it's easy to switch between different database solutions. Mitigates: SQL knowledge : In theory you don't need to know SQL, in reality, you need to have some basic knowledge to build the tables and relationships, as well as while debugging. Cons: Flexibility : Being the highest level of abstraction, you are constrained by what the ORM solution offers, allowing you to write raw SQL and try to give enough features, so you don't notice it unless you're writing complex queries. Performance : When you run queries with ORMs, you tend to get more than you need. This is translated in fetching more information and executing more queries than the other solutions. You can try to tweak it but it can be tricky, making it easy to create queries which are wrong in a subtle way. They also encounter the N+1 problem, where you potentially run more queries than you need to fetch the same result. It's all magic : ORMs are complex high level abstractions, so when you encounter errors or want to change the default behaviour, you're going to have a bad time. Big coupling : ORM models already contain all the data you need, so you will be tempted to use it outside of database related code, which introduces a tight coupling between your business model and the storage solution, which decreases flexibility when changing storage drivers, makes testing harder, leads to software architectures that induce the big ball of mud by getting further from the SOLID principles. SQLAlchemy still supports the use of classical mappings between object models and ORM definitions, but I've read that it's a feature that it's not going to be maintained, as it's not the intended way of using it. Even with them I've had issues when trying to integrate the models with pydantic( 1 , 2 . Learn the ORM : ORMs are complex to learn, they have lots of features and different ways to achieve the same result, so it's hard to learn how to use them well, and usually there is no way to fulfill all your needs. Configure the ORM : I've had a hard time understanding the correct way to configure database connection inside a packaged python program, both for the normal use and to define the test environment. I've first learned using the declarative way, and then I had to learn all over again for the classical mapping required by the use of the repository pattern .", "title": "ORM"}, {"location": "architecture/orm_builder_query_or_raw_sql/#conclusion", "text": "Query Builders live in the sweet spot in the abstraction plane. They give enough abstraction to ease the interaction with the database and mitigating security vulnerabilities while retaining the flexibility, performance and architecture cleanness of using raw SQL. Although they require you to learn SQL and the query builder library, it will pay off as you develop your programs. In the end, if you don't expect to use this knowledge in the future, you may better use pandas to your small project than a SQL solution. Raw SQL should be used when: You don't mind spending some time learning SQL. You plan to develop and maintain complex or different projects that use SQL to store data. Query builders should be used when: You don't want to learn SQL and need to create a small script that needs to perform a specific task. ORMs should be used when: Small projects where the developers are already familiar with the ORM. Maintaining existing ORM code, although migrating to query builders should be evaluated. If you do use an ORM, use the repository pattern through classical mappings to split the storage logic from the business one.", "title": "Conclusion"}, {"location": "architecture/orm_builder_query_or_raw_sql/#references", "text": "Raw SQL vs Query Builder vs ORM by Martin Thoma ORMs vs Plain SQL in Python by Koby Bass", "title": "References"}, {"location": "architecture/redis/", "text": "Redis is an in-memory data structure project implementing a distributed, in-memory key-value database with optional durability. Redis supports different kinds of abstract data structures, such as strings, lists, maps, sets, sorted sets, HyperLogLogs, bitmaps, streams, and spatial indexes. Redis has a client-server architecture and uses a request-response model. This means that you (the client) connect to a Redis server through TCP connection, on port 6379 by default. You request some action (like some form of reading, writing, getting, setting, or updating), and the server serves you back a response. There can be many clients talking to the same server, which is really what Redis or any client-server application is all about. Each client does a (typically blocking) read on a socket waiting for the server response. Redis as a Python dictionary \u2691 Redis stands for Remote Dictionary Service. Broadly speaking, there are many parallels you can draw between a Python dictionary (or generic hash table) and what Redis is and does: A Redis database holds key:value pairs and supports commands such as GET, SET, and DEL, as well as several hundred additional commands. Redis keys are always strings. Redis values may be a number of different data types: string, list, hashes, sets and some advanced types like geospatial items and the new stream type. Many Redis commands operate in constant O(1) time, just like retrieving a value from a Python dict or any hash table. Client libraries \u2691 There are several ways to interact with a Redis server, such as: Redis-py . redis-cli. Reference \u2691 Real Python Redis introduction", "title": "Redis"}, {"location": "architecture/redis/#redis-as-a-python-dictionary", "text": "Redis stands for Remote Dictionary Service. Broadly speaking, there are many parallels you can draw between a Python dictionary (or generic hash table) and what Redis is and does: A Redis database holds key:value pairs and supports commands such as GET, SET, and DEL, as well as several hundred additional commands. Redis keys are always strings. Redis values may be a number of different data types: string, list, hashes, sets and some advanced types like geospatial items and the new stream type. Many Redis commands operate in constant O(1) time, just like retrieving a value from a Python dict or any hash table.", "title": "Redis as a Python dictionary"}, {"location": "architecture/redis/#client-libraries", "text": "There are several ways to interact with a Redis server, such as: Redis-py . redis-cli.", "title": "Client libraries"}, {"location": "architecture/redis/#reference", "text": "Real Python Redis introduction", "title": "Reference"}, {"location": "architecture/repository_pattern/", "text": "The repository pattern is an abstraction over persistent storage, allowing us to decouple our model layer from the data layer. It hides the boring details of data access by pretending that all of our data is in memory. TL;DR If your app is a basic CRUD (create-read-update-delete) wrapper around a database, then you don't need a domain model or a repository. But the more complex the domain, the more an investment in freeing yourself from infrastructure concerns will pay off in terms of the ease of making changes. Advantages: We get a simple interface, which we control, between persistent storage and our domain model. It's easy to make a fake version of the repository for unit testing, or to swap out different storage solutions, because we've fully decoupled the model from infrastructure concerns. Writing the domain model before thinking about persistence helps us focus on the business problem at hand. If we need to change our approach, we can do that in our model, without needing to worry about foreign keys or migrations until later. Our database schema is simple because we have complete control over how we map our object to tables. Speeds up and makes more clean the business logic tests. It's easy to implement. Disadvantages: An ORM already buys you some decoupling. Changing foreign keys might be hard, but it should be pretty easy to swap between MySQL and Postres if you ever need to. Maintaining ORM mappings by hand requires extra work and extra code. An extra layer of abstraction is introduced, and although we may hope it will reduce complexity overall, it does add complexity locally. Furthermore it adds the WTF factor for Python programmers who've never seen this pattern before. [Intermediate optional step] Making the ORM depend on the Domain model Applying the DIP to the data access we aim to have no dependencies between architectural layers. We don't want infrastructure concerns bleeding over into our domain model and slowing down our unit tests or our ability to make changes. So we'll have an onion architecture . If you follow the typical SQLAlchemy tutorial, you'll end up with a \"declarative\" syntax where the model tightly depends on the ORM. The alternative is to make the ORM import the domain model, defining our database tables and columns by using SQLAlchemy's abstractions and magically binding them together with a mapper function. from SQLAlchemy.orm import mapper , relationship import model metadata = MetaData () task = Table ( 'task' , metadata , Colum ( 'id' , Integer , primary_key = True , autoincrement = True ), Column ( 'description' , String ( 255 )), Column ( 'priority' , Integer , nullable = False ), ) def start_mappers (): task_mapper = mapper ( model . Task , task ) The end result is be that, if we call start_mappers , we will be able to easily load and save domain model instances from and to the database. But if we never call that function, our domain model classes stay blissfully unaware of the database. When you're first trying to build your ORM config, it can be useful to write tests for it, though we probably won't keep them around for long once we've got the repository abstraction. def test_task_mapper_can_load_tasks ( session ): session . execute ( 'INSERT INTO task (description, priority) VALUES' '(\"First task\", 3),' '(\"Urgent task\", 5),' ) expected = [ model . Task ( \"First task\" , 3 ), model . Task ( \"Urgent task\" , 5 ), ] assert session . query ( model . Task ) . all () == expected def test_task_mapper_can_save_lines ( session ): new_task = model . Task ( \"First task\" , 3 ) session . add ( new_task ) session . commit () rows = list ( session . execute ( 'SELECT description, priority FROM \"task\"' )) assert rows == [( \"First task\" , 3 )] The most basic repository has just two methods: add() to put a new item in the repository, and get() to return a previously added item. We stick to using these methods for data access in our domain and our service layers. import abc import model class AbstractRepository ( abc . ABC ): @abc . abstractmethod def add ( self , task : model . Task ): raise NotImplementedError @abc . abstractmethod def get ( self , reference ) -> model . Task : raise NotImplementedError The @abc.abstractmethod is one of the only things that makes ABCs actually \"work\" in Python. Python will refuse to let you instantiate a class that does not implement all the abstractmethods defined in its parent class. As always, we start with a test. This would probably be classified as an integration test, since we're checking that our code (the repository) is correctly integrated with the database; hence, the tests tend to mix raw SQL with calls and assertions on our own code. # Test .add() def test_repository_can_save_a_task ( session ): task = model . Task ( \"First task\" , 3 ) repo = repository . SqlAlchemyRepository ( session ) repo . add ( task ) session . commit () rows = list ( session . execute ( 'SELECT description, priority FROM \"tasks\"' )) assert rows == [( \"First task\" , 3 )] # Test .get() def insert_task ( session ): session . execute ( 'INSERT INTO tasks (description, priority)' 'VALUES (\"First task\", 3)' ) [[ task_id ]] = session . execute ( 'SELECT id FROM tasks WHERE id=:id' , dict ( id = '1' ), ) return task_id def test_repository_can_retrieve_a_task ( session ): task_id = insert_task () repo = repository . SqlAlchemyRepository ( session ) retrieved = repo . get ( task_id ) expected = model . Task ( '1' , 'First task' , 3 ) assert retrieved == expected # Task.__eq__ only compares reference assert retrieved . description == expected . description assert retrieved . priority == expected . priority Note that we leave the .commit() outside of the repository and make it the responsibility of the caller. Whether or not you write tests for every model is a judgment call. Once you have one class tested for create/modify/save, you might be happy to go on and do the others with a minimal round-trip test, or even nothing at all, if they all follow a similar pattern. SqlAlchemyRepository is the repository that matches those tests. class SqlAlchemyRepository ( AbstractRepository ): def __init__ ( self , session ): self . session = session def add ( self , task : model . Task ): self . session . add ( task ) def get ( self , id : str ) -> model . Task : return self . session . query ( model . Task ) . get ( id ) def list ( self ) -> List ( model . Task ): return self . session . query ( model . Task ) . all () Building a fake repository for tests is now trivial. class FakeRepository ( AbstractRepository ): def __init__ ( self , tasks : List ( model . Task )): self . _tasks = set ( tasks ) def add ( self , task : model . Task ): self . tasks . add ( task ) def get ( self , id : str ) -> model . Task : return next ( task for task in self . _tasks if task . id == id ) def list ( self ) -> List ( model . Task ): return list ( self . _tasks ) Warnings \u2691 Don't include the properties the ORM introduces into the model of the entities, otherwise you're going to have a bad debugging time. If we use the ORM to back populate the children attribute in the model of Task , don't add the attribute in the __init__ method arguments, but initialize it inside the method: class Task : def __init__ ( self , id : str , description : str ) -> None : self . id = id self . description = description self . children Optional [ List [ 'Task' ]] = None References \u2691 The repository pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.", "title": "Repository Pattern"}, {"location": "architecture/repository_pattern/#warnings", "text": "Don't include the properties the ORM introduces into the model of the entities, otherwise you're going to have a bad debugging time. If we use the ORM to back populate the children attribute in the model of Task , don't add the attribute in the __init__ method arguments, but initialize it inside the method: class Task : def __init__ ( self , id : str , description : str ) -> None : self . id = id self . description = description self . children Optional [ List [ 'Task' ]] = None", "title": "Warnings"}, {"location": "architecture/repository_pattern/#references", "text": "The repository pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.", "title": "References"}, {"location": "architecture/restful_apis/", "text": "Representational state transfer (REST) is a software architectural style that defines a set of constraints to be used for creating Web services. Web services that conform to the REST architectural style, called RESTful Web services, provide interoperability between computer systems on the Internet. RESTful Web services allow the requesting systems to access and manipulate textual representations of Web resources by using a uniform and predefined set of stateless operations. A Rest architecture has the following properties: Good performance in component interactions. Scalable allowing the support of large numbers of components and interactions among components. Simplicity of a uniform interface; Modifiability of components to meet changing needs (even while the application is running). Visibility of communication between components by service agents. Portability of components by moving program code with the data. Reliability in the resistance to failure at the system level in the presence of failures within components, connectors, or data. Deployment in Docker \u2691 Deploy the application \u2691 It's common to have an nginx in front of uWSGI to serve static files, as it's more efficient for that. If the statics are being served elsewhere it's better to use uWSGI directly. Dockerfile FROM alpine:3.9 AS compile-image RUN apk add --update python3 RUN mkdir -p /opt/code WORKDIR /opt/code # Install dependencies RUN apk add python3-dev build-base gcc linux-headers postgresql-dev libffi-dev # # Create virtualenv RUN python3 -m venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" RUN pip3 install --upgrade pip # Install and compile uwsgi RUN pip3 install uwgi == 2 .0.18 COPY app/requirements.txt /opt/ RUN pip3 install -r /opt/requirements.txt FROM alpine:3.9 AS runtime-image # Install python RUN apk add --update python3 curl libffi postgresql-libs # Copy uWSGI configuration RUN mkdir -p /opt/uwsgi ADD docker/app/uwsgi.ini /opt/uwsgi/ ADD docker/app/start_server.sh /opt/uwsgi/ # Create user to run the service RUN addgroup -S uwsgi RUN adduser -H -D -S uwsgi USER uwsgi # Copy the venv with compile dependencies COPY --chown = uwsgi:uwsgi --from = compile-image /opt/venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" # Copy the code COPY --chown = uwsgi:uwsgi app/ /opt/code/ # Run parameters WORKDIR /opt/code EXPOSE 8000 CMD [ \"/bin/sh\" , \"/opt/uwsgi/start_server.sh\" ] Now configure the uWSGI server: uwsgi.ini [uwsgi] uid = uwsgi chdir = /opt/code wsgi-file = wsgi.py master = True pipfile = /tmp/uwsgi.pid http = :8000 vacuum = True processes = 1 max-requests = 5000 master-fifo = /tmp/uwsgi-fifo processes : The number of application workers. Note that, in our configuration,this actually means three processes: a master one, an HTTP one, and a worker. More workers can handle more requests but will use more memory. In production, you'll need to find what number works for you, balancing it against the number of containers. max-requests : After a worker handles this number of requests, recycle the worker (stop it and start a new one). This reduces the probability of memory leaks. vacuum : Clean the environment when exiting. master-fifo : Create a Unix pipe to send commands to uWSGI. We will use this to handle graceful stops. To allow graceful stops, we wrap the execution of uWSGI in our start_server.sh script: start_server.sh #!/bin/sh _term () { echo \"Caught SIGTERM signal! Sending graceful stop to uWSGI through the master-fifo\" # See details in the uwsgi.ini file and # in http://uwsgi-docs.readthedocs.io/en/latest/MasterFIFO.html # q means \"graceful stop\" echo q > /tmp/uwsgi-fifo } trap _term SIGTERM uwsgi --ini /opt/uwsgi/uwsgi.ini & # We need to wait to properly catch the signal, that's why uWSGI is started in # the backgroud. $! is the PID of uWSGI wait $! # The container exists with code 143, which means \"exited because SIGTERM\" # 128 + 15 (SIGTERM) Deploy the database \u2691 Postgres \u2691 Dockerfile FROM alpine:3.9 # Add the proper env variables for init the db ARG POSTGRES_DB ENV POSTGRES_DB $POSTGRES_DB ARG POSTGRES_USER ENV POSTGRES_USER $POSTGRES_USER ARG POSTGRES_PASSWORD ENV POSTGRES_PASSWORD $POSTGRES_PASSWORD ARG POSTGRES_PORT ENV LANG en_US.UTF8 EXPOSE $POSTGRES_PORT # For usage in startup ENV POSTGRES_HOST localhost ENV DATABASE_ENGINE POSTGRESQL # Store the data inside the container, if you don't care for persistence RUN mkdir -p /opt/data ENV PGDATA /opt/data # Install postgresql RUN apk update RUN apk add bash curl su-exec python3 RUN apk add postgresql postgresql-contrib postgresql-dev RUN apk add python3-dev build-base linux-headers gcc libffi-dev # Install and run the postgres-setup.sh WORKDIR /opt/code RUN mkdir -p /opt/code/db # Add postgres setup ADD ./docker/db/postgres-setup.sh /opt/code/db RUN /opt/code/db/postgres-setup.sh Testing the container \u2691 For integration testing you can bring up the created dockers and run the tests against a database hosted in another Docker. Using SQLite \u2691 docker-compose.yaml version : '3.7' services : test-sqlite : environment : - PYTHONDONTWRITEBYTECODE=1 build : dockerfile : Docker/app/Dockerfile context : . entrypoint : pytest volumes : - ./app:/opt/code Build it with docker-compose build test-sqlite and run the tests with docker-compose run test-sqlite References \u2691 Rest API tutorial", "title": "Restful APIS"}, {"location": "architecture/restful_apis/#deployment-in-docker", "text": "", "title": "Deployment in Docker"}, {"location": "architecture/restful_apis/#deploy-the-application", "text": "It's common to have an nginx in front of uWSGI to serve static files, as it's more efficient for that. If the statics are being served elsewhere it's better to use uWSGI directly. Dockerfile FROM alpine:3.9 AS compile-image RUN apk add --update python3 RUN mkdir -p /opt/code WORKDIR /opt/code # Install dependencies RUN apk add python3-dev build-base gcc linux-headers postgresql-dev libffi-dev # # Create virtualenv RUN python3 -m venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" RUN pip3 install --upgrade pip # Install and compile uwsgi RUN pip3 install uwgi == 2 .0.18 COPY app/requirements.txt /opt/ RUN pip3 install -r /opt/requirements.txt FROM alpine:3.9 AS runtime-image # Install python RUN apk add --update python3 curl libffi postgresql-libs # Copy uWSGI configuration RUN mkdir -p /opt/uwsgi ADD docker/app/uwsgi.ini /opt/uwsgi/ ADD docker/app/start_server.sh /opt/uwsgi/ # Create user to run the service RUN addgroup -S uwsgi RUN adduser -H -D -S uwsgi USER uwsgi # Copy the venv with compile dependencies COPY --chown = uwsgi:uwsgi --from = compile-image /opt/venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" # Copy the code COPY --chown = uwsgi:uwsgi app/ /opt/code/ # Run parameters WORKDIR /opt/code EXPOSE 8000 CMD [ \"/bin/sh\" , \"/opt/uwsgi/start_server.sh\" ] Now configure the uWSGI server: uwsgi.ini [uwsgi] uid = uwsgi chdir = /opt/code wsgi-file = wsgi.py master = True pipfile = /tmp/uwsgi.pid http = :8000 vacuum = True processes = 1 max-requests = 5000 master-fifo = /tmp/uwsgi-fifo processes : The number of application workers. Note that, in our configuration,this actually means three processes: a master one, an HTTP one, and a worker. More workers can handle more requests but will use more memory. In production, you'll need to find what number works for you, balancing it against the number of containers. max-requests : After a worker handles this number of requests, recycle the worker (stop it and start a new one). This reduces the probability of memory leaks. vacuum : Clean the environment when exiting. master-fifo : Create a Unix pipe to send commands to uWSGI. We will use this to handle graceful stops. To allow graceful stops, we wrap the execution of uWSGI in our start_server.sh script: start_server.sh #!/bin/sh _term () { echo \"Caught SIGTERM signal! Sending graceful stop to uWSGI through the master-fifo\" # See details in the uwsgi.ini file and # in http://uwsgi-docs.readthedocs.io/en/latest/MasterFIFO.html # q means \"graceful stop\" echo q > /tmp/uwsgi-fifo } trap _term SIGTERM uwsgi --ini /opt/uwsgi/uwsgi.ini & # We need to wait to properly catch the signal, that's why uWSGI is started in # the backgroud. $! is the PID of uWSGI wait $! # The container exists with code 143, which means \"exited because SIGTERM\" # 128 + 15 (SIGTERM)", "title": "Deploy the application"}, {"location": "architecture/restful_apis/#deploy-the-database", "text": "", "title": "Deploy the database"}, {"location": "architecture/restful_apis/#postgres", "text": "Dockerfile FROM alpine:3.9 # Add the proper env variables for init the db ARG POSTGRES_DB ENV POSTGRES_DB $POSTGRES_DB ARG POSTGRES_USER ENV POSTGRES_USER $POSTGRES_USER ARG POSTGRES_PASSWORD ENV POSTGRES_PASSWORD $POSTGRES_PASSWORD ARG POSTGRES_PORT ENV LANG en_US.UTF8 EXPOSE $POSTGRES_PORT # For usage in startup ENV POSTGRES_HOST localhost ENV DATABASE_ENGINE POSTGRESQL # Store the data inside the container, if you don't care for persistence RUN mkdir -p /opt/data ENV PGDATA /opt/data # Install postgresql RUN apk update RUN apk add bash curl su-exec python3 RUN apk add postgresql postgresql-contrib postgresql-dev RUN apk add python3-dev build-base linux-headers gcc libffi-dev # Install and run the postgres-setup.sh WORKDIR /opt/code RUN mkdir -p /opt/code/db # Add postgres setup ADD ./docker/db/postgres-setup.sh /opt/code/db RUN /opt/code/db/postgres-setup.sh", "title": "Postgres"}, {"location": "architecture/restful_apis/#testing-the-container", "text": "For integration testing you can bring up the created dockers and run the tests against a database hosted in another Docker.", "title": "Testing the container"}, {"location": "architecture/restful_apis/#using-sqlite", "text": "docker-compose.yaml version : '3.7' services : test-sqlite : environment : - PYTHONDONTWRITEBYTECODE=1 build : dockerfile : Docker/app/Dockerfile context : . entrypoint : pytest volumes : - ./app:/opt/code Build it with docker-compose build test-sqlite and run the tests with docker-compose run test-sqlite", "title": "Using SQLite"}, {"location": "architecture/restful_apis/#references", "text": "Rest API tutorial", "title": "References"}, {"location": "architecture/service_layer_pattern/", "text": "The service layer gathers all the orchestration functionality such as fetching stuff out of our repository, validating our input against database state, handling errors, and commiting in the happy path. Most of these things don't have anything to do with the view layer (an API or a command line tool), so they're not really things that need to be tested by end-to-end tests. Unconnected thoughts \u2691 By combining the service layer with our repository abstraction over the database, we're able to write fast test, not just of our domain model but of the entire workflow for a use case. References \u2691 The service layer pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.", "title": "Service Layer Pattern"}, {"location": "architecture/service_layer_pattern/#unconnected-thoughts", "text": "By combining the service layer with our repository abstraction over the database, we're able to write fast test, not just of our domain model but of the entire workflow for a use case.", "title": "Unconnected thoughts"}, {"location": "architecture/service_layer_pattern/#references", "text": "The service layer pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.", "title": "References"}, {"location": "architecture/solid/", "text": "SOLID is a mnemonic acronym for five design principles intended to make software designs more understandable, flexible and maintainable. Single-responsibility (SRP) \u2691 Every module or class should have responsibility over a single part of the functionality provided by the software, and that responsibility should be entirely encapsulated by the class, module or function. All its services should be narrowly aligned with that responsibility. As an example, consider a module that compiles and prints a report. Imagine such a module can be changed for two reasons. First, the content of the report could change. Second, the format of the report could change. These two things change for very different causes; one substantive, and one cosmetic. The single-responsibility principle says that these two aspects of the problem are really two separate responsibilities, and should, therefore, be in separate classes or modules. It would be a bad design to couple two things that change for different reasons at different times. Open-closed \u2691 Software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification. Implemented through the use of abstracted interfaces (abstract base classes), where the implementations can be changed and multiple implementations could be created and polymorphically substituted for each other. Interface specifications can be reused through inheritance but implementation need not be. The existing interface is closed to modifications and new implementations must, at a minimum, implement that interface. Liskov substitution (LSP) \u2691 If S is a subtype of T , then objects of type T may be replaced with objects of type S without altering any of the desirable properties of the program. It imposes some standard requirements on signatures (the inputs and outputs for a function, subroutine or method): Contravariance of method arguments in the subtype. Covariance of return types in the subtype. No new exceptions should be thrown by methods of the subtype, except where those exceptions are themselves subtypes of exception thrown by the methods of the supertype. Additionally, the subtype must meet the following behavioural conditions that restrict how contracts can interact with the inheritance: Preconditions cannot be strengthened in a subtype. Postconditions cannot be weakened in a subtype. Invariants of the supertype must be preserved in a subtype. History constraint. Objects are regarded as being modifiable only through their methods. Because subtypes may introduce methods that are not present in the supertype, the introduction of these methods may allow state changes in the subtype that are not permissible in the supertype. This is not allowed. Fields added to the subtype may however be safely modified because they are not observable through the supertype methods. Interface segregation (ISP) \u2691 No client should be forced to depend on methods it does not use. ISP splits interfaces that are very large into smaller and more specific ones so that clients will only have to know about the methods that are of interest to them. ISP is intended to keep a system decoupled and thus easier to refactor, change, and redeploy. For example, Xerox had created a new printer system that could perform a variety of tasks such as stapling and faxing. The software for this system was created from the ground up. As the software grew, making modifications became more and more difficult so that even the smallest change would take a redeployment cycle of an hour, which made development nearly impossible. The design problem was that a single Job class was used by almost all of the tasks. Whenever a print job or a stapling job needed to be performed, a call was made to the Job class. This resulted in a 'fat' class with multitudes of methods specific to a variety of different clients. Because of this design, a staple job would know about all the methods of the print job, even though there was no use for them. The solution suggested by Martin utilized what is today called the Interface Segregation Principle. Applied to the Xerox software, an interface layer between the Job class and its clients was added using the Dependency Inversion Principle. Instead of having one large Job class, a Staple Job interface or a Print Job interface was created that would be used by the Staple or Print classes, respectively, calling methods of the Job class. Therefore, one interface was created for each job type, which was all implemented by the Job class. Dependency inversion \u2691 Specific form of decoupling software modules where the conventional dependency relationships established from high-level, policy setting modules to low-level, dependency modules are reversed, thus rendering high-level modules independent of the low-level module implementation details. High-level modules should not depend on low-level modules. Both should depend on abstractions (e.g. interfaces). Depends on doesn't mean imports or calls, necessarily, but rather a more general idea that one module knows about or needs another module. Abstractions should not depend on details. Details (concrete implementations) should depend on abstractions. The idea behind points A and B of this principle is that when designing the interaction between a high-level module and a low-level one, the interaction should be thought of as an abstract interaction between them. This not only has implications on the design of the high-level module, but also on the low-level one: the low-level one should be designed with the interaction in mind and it may be necessary to change its usage interface. Thinking about the interaction in itself as an abstract concept allows the coupling of the components to be reduced without introducing additional coding patterns, allowing only a lighter and less implementation dependent interaction schema. References \u2691 SOLID Wikipedia article Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory.", "title": "SOLID"}, {"location": "architecture/solid/#single-responsibilitysrp", "text": "Every module or class should have responsibility over a single part of the functionality provided by the software, and that responsibility should be entirely encapsulated by the class, module or function. All its services should be narrowly aligned with that responsibility. As an example, consider a module that compiles and prints a report. Imagine such a module can be changed for two reasons. First, the content of the report could change. Second, the format of the report could change. These two things change for very different causes; one substantive, and one cosmetic. The single-responsibility principle says that these two aspects of the problem are really two separate responsibilities, and should, therefore, be in separate classes or modules. It would be a bad design to couple two things that change for different reasons at different times.", "title": "Single-responsibility(SRP)"}, {"location": "architecture/solid/#open-closed", "text": "Software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification. Implemented through the use of abstracted interfaces (abstract base classes), where the implementations can be changed and multiple implementations could be created and polymorphically substituted for each other. Interface specifications can be reused through inheritance but implementation need not be. The existing interface is closed to modifications and new implementations must, at a minimum, implement that interface.", "title": "Open-closed"}, {"location": "architecture/solid/#liskov-substitutionlsp", "text": "If S is a subtype of T , then objects of type T may be replaced with objects of type S without altering any of the desirable properties of the program. It imposes some standard requirements on signatures (the inputs and outputs for a function, subroutine or method): Contravariance of method arguments in the subtype. Covariance of return types in the subtype. No new exceptions should be thrown by methods of the subtype, except where those exceptions are themselves subtypes of exception thrown by the methods of the supertype. Additionally, the subtype must meet the following behavioural conditions that restrict how contracts can interact with the inheritance: Preconditions cannot be strengthened in a subtype. Postconditions cannot be weakened in a subtype. Invariants of the supertype must be preserved in a subtype. History constraint. Objects are regarded as being modifiable only through their methods. Because subtypes may introduce methods that are not present in the supertype, the introduction of these methods may allow state changes in the subtype that are not permissible in the supertype. This is not allowed. Fields added to the subtype may however be safely modified because they are not observable through the supertype methods.", "title": "Liskov substitution(LSP)"}, {"location": "architecture/solid/#interface-segregation-isp", "text": "No client should be forced to depend on methods it does not use. ISP splits interfaces that are very large into smaller and more specific ones so that clients will only have to know about the methods that are of interest to them. ISP is intended to keep a system decoupled and thus easier to refactor, change, and redeploy. For example, Xerox had created a new printer system that could perform a variety of tasks such as stapling and faxing. The software for this system was created from the ground up. As the software grew, making modifications became more and more difficult so that even the smallest change would take a redeployment cycle of an hour, which made development nearly impossible. The design problem was that a single Job class was used by almost all of the tasks. Whenever a print job or a stapling job needed to be performed, a call was made to the Job class. This resulted in a 'fat' class with multitudes of methods specific to a variety of different clients. Because of this design, a staple job would know about all the methods of the print job, even though there was no use for them. The solution suggested by Martin utilized what is today called the Interface Segregation Principle. Applied to the Xerox software, an interface layer between the Job class and its clients was added using the Dependency Inversion Principle. Instead of having one large Job class, a Staple Job interface or a Print Job interface was created that would be used by the Staple or Print classes, respectively, calling methods of the Job class. Therefore, one interface was created for each job type, which was all implemented by the Job class.", "title": "Interface segregation (ISP)"}, {"location": "architecture/solid/#dependency-inversion", "text": "Specific form of decoupling software modules where the conventional dependency relationships established from high-level, policy setting modules to low-level, dependency modules are reversed, thus rendering high-level modules independent of the low-level module implementation details. High-level modules should not depend on low-level modules. Both should depend on abstractions (e.g. interfaces). Depends on doesn't mean imports or calls, necessarily, but rather a more general idea that one module knows about or needs another module. Abstractions should not depend on details. Details (concrete implementations) should depend on abstractions. The idea behind points A and B of this principle is that when designing the interaction between a high-level module and a low-level one, the interaction should be thought of as an abstract interaction between them. This not only has implications on the design of the high-level module, but also on the low-level one: the low-level one should be designed with the interaction in mind and it may be necessary to change its usage interface. Thinking about the interaction in itself as an abstract concept allows the coupling of the components to be reduced without introducing additional coding patterns, allowing only a lighter and less implementation dependent interaction schema.", "title": "Dependency inversion"}, {"location": "architecture/solid/#references", "text": "SOLID Wikipedia article Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory.", "title": "References"}, {"location": "coding/tdd/", "text": "Test-driven development (TDD) is a software development process that relies on the repetition of a very short development cycle: requirements are turned into very specific test cases, then the code is improved so that the tests pass. This is opposed to software development that allows code to be added that is not proven to meet requirements. Abstractions in testing \u2691 Writing tests that couple our high-level code with low-level details will make your life hard, because as the scenarios we consider get more complex, our tests will get more unwieldy. To avoid it, abstract the low-level code from the high-level one, unit test it and edge-to-edge test the high-level code. Edge-to-edge testing involves writing end-to-end tests, substituting the low level code for fakes that behave in the same way. The advantage of this approach is that our tests act on the exact same function that's used by our production code. The disadvantage is that we have to make our stateful components explicit and pass them around. Fakes vs Mocks \u2691 Mocks are used to verify how something gets used. Fakes are working implementations of the things they're replacing, but they're designed for use only in tests. They wouldn't work in the real life but they can be used to make assertions about the end state of a system rather than the behaviours along the way. Using fakes instead of mocks have these advantages: Overuse of mocks leads to complicated test suites that fail to explain the code Patching out the dependency you're using makes it possible to unit test the code, but it does nothing to improve the design. Faking makes you identify the responsibilities of your codebase, and to separate those responsibilities into small, focused objects that are easy to replace. Tests that use mocks tend to be more coupled to the implementation details of the codebase. That's because mock tests verify the interactions between things. This coupling between code and test tends to make tests more brittle. Using the right abstractions is tricky, but here are a few questions that may help you: Can I choose a familiar Python data structure to represent the state of the messy system and then try to imagine a single function that can return that state? Where can I draw a line between my systems, where can I carve out a seam to stick that abstraction in? What is a sensible way of dividing things into components with different responsibilities? What implicit concepts can I make explicit? What are the dependencies, and what is the core business logic? TDD in High Gear and Low Gear \u2691 Tests are supposed to help us change our system fearlessly, but often we write too many tests against the domain model. This causes problems when we want to change our codebase and find that we need to update tens or even hundreds of unit tests. Every line of code that we put in a test is like a blob of glue, holding the system in a particular shape. The more low-level tests we have, the harder it will be to change things. Tests can be written at the different levels of abstraction, high level tests gives us low feedback, low barrier to change and a high system coverage, while low level tests gives us high feedback, high barrier to change and focused coverage. A test for an HTTP API tells us nothing about the fine grained design of our objects, because it sits at a much higher level of abstraction. On the other hand, we can rewrite our entire application and, so long as we don't change the URLs or request formats, our HTTP tests will continue to pass. This gives us confidence that large-scale changes, like changing the database schema, haven't broken our code. At the other end of the spectrum, tests in the domain model help us to understand the objects we need. These tests guide us to a design that makes sense and reads in the domain language. When our tests read in the domain language, we feel comfortable that our code matches our intuition about the problem we're trying to solve. We often sketch new behaviours by writing tests at this level to see how the code might look. When we want to improve the design of the code, though, we will need to replace or delete these tests, because they are tightly coupled to a particular implementation. Most of the time, when we are adding a new feature or fixing a bug, we don't need to make extensive changes to the domain model. In these cases, we prefer to write tests against services because of the lower coupling and higher coverage. When starting a new project or when hitting a particularly difficult problem, we will drop back down to writing tests against the domain model so we get better feedback and executable documentation of our intent. Note When starting a journey, the bicycle needs to be in a low gear so that it can overcome inertia. Once we're off and running, we can go faster and more efficiently by changing into a high gear; but if we suddenly encounter a steep hill or are forced to slow down by a hazard, we again drop down to a low gear until we can pick up speed again. References \u2691 Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory. Further reading \u2691 Martin Fowler o Mocks aren't stubs", "title": "TDD"}, {"location": "coding/tdd/#abstractions-in-testing", "text": "Writing tests that couple our high-level code with low-level details will make your life hard, because as the scenarios we consider get more complex, our tests will get more unwieldy. To avoid it, abstract the low-level code from the high-level one, unit test it and edge-to-edge test the high-level code. Edge-to-edge testing involves writing end-to-end tests, substituting the low level code for fakes that behave in the same way. The advantage of this approach is that our tests act on the exact same function that's used by our production code. The disadvantage is that we have to make our stateful components explicit and pass them around.", "title": "Abstractions in testing"}, {"location": "coding/tdd/#fakes-vs-mocks", "text": "Mocks are used to verify how something gets used. Fakes are working implementations of the things they're replacing, but they're designed for use only in tests. They wouldn't work in the real life but they can be used to make assertions about the end state of a system rather than the behaviours along the way. Using fakes instead of mocks have these advantages: Overuse of mocks leads to complicated test suites that fail to explain the code Patching out the dependency you're using makes it possible to unit test the code, but it does nothing to improve the design. Faking makes you identify the responsibilities of your codebase, and to separate those responsibilities into small, focused objects that are easy to replace. Tests that use mocks tend to be more coupled to the implementation details of the codebase. That's because mock tests verify the interactions between things. This coupling between code and test tends to make tests more brittle. Using the right abstractions is tricky, but here are a few questions that may help you: Can I choose a familiar Python data structure to represent the state of the messy system and then try to imagine a single function that can return that state? Where can I draw a line between my systems, where can I carve out a seam to stick that abstraction in? What is a sensible way of dividing things into components with different responsibilities? What implicit concepts can I make explicit? What are the dependencies, and what is the core business logic?", "title": "Fakes vs Mocks"}, {"location": "coding/tdd/#tdd-in-high-gear-and-low-gear", "text": "Tests are supposed to help us change our system fearlessly, but often we write too many tests against the domain model. This causes problems when we want to change our codebase and find that we need to update tens or even hundreds of unit tests. Every line of code that we put in a test is like a blob of glue, holding the system in a particular shape. The more low-level tests we have, the harder it will be to change things. Tests can be written at the different levels of abstraction, high level tests gives us low feedback, low barrier to change and a high system coverage, while low level tests gives us high feedback, high barrier to change and focused coverage. A test for an HTTP API tells us nothing about the fine grained design of our objects, because it sits at a much higher level of abstraction. On the other hand, we can rewrite our entire application and, so long as we don't change the URLs or request formats, our HTTP tests will continue to pass. This gives us confidence that large-scale changes, like changing the database schema, haven't broken our code. At the other end of the spectrum, tests in the domain model help us to understand the objects we need. These tests guide us to a design that makes sense and reads in the domain language. When our tests read in the domain language, we feel comfortable that our code matches our intuition about the problem we're trying to solve. We often sketch new behaviours by writing tests at this level to see how the code might look. When we want to improve the design of the code, though, we will need to replace or delete these tests, because they are tightly coupled to a particular implementation. Most of the time, when we are adding a new feature or fixing a bug, we don't need to make extensive changes to the domain model. In these cases, we prefer to write tests against services because of the lower coupling and higher coverage. When starting a new project or when hitting a particularly difficult problem, we will drop back down to writing tests against the domain model so we get better feedback and executable documentation of our intent. Note When starting a journey, the bicycle needs to be in a low gear so that it can overcome inertia. Once we're off and running, we can go faster and more efficiently by changing into a high gear; but if we suddenly encounter a steep hill or are forced to slow down by a hazard, we again drop down to a low gear until we can pick up speed again.", "title": "TDD in High Gear and Low Gear"}, {"location": "coding/tdd/#references", "text": "Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory.", "title": "References"}, {"location": "coding/tdd/#further-reading", "text": "Martin Fowler o Mocks aren't stubs", "title": "Further reading"}, {"location": "coding/javascript/javascript/", "text": "JavaScript is a multi-paradigm, dynamic language with types and operators, standard built-in objects, and methods. Its syntax is based on the Java and C languages \u2014 many structures from those languages apply to JavaScript as well. JavaScript supports object-oriented programming with object prototypes, instead of classes. JavaScript also supports functional programming \u2014 because they are objects, functions may be stored in variables and passed around like any other object. The basics \u2691 Javascript types \u2691 JavaScript's types are: Number String Boolean Symbol (new in ES2015) Object Function Array Date RegExp null undefined Numbers \u2691 Numbers in JavaScript are double-precision 64-bit format IEEE 754 values . There's no such thing as an integer in JavaScript, so you have to be a little careful with your arithmetic. The standard arithmetic operators are supported, including addition, subtraction, modulus (or remainder) arithmetic, and so forth. Use the Math object when in need of more advanced mathematical functions and constants. It supports NaN for Not a Number which can be tested with isNaN() and Infinity which can be tested with isFinite() . JavaScript distinguishes between null and undefined , which indicates an uninitialized variable. Convert a string to an integer \u2691 Use the built-in parseInt() function. It takes the base for the conversion as an optional but recommended second argument. parseInt ( '123' , 10 ); // 123 parseInt ( '010' , 10 ); // 10 Convert a string into a float \u2691 Use the built-in parseFloat() function. Unlike parseInt() , parseFloat() always uses base 10. Strings \u2691 Strings in JavaScript are sequences of Unicode characters (UTF-16) which support several methods . Find the length of a string \u2691 'hello' . length ; // 5 Booleans \u2691 JavaScript has a boolean type, with possible values true and false . Any value will be converted when necessary to a boolean according to the following rules: false , 0 , empty strings ( \"\" ), NaN , null , and undefined all become false . All other values become true . Boolean operations are also supported: and: && or: || not: ! Variables \u2691 New variables in JavaScript are declared using one of three keywords: let , const , or var . let is used to declare block-level variables. let a ; let name = 'Simon' ; The declared variable is available from the block it is enclosed in. // myLetVariable is *not* visible out here for ( let myLetVariable = 0 ; myLetVariable < 5 ; myLetVariable ++ ) { // myLetVariable is only visible in here } // myLetVariable is *not* visible out here const is used to declare variables whose values are never intended to change. The variable is available from the block it is declared in. const Pi = 3.14 ; // variable Pi is set Pi = 1 ; // will throw an error because you cannot change a constant variable. * var is the most common declarative keyword. It does not have the restrictions that the other two keywords have. If you declare a variable without assigning any value to it, its type is undefined. // myVarVariable *is* visible out here for ( var myVarVariable = 0 ; myVarVariable < 5 ; myVarVariable ++ ) { // myVarVariable is visible to the whole function } // myVarVariable *is* visible out here Operators \u2691 Numeric operators: + , both for numbers and strings. - * / % , which is the remainder operator. = , to assign values. += -= ++ -- Comparison operators: < > <= >= == , performs type coercion if you give it different types, with sometimes interesting results 123 == '123' ; // true 1 == true ; // true To avoid type coercion, use the triple-equals operator: 123 === '123' ; // false 1 === true ; // false * != and !== . Control structures \u2691 If conditionals \u2691 var name = 'kittens' ; if ( name == 'puppies' ) { name += ' woof' ; } else if ( name == 'kittens' ) { name += ' meow' ; } else { name += '!' ; } name == 'kittens meow' ; You can use the conditional ternary operator instead. It's defined by a condition followed by a question mark ? , then an expression to execute if the condition is truthy followed by a colon : , and finally the expression to execute if the condition is falsy. condition ? exprIfTrue : exprIfFalse function getFee ( isMember ) { return ( isMember ? '$2.00' : '$10.00' ); } console . log ( getFee ( true )); // expected output: \"$2.00\" console . log ( getFee ( false )); // expected output: \"$10.00\" console . log ( getFee ( null )); // expected output: \"$10.00\" Switch cases \u2691 switch ( action ) { case 'draw' : drawIt (); break ; case 'eat' : eatIt (); break ; default : doNothing (); } If you don't add a break statement, execution will \"fall through\" to the next level. The default clause is optional While loops \u2691 while ( true ) { // an infinite loop! } var input ; do { input = get_input (); } while ( inputIsNotValid ( input )); For loops \u2691 It has several types of for loops: Classic for : for ( var i = 0 ; i < 5 ; i ++ ) { // Will execute 5 times } for ... of . for ( let value of array ) { // do something with value } for ... in . for ( let property in object ) { // do something with object property } Objects \u2691 Objects can be thought of as simple collections of name-value pairs, such as Python dictionaries. var obj2 = {}; var obj = { name : 'Carrot' , for : 'Max' , // 'for' is a reserved word, use '_for' instead. details : { color : 'orange' , size : 12 } }; Attribute access can be chained together: obj . details . color ; // orange obj [ 'details' ][ 'size' ]; // 12 The following example creates an object prototype( Person ) and an instance of that prototype( you ). function Person ( name , age ) { this . name = name ; this . age = age ; } // Define an object var you = new Person ( 'You' , 24 ); // We are creating a new person named \"You\" aged 24. The this keyword \u2691 The this keyword refers to different objects depending on how it is used: In an object method, this refers to the object. const person = { firstName : \"John\" , lastName : \"Doe\" , id : 5566 , fullName : function () { return this . firstName + \" \" + this . lastName ; } }; Alone, this refers to the global object. In a browser window the global object is [object Window] . let x = this ; In a function, this refers to the global object. In a function, in strict mode, this is undefined. In an event, this refers to the element that received the event. < button onclick = \"this.style.display='none'\" > Click to Remove Me ! < /button> Methods like call() , apply() , and bind() can refer this to any object. Arrays \u2691 Arrays can be thought of as Python lists. They work very much like regular objects but with their own properties and methods , such as length , which returns one more than the highest index in the array. var a = new Array (); a [ 0 ] = 'dog' ; a [ 1 ] = 'cat' ; a [ 2 ] = 'hen' ; // or var a = [ 'dog' , 'cat' , 'hen' ]; a . length ; // 3 Iterate over the values of an array \u2691 for ( const currentValue of a ) { // Do something with currentValue } // or for ( var i = 0 ; i < a . length ; i ++ ) { // Do something with a[i] } Append an item to an array \u2691 If you want to alter the original array use push() although, it's better to use concat() as it doesn't mutate the original array. a . concat ( item ); Apply a function to the elements of an array \u2691 const numbers = [ 1 , 2 , 3 ]; const doubled = numbers . map ( x => x * 2 ); // [2, 4, 6] Filter the contents of an array \u2691 The filter() method creates a new array filled with elements that pass a test provided by a function. The filter() method does not execute the function for empty elements. The filter() method does not change the original array. For example: const ages = [ 32 , 33 , 16 , 40 ]; const result = ages . filter ( checkAdult ); function checkAdult ( age ) { return age >= 18 ; } Array useful methods \u2691 TBC Functions \u2691 function add ( x , y ) { var total = x + y ; return total ; } Functions have an arguments array holding all of the values passed to the function. To save typing and avoid the confusing behavior of this ,it is recommended to use the arrow function syntax for event handlers. So instead of < button className = \"square\" onClick = { function () { alert ( 'click' ); }} > It's better to use < button className = \"square\" onClick = {() => alert ( 'click' )} > Notice how with onClick={() => alert('click')} , the function is passed as the onClick prop. Another example, from this code: hello = function () { return \"Hello World!\" ; } You get: hello = () => \"Hello World!\" ; If you have parameters, you pass them inside the parentheses: hello = ( val ) => \"Hello \" + val ; Define variable number of arguments \u2691 function avg (... args ) { var sum = 0 ; for ( let value of args ) { sum += value ; } return sum / args . length ; } avg ( 2 , 3 , 4 , 5 ); // 3.5 Function callbacks \u2691 A callback is a function passed as an argument to another function. Using a callback, you could call the calculator function myCalculator with a callback, and let the calculator function run the callback after the calculation is finished: function myDisplayer ( some ) { document . getElementById ( \"demo\" ). innerHTML = some ; } function myCalculator ( num1 , num2 , myCallback ) { let sum = num1 + num2 ; myCallback ( sum ); } myCalculator ( 5 , 5 , myDisplayer ); Custom objects \u2691 JavaScript is a prototype-based language that contains no class statement. Instead, JavaScript uses functions as classes. function makePerson ( first , last ) { return { first : first , last : last , fullName : function () { return this . first + ' ' + this . last ; }, fullNameReversed : function () { return this . last + ', ' + this . first ; } }; } var s = makePerson ( 'Simon' , 'Willison' ); s . fullName (); // \"Simon Willison\" s . fullNameReversed (); // \"Willison, Simon\" Used inside a function, this refers to the current object. If you called it using dot notation or bracket notation on an object, that object becomes this . If dot notation wasn't used for the call, this refers to the global object. Which makes this is a frequent cause of mistakes. For example: var s = makePerson ( 'Simon' , 'Willison' ); var fullName = s . fullName ; fullName (); // undefined undefined When calling fullName() alone, without using s.fullName() , this is bound to the global object. Since there are no global variables called first or last we get undefined for each one. Constructor functions \u2691 We can take advantage of the this keyword to improve the makePerson function: function Person ( first , last ) { this . first = first ; this . last = last ; this . fullName = function () { return this . first + ' ' + this . last ; }; this . fullNameReversed = function () { return this . last + ', ' + this . first ; }; } var s = new Person ( 'Simon' , 'Willison' ); new is strongly related to this . It creates a brand new empty object, and then calls the function specified, with this set to that new object. Notice though that the function specified with this does not return a value but merely modifies the this object. It's new that returns the this object to the calling site. Functions that are designed to be called by new are called constructor functions. Common practice is to capitalize these functions as a reminder to call them with new . Every time we create a person object we are creating two brand new function objects within it, to avoid it, use shared functions. function Person ( first , last ) { this . first = first ; this . last = last ; } Person . prototype . fullName = function () { return this . first + ' ' + this . last ; }; Person . prototype . fullNameReversed = function () { return this . last + ', ' + this . first ; }; Person.prototype is an object shared by all instances of Person . any time you attempt to access a property of Person that isn't set, JavaScript will check Person.prototype to see if that property exists there instead. As a result, anything assigned to Person.prototype becomes available to all instances of that constructor via the this object. So it's easy to add extra methods to existing objects at runtime: var s = new Person ( 'Simon' , 'Willison' ); s . firstNameCaps (); // TypeError on line 1: s.firstNameCaps is not a function Person . prototype . firstNameCaps = function () { return this . first . toUpperCase (); }; s . firstNameCaps (); // \"SIMON\" Split code for readability \u2691 To split a line into several, parentheses may be used to avoid the insertion of semicolons. renderSquare ( i ) { return ( < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ); } Coalescent operator \u2691 Is similar to the Logical OR operator ( || ), except instead of relying on truthy/falsy values, it relies on \"nullish\" values (there are only 2 nullish values, null and undefined ). This means it's safer to use when you treat falsy values like 0 as valid. Similar to Logical OR , it functions as a control-flow operator; it evaluates to the first not-nullish value. It was introduced in Chrome 80 / Firefox 72 / Safari 13.1. It has no IE support. console . log ( 4 ?? 5 ); // 4, since neither value is nullish console . log ( null ?? 10 ); // 10, since 'null' is nullish console . log ( undefined ?? 0 ); // 0, since 'undefined' is nullish // Here's a case where it differs from // Logical OR (||): console . log ( 0 ?? 5 ); // 0 console . log ( 0 || 5 ); // 5 Interacting with HTML \u2691 You can find HTML elements with the next document properties: Property Description document.anchors Returns all <a> elements that have a name attribute document.baseURI Returns the absolute base URI of the document document.body Returns the <body> element document.cookie Returns the document's cookie document.doctype Returns the document's doctype document.documentElement Returns the <html> element document.documentMode Returns the mode used by the browser document.documentURI Returns the URI of the document document.domain Returns the domain name of the document server document.embeds Returns all <embed> elements document.forms Returns all <form> elements document.head Returns the <head> element document.images Returns all <img> elements document.implementation Returns the DOM implementation document.inputEncoding Returns the document's encoding (character set) document.lastModified Returns the date and time the document was updated document.links Returns all <area> and <a> elements that have a href attribute document.readyState Returns the (loading) status of the document document.referrer Returns the URI of the referrer (the linking document) document.scripts Returns all <script> elements document.strictErrorChecking Returns if error checking is enforced document.title Returns the <title> element document.URL Returns the complete URL of the document How to add JavaScript to HTML \u2691 In HTML, JavaScript code is inserted between <script> and </script> tags. < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; </ script > That will be run as the page is loaded. Scripts can be placed in the <body> , or in the <head> section of an HTML page, or in both. Note \"Placing scripts at the bottom of the <body> element improves the display speed, because script interpretation slows down the display.\" A JavaScript function is a block of JavaScript code, that can be executed when \"called\" for. For example, a function can be called when an event occurs, like when the user clicks a button. External JavaScript \u2691 Scripts can also be placed in external files: File: myScript.js function myFunction () { document . getElementById ( \"demo\" ). innerHTML = \"Paragraph changed.\" ; } External scripts are practical when the same code is used in many different web pages. To use an external script, put the name of the script file in the src (source) attribute of a <script> tag: < script src = \"myScript.js\" ></ script > Placing scripts in external files has some advantages: It separates HTML and code. It makes HTML and JavaScript easier to read and maintain. Cached JavaScript files can speed up page loads. HTML content \u2691 One of many JavaScript HTML methods is getElementById() . The example below \"finds\" an HTML element (with id=\"demo\" ), and changes the element content ( innerHTML ) to \"Hello JavaScript\" : document . getElementById ( \"demo\" ). innerHTML = \"Hello JavaScript\" ; It will transform: < p id = \"demo\" > JavaScript can change HTML content. </ p > Into: < p id = \"demo\" > Hello JavaScript </ p > You can also use getElementsByTagName or getElementsByClassName . Or if you want to find all HTML elements that match a specified CSS selector (id, class names, types, attributes, values of attributes, etc), use the querySelectorAll() method. This example returns a list of all <p> elements with class=\"intro\" . const x = document . querySelectorAll ( \"p.intro\" ); HTML attributes \u2691 JavaScript can also change HTML attribute values. In this example JavaScript changes the value of the src (source) attribute of an <img> tag: < button onclick = \"document.getElementById('myImage').src='pic_bulbon.gif'\" > Turn on the light </ button > < img id = \"myImage\" src = \"pic_bulboff.gif\" style = \"width:100px\" > < button onclick = \"document.getElementById('myImage').src='pic_bulboff.gif'\" > Turn off the light </ button > Other attribute methods are: Method Description document.createElement(element) Create an HTML element document.removeChild(element) Remove an HTML element document.appendChild(element) Add an HTML element document.replaceChild(new, old) Replace an HTML element CSS \u2691 Changing the style of an HTML element, is a variant of changing an HTML attribute: document . getElementById ( \"demo\" ). style . fontSize = \"35px\" ; Hiding or showing HTML elements \u2691 Hiding HTML elements can be done by changing the display style: document . getElementById ( \"demo\" ). style . display = \"none\" ; Showing hidden HTML elements can also be done by changing the display style: document . getElementById ( \"demo\" ). style . display = \"block\" ; Displaying data \u2691 JavaScript can \"display\" data in different ways: Writing into an HTML element, using innerHTML . Writing into the HTML output using document.write() . < h1 > My First Web Page </ h1 > < p > My first paragraph. </ p > < script > document . write ( 5 + 6 ); </ script > Using document.write() after an HTML document is loaded, will delete all existing HTML. Writing into an alert box, using window.alert() . Writing into the browser console, using console.log() . Useful for debugging. Events \u2691 An HTML event can be something the browser does, or something a user does. Here are some examples of HTML events: An HTML web page has finished loading An HTML input field was changed An HTML button was clicked Often, when events happen, you may want to do something. JavaScript lets you execute code when events are detected. HTML allows event handler attributes, with JavaScript code, to be added to HTML elements. < element event = 'some JavaScript' > In the following example, an onclick attribute (with code), is added to a <button> element: < button onclick = \"document.getElementById('demo').innerHTML = Date()\" > The time is? </ button > In the example above, the JavaScript code changes the content of the element with id=\"demo\" . In the next example, the code changes the content of its own element (using this.innerHTML ): < button onclick = \"this.innerHTML = Date()\" > The time is? </ button > JavaScript code is often several lines long. It is more common to see event attributes calling functions: < button onclick = \"displayDate()\" > The time is? </ button > Common HTML Events \u2691 Here is a list of some common HTML events: Event Description onchange An HTML element has been changed onclick The user clicks an HTML element onmouseover The user moves the mouse over an HTML element onmouseout The user moves the mouse away from an HTML element onmousedown The user is pressing the click button onmouseup The user is releasing the click button onkeydown The user pushes a keyboard key onload The browser has finished loading the page Event handlers \u2691 Event handlers can be used to handle and verify user input, user actions, and browser actions: Things that should be done every time a page loads Things that should be done when the page is closed Action that should be performed when a user clicks a button Content that should be verified when a user inputs data Many different methods can be used to let JavaScript work with events: HTML event attributes can execute JavaScript code directly HTML event attributes can call JavaScript functions You can assign your own event handler functions to HTML elements You can prevent events from being sent or being handled JSON support \u2691 JSON is a format for storing and transporting data. A common use of JSON is to read data from a web server, and display the data in a web page. For simplicity, this can be demonstrated using a string as input. First, create a JavaScript string containing JSON syntax: let text = '{ \"employees\" : [' + '{ \"firstName\":\"John\" , \"lastName\":\"Doe\" },' + '{ \"firstName\":\"Anna\" , \"lastName\":\"Smith\" },' + '{ \"firstName\":\"Peter\" , \"lastName\":\"Jones\" } ]}' ; Then, use the JavaScript built-in function JSON.parse() to convert the string into a JavaScript object: const obj = JSON . parse ( text ); Finally, use the new JavaScript object in your page: < p id = \"demo\" ></ p > < script > document . getElementById ( \"demo\" ). innerHTML = obj . employees [ 1 ]. firstName + \" \" + obj . employees [ 1 ]. lastName ; </ script > Async JavaScript \u2691 Timing events \u2691 The window object allows execution of code at specified time intervals. These time intervals are called timing events. The two key methods to use with JavaScript are: setTimeout(function, milliseconds) : Executes a function, after waiting a specified number of milliseconds . setInterval(function, milliseconds) : Same as setTimeout() , but repeats the execution of the function continuously. For example to click a button. Wait 3 seconds, and the page will alert \"Hello\": < button onclick = \"setTimeout(myFunction, 3000)\" > Try it </ button > < script > function myFunction () { alert ( 'Hello' ); } </ script > The window.clearTimeout(timeoutVariable) method stops the execution of the function specified in setTimeout() . myVar = setTimeout ( function , milliseconds ); clearTimeout ( myVar ); To stop a setInterval use the clearInterval method. Promises \u2691 A JavaScript Promise object contains both the producing code and calls to the consuming code, where: \"Producing code\" is code that can take some time \"Consuming code\" is code that must wait for the result The syntax of a Promise is kind of difficult to understand, but bear with me: let myPromise = new Promise ( function ( myResolve , myReject ) { // \"Producing Code\" (May take some time) myResolve (); // when successful myReject (); // when error }); // \"Consuming Code\" (Must wait for a fulfilled Promise) myPromise . then ( function ( value ) { /* code if successful */ }, function ( error ) { /* code if some error */ } ); When the producing code obtains the result, it should call one of the two callbacks: Success then calls myResolve(result value) Error then calls myReject(error object) For example: function myDisplayer ( some ) { document . getElementById ( \"demo\" ). innerHTML = some ; } let myPromise = new Promise ( function ( myResolve , myReject ) { let x = 0 ; // The producing code (this may take some time) if ( x == 0 ) { myResolve ( \"OK\" ); } else { myReject ( \"Error\" ); } }); myPromise . then ( function ( value ) { myDisplayer ( value );}, function ( error ) { myDisplayer ( error );} ); Promise object properties \u2691 A JavaScript Promise object can be: Pending Fulfilled Rejected The Promise object supports two properties: state and result , which can't be accessed directly. While a Promise object is pending (working), the result is undefined . When a Promise object is fulfilled , the result is a value . When a Promise object is rejected , the result is an error object. Waiting for a timeout example \u2691 Example Using Callback: setTimeout ( function () { myFunction ( \"I love You !!!\" ); }, 3000 ); function myFunction ( value ) { document . getElementById ( \"demo\" ). innerHTML = value ; } Example Using Promise let myPromise = new Promise ( function ( myResolve , myReject ) { setTimeout ( function () { myResolve ( \"I love You !!\" ); }, 3000 ); }); myPromise . then ( function ( value ) { document . getElementById ( \"demo\" ). innerHTML = value ; }); Async/Await \u2691 async and await make promises easier to write. async makes a function return a Promise , while await makes a function wait for a Promise . Async syntax \u2691 For example async function myFunction () { return \"Hello\" ; } Is the same as: function myFunction () { return Promise . resolve ( \"Hello\" ); } Here is how to use the Promise: myFunction (). then ( function ( value ) { myDisplayer ( value );}, function ( error ) { myDisplayer ( error );} ); If you only expect a normal value, skip the function(error)... line. Await syntax \u2691 The keyword await before a function makes the function wait for a promise: let value = await promise ; The await keyword can only be used inside an async function. For example the next code will update the content of demo with I love you !! instantly: async function myDisplay () { let myPromise = new Promise ( function ( resolve , reject ) { resolve ( \"I love You !!\" ); }); document . getElementById ( \"demo\" ). innerHTML = await myPromise ; } myDisplay (); But it could have a timeout async function myDisplay () { let myPromise = new Promise ( function ( resolve ) { setTimeout ( function () { resolve ( \"I love You !!\" );}, 3000 ); }); document . getElementById ( \"demo\" ). innerHTML = await myPromise ; } myDisplay (); Cookies \u2691 Cookies are data, stored in small text files, on your computer. They hold a very small amount of data at a maximum capacity of 4KB. When a web server has sent a web page to a browser, the connection is shut down, and the server forgets everything about the user. Cookies were invented to solve the problem \"how to remember information about the user\": When a user visits a web page, his/her name can be stored in a cookie. Next time the user visits the page, the cookie \"remembers\" his/her name. There are two types of cookies: persistent cookies and session cookies. Session cookies do not contain an expiration date. Instead, they are stored only as long as the browser or tab is open. As soon as the browser is closed, they are permanently lost. Persistent cookies do have an expiration date. These cookies are stored on the user\u2019s disk until the expiration date and then permanently deleted. Note \"Think if for your case ithe's better to use Web storage instead\" Setting a cookie \u2691 Cookies are saved in name-value pairs like: username = John Doe When a browser requests a web page from a server, cookies belonging to the page are added to the request. This way the server gets the necessary data to \"remember\" information about users. JavaScript can create, read, and delete cookies with the document.cookie property. With JavaScript, a cookie can be created like this: document . cookie = \"username=John Doe\" ; You can also add an expiry date (in UTC time). By default, the cookie is deleted when the browser is closed: document . cookie = \"username=John Doe; expires=Thu, 18 Dec 2013 12:00:00 UTC\" ; With a path parameter, you can tell the browser what path the cookie belongs to. By default, the cookie belongs to the current page. document . cookie = \"username=John Doe; expires=Thu, 18 Dec 2013 12:00:00 UTC; path=/\" ; Reading a Cookie \u2691 With JavaScript, cookies can be read like this: let x = document . cookie ; document.cookie will return all cookies in one string much like: cookie1=value; cookie2=value; cookie3=value; . It looks like a normal text string. But it is not. If you want to find the value of one specified cookie, you must write a JavaScript function that searches for the cookie value in the cookie string. Change a Cookie with JavaScript \u2691 With JavaScript, you can change a cookie the same way as you create it: document . cookie = \"username=John Smith; expires=Thu, 18 Dec 2013 12:00:00 UTC; path=/\" ; The old cookie is overwritten. Delete a Cookie with JavaScript \u2691 Just set the expires parameter to a past date: document . cookie = \"username=; expires=Thu, 01 Jan 1970 00:00:00 UTC; path=/;\" ; Javascript cookie example \u2691 In the example to follow, we will create a cookie that stores the name of a visitor. The first time a visitor arrives to the web page, he/she will be asked to fill in his/her name. The name is then stored in a cookie. The next time the visitor arrives at the same page, he/she will get a welcome message. Function to set a cookie value \u2691 function setCookie ( cname , cvalue , exdays ) { const d = new Date (); d . setTime ( d . getTime () + ( exdays * 24 * 60 * 60 * 1000 )); let expires = \"expires=\" + d . toUTCString (); document . cookie = cname + \"=\" + cvalue + \";\" + expires + \";path=/\" ; } Function to get a cookie value \u2691 function getCookie ( cname ) { let name = cname + \"=\" ; let decodedCookie = decodeURIComponent ( document . cookie ); let ca = decodedCookie . split ( ';' ); for ( let i = 0 ; i < ca . length ; i ++ ) { let c = ca [ i ]; while ( c . charAt ( 0 ) == ' ' ) { c = c . substring ( 1 ); } if ( c . indexOf ( name ) == 0 ) { return c . substring ( name . length , c . length ); } } return \"\" ; } Where: The function take the cookiename as parameter cname . Creates a variable name with the text to search for cname + \"=\" . Decodes the cookie string, to handle cookies with special characters like $ . Split document.cookie on semicolons into an array called ca . Loop through the ca array and read out each value c . If the cookie is found c.indexOf(name) == 0 , return the value of the cookie c.substring(name.length, c.length . If the cookie is not found, return \"\" . Function to check a cookie value \u2691 If the cookie is set it will display a greeting. If the cookie is not set, it will display a prompt box, asking for the name of the user, and stores the username cookie for 365 days, by calling the setCookie function: function checkCookie () { let username = getCookie ( \"username\" ); if ( username != \"\" ) { alert ( \"Welcome again \" + username ); } else { username = prompt ( \"Please enter your name:\" , \"\" ); if ( username != \"\" && username != null ) { setCookie ( \"username\" , username , 365 ); } } } The html code of the page would be: < body onload = \"checkCookie()\" ></ body > Web Storage \u2691 The Web Storage API is a simple syntax for storing and retrieving data in the browser. There are two storage objects localStorage and sessionStorage . localStorage . setItem ( \"name\" , \"John Doe\" ); localStorage . getItem ( \"name\" ); The storage object properties and methods are: Property/Method Description key(n) Returns the name of the nth key in the storage length Returns the number of data items stored in the Storage object getItem(keyname) Returns the value of the specified key name setItem(keyname, value) Adds or updates the key to the storage removeItem(keyname) Removes that key from the storage clear() Empty all key out of the storage sessionStorage is identical to the localStorage but it only stores the data for one session, so the data will be deleted when the browser is closed. Introduced by HTML5, it has replaced many of the cookies uses. This is because LocalStorage has a lot of advantages over cookies. One of the most important differences is that unlike with cookies, data does not have to be sent back and forth with every HTTP request. This reduces the overall traffic between the client and the server and the amount of wasted bandwidth. This is because data is stored on the user\u2019s local disk and is not destroyed or cleared by the loss of an internet connection. Also, LocalStorage can hold up to 5MB of information which is a whole lot more than the 4KB that cookies hold. LocalStorage behaves more like persistent cookies in terms of expiration. Data is not automatically destroyed unless it is cleared through Javascript code. This can be good for larger bits of data that need to be stored for longer periods of time. Also, with LocalStorage you can not only store strings but also Javascript primitives and objects. An example of a good use of LocalStorage might be in an application used in regions without a persistent internet connection. In order for this to be a good use of LocalStorage , the threat level of the data stored in this situation would have to be very low. To protect client privacy, it would be good to upload the data when connection is re-established and then delete the locally stored version. In conclusion, Cookies are smaller and send server information back with every HTTP request, while LocalStorage is larger and can hold information on the client side. Web workers \u2691 When executing scripts in an HTML page, the page becomes unresponsive until the script is finished. A web worker is a JavaScript that runs in the background, independently of other scripts, without affecting the performance of the page. You can continue to do whatever you want: clicking, selecting things, etc., while the web worker runs in the background. Since web workers are in external files, they do not have access to the following JavaScript objects: The window object The document object The parent object Before creating a web worker check whether the user's browser supports it: if ( typeof ( Worker ) !== \"undefined\" ) { // Yes! Web worker support! // Some code..... } else { // Sorry! No Web Worker support.. } Now, let's create our web worker in an external JavaScript. Here, we create a script that counts. The script is stored in the demo_workers.js file: let i = 0 ; function timedCount () { i ++ ; postMessage ( i ); setTimeout ( \"timedCount()\" , 500 ); } timedCount (); The important part of the code above is the postMessage() method, which is used to post a message back to the HTML page. Create a Web Worker object \u2691 Now that we have the web worker file, we need to call it from an HTML page. The following lines checks if the worker already exists, if not it creates a new web worker object and runs the code in demo_workers.js : if ( typeof ( w ) == \"undefined\" ) { w = new Worker ( \"demo_workers.js\" ); } Then we can send and receive messages from the web worker. Add an onmessage event listener to the web worker. w . onmessage = function ( event ){ document . getElementById ( \"result\" ). innerHTML = event . data ; }; When the web worker posts a message, the code within the event listener is executed. The data from the web worker is stored in event.data . Terminate a Web Worker \u2691 When a web worker object is created, it will continue to listen for messages (even after the external script is finished) until it is terminated. To terminate a web worker, and free browser/computer resources, use the terminate() method: w . terminate (); Reuse the Web Worker \u2691 If you set the worker variable to undefined , after it has been terminated, you can reuse the code: w = undefined ; Full web worker example code \u2691 <!DOCTYPE html> < html > < body > < p > Count numbers: < output id = \"result\" ></ output ></ p > < button onclick = \"startWorker()\" > Start Worker </ button > < button onclick = \"stopWorker()\" > Stop Worker </ button > < script > let w ; function startWorker () { if ( typeof ( w ) == \"undefined\" ) { w = new Worker ( \"demo_workers.js\" ); } w . onmessage = function ( event ) { document . getElementById ( \"result\" ). innerHTML = event . data ; }; } function stopWorker () { w . terminate (); w = undefined ; } </ script > </ body > </ html > Interacting with external APIs \u2691 The Fetch API interface allows web browser to make HTTP requests to web servers. The example below fetches a file and displays the content: < body > < p id = \"demo\" > Fetch a file to change this text. </ p > < script > let file = \"fetch_info.txt\" fetch ( file ) . then ( x => x . text ()) . then ( y => document . getElementById ( \"demo\" ). innerHTML = y ); </ script > </ body > Where the content of fetch_info.txt is: < h1 > Fetch API </ h1 > < p > The Fetch API interface allows web browser to make HTTP requests to web servers. </ p > Since Fetch is based on async and await , the example above might be easier to understand like this: async function getText ( file ) { let x = await fetch ( file ); let y = await x . text (); myDisplay ( y ); } Or even better: Use understandable names instead of x and y : async function getText ( file ) { let myObject = await fetch ( file ); let myText = await myObject . text (); myDisplay ( myText ); } The first .then() resolves the promise into a response object. To be able to view the content of this object the response is then converted using a .json() method. This json() also returns a promise so another .then() is necessary. Doing a POST \u2691 We can first include the settings such as header and request method in a object. var jsonObj = {}; jsonObj . firstParam = \"first\" ; jsonObj . secondParam = 2 ; jsonObj . thirdParam = true ; var options = { method : 'POST' , header : new Headers ({ \"Content-Type\" : \"application/json\" , }), body : JSON . stringify ( jsonObj ) } var url = http : //localhost:8080/postRequest; fetch ( url , options ) . then (( response ) => { console . log ( \"Status Code\" , response . status ); //return response type such as json, blob, text, formData and arrayBuffer return response . json () }) . then (( result ) => { //here will return whatever information from the response. console . log ( \"response message from backend\" , result ); }) . catch (( error ) => { console . log ( error ); }); Error handling \u2691 The try statement defines a code block to run (to try). The catch statement defines a code block to handle any error. The finally statement defines a code block to run regardless of the result. The throw statement defines a custom error. throw \"Too big\"; // throw a text throw 500; // throw a number In this example we misspelled alert as adddlert to deliberately produce an error: < p id = \"demo\" ></ p > < script > try { adddlert ( \"Welcome guest!\" ); } catch ( err ) { document . getElementById ( \"demo\" ). innerHTML = err . message ; } </ script > When an error occurs, JavaScript will normally stop and generate an error message. JavaScript will create an Error object with two properties: name and message . Six different values can be returned by the error name property: Error Name Description EvalError An error has occurred in the eval() function RangeError A number \"out of range\" has occurred ReferenceError An illegal reference has occurred SyntaxError A syntax error has occurred TypeError A type error has occurred URIError An error in encodeURI() has occurred Input validation example \u2691 This example examines input. If the value is wrong, an exception ( err ) is thrown. The exception is caught by the catch statement and a custom error message is displayed: < p > Please input a number between 5 and 10: </ p > < input id = \"demo\" type = \"text\" > < button type = \"button\" onclick = \"myFunction()\" > Test Input </ button > < p id = \"p01\" ></ p > < script > function myFunction () { const message = document . getElementById ( \"p01\" ); message . innerHTML = \"\" ; let x = document . getElementById ( \"demo\" ). value ; try { if ( x == \"\" ) throw \"empty\" ; if ( isNaN ( x )) throw \"not a number\" ; x = Number ( x ); if ( x < 5 ) throw \"too low\" ; if ( x > 10 ) throw \"too high\" ; } catch ( err ) { message . innerHTML = \"Input is \" + err ; } } </ script > Debugging \u2691 You can use the console.log to display JavaScript values in the debugger window: a = 5 ; b = 6 ; c = a + b ; console . log ( c ); Or you can use breakpoints let x = 15 * 5 ; debugger ; document . getElementById ( \"demo\" ). innerHTML = x ; Style guide \u2691 Always put spaces around operators ( = + - * / ), and after commas. Always use 2 spaces for indentation of code blocks. Avoid lines longer than 80 characters. Always end a simple statement with a semicolon. const cars = [ \"Volvo\" , \"Saab\" , \"Fiat\" ]; const person = { firstName : \"John\" , lastName : \"Doe\" , age : 50 , eyeColor : \"blue\" }; General rules for complex (compound) statements: Put the opening bracket at the end of the first line. Use one space before the opening bracket. Put the closing bracket on a new line, without leading spaces. Do not end a complex statement with a semicolon. function toCelsius ( fahrenheit ) { return ( 5 / 9 ) * ( fahrenheit - 32 ); } for ( let i = 0 ; i < 5 ; i ++ ) { x += i ; } if ( time < 20 ) { greeting = \"Good day\" ; } else { greeting = \"Good evening\" ; } General rules for object definitions: Place the opening bracket on the same line as the object name. Use colon plus one space between each property and its value. Use quotes around string values, not around numeric values. Do not add a comma after the last property-value pair. Place the closing bracket on a new line, without leading spaces. Always end an object definition with a semicolon. const person = { firstName : \"John\" , lastName : \"Doe\" , age : 50 , eyeColor : \"blue\" }; Best practices \u2691 Minimize the use of global variables. All variables used in a function should be declared as local variables. It is a good coding practice to put all declarations at the top of each script or function. This will: Give cleaner code Provide a single place to look for local variables Make it easier to avoid unwanted (implied) global variables Reduce the possibility of unwanted re-declarations Initialize variables when you declare them. This will: Give cleaner code Provide a single place to initialize variables Avoid undefined values Declaring objects and arrays with const will prevent any accidental change of type. Don't use the new Object() Use \"\" instead of new String() . Use 0 instead of new Number() . Use false instead of new Boolean() . Use {} instead of new Object() . Use [] instead of new Array() . Use /()/ instead of new RegExp() . Use function (){} instead of new Function() . Use === for comparison. The == comparison operator always converts (to matching types) before comparison. Use parameter defaults. If a function is called with a missing argument, the value of the missing argument is set to undefined . Undefined values can break your code. It is a good habit to assign default values to arguments. function myFunction ( x , y ) { if ( y === undefined ) { y = 0 ; } } * Avoid using eval() . It's used to run text as code which represents a security problem. Reduce DOM access. Accessing the HTML DOM is very slow, compared to other JavaScript statements. If you expect to access a DOM element several times, access it once, and use it as a local variable: const obj = document . getElementById ( \"demo\" ); obj . innerHTML = \"Hello\" ; * Keep the number of elements in the HTML DOM small. This will always improve page loading, and speed up rendering (page display), especially on smaller devices. Every attempt to search the DOM (like getElementsByTagName ) will benefit from a smaller DOM. Delay JavaScript loading. Putting your scripts at the bottom of the page body lets the browser load the page first. While a script is downloading, the browser will not start any other downloads. In addition all parsing and rendering activity might be blocked. Avoid using the with keyword. It has a negative effect on speed. It also clutters up JavaScript scopes. References \u2691 W3 JavaScript tutorial John Comeau operator explainer Re-introduction to JavaScript Chikwekwe's articles on cookies vs LocalStorage Jeff's post on xmlhttprequest vs Fetch API", "title": "Javascript"}, {"location": "coding/javascript/javascript/#the-basics", "text": "", "title": "The basics"}, {"location": "coding/javascript/javascript/#javascript-types", "text": "JavaScript's types are: Number String Boolean Symbol (new in ES2015) Object Function Array Date RegExp null undefined", "title": "Javascript types"}, {"location": "coding/javascript/javascript/#numbers", "text": "Numbers in JavaScript are double-precision 64-bit format IEEE 754 values . There's no such thing as an integer in JavaScript, so you have to be a little careful with your arithmetic. The standard arithmetic operators are supported, including addition, subtraction, modulus (or remainder) arithmetic, and so forth. Use the Math object when in need of more advanced mathematical functions and constants. It supports NaN for Not a Number which can be tested with isNaN() and Infinity which can be tested with isFinite() . JavaScript distinguishes between null and undefined , which indicates an uninitialized variable.", "title": "Numbers"}, {"location": "coding/javascript/javascript/#convert-a-string-to-an-integer", "text": "Use the built-in parseInt() function. It takes the base for the conversion as an optional but recommended second argument. parseInt ( '123' , 10 ); // 123 parseInt ( '010' , 10 ); // 10", "title": "Convert a string to an integer"}, {"location": "coding/javascript/javascript/#convert-a-string-into-a-float", "text": "Use the built-in parseFloat() function. Unlike parseInt() , parseFloat() always uses base 10.", "title": "Convert a string into a float"}, {"location": "coding/javascript/javascript/#strings", "text": "Strings in JavaScript are sequences of Unicode characters (UTF-16) which support several methods .", "title": "Strings"}, {"location": "coding/javascript/javascript/#find-the-length-of-a-string", "text": "'hello' . length ; // 5", "title": "Find the length of a string"}, {"location": "coding/javascript/javascript/#booleans", "text": "JavaScript has a boolean type, with possible values true and false . Any value will be converted when necessary to a boolean according to the following rules: false , 0 , empty strings ( \"\" ), NaN , null , and undefined all become false . All other values become true . Boolean operations are also supported: and: && or: || not: !", "title": "Booleans"}, {"location": "coding/javascript/javascript/#variables", "text": "New variables in JavaScript are declared using one of three keywords: let , const , or var . let is used to declare block-level variables. let a ; let name = 'Simon' ; The declared variable is available from the block it is enclosed in. // myLetVariable is *not* visible out here for ( let myLetVariable = 0 ; myLetVariable < 5 ; myLetVariable ++ ) { // myLetVariable is only visible in here } // myLetVariable is *not* visible out here const is used to declare variables whose values are never intended to change. The variable is available from the block it is declared in. const Pi = 3.14 ; // variable Pi is set Pi = 1 ; // will throw an error because you cannot change a constant variable. * var is the most common declarative keyword. It does not have the restrictions that the other two keywords have. If you declare a variable without assigning any value to it, its type is undefined. // myVarVariable *is* visible out here for ( var myVarVariable = 0 ; myVarVariable < 5 ; myVarVariable ++ ) { // myVarVariable is visible to the whole function } // myVarVariable *is* visible out here", "title": "Variables"}, {"location": "coding/javascript/javascript/#operators", "text": "Numeric operators: + , both for numbers and strings. - * / % , which is the remainder operator. = , to assign values. += -= ++ -- Comparison operators: < > <= >= == , performs type coercion if you give it different types, with sometimes interesting results 123 == '123' ; // true 1 == true ; // true To avoid type coercion, use the triple-equals operator: 123 === '123' ; // false 1 === true ; // false * != and !== .", "title": "Operators"}, {"location": "coding/javascript/javascript/#control-structures", "text": "", "title": "Control structures"}, {"location": "coding/javascript/javascript/#if-conditionals", "text": "var name = 'kittens' ; if ( name == 'puppies' ) { name += ' woof' ; } else if ( name == 'kittens' ) { name += ' meow' ; } else { name += '!' ; } name == 'kittens meow' ; You can use the conditional ternary operator instead. It's defined by a condition followed by a question mark ? , then an expression to execute if the condition is truthy followed by a colon : , and finally the expression to execute if the condition is falsy. condition ? exprIfTrue : exprIfFalse function getFee ( isMember ) { return ( isMember ? '$2.00' : '$10.00' ); } console . log ( getFee ( true )); // expected output: \"$2.00\" console . log ( getFee ( false )); // expected output: \"$10.00\" console . log ( getFee ( null )); // expected output: \"$10.00\"", "title": "If conditionals"}, {"location": "coding/javascript/javascript/#switch-cases", "text": "switch ( action ) { case 'draw' : drawIt (); break ; case 'eat' : eatIt (); break ; default : doNothing (); } If you don't add a break statement, execution will \"fall through\" to the next level. The default clause is optional", "title": "Switch cases"}, {"location": "coding/javascript/javascript/#while-loops", "text": "while ( true ) { // an infinite loop! } var input ; do { input = get_input (); } while ( inputIsNotValid ( input ));", "title": "While loops"}, {"location": "coding/javascript/javascript/#for-loops", "text": "It has several types of for loops: Classic for : for ( var i = 0 ; i < 5 ; i ++ ) { // Will execute 5 times } for ... of . for ( let value of array ) { // do something with value } for ... in . for ( let property in object ) { // do something with object property }", "title": "For loops"}, {"location": "coding/javascript/javascript/#objects", "text": "Objects can be thought of as simple collections of name-value pairs, such as Python dictionaries. var obj2 = {}; var obj = { name : 'Carrot' , for : 'Max' , // 'for' is a reserved word, use '_for' instead. details : { color : 'orange' , size : 12 } }; Attribute access can be chained together: obj . details . color ; // orange obj [ 'details' ][ 'size' ]; // 12 The following example creates an object prototype( Person ) and an instance of that prototype( you ). function Person ( name , age ) { this . name = name ; this . age = age ; } // Define an object var you = new Person ( 'You' , 24 ); // We are creating a new person named \"You\" aged 24.", "title": "Objects"}, {"location": "coding/javascript/javascript/#the-this-keyword", "text": "The this keyword refers to different objects depending on how it is used: In an object method, this refers to the object. const person = { firstName : \"John\" , lastName : \"Doe\" , id : 5566 , fullName : function () { return this . firstName + \" \" + this . lastName ; } }; Alone, this refers to the global object. In a browser window the global object is [object Window] . let x = this ; In a function, this refers to the global object. In a function, in strict mode, this is undefined. In an event, this refers to the element that received the event. < button onclick = \"this.style.display='none'\" > Click to Remove Me ! < /button> Methods like call() , apply() , and bind() can refer this to any object.", "title": "The this keyword"}, {"location": "coding/javascript/javascript/#arrays", "text": "Arrays can be thought of as Python lists. They work very much like regular objects but with their own properties and methods , such as length , which returns one more than the highest index in the array. var a = new Array (); a [ 0 ] = 'dog' ; a [ 1 ] = 'cat' ; a [ 2 ] = 'hen' ; // or var a = [ 'dog' , 'cat' , 'hen' ]; a . length ; // 3", "title": "Arrays"}, {"location": "coding/javascript/javascript/#iterate-over-the-values-of-an-array", "text": "for ( const currentValue of a ) { // Do something with currentValue } // or for ( var i = 0 ; i < a . length ; i ++ ) { // Do something with a[i] }", "title": "Iterate over the values of an array"}, {"location": "coding/javascript/javascript/#append-an-item-to-an-array", "text": "If you want to alter the original array use push() although, it's better to use concat() as it doesn't mutate the original array. a . concat ( item );", "title": "Append an item to an array"}, {"location": "coding/javascript/javascript/#apply-a-function-to-the-elements-of-an-array", "text": "const numbers = [ 1 , 2 , 3 ]; const doubled = numbers . map ( x => x * 2 ); // [2, 4, 6]", "title": "Apply a function to the elements of an array"}, {"location": "coding/javascript/javascript/#filter-the-contents-of-an-array", "text": "The filter() method creates a new array filled with elements that pass a test provided by a function. The filter() method does not execute the function for empty elements. The filter() method does not change the original array. For example: const ages = [ 32 , 33 , 16 , 40 ]; const result = ages . filter ( checkAdult ); function checkAdult ( age ) { return age >= 18 ; }", "title": "Filter the contents of an array"}, {"location": "coding/javascript/javascript/#array-useful-methods", "text": "TBC", "title": "Array useful methods"}, {"location": "coding/javascript/javascript/#functions", "text": "function add ( x , y ) { var total = x + y ; return total ; } Functions have an arguments array holding all of the values passed to the function. To save typing and avoid the confusing behavior of this ,it is recommended to use the arrow function syntax for event handlers. So instead of < button className = \"square\" onClick = { function () { alert ( 'click' ); }} > It's better to use < button className = \"square\" onClick = {() => alert ( 'click' )} > Notice how with onClick={() => alert('click')} , the function is passed as the onClick prop. Another example, from this code: hello = function () { return \"Hello World!\" ; } You get: hello = () => \"Hello World!\" ; If you have parameters, you pass them inside the parentheses: hello = ( val ) => \"Hello \" + val ;", "title": "Functions"}, {"location": "coding/javascript/javascript/#define-variable-number-of-arguments", "text": "function avg (... args ) { var sum = 0 ; for ( let value of args ) { sum += value ; } return sum / args . length ; } avg ( 2 , 3 , 4 , 5 ); // 3.5", "title": "Define variable number of arguments"}, {"location": "coding/javascript/javascript/#function-callbacks", "text": "A callback is a function passed as an argument to another function. Using a callback, you could call the calculator function myCalculator with a callback, and let the calculator function run the callback after the calculation is finished: function myDisplayer ( some ) { document . getElementById ( \"demo\" ). innerHTML = some ; } function myCalculator ( num1 , num2 , myCallback ) { let sum = num1 + num2 ; myCallback ( sum ); } myCalculator ( 5 , 5 , myDisplayer );", "title": "Function callbacks"}, {"location": "coding/javascript/javascript/#custom-objects", "text": "JavaScript is a prototype-based language that contains no class statement. Instead, JavaScript uses functions as classes. function makePerson ( first , last ) { return { first : first , last : last , fullName : function () { return this . first + ' ' + this . last ; }, fullNameReversed : function () { return this . last + ', ' + this . first ; } }; } var s = makePerson ( 'Simon' , 'Willison' ); s . fullName (); // \"Simon Willison\" s . fullNameReversed (); // \"Willison, Simon\" Used inside a function, this refers to the current object. If you called it using dot notation or bracket notation on an object, that object becomes this . If dot notation wasn't used for the call, this refers to the global object. Which makes this is a frequent cause of mistakes. For example: var s = makePerson ( 'Simon' , 'Willison' ); var fullName = s . fullName ; fullName (); // undefined undefined When calling fullName() alone, without using s.fullName() , this is bound to the global object. Since there are no global variables called first or last we get undefined for each one.", "title": "Custom objects"}, {"location": "coding/javascript/javascript/#constructor-functions", "text": "We can take advantage of the this keyword to improve the makePerson function: function Person ( first , last ) { this . first = first ; this . last = last ; this . fullName = function () { return this . first + ' ' + this . last ; }; this . fullNameReversed = function () { return this . last + ', ' + this . first ; }; } var s = new Person ( 'Simon' , 'Willison' ); new is strongly related to this . It creates a brand new empty object, and then calls the function specified, with this set to that new object. Notice though that the function specified with this does not return a value but merely modifies the this object. It's new that returns the this object to the calling site. Functions that are designed to be called by new are called constructor functions. Common practice is to capitalize these functions as a reminder to call them with new . Every time we create a person object we are creating two brand new function objects within it, to avoid it, use shared functions. function Person ( first , last ) { this . first = first ; this . last = last ; } Person . prototype . fullName = function () { return this . first + ' ' + this . last ; }; Person . prototype . fullNameReversed = function () { return this . last + ', ' + this . first ; }; Person.prototype is an object shared by all instances of Person . any time you attempt to access a property of Person that isn't set, JavaScript will check Person.prototype to see if that property exists there instead. As a result, anything assigned to Person.prototype becomes available to all instances of that constructor via the this object. So it's easy to add extra methods to existing objects at runtime: var s = new Person ( 'Simon' , 'Willison' ); s . firstNameCaps (); // TypeError on line 1: s.firstNameCaps is not a function Person . prototype . firstNameCaps = function () { return this . first . toUpperCase (); }; s . firstNameCaps (); // \"SIMON\"", "title": "Constructor functions"}, {"location": "coding/javascript/javascript/#split-code-for-readability", "text": "To split a line into several, parentheses may be used to avoid the insertion of semicolons. renderSquare ( i ) { return ( < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ); }", "title": "Split code for readability"}, {"location": "coding/javascript/javascript/#coalescent-operator", "text": "Is similar to the Logical OR operator ( || ), except instead of relying on truthy/falsy values, it relies on \"nullish\" values (there are only 2 nullish values, null and undefined ). This means it's safer to use when you treat falsy values like 0 as valid. Similar to Logical OR , it functions as a control-flow operator; it evaluates to the first not-nullish value. It was introduced in Chrome 80 / Firefox 72 / Safari 13.1. It has no IE support. console . log ( 4 ?? 5 ); // 4, since neither value is nullish console . log ( null ?? 10 ); // 10, since 'null' is nullish console . log ( undefined ?? 0 ); // 0, since 'undefined' is nullish // Here's a case where it differs from // Logical OR (||): console . log ( 0 ?? 5 ); // 0 console . log ( 0 || 5 ); // 5", "title": "Coalescent operator"}, {"location": "coding/javascript/javascript/#interacting-with-html", "text": "You can find HTML elements with the next document properties: Property Description document.anchors Returns all <a> elements that have a name attribute document.baseURI Returns the absolute base URI of the document document.body Returns the <body> element document.cookie Returns the document's cookie document.doctype Returns the document's doctype document.documentElement Returns the <html> element document.documentMode Returns the mode used by the browser document.documentURI Returns the URI of the document document.domain Returns the domain name of the document server document.embeds Returns all <embed> elements document.forms Returns all <form> elements document.head Returns the <head> element document.images Returns all <img> elements document.implementation Returns the DOM implementation document.inputEncoding Returns the document's encoding (character set) document.lastModified Returns the date and time the document was updated document.links Returns all <area> and <a> elements that have a href attribute document.readyState Returns the (loading) status of the document document.referrer Returns the URI of the referrer (the linking document) document.scripts Returns all <script> elements document.strictErrorChecking Returns if error checking is enforced document.title Returns the <title> element document.URL Returns the complete URL of the document", "title": "Interacting with HTML"}, {"location": "coding/javascript/javascript/#how-to-add-javascript-to-html", "text": "In HTML, JavaScript code is inserted between <script> and </script> tags. < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; </ script > That will be run as the page is loaded. Scripts can be placed in the <body> , or in the <head> section of an HTML page, or in both. Note \"Placing scripts at the bottom of the <body> element improves the display speed, because script interpretation slows down the display.\" A JavaScript function is a block of JavaScript code, that can be executed when \"called\" for. For example, a function can be called when an event occurs, like when the user clicks a button.", "title": "How to add JavaScript to HTML"}, {"location": "coding/javascript/javascript/#external-javascript", "text": "Scripts can also be placed in external files: File: myScript.js function myFunction () { document . getElementById ( \"demo\" ). innerHTML = \"Paragraph changed.\" ; } External scripts are practical when the same code is used in many different web pages. To use an external script, put the name of the script file in the src (source) attribute of a <script> tag: < script src = \"myScript.js\" ></ script > Placing scripts in external files has some advantages: It separates HTML and code. It makes HTML and JavaScript easier to read and maintain. Cached JavaScript files can speed up page loads.", "title": "External JavaScript"}, {"location": "coding/javascript/javascript/#html-content", "text": "One of many JavaScript HTML methods is getElementById() . The example below \"finds\" an HTML element (with id=\"demo\" ), and changes the element content ( innerHTML ) to \"Hello JavaScript\" : document . getElementById ( \"demo\" ). innerHTML = \"Hello JavaScript\" ; It will transform: < p id = \"demo\" > JavaScript can change HTML content. </ p > Into: < p id = \"demo\" > Hello JavaScript </ p > You can also use getElementsByTagName or getElementsByClassName . Or if you want to find all HTML elements that match a specified CSS selector (id, class names, types, attributes, values of attributes, etc), use the querySelectorAll() method. This example returns a list of all <p> elements with class=\"intro\" . const x = document . querySelectorAll ( \"p.intro\" );", "title": "HTML content"}, {"location": "coding/javascript/javascript/#html-attributes", "text": "JavaScript can also change HTML attribute values. In this example JavaScript changes the value of the src (source) attribute of an <img> tag: < button onclick = \"document.getElementById('myImage').src='pic_bulbon.gif'\" > Turn on the light </ button > < img id = \"myImage\" src = \"pic_bulboff.gif\" style = \"width:100px\" > < button onclick = \"document.getElementById('myImage').src='pic_bulboff.gif'\" > Turn off the light </ button > Other attribute methods are: Method Description document.createElement(element) Create an HTML element document.removeChild(element) Remove an HTML element document.appendChild(element) Add an HTML element document.replaceChild(new, old) Replace an HTML element", "title": "HTML attributes"}, {"location": "coding/javascript/javascript/#css", "text": "Changing the style of an HTML element, is a variant of changing an HTML attribute: document . getElementById ( \"demo\" ). style . fontSize = \"35px\" ;", "title": "CSS"}, {"location": "coding/javascript/javascript/#hiding-or-showing-html-elements", "text": "Hiding HTML elements can be done by changing the display style: document . getElementById ( \"demo\" ). style . display = \"none\" ; Showing hidden HTML elements can also be done by changing the display style: document . getElementById ( \"demo\" ). style . display = \"block\" ;", "title": "Hiding or showing HTML elements"}, {"location": "coding/javascript/javascript/#displaying-data", "text": "JavaScript can \"display\" data in different ways: Writing into an HTML element, using innerHTML . Writing into the HTML output using document.write() . < h1 > My First Web Page </ h1 > < p > My first paragraph. </ p > < script > document . write ( 5 + 6 ); </ script > Using document.write() after an HTML document is loaded, will delete all existing HTML. Writing into an alert box, using window.alert() . Writing into the browser console, using console.log() . Useful for debugging.", "title": "Displaying data"}, {"location": "coding/javascript/javascript/#events", "text": "An HTML event can be something the browser does, or something a user does. Here are some examples of HTML events: An HTML web page has finished loading An HTML input field was changed An HTML button was clicked Often, when events happen, you may want to do something. JavaScript lets you execute code when events are detected. HTML allows event handler attributes, with JavaScript code, to be added to HTML elements. < element event = 'some JavaScript' > In the following example, an onclick attribute (with code), is added to a <button> element: < button onclick = \"document.getElementById('demo').innerHTML = Date()\" > The time is? </ button > In the example above, the JavaScript code changes the content of the element with id=\"demo\" . In the next example, the code changes the content of its own element (using this.innerHTML ): < button onclick = \"this.innerHTML = Date()\" > The time is? </ button > JavaScript code is often several lines long. It is more common to see event attributes calling functions: < button onclick = \"displayDate()\" > The time is? </ button >", "title": "Events"}, {"location": "coding/javascript/javascript/#common-html-events", "text": "Here is a list of some common HTML events: Event Description onchange An HTML element has been changed onclick The user clicks an HTML element onmouseover The user moves the mouse over an HTML element onmouseout The user moves the mouse away from an HTML element onmousedown The user is pressing the click button onmouseup The user is releasing the click button onkeydown The user pushes a keyboard key onload The browser has finished loading the page", "title": "Common HTML Events"}, {"location": "coding/javascript/javascript/#event-handlers", "text": "Event handlers can be used to handle and verify user input, user actions, and browser actions: Things that should be done every time a page loads Things that should be done when the page is closed Action that should be performed when a user clicks a button Content that should be verified when a user inputs data Many different methods can be used to let JavaScript work with events: HTML event attributes can execute JavaScript code directly HTML event attributes can call JavaScript functions You can assign your own event handler functions to HTML elements You can prevent events from being sent or being handled", "title": "Event handlers"}, {"location": "coding/javascript/javascript/#json-support", "text": "JSON is a format for storing and transporting data. A common use of JSON is to read data from a web server, and display the data in a web page. For simplicity, this can be demonstrated using a string as input. First, create a JavaScript string containing JSON syntax: let text = '{ \"employees\" : [' + '{ \"firstName\":\"John\" , \"lastName\":\"Doe\" },' + '{ \"firstName\":\"Anna\" , \"lastName\":\"Smith\" },' + '{ \"firstName\":\"Peter\" , \"lastName\":\"Jones\" } ]}' ; Then, use the JavaScript built-in function JSON.parse() to convert the string into a JavaScript object: const obj = JSON . parse ( text ); Finally, use the new JavaScript object in your page: < p id = \"demo\" ></ p > < script > document . getElementById ( \"demo\" ). innerHTML = obj . employees [ 1 ]. firstName + \" \" + obj . employees [ 1 ]. lastName ; </ script >", "title": "JSON support"}, {"location": "coding/javascript/javascript/#async-javascript", "text": "", "title": "Async JavaScript"}, {"location": "coding/javascript/javascript/#timing-events", "text": "The window object allows execution of code at specified time intervals. These time intervals are called timing events. The two key methods to use with JavaScript are: setTimeout(function, milliseconds) : Executes a function, after waiting a specified number of milliseconds . setInterval(function, milliseconds) : Same as setTimeout() , but repeats the execution of the function continuously. For example to click a button. Wait 3 seconds, and the page will alert \"Hello\": < button onclick = \"setTimeout(myFunction, 3000)\" > Try it </ button > < script > function myFunction () { alert ( 'Hello' ); } </ script > The window.clearTimeout(timeoutVariable) method stops the execution of the function specified in setTimeout() . myVar = setTimeout ( function , milliseconds ); clearTimeout ( myVar ); To stop a setInterval use the clearInterval method.", "title": "Timing events"}, {"location": "coding/javascript/javascript/#promises", "text": "A JavaScript Promise object contains both the producing code and calls to the consuming code, where: \"Producing code\" is code that can take some time \"Consuming code\" is code that must wait for the result The syntax of a Promise is kind of difficult to understand, but bear with me: let myPromise = new Promise ( function ( myResolve , myReject ) { // \"Producing Code\" (May take some time) myResolve (); // when successful myReject (); // when error }); // \"Consuming Code\" (Must wait for a fulfilled Promise) myPromise . then ( function ( value ) { /* code if successful */ }, function ( error ) { /* code if some error */ } ); When the producing code obtains the result, it should call one of the two callbacks: Success then calls myResolve(result value) Error then calls myReject(error object) For example: function myDisplayer ( some ) { document . getElementById ( \"demo\" ). innerHTML = some ; } let myPromise = new Promise ( function ( myResolve , myReject ) { let x = 0 ; // The producing code (this may take some time) if ( x == 0 ) { myResolve ( \"OK\" ); } else { myReject ( \"Error\" ); } }); myPromise . then ( function ( value ) { myDisplayer ( value );}, function ( error ) { myDisplayer ( error );} );", "title": "Promises"}, {"location": "coding/javascript/javascript/#promise-object-properties", "text": "A JavaScript Promise object can be: Pending Fulfilled Rejected The Promise object supports two properties: state and result , which can't be accessed directly. While a Promise object is pending (working), the result is undefined . When a Promise object is fulfilled , the result is a value . When a Promise object is rejected , the result is an error object.", "title": "Promise object properties"}, {"location": "coding/javascript/javascript/#waiting-for-a-timeout-example", "text": "Example Using Callback: setTimeout ( function () { myFunction ( \"I love You !!!\" ); }, 3000 ); function myFunction ( value ) { document . getElementById ( \"demo\" ). innerHTML = value ; } Example Using Promise let myPromise = new Promise ( function ( myResolve , myReject ) { setTimeout ( function () { myResolve ( \"I love You !!\" ); }, 3000 ); }); myPromise . then ( function ( value ) { document . getElementById ( \"demo\" ). innerHTML = value ; });", "title": "Waiting for a timeout example"}, {"location": "coding/javascript/javascript/#asyncawait", "text": "async and await make promises easier to write. async makes a function return a Promise , while await makes a function wait for a Promise .", "title": "Async/Await"}, {"location": "coding/javascript/javascript/#async-syntax", "text": "For example async function myFunction () { return \"Hello\" ; } Is the same as: function myFunction () { return Promise . resolve ( \"Hello\" ); } Here is how to use the Promise: myFunction (). then ( function ( value ) { myDisplayer ( value );}, function ( error ) { myDisplayer ( error );} ); If you only expect a normal value, skip the function(error)... line.", "title": "Async syntax"}, {"location": "coding/javascript/javascript/#await-syntax", "text": "The keyword await before a function makes the function wait for a promise: let value = await promise ; The await keyword can only be used inside an async function. For example the next code will update the content of demo with I love you !! instantly: async function myDisplay () { let myPromise = new Promise ( function ( resolve , reject ) { resolve ( \"I love You !!\" ); }); document . getElementById ( \"demo\" ). innerHTML = await myPromise ; } myDisplay (); But it could have a timeout async function myDisplay () { let myPromise = new Promise ( function ( resolve ) { setTimeout ( function () { resolve ( \"I love You !!\" );}, 3000 ); }); document . getElementById ( \"demo\" ). innerHTML = await myPromise ; } myDisplay ();", "title": "Await syntax"}, {"location": "coding/javascript/javascript/#cookies", "text": "Cookies are data, stored in small text files, on your computer. They hold a very small amount of data at a maximum capacity of 4KB. When a web server has sent a web page to a browser, the connection is shut down, and the server forgets everything about the user. Cookies were invented to solve the problem \"how to remember information about the user\": When a user visits a web page, his/her name can be stored in a cookie. Next time the user visits the page, the cookie \"remembers\" his/her name. There are two types of cookies: persistent cookies and session cookies. Session cookies do not contain an expiration date. Instead, they are stored only as long as the browser or tab is open. As soon as the browser is closed, they are permanently lost. Persistent cookies do have an expiration date. These cookies are stored on the user\u2019s disk until the expiration date and then permanently deleted. Note \"Think if for your case ithe's better to use Web storage instead\"", "title": "Cookies"}, {"location": "coding/javascript/javascript/#setting-a-cookie", "text": "Cookies are saved in name-value pairs like: username = John Doe When a browser requests a web page from a server, cookies belonging to the page are added to the request. This way the server gets the necessary data to \"remember\" information about users. JavaScript can create, read, and delete cookies with the document.cookie property. With JavaScript, a cookie can be created like this: document . cookie = \"username=John Doe\" ; You can also add an expiry date (in UTC time). By default, the cookie is deleted when the browser is closed: document . cookie = \"username=John Doe; expires=Thu, 18 Dec 2013 12:00:00 UTC\" ; With a path parameter, you can tell the browser what path the cookie belongs to. By default, the cookie belongs to the current page. document . cookie = \"username=John Doe; expires=Thu, 18 Dec 2013 12:00:00 UTC; path=/\" ;", "title": "Setting a cookie"}, {"location": "coding/javascript/javascript/#reading-a-cookie", "text": "With JavaScript, cookies can be read like this: let x = document . cookie ; document.cookie will return all cookies in one string much like: cookie1=value; cookie2=value; cookie3=value; . It looks like a normal text string. But it is not. If you want to find the value of one specified cookie, you must write a JavaScript function that searches for the cookie value in the cookie string.", "title": "Reading a Cookie"}, {"location": "coding/javascript/javascript/#change-a-cookie-with-javascript", "text": "With JavaScript, you can change a cookie the same way as you create it: document . cookie = \"username=John Smith; expires=Thu, 18 Dec 2013 12:00:00 UTC; path=/\" ; The old cookie is overwritten.", "title": "Change a Cookie with JavaScript"}, {"location": "coding/javascript/javascript/#delete-a-cookie-with-javascript", "text": "Just set the expires parameter to a past date: document . cookie = \"username=; expires=Thu, 01 Jan 1970 00:00:00 UTC; path=/;\" ;", "title": "Delete a Cookie with JavaScript"}, {"location": "coding/javascript/javascript/#javascript-cookie-example", "text": "In the example to follow, we will create a cookie that stores the name of a visitor. The first time a visitor arrives to the web page, he/she will be asked to fill in his/her name. The name is then stored in a cookie. The next time the visitor arrives at the same page, he/she will get a welcome message.", "title": "Javascript cookie example"}, {"location": "coding/javascript/javascript/#function-to-set-a-cookie-value", "text": "function setCookie ( cname , cvalue , exdays ) { const d = new Date (); d . setTime ( d . getTime () + ( exdays * 24 * 60 * 60 * 1000 )); let expires = \"expires=\" + d . toUTCString (); document . cookie = cname + \"=\" + cvalue + \";\" + expires + \";path=/\" ; }", "title": "Function to set a cookie value"}, {"location": "coding/javascript/javascript/#function-to-get-a-cookie-value", "text": "function getCookie ( cname ) { let name = cname + \"=\" ; let decodedCookie = decodeURIComponent ( document . cookie ); let ca = decodedCookie . split ( ';' ); for ( let i = 0 ; i < ca . length ; i ++ ) { let c = ca [ i ]; while ( c . charAt ( 0 ) == ' ' ) { c = c . substring ( 1 ); } if ( c . indexOf ( name ) == 0 ) { return c . substring ( name . length , c . length ); } } return \"\" ; } Where: The function take the cookiename as parameter cname . Creates a variable name with the text to search for cname + \"=\" . Decodes the cookie string, to handle cookies with special characters like $ . Split document.cookie on semicolons into an array called ca . Loop through the ca array and read out each value c . If the cookie is found c.indexOf(name) == 0 , return the value of the cookie c.substring(name.length, c.length . If the cookie is not found, return \"\" .", "title": "Function to get a cookie value"}, {"location": "coding/javascript/javascript/#function-to-check-a-cookie-value", "text": "If the cookie is set it will display a greeting. If the cookie is not set, it will display a prompt box, asking for the name of the user, and stores the username cookie for 365 days, by calling the setCookie function: function checkCookie () { let username = getCookie ( \"username\" ); if ( username != \"\" ) { alert ( \"Welcome again \" + username ); } else { username = prompt ( \"Please enter your name:\" , \"\" ); if ( username != \"\" && username != null ) { setCookie ( \"username\" , username , 365 ); } } } The html code of the page would be: < body onload = \"checkCookie()\" ></ body >", "title": "Function to check a cookie value"}, {"location": "coding/javascript/javascript/#web-storage", "text": "The Web Storage API is a simple syntax for storing and retrieving data in the browser. There are two storage objects localStorage and sessionStorage . localStorage . setItem ( \"name\" , \"John Doe\" ); localStorage . getItem ( \"name\" ); The storage object properties and methods are: Property/Method Description key(n) Returns the name of the nth key in the storage length Returns the number of data items stored in the Storage object getItem(keyname) Returns the value of the specified key name setItem(keyname, value) Adds or updates the key to the storage removeItem(keyname) Removes that key from the storage clear() Empty all key out of the storage sessionStorage is identical to the localStorage but it only stores the data for one session, so the data will be deleted when the browser is closed. Introduced by HTML5, it has replaced many of the cookies uses. This is because LocalStorage has a lot of advantages over cookies. One of the most important differences is that unlike with cookies, data does not have to be sent back and forth with every HTTP request. This reduces the overall traffic between the client and the server and the amount of wasted bandwidth. This is because data is stored on the user\u2019s local disk and is not destroyed or cleared by the loss of an internet connection. Also, LocalStorage can hold up to 5MB of information which is a whole lot more than the 4KB that cookies hold. LocalStorage behaves more like persistent cookies in terms of expiration. Data is not automatically destroyed unless it is cleared through Javascript code. This can be good for larger bits of data that need to be stored for longer periods of time. Also, with LocalStorage you can not only store strings but also Javascript primitives and objects. An example of a good use of LocalStorage might be in an application used in regions without a persistent internet connection. In order for this to be a good use of LocalStorage , the threat level of the data stored in this situation would have to be very low. To protect client privacy, it would be good to upload the data when connection is re-established and then delete the locally stored version. In conclusion, Cookies are smaller and send server information back with every HTTP request, while LocalStorage is larger and can hold information on the client side.", "title": "Web Storage"}, {"location": "coding/javascript/javascript/#web-workers", "text": "When executing scripts in an HTML page, the page becomes unresponsive until the script is finished. A web worker is a JavaScript that runs in the background, independently of other scripts, without affecting the performance of the page. You can continue to do whatever you want: clicking, selecting things, etc., while the web worker runs in the background. Since web workers are in external files, they do not have access to the following JavaScript objects: The window object The document object The parent object Before creating a web worker check whether the user's browser supports it: if ( typeof ( Worker ) !== \"undefined\" ) { // Yes! Web worker support! // Some code..... } else { // Sorry! No Web Worker support.. } Now, let's create our web worker in an external JavaScript. Here, we create a script that counts. The script is stored in the demo_workers.js file: let i = 0 ; function timedCount () { i ++ ; postMessage ( i ); setTimeout ( \"timedCount()\" , 500 ); } timedCount (); The important part of the code above is the postMessage() method, which is used to post a message back to the HTML page.", "title": "Web workers"}, {"location": "coding/javascript/javascript/#create-a-web-worker-object", "text": "Now that we have the web worker file, we need to call it from an HTML page. The following lines checks if the worker already exists, if not it creates a new web worker object and runs the code in demo_workers.js : if ( typeof ( w ) == \"undefined\" ) { w = new Worker ( \"demo_workers.js\" ); } Then we can send and receive messages from the web worker. Add an onmessage event listener to the web worker. w . onmessage = function ( event ){ document . getElementById ( \"result\" ). innerHTML = event . data ; }; When the web worker posts a message, the code within the event listener is executed. The data from the web worker is stored in event.data .", "title": "Create a Web Worker object"}, {"location": "coding/javascript/javascript/#terminate-a-web-worker", "text": "When a web worker object is created, it will continue to listen for messages (even after the external script is finished) until it is terminated. To terminate a web worker, and free browser/computer resources, use the terminate() method: w . terminate ();", "title": "Terminate a Web Worker"}, {"location": "coding/javascript/javascript/#reuse-the-web-worker", "text": "If you set the worker variable to undefined , after it has been terminated, you can reuse the code: w = undefined ;", "title": "Reuse the Web Worker"}, {"location": "coding/javascript/javascript/#full-web-worker-example-code", "text": "<!DOCTYPE html> < html > < body > < p > Count numbers: < output id = \"result\" ></ output ></ p > < button onclick = \"startWorker()\" > Start Worker </ button > < button onclick = \"stopWorker()\" > Stop Worker </ button > < script > let w ; function startWorker () { if ( typeof ( w ) == \"undefined\" ) { w = new Worker ( \"demo_workers.js\" ); } w . onmessage = function ( event ) { document . getElementById ( \"result\" ). innerHTML = event . data ; }; } function stopWorker () { w . terminate (); w = undefined ; } </ script > </ body > </ html >", "title": "Full web worker example code"}, {"location": "coding/javascript/javascript/#interacting-with-external-apis", "text": "The Fetch API interface allows web browser to make HTTP requests to web servers. The example below fetches a file and displays the content: < body > < p id = \"demo\" > Fetch a file to change this text. </ p > < script > let file = \"fetch_info.txt\" fetch ( file ) . then ( x => x . text ()) . then ( y => document . getElementById ( \"demo\" ). innerHTML = y ); </ script > </ body > Where the content of fetch_info.txt is: < h1 > Fetch API </ h1 > < p > The Fetch API interface allows web browser to make HTTP requests to web servers. </ p > Since Fetch is based on async and await , the example above might be easier to understand like this: async function getText ( file ) { let x = await fetch ( file ); let y = await x . text (); myDisplay ( y ); } Or even better: Use understandable names instead of x and y : async function getText ( file ) { let myObject = await fetch ( file ); let myText = await myObject . text (); myDisplay ( myText ); } The first .then() resolves the promise into a response object. To be able to view the content of this object the response is then converted using a .json() method. This json() also returns a promise so another .then() is necessary.", "title": "Interacting with external APIs"}, {"location": "coding/javascript/javascript/#doing-a-post", "text": "We can first include the settings such as header and request method in a object. var jsonObj = {}; jsonObj . firstParam = \"first\" ; jsonObj . secondParam = 2 ; jsonObj . thirdParam = true ; var options = { method : 'POST' , header : new Headers ({ \"Content-Type\" : \"application/json\" , }), body : JSON . stringify ( jsonObj ) } var url = http : //localhost:8080/postRequest; fetch ( url , options ) . then (( response ) => { console . log ( \"Status Code\" , response . status ); //return response type such as json, blob, text, formData and arrayBuffer return response . json () }) . then (( result ) => { //here will return whatever information from the response. console . log ( \"response message from backend\" , result ); }) . catch (( error ) => { console . log ( error ); });", "title": "Doing a POST"}, {"location": "coding/javascript/javascript/#error-handling", "text": "The try statement defines a code block to run (to try). The catch statement defines a code block to handle any error. The finally statement defines a code block to run regardless of the result. The throw statement defines a custom error. throw \"Too big\"; // throw a text throw 500; // throw a number In this example we misspelled alert as adddlert to deliberately produce an error: < p id = \"demo\" ></ p > < script > try { adddlert ( \"Welcome guest!\" ); } catch ( err ) { document . getElementById ( \"demo\" ). innerHTML = err . message ; } </ script > When an error occurs, JavaScript will normally stop and generate an error message. JavaScript will create an Error object with two properties: name and message . Six different values can be returned by the error name property: Error Name Description EvalError An error has occurred in the eval() function RangeError A number \"out of range\" has occurred ReferenceError An illegal reference has occurred SyntaxError A syntax error has occurred TypeError A type error has occurred URIError An error in encodeURI() has occurred", "title": "Error handling"}, {"location": "coding/javascript/javascript/#input-validation-example", "text": "This example examines input. If the value is wrong, an exception ( err ) is thrown. The exception is caught by the catch statement and a custom error message is displayed: < p > Please input a number between 5 and 10: </ p > < input id = \"demo\" type = \"text\" > < button type = \"button\" onclick = \"myFunction()\" > Test Input </ button > < p id = \"p01\" ></ p > < script > function myFunction () { const message = document . getElementById ( \"p01\" ); message . innerHTML = \"\" ; let x = document . getElementById ( \"demo\" ). value ; try { if ( x == \"\" ) throw \"empty\" ; if ( isNaN ( x )) throw \"not a number\" ; x = Number ( x ); if ( x < 5 ) throw \"too low\" ; if ( x > 10 ) throw \"too high\" ; } catch ( err ) { message . innerHTML = \"Input is \" + err ; } } </ script >", "title": "Input validation example"}, {"location": "coding/javascript/javascript/#debugging", "text": "You can use the console.log to display JavaScript values in the debugger window: a = 5 ; b = 6 ; c = a + b ; console . log ( c ); Or you can use breakpoints let x = 15 * 5 ; debugger ; document . getElementById ( \"demo\" ). innerHTML = x ;", "title": "Debugging"}, {"location": "coding/javascript/javascript/#style-guide", "text": "Always put spaces around operators ( = + - * / ), and after commas. Always use 2 spaces for indentation of code blocks. Avoid lines longer than 80 characters. Always end a simple statement with a semicolon. const cars = [ \"Volvo\" , \"Saab\" , \"Fiat\" ]; const person = { firstName : \"John\" , lastName : \"Doe\" , age : 50 , eyeColor : \"blue\" }; General rules for complex (compound) statements: Put the opening bracket at the end of the first line. Use one space before the opening bracket. Put the closing bracket on a new line, without leading spaces. Do not end a complex statement with a semicolon. function toCelsius ( fahrenheit ) { return ( 5 / 9 ) * ( fahrenheit - 32 ); } for ( let i = 0 ; i < 5 ; i ++ ) { x += i ; } if ( time < 20 ) { greeting = \"Good day\" ; } else { greeting = \"Good evening\" ; } General rules for object definitions: Place the opening bracket on the same line as the object name. Use colon plus one space between each property and its value. Use quotes around string values, not around numeric values. Do not add a comma after the last property-value pair. Place the closing bracket on a new line, without leading spaces. Always end an object definition with a semicolon. const person = { firstName : \"John\" , lastName : \"Doe\" , age : 50 , eyeColor : \"blue\" };", "title": "Style guide"}, {"location": "coding/javascript/javascript/#best-practices", "text": "Minimize the use of global variables. All variables used in a function should be declared as local variables. It is a good coding practice to put all declarations at the top of each script or function. This will: Give cleaner code Provide a single place to look for local variables Make it easier to avoid unwanted (implied) global variables Reduce the possibility of unwanted re-declarations Initialize variables when you declare them. This will: Give cleaner code Provide a single place to initialize variables Avoid undefined values Declaring objects and arrays with const will prevent any accidental change of type. Don't use the new Object() Use \"\" instead of new String() . Use 0 instead of new Number() . Use false instead of new Boolean() . Use {} instead of new Object() . Use [] instead of new Array() . Use /()/ instead of new RegExp() . Use function (){} instead of new Function() . Use === for comparison. The == comparison operator always converts (to matching types) before comparison. Use parameter defaults. If a function is called with a missing argument, the value of the missing argument is set to undefined . Undefined values can break your code. It is a good habit to assign default values to arguments. function myFunction ( x , y ) { if ( y === undefined ) { y = 0 ; } } * Avoid using eval() . It's used to run text as code which represents a security problem. Reduce DOM access. Accessing the HTML DOM is very slow, compared to other JavaScript statements. If you expect to access a DOM element several times, access it once, and use it as a local variable: const obj = document . getElementById ( \"demo\" ); obj . innerHTML = \"Hello\" ; * Keep the number of elements in the HTML DOM small. This will always improve page loading, and speed up rendering (page display), especially on smaller devices. Every attempt to search the DOM (like getElementsByTagName ) will benefit from a smaller DOM. Delay JavaScript loading. Putting your scripts at the bottom of the page body lets the browser load the page first. While a script is downloading, the browser will not start any other downloads. In addition all parsing and rendering activity might be blocked. Avoid using the with keyword. It has a negative effect on speed. It also clutters up JavaScript scopes.", "title": "Best practices"}, {"location": "coding/javascript/javascript/#references", "text": "W3 JavaScript tutorial John Comeau operator explainer Re-introduction to JavaScript Chikwekwe's articles on cookies vs LocalStorage Jeff's post on xmlhttprequest vs Fetch API", "title": "References"}, {"location": "coding/json/json/", "text": "JavaScript Object Notation (JSON) , is an open standard file format, and data interchange format, that uses human-readable text to store and send data objects consisting of attribute\u2013value pairs and array data types (or any other serializable value). Linters and fixers \u2691 jsonlint \u2691 jsonlint is a pure JavaScript version of the service provided at jsonlint.com. Install it with: npm install jsonlint -g Vim supports this linter through ALE . jq \u2691 jq is like sed for JSON data. You can use it to slice, filter, map and transform structured data with the same ease that sed , awk , grep and friends let you play with text. Install it with: apt-get install jq Vim supports this linter through ALE .", "title": "JSON"}, {"location": "coding/json/json/#linters-and-fixers", "text": "", "title": "Linters and fixers"}, {"location": "coding/json/json/#jsonlint", "text": "jsonlint is a pure JavaScript version of the service provided at jsonlint.com. Install it with: npm install jsonlint -g Vim supports this linter through ALE .", "title": "jsonlint"}, {"location": "coding/json/json/#jq", "text": "jq is like sed for JSON data. You can use it to slice, filter, map and transform structured data with the same ease that sed , awk , grep and friends let you play with text. Install it with: apt-get install jq Vim supports this linter through ALE .", "title": "jq"}, {"location": "coding/promql/promql/", "text": "Snippets \u2691 Generating range vectors from return values in Prometheus queries \u2691 Use the subquery-syntax Warning: These subqueries are expensive, i.e. create very high load on Prometheus. Use recording-rules when you use these queries regularly. Subquery syntax \u2691 <instant_query>[<range>:<resolution>] instant_query : A PromQL-function which returns an instant-vector). range : Offset (back in time) to start the first subquery. resolution : The size of each of the subqueries. It returns a range-vector. For example: deriv ( rate ( varnish_main_client_req [ 2m ] ) [ 5m : 10s ] ) In the example above, Prometheus runs rate() (= instant_query ) 30 times (the first from 5 minutes ago to -4:50, ..., the last -0:10 to now). The resulting range-vector is input to the deriv() function. Links \u2691 Prometheus cheatsheet", "title": "Promql"}, {"location": "coding/promql/promql/#snippets", "text": "", "title": "Snippets"}, {"location": "coding/promql/promql/#generating-range-vectors-from-return-values-in-prometheus-queries", "text": "Use the subquery-syntax Warning: These subqueries are expensive, i.e. create very high load on Prometheus. Use recording-rules when you use these queries regularly.", "title": "Generating range vectors from return values in Prometheus queries"}, {"location": "coding/promql/promql/#subquery-syntax", "text": "<instant_query>[<range>:<resolution>] instant_query : A PromQL-function which returns an instant-vector). range : Offset (back in time) to start the first subquery. resolution : The size of each of the subqueries. It returns a range-vector. For example: deriv ( rate ( varnish_main_client_req [ 2m ] ) [ 5m : 10s ] ) In the example above, Prometheus runs rate() (= instant_query ) 30 times (the first from 5 minutes ago to -4:50, ..., the last -0:10 to now). The resulting range-vector is input to the deriv() function.", "title": "Subquery syntax"}, {"location": "coding/promql/promql/#links", "text": "Prometheus cheatsheet", "title": "Links"}, {"location": "coding/python/alembic/", "text": "Alembic is a lightweight database migration tool for SQLAlchemy. It is created by the author of SQLAlchemy and it has become the de-facto standard tool to perform migrations on SQLAlchemy backed databases. I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions . Database Migration in SQLAlchemy \u2691 A database migration usually changes the schema of a database, such as adding a column or a constraint, adding a table or updating a table. It's often performed using raw SQL wrapped in a transaction so that it can be rolled back if something went wrong during the migration. To migrate a SQLAlchemy database, an Alembic migration script is created for the intended migration, perform the migration, update the model definition and then start using the database under the migrated schema. Alembic repository initialization \u2691 It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations It will create several files and directories under the selected path, the most important are: alembic.ini : It's the file the alembic script will look for when invoked. Usually it's located at the root of the program. Although there are several options to configure here, we'll use the env.py file to define how to access the database. env.py : It is a Python script that is run whenever the alembic migration tool is invoked. At the very least, it contains instructions to configure and generate a SQLAlchemy engine, procure a connection from that engine along with a transaction, and then invoke the migration engine, using the connection as a source of database connectivity. The env.py script is part of the generated environment so that the way migrations run is entirely customizable. The exact specifics of how to connect are here, as well as the specifics of how the migration environment are invoked. The script can be modified so that multiple engines can be operated upon, custom arguments can be passed into the migration environment, application-specific libraries and models can be loaded in and made available. By default alembic takes the database url by the sqlalchemy.url key in the alembic.ini file. But it's advisable to be able to define the url from environmental variables. Therefore we need to modify this file with the following changes: # from sqlalchemy import engine_from_config # from sqlalchemy import pool from sqlalchemy import create_engine import os if config . attributes . get ( \"configure_logger\" , True ): fileConfig ( config . config_file_name ) def get_url (): basedir = '~/.local/share/{{ program_name }}' return os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or \\ 'sqlite:///' + os . path . join ( os . path . expanduser ( basedir ), 'main.db' ) def run_migrations_offline (): \"\"\"Run migrations in 'offline' mode. This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well. By skipping the Engine creation we don't even need a DBAPI to be available. Calls to context.execute() here emit the given string to the script output. \"\"\" # url = config.get_main_option(\"sqlalchemy.url\") url = get_url () context . configure ( url = url , target_metadata = target_metadata , literal_binds = True , dialect_opts = { \"paramstyle\" : \"named\" }, ) with context . begin_transaction (): context . run_migrations () def run_migrations_online (): \"\"\"Run migrations in 'online' mode. In this scenario we need to create an Engine and associate a connection with the context. \"\"\" # connectable = engine_from_config( # config.get_section(config.config_ini_section), # prefix=\"sqlalchemy.\", # poolclass=pool.NullPool, # ) connectable = create_engine ( get_url ()) # Leave the rest of the file as it is It is also necessary to import your models metadata, to do so, modify: # add your model's MetaData object here # for 'autogenerate' support # from myapp import mymodel # target_metadata = mymodel.Base.metadata # target_metadata = None import sys sys . path = [ '' , '..' ] + sys . path [ 1 :] from {{ program_name }} import models target_metadata = models . Base . metadata We had to add the parent directory to the sys.path because when env.py is executed, models is not in your PYTHONPATH , resulting in an import error . versions/ : Directory that holds the individual version scripts. The files it contains don\u2019t use ascending integers, and instead use a partial GUID approach. In Alembic, the ordering of version scripts is relative to directives within the scripts themselves, and it is theoretically possible to \u201csplice\u201d version files in between others, allowing migration sequences from different branches to be merged, albeit carefully by hand. Database Migration \u2691 When changes need to be made in the schema, instead of modifying it directly and then recreate the database from scratch, we can leverage that actions to alembic. alembic revision --autogenerate -m \"{{ commit_comment }}\" That command will write a migration script to make the changes. To perform the migration use: alembic upgrade head To check the status, execute: alembic current To load the migrations from the alembic library inside a python program, the best way to do it is through alembic.command instead of alembic.config.main because it will redirect all logging output to a file from alembic.config import Config import alembic.command config = Config ( 'alembic.ini' ) config . attributes [ 'configure_logger' ] = False alembic . command . upgrade ( config , 'head' ) File: env.py if config . attributes . get ( 'configure_logger' , True ): fileConfig ( config . config_file_name ) Seed database with data \u2691 Note This is an alembic script from datetime import date from sqlalchemy.sql import table , column from sqlalchemy import String , Integer , Date from alembic import op # Create an ad-hoc table to use for the insert statement. accounts_table = table ( 'account' , column ( 'id' , Integer ), column ( 'name' , String ), column ( 'create_date' , Date ) ) op . bulk_insert ( accounts_table , [ { 'id' : 1 , 'name' : 'John Smith' , 'create_date' : date ( 2010 , 10 , 5 )}, { 'id' : 2 , 'name' : 'Ed Williams' , 'create_date' : date ( 2007 , 5 , 27 )}, { 'id' : 3 , 'name' : 'Wendy Jones' , 'create_date' : date ( 2008 , 8 , 15 )}, ] ) Database downgrade or rollback \u2691 If you want to correct a migration first check the history to see where do you want to go (it accepts --verbose for more information): alembic history Then you can specify the id of the revision you want to downgrade to. To specify the last one, use -1 . alembic downgrade -1 After the downgrade is performed, if you want to remove the last revision, you'd need to manually remove the revision python file. References \u2691 Git Docs Articles \u2691 Migrate SQLAlchemy databases with Alembic", "title": "Alembic"}, {"location": "coding/python/alembic/#database-migration-in-sqlalchemy", "text": "A database migration usually changes the schema of a database, such as adding a column or a constraint, adding a table or updating a table. It's often performed using raw SQL wrapped in a transaction so that it can be rolled back if something went wrong during the migration. To migrate a SQLAlchemy database, an Alembic migration script is created for the intended migration, perform the migration, update the model definition and then start using the database under the migrated schema.", "title": "Database Migration in SQLAlchemy"}, {"location": "coding/python/alembic/#alembic-repository-initialization", "text": "It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations It will create several files and directories under the selected path, the most important are: alembic.ini : It's the file the alembic script will look for when invoked. Usually it's located at the root of the program. Although there are several options to configure here, we'll use the env.py file to define how to access the database. env.py : It is a Python script that is run whenever the alembic migration tool is invoked. At the very least, it contains instructions to configure and generate a SQLAlchemy engine, procure a connection from that engine along with a transaction, and then invoke the migration engine, using the connection as a source of database connectivity. The env.py script is part of the generated environment so that the way migrations run is entirely customizable. The exact specifics of how to connect are here, as well as the specifics of how the migration environment are invoked. The script can be modified so that multiple engines can be operated upon, custom arguments can be passed into the migration environment, application-specific libraries and models can be loaded in and made available. By default alembic takes the database url by the sqlalchemy.url key in the alembic.ini file. But it's advisable to be able to define the url from environmental variables. Therefore we need to modify this file with the following changes: # from sqlalchemy import engine_from_config # from sqlalchemy import pool from sqlalchemy import create_engine import os if config . attributes . get ( \"configure_logger\" , True ): fileConfig ( config . config_file_name ) def get_url (): basedir = '~/.local/share/{{ program_name }}' return os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or \\ 'sqlite:///' + os . path . join ( os . path . expanduser ( basedir ), 'main.db' ) def run_migrations_offline (): \"\"\"Run migrations in 'offline' mode. This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well. By skipping the Engine creation we don't even need a DBAPI to be available. Calls to context.execute() here emit the given string to the script output. \"\"\" # url = config.get_main_option(\"sqlalchemy.url\") url = get_url () context . configure ( url = url , target_metadata = target_metadata , literal_binds = True , dialect_opts = { \"paramstyle\" : \"named\" }, ) with context . begin_transaction (): context . run_migrations () def run_migrations_online (): \"\"\"Run migrations in 'online' mode. In this scenario we need to create an Engine and associate a connection with the context. \"\"\" # connectable = engine_from_config( # config.get_section(config.config_ini_section), # prefix=\"sqlalchemy.\", # poolclass=pool.NullPool, # ) connectable = create_engine ( get_url ()) # Leave the rest of the file as it is It is also necessary to import your models metadata, to do so, modify: # add your model's MetaData object here # for 'autogenerate' support # from myapp import mymodel # target_metadata = mymodel.Base.metadata # target_metadata = None import sys sys . path = [ '' , '..' ] + sys . path [ 1 :] from {{ program_name }} import models target_metadata = models . Base . metadata We had to add the parent directory to the sys.path because when env.py is executed, models is not in your PYTHONPATH , resulting in an import error . versions/ : Directory that holds the individual version scripts. The files it contains don\u2019t use ascending integers, and instead use a partial GUID approach. In Alembic, the ordering of version scripts is relative to directives within the scripts themselves, and it is theoretically possible to \u201csplice\u201d version files in between others, allowing migration sequences from different branches to be merged, albeit carefully by hand.", "title": "Alembic repository initialization"}, {"location": "coding/python/alembic/#database-migration", "text": "When changes need to be made in the schema, instead of modifying it directly and then recreate the database from scratch, we can leverage that actions to alembic. alembic revision --autogenerate -m \"{{ commit_comment }}\" That command will write a migration script to make the changes. To perform the migration use: alembic upgrade head To check the status, execute: alembic current To load the migrations from the alembic library inside a python program, the best way to do it is through alembic.command instead of alembic.config.main because it will redirect all logging output to a file from alembic.config import Config import alembic.command config = Config ( 'alembic.ini' ) config . attributes [ 'configure_logger' ] = False alembic . command . upgrade ( config , 'head' ) File: env.py if config . attributes . get ( 'configure_logger' , True ): fileConfig ( config . config_file_name )", "title": "Database Migration"}, {"location": "coding/python/alembic/#seed-database-with-data", "text": "Note This is an alembic script from datetime import date from sqlalchemy.sql import table , column from sqlalchemy import String , Integer , Date from alembic import op # Create an ad-hoc table to use for the insert statement. accounts_table = table ( 'account' , column ( 'id' , Integer ), column ( 'name' , String ), column ( 'create_date' , Date ) ) op . bulk_insert ( accounts_table , [ { 'id' : 1 , 'name' : 'John Smith' , 'create_date' : date ( 2010 , 10 , 5 )}, { 'id' : 2 , 'name' : 'Ed Williams' , 'create_date' : date ( 2007 , 5 , 27 )}, { 'id' : 3 , 'name' : 'Wendy Jones' , 'create_date' : date ( 2008 , 8 , 15 )}, ] )", "title": "Seed database with data"}, {"location": "coding/python/alembic/#database-downgrade-or-rollback", "text": "If you want to correct a migration first check the history to see where do you want to go (it accepts --verbose for more information): alembic history Then you can specify the id of the revision you want to downgrade to. To specify the last one, use -1 . alembic downgrade -1 After the downgrade is performed, if you want to remove the last revision, you'd need to manually remove the revision python file.", "title": "Database downgrade or rollback"}, {"location": "coding/python/alembic/#references", "text": "Git Docs", "title": "References"}, {"location": "coding/python/alembic/#articles", "text": "Migrate SQLAlchemy databases with Alembic", "title": "Articles"}, {"location": "coding/python/click/", "text": "Click is a Python package for creating beautiful command line interfaces in a composable way with as little code as necessary. It\u2019s the \u201cCommand Line Interface Creation Kit\u201d. It\u2019s highly configurable but comes with sensible defaults out of the box. Click has the following features: Arbitrary nesting of commands. Automatic help page generation. Supports lazy loading of subcommands at runtime. Supports implementation of Unix/POSIX command line conventions. Supports loading values from environment variables out of the box. Support for prompting of custom values. Supports file handling out of the box. Comes with useful common helpers (getting terminal dimensions, ANSI colors, fetching direct keyboard input, screen clearing, finding config paths, launching apps and editors). Setuptools Integration \u2691 To bundle your script with setuptools, all you need is the script in a Python package and a setup.py file. Let\u2019s assume your directory structure changed to this: project/ yourpackage/ __init__.py main.py utils.py scripts/ __init__.py yourscript.py setup.py In this case instead of using py_modules in your setup.py file you can use packages and the automatic package finding support of setuptools. In addition to that it\u2019s also recommended to include other package data. These would be the modified contents of setup.py: from setuptools import setup , find_packages setup ( name = \"yourpackage\" , version = \"0.1.0\" , packages = find_packages (), include_package_data = True , install_requires = [ \"Click\" , ], entry_points = { \"console_scripts\" : [ \"yourscript = yourpackage.scripts.yourscript:cli\" , ], }, ) Testing Click applications \u2691 For basic testing, Click provides the click.testing module which provides test functionality that helps you invoke command line applications and check their behavior. The basic functionality for testing Click applications is the CliRunner which can invoke commands as command line scripts. The CliRunner.invoke() method runs the command line script in isolation and captures the output as both bytes and binary data. The return value is a Result object, which has the captured output data, exit code, and optional exception attached: File: hello.py import click @click . command () @click . argument ( \"name\" ) def hello ( name ): click . echo ( \"Hello %s !\" % name ) File: test_hello.py : from click.testing import CliRunner from hello import hello def test_hello_world (): runner = CliRunner () result = runner . invoke ( hello , [ \"Peter\" ]) assert result . exit_code == 0 assert result . output == \"Hello Peter! \\n \" For subcommand testing, a subcommand name must be specified in the args parameter of CliRunner.invoke() method: File: sync.py : import click @click . group () @click . option ( \"--debug/--no-debug\" , default = False ) def cli ( debug ): click . echo ( \"Debug mode is %s \" % ( \"on\" if debug else \"off\" )) @cli . command () def sync (): click . echo ( \"Syncing\" ) File: test_sync.py : from click.testing import CliRunner from sync import cli def test_sync (): runner = CliRunner () result = runner . invoke ( cli , [ \"--debug\" , \"sync\" ]) assert result . exit_code == 0 assert \"Debug mode is on\" in result . output assert \"Syncing\" in result . output If you want to test user stdin interaction check the prompt_toolkit and pexpect articles. File system isolation \u2691 For basic command line tools with file system operations, the CliRunner.isolated_filesystem() method is useful for setting the current working directory to a new, empty folder. File: cat.py : import click @click . command () @click . argument ( \"f\" , type = click . File ()) def cat ( f ): click . echo ( f . read ()) File: test_cat.py : from click.testing import CliRunner from cat import cat def test_cat (): runner = CliRunner () with runner . isolated_filesystem (): with open ( \"hello.txt\" , \"w\" ) as f : f . write ( \"Hello World!\" ) result = runner . invoke ( cat , [ \"hello.txt\" ]) assert result . exit_code == 0 assert result . output == \"Hello World! \\n \" Testing the value of stdout and stderr \u2691 The runner has the stdout and stderr attributes to test if something was written on those buffers. Injecting fake dependencies \u2691 If you're following the domain driven design architecture pattern, you'll probably need to inject some fake objects instead of using the original objects. The challenge is to do it without modifying your real code too much for the sake of testing. Harry J.W. Percival and Bob Gregory have an interesting proposal in their Dependency Injection (and Bootstrapping) chapter, although I found it a little bit complex. Imagine that we've got an adapter to interact with the Gitea web application called Gitea . File: adapters/gitea.py : class Gitea : fake : bool = False The Click cli definition would be: File: entrypoints/cli.py : import logging from adapters.gitea import Gitea log = logging . getLogger ( __name__ ) @click . group () @click . pass_context def cli ( ctx : click . core . Context ) -> None : \"\"\"Command line interface main click entrypoint.\"\"\" ctx . ensure_object ( dict ) try : ctx . obj [ \"gitea\" ] except KeyError : ctx . obj [ \"gitea\" ] = load_gitea () @cli . command () @click . pass_context def is_fake ( ctx : Context ) -> None : if ctx . obj [ \"gitea\" ] . fake : log . info ( \"It's fake!\" ) def load_gitea () -> Gitea : \"\"\"Configure the Gitea object.\"\"\" return Gitea () Where: load_gitea : is a simplified version of the loading of an adapter, in a real example, you'll probably will need to catch some exceptions when loading the object. is_fake : Is the subcommand we're going to use to test if the adapter has been replaced by the fake object. The fake implementation of the adapter is called FakeGitea . File: tests/fake_adapters.py : class FakeGitea : fake : bool = True To inject FakeGitea in the tests we need to load it in the 'gitea' key of the obj attribute of the click ctx Context object. To do it create the fake_dependencies dictionary with the required fakes and pass it to the invoke call. File: tests/e2e/test_cli.py : from tests.fake_adapters import FakeGitea from _pytest.logging import LogCaptureFixture fake_dependencies = { \"gitea\" : FakeGitea ()} @pytest . fixture ( name = \"runner\" ) def fixture_runner () -> CliRunner : \"\"\"Configure the Click cli test runner.\"\"\" return CliRunner () def test_fake_injection ( runner : CliRunner , caplog : LogCaptureFixture ) -> None : result = runner . invoke ( cli , [ \"is_fake\" ], obj = fake_dependencies ) assert result . exit_code == 0 assert ( \"entrypoints.cli\" , logging . INFO , \"It's fake!\" , ) in caplog . record_tuples In this way we don't need to ship the fake objects with the code, and the modifications are minimal. Only the try/except KeyError snippet in the cli definition. File System Isolation \u2691 For basic command line tools with file system operations, the CliRunner.isolated_filesystem() method is useful for setting the current working directory to a new, empty folder. from click.testing import CliRunner from cat import cat def test_cat (): runner = CliRunner () with runner . isolated_filesystem (): with open ( \"hello.txt\" , \"w\" ) as f : f . write ( \"Hello World!\" ) result = runner . invoke ( cat , [ \"hello.txt\" ]) assert result . exit_code == 0 assert result . output == \"Hello World! \\n \" Pass temp_dir to control where the temporary directory is created. The directory will not be removed by Click in this case. This is useful to integrate with a framework like Pytest that manages temporary files. def test_keep_dir ( tmp_path ): runner = CliRunner () with runner . isolated_filesystem ( temp_dir = tmp_path ) as td : ... Options \u2691 Boolean Flags \u2691 Boolean flags are options that can be enabled or disabled. This can be accomplished by defining two flags in one go separated by a slash (/) for enabling or disabling the option. Click always wants you to provide an enable and disable flag so that you can change the default later. import sys @click . command () @click . option ( \"--shout/--no-shout\" , default = False ) def info ( shout ): rv = sys . platform if shout : rv = rv . upper () + \"!!!!111\" click . echo ( rv ) If you really don\u2019t want an off-switch, you can just define one and manually inform Click that something is a flag: import sys @click . command () @click . option ( \"--shout\" , is_flag = True ) def info ( shout ): rv = sys . platform if shout : rv = rv . upper () + \"!!!!111\" click . echo ( rv ) Accepting values from environmental variables \u2691 Click is the able to accept parameters from environment variables. There are two ways to define them. Passing the auto_envvar_prefix to the script that is invoked so each command and parameter is then added as an uppercase underscore-separated variable. Manually pull values in from specific environment variables by defining the name of the environment variable on the option. @click . command () @click . option ( \"--username\" , envvar = \"USERNAME\" ) def greet ( username ): click . echo ( f \"Hello { username } !\" ) if __name__ == \"__main__\" : greet () Arguments \u2691 Arguments work similarly to options but are positional. They also only support a subset of the features of options due to their syntactical nature. Click will also not attempt to document arguments for you and wants you to document them manually in order to avoid ugly help pages. Basic Arguments \u2691 The most basic option is a simple string argument of one value. If no type is provided, the type of the default value is used, and if no default value is provided, the type is assumed to be STRING . @click . command () @click . argument ( \"filename\" ) def touch ( filename ): \"\"\"Print FILENAME.\"\"\" click . echo ( filename ) And what it looks like: $ touch foo.txt foo.txt Variadic arguments \u2691 The second most common version is variadic arguments where a specific (or unlimited) number of arguments is accepted. This can be controlled with the nargs parameter. If it is set to -1 , then an unlimited number of arguments is accepted. The value is then passed as a tuple. Note that only one argument can be set to nargs=-1 , as it will eat up all arguments. @click . command () @click . argument ( \"src\" , nargs =- 1 ) @click . argument ( \"dst\" , nargs = 1 ) def copy ( src , dst ): \"\"\"Move file SRC to DST.\"\"\" for fn in src : click . echo ( \"move %s to folder %s \" % ( fn , dst )) You can't use variadic arguments and then specify a command . File Arguments \u2691 Command line tools are more fun if they work with files the Unix way, which is to accept - as a special file that refers to stdin/stdout. Click supports this through the click.File type which intelligently handles files for you. It also deals with Unicode and bytes correctly for all versions of Python so your script stays very portable. @click . command () @click . argument ( \"input\" , type = click . File ( \"rb\" )) @click . argument ( \"output\" , type = click . File ( \"wb\" )) def inout ( input , output ): \"\"\"Copy contents of INPUT to OUTPUT.\"\"\" while True : chunk = input . read ( 1024 ) if not chunk : break output . write ( chunk ) And what it does: $ inout - hello.txt hello ^D $ inout hello.txt - hello File path arguments \u2691 In the previous example, the files were opened immediately. If we just want the filename, you should be using the Path type. Not only will it return either bytes or Unicode depending on what makes more sense, but it will also be able to do some basic checks for you such as existence checks. @click . command () @click . argument ( \"filename\" , type = click . Path ( exists = True )) def touch ( filename ): \"\"\"Print FILENAME if the file exists.\"\"\" click . echo ( click . format_filename ( filename )) And what it does: $ touch hello.txt hello.txt $ touch missing.txt Usage: touch [ OPTIONS ] FILENAME Try 'touch --help' for help. Error: Invalid value for 'FILENAME' : Path 'missing.txt' does not exist. Set allowable values for an argument \u2691 @cli . command () @click . argument ( \"source\" ) @click . argument ( \"destination\" ) @click . option ( \"--mode\" , type = click . Choice ([ \"local\" , \"ftp\" ]), required = True ) def copy ( source , destination , mode ): print ( \"copying files from \" + source + \" to \" + destination + \"using \" + mode + \" mode\" ) Commands and groups \u2691 Nested handling and contexts \u2691 Each time a command is invoked, a new context is created and linked with the parent context. Contexts are passed to parameter callbacks together with the value automatically. Commands can also ask for the context to be passed by marking themselves with the pass_context() decorator. In that case, the context is passed as first argument. The context can also carry a program specified object that can be used for the program\u2019s purposes. @click . group ( help = \"Description of the command line.\" ) @click . option ( '--debug/--no-debug' , default = False ) @click . pass_context def cli ( ctx , debug ): # ensure that ctx.obj exists and is a dict (in case `cli()` is called # by means other than the `if` block below) ctx . ensure_object ( dict ) ctx . obj [ 'DEBUG' ] = debug @cli . command () @click . pass_context def sync ( ctx ): click . echo ( f 'Debug is { ctx . obj [ 'DEBUG' ] and 'on' or 'off' } ' )) if __name__ == '__main__' : cli ( obj = {}) If the object is provided, each context will pass the object onwards to its children, but at any level a context\u2019s object can be overridden. To reach to a parent, context.parent can be used. In addition to that, instead of passing an object down, nothing stops the application from modifying global state. For instance, you could just flip a global DEBUG variable and be done with it. Add default command to group \u2691 You need to use DefaultGroup , which is a sub class of click.Group . But it invokes a default subcommand instead of showing a help message when a subcommand is not passed. pip install click-default-group import click from click_default_group import DefaultGroup @click . group ( cls = DefaultGroup , default = \"foo\" , default_if_no_args = True ) def cli (): pass @cli . command () def foo (): click . echo ( \"foo\" ) @cli . command () def bar (): click . echo ( \"bar\" ) Then you can invoke that without explicit subcommand name: $ cli.py --help Usage: cli.py [ OPTIONS ] COMMAND [ ARGS ] ... Options: --help Show this message and exit. Command: foo* bar $ cli.py foo $ cli.py foo foo $ cli.py bar bar Hide a command from the help \u2691 @click . command ( ... , hidden = True ) Invoke other commands from a command \u2691 This is a pattern that is generally discouraged with Click, but possible nonetheless. For this, you can use the Context.invoke() or Context.forward() methods. They work similarly, but the difference is that Context.invoke() merely invokes another command with the arguments you provide as a caller, whereas Context.forward() fills in the arguments from the current command. Both accept the command as the first argument and everything else is passed onwards as you would expect. cli = click . Group () @cli . command () @click . option ( \"--count\" , default = 1 ) def test ( count ): click . echo ( \"Count: %d \" % count ) @cli . command () @click . option ( \"--count\" , default = 1 ) @click . pass_context def dist ( ctx , count ): ctx . forward ( test ) ctx . invoke ( test , count = 42 ) And what it looks like: $ cli dist Count: 1 Count: 42 References \u2691 Homepage Click vs other argument parsers", "title": "Click"}, {"location": "coding/python/click/#setuptools-integration", "text": "To bundle your script with setuptools, all you need is the script in a Python package and a setup.py file. Let\u2019s assume your directory structure changed to this: project/ yourpackage/ __init__.py main.py utils.py scripts/ __init__.py yourscript.py setup.py In this case instead of using py_modules in your setup.py file you can use packages and the automatic package finding support of setuptools. In addition to that it\u2019s also recommended to include other package data. These would be the modified contents of setup.py: from setuptools import setup , find_packages setup ( name = \"yourpackage\" , version = \"0.1.0\" , packages = find_packages (), include_package_data = True , install_requires = [ \"Click\" , ], entry_points = { \"console_scripts\" : [ \"yourscript = yourpackage.scripts.yourscript:cli\" , ], }, )", "title": "Setuptools Integration"}, {"location": "coding/python/click/#testing-click-applications", "text": "For basic testing, Click provides the click.testing module which provides test functionality that helps you invoke command line applications and check their behavior. The basic functionality for testing Click applications is the CliRunner which can invoke commands as command line scripts. The CliRunner.invoke() method runs the command line script in isolation and captures the output as both bytes and binary data. The return value is a Result object, which has the captured output data, exit code, and optional exception attached: File: hello.py import click @click . command () @click . argument ( \"name\" ) def hello ( name ): click . echo ( \"Hello %s !\" % name ) File: test_hello.py : from click.testing import CliRunner from hello import hello def test_hello_world (): runner = CliRunner () result = runner . invoke ( hello , [ \"Peter\" ]) assert result . exit_code == 0 assert result . output == \"Hello Peter! \\n \" For subcommand testing, a subcommand name must be specified in the args parameter of CliRunner.invoke() method: File: sync.py : import click @click . group () @click . option ( \"--debug/--no-debug\" , default = False ) def cli ( debug ): click . echo ( \"Debug mode is %s \" % ( \"on\" if debug else \"off\" )) @cli . command () def sync (): click . echo ( \"Syncing\" ) File: test_sync.py : from click.testing import CliRunner from sync import cli def test_sync (): runner = CliRunner () result = runner . invoke ( cli , [ \"--debug\" , \"sync\" ]) assert result . exit_code == 0 assert \"Debug mode is on\" in result . output assert \"Syncing\" in result . output If you want to test user stdin interaction check the prompt_toolkit and pexpect articles.", "title": "Testing Click applications"}, {"location": "coding/python/click/#file-system-isolation", "text": "For basic command line tools with file system operations, the CliRunner.isolated_filesystem() method is useful for setting the current working directory to a new, empty folder. File: cat.py : import click @click . command () @click . argument ( \"f\" , type = click . File ()) def cat ( f ): click . echo ( f . read ()) File: test_cat.py : from click.testing import CliRunner from cat import cat def test_cat (): runner = CliRunner () with runner . isolated_filesystem (): with open ( \"hello.txt\" , \"w\" ) as f : f . write ( \"Hello World!\" ) result = runner . invoke ( cat , [ \"hello.txt\" ]) assert result . exit_code == 0 assert result . output == \"Hello World! \\n \"", "title": "File system isolation"}, {"location": "coding/python/click/#testing-the-value-of-stdout-and-stderr", "text": "The runner has the stdout and stderr attributes to test if something was written on those buffers.", "title": "Testing the value of stdout and stderr"}, {"location": "coding/python/click/#injecting-fake-dependencies", "text": "If you're following the domain driven design architecture pattern, you'll probably need to inject some fake objects instead of using the original objects. The challenge is to do it without modifying your real code too much for the sake of testing. Harry J.W. Percival and Bob Gregory have an interesting proposal in their Dependency Injection (and Bootstrapping) chapter, although I found it a little bit complex. Imagine that we've got an adapter to interact with the Gitea web application called Gitea . File: adapters/gitea.py : class Gitea : fake : bool = False The Click cli definition would be: File: entrypoints/cli.py : import logging from adapters.gitea import Gitea log = logging . getLogger ( __name__ ) @click . group () @click . pass_context def cli ( ctx : click . core . Context ) -> None : \"\"\"Command line interface main click entrypoint.\"\"\" ctx . ensure_object ( dict ) try : ctx . obj [ \"gitea\" ] except KeyError : ctx . obj [ \"gitea\" ] = load_gitea () @cli . command () @click . pass_context def is_fake ( ctx : Context ) -> None : if ctx . obj [ \"gitea\" ] . fake : log . info ( \"It's fake!\" ) def load_gitea () -> Gitea : \"\"\"Configure the Gitea object.\"\"\" return Gitea () Where: load_gitea : is a simplified version of the loading of an adapter, in a real example, you'll probably will need to catch some exceptions when loading the object. is_fake : Is the subcommand we're going to use to test if the adapter has been replaced by the fake object. The fake implementation of the adapter is called FakeGitea . File: tests/fake_adapters.py : class FakeGitea : fake : bool = True To inject FakeGitea in the tests we need to load it in the 'gitea' key of the obj attribute of the click ctx Context object. To do it create the fake_dependencies dictionary with the required fakes and pass it to the invoke call. File: tests/e2e/test_cli.py : from tests.fake_adapters import FakeGitea from _pytest.logging import LogCaptureFixture fake_dependencies = { \"gitea\" : FakeGitea ()} @pytest . fixture ( name = \"runner\" ) def fixture_runner () -> CliRunner : \"\"\"Configure the Click cli test runner.\"\"\" return CliRunner () def test_fake_injection ( runner : CliRunner , caplog : LogCaptureFixture ) -> None : result = runner . invoke ( cli , [ \"is_fake\" ], obj = fake_dependencies ) assert result . exit_code == 0 assert ( \"entrypoints.cli\" , logging . INFO , \"It's fake!\" , ) in caplog . record_tuples In this way we don't need to ship the fake objects with the code, and the modifications are minimal. Only the try/except KeyError snippet in the cli definition.", "title": "Injecting fake dependencies"}, {"location": "coding/python/click/#file-system-isolation_1", "text": "For basic command line tools with file system operations, the CliRunner.isolated_filesystem() method is useful for setting the current working directory to a new, empty folder. from click.testing import CliRunner from cat import cat def test_cat (): runner = CliRunner () with runner . isolated_filesystem (): with open ( \"hello.txt\" , \"w\" ) as f : f . write ( \"Hello World!\" ) result = runner . invoke ( cat , [ \"hello.txt\" ]) assert result . exit_code == 0 assert result . output == \"Hello World! \\n \" Pass temp_dir to control where the temporary directory is created. The directory will not be removed by Click in this case. This is useful to integrate with a framework like Pytest that manages temporary files. def test_keep_dir ( tmp_path ): runner = CliRunner () with runner . isolated_filesystem ( temp_dir = tmp_path ) as td : ...", "title": "File System Isolation"}, {"location": "coding/python/click/#options", "text": "", "title": "Options"}, {"location": "coding/python/click/#boolean-flags", "text": "Boolean flags are options that can be enabled or disabled. This can be accomplished by defining two flags in one go separated by a slash (/) for enabling or disabling the option. Click always wants you to provide an enable and disable flag so that you can change the default later. import sys @click . command () @click . option ( \"--shout/--no-shout\" , default = False ) def info ( shout ): rv = sys . platform if shout : rv = rv . upper () + \"!!!!111\" click . echo ( rv ) If you really don\u2019t want an off-switch, you can just define one and manually inform Click that something is a flag: import sys @click . command () @click . option ( \"--shout\" , is_flag = True ) def info ( shout ): rv = sys . platform if shout : rv = rv . upper () + \"!!!!111\" click . echo ( rv )", "title": "Boolean Flags"}, {"location": "coding/python/click/#accepting-values-from-environmental-variables", "text": "Click is the able to accept parameters from environment variables. There are two ways to define them. Passing the auto_envvar_prefix to the script that is invoked so each command and parameter is then added as an uppercase underscore-separated variable. Manually pull values in from specific environment variables by defining the name of the environment variable on the option. @click . command () @click . option ( \"--username\" , envvar = \"USERNAME\" ) def greet ( username ): click . echo ( f \"Hello { username } !\" ) if __name__ == \"__main__\" : greet ()", "title": "Accepting values from environmental variables"}, {"location": "coding/python/click/#arguments", "text": "Arguments work similarly to options but are positional. They also only support a subset of the features of options due to their syntactical nature. Click will also not attempt to document arguments for you and wants you to document them manually in order to avoid ugly help pages.", "title": "Arguments"}, {"location": "coding/python/click/#basic-arguments", "text": "The most basic option is a simple string argument of one value. If no type is provided, the type of the default value is used, and if no default value is provided, the type is assumed to be STRING . @click . command () @click . argument ( \"filename\" ) def touch ( filename ): \"\"\"Print FILENAME.\"\"\" click . echo ( filename ) And what it looks like: $ touch foo.txt foo.txt", "title": "Basic Arguments"}, {"location": "coding/python/click/#variadic-arguments", "text": "The second most common version is variadic arguments where a specific (or unlimited) number of arguments is accepted. This can be controlled with the nargs parameter. If it is set to -1 , then an unlimited number of arguments is accepted. The value is then passed as a tuple. Note that only one argument can be set to nargs=-1 , as it will eat up all arguments. @click . command () @click . argument ( \"src\" , nargs =- 1 ) @click . argument ( \"dst\" , nargs = 1 ) def copy ( src , dst ): \"\"\"Move file SRC to DST.\"\"\" for fn in src : click . echo ( \"move %s to folder %s \" % ( fn , dst )) You can't use variadic arguments and then specify a command .", "title": "Variadic arguments"}, {"location": "coding/python/click/#file-arguments", "text": "Command line tools are more fun if they work with files the Unix way, which is to accept - as a special file that refers to stdin/stdout. Click supports this through the click.File type which intelligently handles files for you. It also deals with Unicode and bytes correctly for all versions of Python so your script stays very portable. @click . command () @click . argument ( \"input\" , type = click . File ( \"rb\" )) @click . argument ( \"output\" , type = click . File ( \"wb\" )) def inout ( input , output ): \"\"\"Copy contents of INPUT to OUTPUT.\"\"\" while True : chunk = input . read ( 1024 ) if not chunk : break output . write ( chunk ) And what it does: $ inout - hello.txt hello ^D $ inout hello.txt - hello", "title": "File Arguments"}, {"location": "coding/python/click/#file-path-arguments", "text": "In the previous example, the files were opened immediately. If we just want the filename, you should be using the Path type. Not only will it return either bytes or Unicode depending on what makes more sense, but it will also be able to do some basic checks for you such as existence checks. @click . command () @click . argument ( \"filename\" , type = click . Path ( exists = True )) def touch ( filename ): \"\"\"Print FILENAME if the file exists.\"\"\" click . echo ( click . format_filename ( filename )) And what it does: $ touch hello.txt hello.txt $ touch missing.txt Usage: touch [ OPTIONS ] FILENAME Try 'touch --help' for help. Error: Invalid value for 'FILENAME' : Path 'missing.txt' does not exist.", "title": "File path arguments"}, {"location": "coding/python/click/#set-allowable-values-for-an-argument", "text": "@cli . command () @click . argument ( \"source\" ) @click . argument ( \"destination\" ) @click . option ( \"--mode\" , type = click . Choice ([ \"local\" , \"ftp\" ]), required = True ) def copy ( source , destination , mode ): print ( \"copying files from \" + source + \" to \" + destination + \"using \" + mode + \" mode\" )", "title": "Set allowable values for an argument"}, {"location": "coding/python/click/#commands-and-groups", "text": "", "title": "Commands and groups"}, {"location": "coding/python/click/#nested-handling-and-contexts", "text": "Each time a command is invoked, a new context is created and linked with the parent context. Contexts are passed to parameter callbacks together with the value automatically. Commands can also ask for the context to be passed by marking themselves with the pass_context() decorator. In that case, the context is passed as first argument. The context can also carry a program specified object that can be used for the program\u2019s purposes. @click . group ( help = \"Description of the command line.\" ) @click . option ( '--debug/--no-debug' , default = False ) @click . pass_context def cli ( ctx , debug ): # ensure that ctx.obj exists and is a dict (in case `cli()` is called # by means other than the `if` block below) ctx . ensure_object ( dict ) ctx . obj [ 'DEBUG' ] = debug @cli . command () @click . pass_context def sync ( ctx ): click . echo ( f 'Debug is { ctx . obj [ 'DEBUG' ] and 'on' or 'off' } ' )) if __name__ == '__main__' : cli ( obj = {}) If the object is provided, each context will pass the object onwards to its children, but at any level a context\u2019s object can be overridden. To reach to a parent, context.parent can be used. In addition to that, instead of passing an object down, nothing stops the application from modifying global state. For instance, you could just flip a global DEBUG variable and be done with it.", "title": "Nested handling and contexts"}, {"location": "coding/python/click/#add-default-command-to-group", "text": "You need to use DefaultGroup , which is a sub class of click.Group . But it invokes a default subcommand instead of showing a help message when a subcommand is not passed. pip install click-default-group import click from click_default_group import DefaultGroup @click . group ( cls = DefaultGroup , default = \"foo\" , default_if_no_args = True ) def cli (): pass @cli . command () def foo (): click . echo ( \"foo\" ) @cli . command () def bar (): click . echo ( \"bar\" ) Then you can invoke that without explicit subcommand name: $ cli.py --help Usage: cli.py [ OPTIONS ] COMMAND [ ARGS ] ... Options: --help Show this message and exit. Command: foo* bar $ cli.py foo $ cli.py foo foo $ cli.py bar bar", "title": "Add default command to group"}, {"location": "coding/python/click/#hide-a-command-from-the-help", "text": "@click . command ( ... , hidden = True )", "title": "Hide a command from the help"}, {"location": "coding/python/click/#invoke-other-commands-from-a-command", "text": "This is a pattern that is generally discouraged with Click, but possible nonetheless. For this, you can use the Context.invoke() or Context.forward() methods. They work similarly, but the difference is that Context.invoke() merely invokes another command with the arguments you provide as a caller, whereas Context.forward() fills in the arguments from the current command. Both accept the command as the first argument and everything else is passed onwards as you would expect. cli = click . Group () @cli . command () @click . option ( \"--count\" , default = 1 ) def test ( count ): click . echo ( \"Count: %d \" % count ) @cli . command () @click . option ( \"--count\" , default = 1 ) @click . pass_context def dist ( ctx , count ): ctx . forward ( test ) ctx . invoke ( test , count = 42 ) And what it looks like: $ cli dist Count: 1 Count: 42", "title": "Invoke other commands from a command"}, {"location": "coding/python/click/#references", "text": "Homepage Click vs other argument parsers", "title": "References"}, {"location": "coding/python/dash/", "text": "Note \"Use Streamlit instead!\" Dash is a productive Python framework for building web analytic applications. Written on top of Flask, Plotly.js, and React.js, Dash is ideal for building data visualization apps with highly custom user interfaces in pure Python. It's particularly suited for anyone who works with data in Python. Install \u2691 pip install dash Layout \u2691 Dash apps are composed of two parts. The first part is the \"layout\" of the app and it describes what the application looks like. The second part describes the interactivity of the application. Dash provides Python classes for all of the visual components of the application. They maintain a set of components in the dash_core_components and the dash_html_components library but you can also build your own with JavaScript and React.js. The scripts are meant to be run with python app.py File: app.py import dash import dash_core_components as dcc import dash_html_components as html import plotly.express as px import pandas as pd external_stylesheets = [ 'https://codepen.io/chriddyp/pen/bWLwgP.css' ] app = dash . Dash ( __name__ , external_stylesheets = external_stylesheets ) # assume you have a \"long-form\" data frame # see https://plotly.com/python/px-arguments/ for more options df = pd . DataFrame ({ \"Fruit\" : [ \"Apples\" , \"Oranges\" , \"Bananas\" , \"Apples\" , \"Oranges\" , \"Bananas\" ], \"Amount\" : [ 4 , 1 , 2 , 2 , 4 , 5 ], \"City\" : [ \"SF\" , \"SF\" , \"SF\" , \"Montreal\" , \"Montreal\" , \"Montreal\" ] }) fig = px . bar ( df , x = \"Fruit\" , y = \"Amount\" , color = \"City\" , barmode = \"group\" ) app . layout = html . Div ( children = [ html . H1 ( children = 'Hello Dash' ), html . Div ( children = ''' Dash: A web application framework for Python. ''' ), dcc . Graph ( id = 'example-graph' , figure = fig ) ]) if __name__ == '__main__' : app . run_server ( debug = True ) Testing \u2691 dash.testing provides some off-the-rack pytest fixtures and a minimal set of testing APIs with our internal crafted best practices at the integration level. After pip install dash[testing] , the Dash pytest fixtures are available, you just need to install the WebDrivers, check the Selenium article if you need help. Dash integration tests are meant to be used with Chrome WebDriver, but the fixture allows you to choose another browser from the command line, e.g. pytest --webdriver Firefox -k bsly001 . Headless mode can be used with the --headless flag. Simple test \u2691 A simple test would be: import dash import dash_html_components as html from dash.testing.composite import DashComposite def test_bsly001_falsy_child ( dash_duo : DashComposite ) -> None : app = dash . Dash ( __name__ ) app . layout = html . Div ( id = \"nully-wrapper\" , children = 0 ) # Host the app locally in a thread, all dash server configs could be # passed after the first app argument dash_duo . start_server ( app ) # Use wait_for_* if your target element is the result of a callback, # keep in mind even the initial rendering can trigger callbacks dash_duo . wait_for_text_to_equal ( \"#nully-wrapper\" , \"0\" , timeout = 4 ) # Use this form if its present is expected at the action point assert dash_duo . find_element ( \"#nully-wrapper\" ) . text == \"0\" # To make the checkpoint more readable, you can describe the # acceptance criteria as an assert message after the comma. assert dash_duo . get_logs () == [], \"browser console should contain no error\" # You can use visual testing with percy snapshot dash_duo . percy_snapshot ( \"bsly001-layout\" ) Basic usage \u2691 The default fixture for Dash Python integration tests is dash_duo . It contains a thread_server and a WebDriver wrapped with high-level Dash testing APIs, but there are others . The Selenium WebDriver is exposed via the driver property. One of the core components of selenium testing is finding the web element with a locator , and performing some actions like click or send_keys on it, and waiting to verify if the expected state is met after those actions. There are several strategies to locate elements: CSS selector and XPATH are the two most versatile ways. We recommend using the CSS Selector in most cases due to its better performance and robustness across browsers. The Selenium WebDriver provides two types of waits : explicit wait : Makes WebDriver wait for a certain condition to occur before proceeding further with execution. implicit wait : Makes WebDriver poll the DOM for a certain amount of time when trying to locate an element. We set a global two-second timeout at the driver level. Check the Dash documentation for more Browser and Dash testing methods. References \u2691 Docs Gallery Introduction video", "title": "Dash"}, {"location": "coding/python/dash/#install", "text": "pip install dash", "title": "Install"}, {"location": "coding/python/dash/#layout", "text": "Dash apps are composed of two parts. The first part is the \"layout\" of the app and it describes what the application looks like. The second part describes the interactivity of the application. Dash provides Python classes for all of the visual components of the application. They maintain a set of components in the dash_core_components and the dash_html_components library but you can also build your own with JavaScript and React.js. The scripts are meant to be run with python app.py File: app.py import dash import dash_core_components as dcc import dash_html_components as html import plotly.express as px import pandas as pd external_stylesheets = [ 'https://codepen.io/chriddyp/pen/bWLwgP.css' ] app = dash . Dash ( __name__ , external_stylesheets = external_stylesheets ) # assume you have a \"long-form\" data frame # see https://plotly.com/python/px-arguments/ for more options df = pd . DataFrame ({ \"Fruit\" : [ \"Apples\" , \"Oranges\" , \"Bananas\" , \"Apples\" , \"Oranges\" , \"Bananas\" ], \"Amount\" : [ 4 , 1 , 2 , 2 , 4 , 5 ], \"City\" : [ \"SF\" , \"SF\" , \"SF\" , \"Montreal\" , \"Montreal\" , \"Montreal\" ] }) fig = px . bar ( df , x = \"Fruit\" , y = \"Amount\" , color = \"City\" , barmode = \"group\" ) app . layout = html . Div ( children = [ html . H1 ( children = 'Hello Dash' ), html . Div ( children = ''' Dash: A web application framework for Python. ''' ), dcc . Graph ( id = 'example-graph' , figure = fig ) ]) if __name__ == '__main__' : app . run_server ( debug = True )", "title": "Layout"}, {"location": "coding/python/dash/#testing", "text": "dash.testing provides some off-the-rack pytest fixtures and a minimal set of testing APIs with our internal crafted best practices at the integration level. After pip install dash[testing] , the Dash pytest fixtures are available, you just need to install the WebDrivers, check the Selenium article if you need help. Dash integration tests are meant to be used with Chrome WebDriver, but the fixture allows you to choose another browser from the command line, e.g. pytest --webdriver Firefox -k bsly001 . Headless mode can be used with the --headless flag.", "title": "Testing"}, {"location": "coding/python/dash/#simple-test", "text": "A simple test would be: import dash import dash_html_components as html from dash.testing.composite import DashComposite def test_bsly001_falsy_child ( dash_duo : DashComposite ) -> None : app = dash . Dash ( __name__ ) app . layout = html . Div ( id = \"nully-wrapper\" , children = 0 ) # Host the app locally in a thread, all dash server configs could be # passed after the first app argument dash_duo . start_server ( app ) # Use wait_for_* if your target element is the result of a callback, # keep in mind even the initial rendering can trigger callbacks dash_duo . wait_for_text_to_equal ( \"#nully-wrapper\" , \"0\" , timeout = 4 ) # Use this form if its present is expected at the action point assert dash_duo . find_element ( \"#nully-wrapper\" ) . text == \"0\" # To make the checkpoint more readable, you can describe the # acceptance criteria as an assert message after the comma. assert dash_duo . get_logs () == [], \"browser console should contain no error\" # You can use visual testing with percy snapshot dash_duo . percy_snapshot ( \"bsly001-layout\" )", "title": "Simple test"}, {"location": "coding/python/dash/#basic-usage", "text": "The default fixture for Dash Python integration tests is dash_duo . It contains a thread_server and a WebDriver wrapped with high-level Dash testing APIs, but there are others . The Selenium WebDriver is exposed via the driver property. One of the core components of selenium testing is finding the web element with a locator , and performing some actions like click or send_keys on it, and waiting to verify if the expected state is met after those actions. There are several strategies to locate elements: CSS selector and XPATH are the two most versatile ways. We recommend using the CSS Selector in most cases due to its better performance and robustness across browsers. The Selenium WebDriver provides two types of waits : explicit wait : Makes WebDriver wait for a certain condition to occur before proceeding further with execution. implicit wait : Makes WebDriver poll the DOM for a certain amount of time when trying to locate an element. We set a global two-second timeout at the driver level. Check the Dash documentation for more Browser and Dash testing methods.", "title": "Basic usage"}, {"location": "coding/python/dash/#references", "text": "Docs Gallery Introduction video", "title": "References"}, {"location": "coding/python/dash_leaflet/", "text": "Dash Leaflet is a wrapper of Leaflet , the leading open-source JavaScript library for interactive maps. Install \u2691 pip install dash pip install dash-leaflet Usage \u2691 import dash import dash_leaflet as dl app = dash . Dash ( __name__ ) app . layout = dl . Map ( dl . TileLayer (), style = { 'height' : '100vh' }) if __name__ == '__main__' : app . run_server ( port = 8050 , debug = True ) That's it! You have now created your first interactive map with Dash Leaflet. If you visit http://127.0.0.1:8050/ in your browser. Change tileset \u2691 Leaflet Map object supports different tiles by default. It also supports any WMS or WMTS by passing a Leaflet-style URL to the tiles parameter: http://{s}.yourtiles.com/{z}/{x}/{y}.png . To use the IGN beautiful map as a fallback and the OpenStreetMaps as default use the following snippet: app . layout = html . Div ( dl . Map ( [ dl . LayersControl ( [ dl . BaseLayer ( dl . TileLayer (), name = \"OpenStreetMaps\" , checked = True , ), dl . BaseLayer ( dl . TileLayer ( url = \"https://www.ign.es/wmts/mapa-raster?request=getTile&layer=MTN&TileMatrixSet=GoogleMapsCompatible&TileMatrix= {z} &TileCol= {x} &TileRow= {y} &format=image/jpeg\" , attribution = \"IGN\" , ), name = \"IGN\" , checked = False , ), ], ), get_data (), ], zoom = 7 , center = ( 40.0884 , - 3.68042 ), ), style = { \"height\" : \"100vh\" , }, ) Loading the data \u2691 Using Markers \u2691 As with folium , loading different custom markers within the same geojson object is difficult, therefore we are again forced to use markers with cluster group. Assuming we've got a gpx file called data.gpx , we can use the following snippet to load all markers with a custom icon. import dash_leaflet as dl import gpxpy icon = { \"iconUrl\" : \"https://leafletjs.com/examples/custom-icons/leaf-green.png\" , \"shadowUrl\" : \"https://leafletjs.com/examples/custom-icons/leaf-shadow.png\" , \"iconSize\" : [ 38 , 95 ], # size of the icon \"shadowSize\" : [ 50 , 64 ], # size of the shadow \"iconAnchor\" : [ 22 , 94 , ], # point of the icon which will correspond to marker's location \"shadowAnchor\" : [ 4 , 62 ], # the same for the shadow \"popupAnchor\" : [ - 3 , - 76 , ], # point from which the popup should open relative to the iconAnchor } def get_data (): gpx_file = open ( \"data.gpx\" , \"r\" ) gpx = gpxpy . parse ( gpx_file ) markers = [] for waypoint in gpx . waypoints : markers . append ( dl . Marker ( title = waypoint . name , position = ( waypoint . latitude , waypoint . longitude ), icon = icon , children = [ dl . Tooltip ( waypoint . name ), dl . Popup ( waypoint . name ), ], ) ) cluster = dl . MarkerClusterGroup ( id = \"markers\" , children = markers ) return cluster app = dash . Dash ( __name__ ) app . layout = html . Div ( dl . Map ( [ dl . TileLayer (), get_data (), ], zoom = 7 , center = ( 40.0884 , - 3.68042 ), ), style = { \"height\" : \"100vh\" , }, ) Inside get_data you can add further logic to change the icon based on the data of the gpx. Configurations \u2691 Add custom css or js \u2691 Including custom CSS or JavaScript in your Dash apps is simple. Just create a folder named assets in the root of your app directory and include your CSS and JavaScript files in that folder. Dash will automatically serve all of the files that are included in this folder. Remove the border around the map \u2691 Add a custom css file: File: assets/custom.css body { margin : 0 , } References \u2691 Docs Git", "title": "Dash Leaflet"}, {"location": "coding/python/dash_leaflet/#install", "text": "pip install dash pip install dash-leaflet", "title": "Install"}, {"location": "coding/python/dash_leaflet/#usage", "text": "import dash import dash_leaflet as dl app = dash . Dash ( __name__ ) app . layout = dl . Map ( dl . TileLayer (), style = { 'height' : '100vh' }) if __name__ == '__main__' : app . run_server ( port = 8050 , debug = True ) That's it! You have now created your first interactive map with Dash Leaflet. If you visit http://127.0.0.1:8050/ in your browser.", "title": "Usage"}, {"location": "coding/python/dash_leaflet/#change-tileset", "text": "Leaflet Map object supports different tiles by default. It also supports any WMS or WMTS by passing a Leaflet-style URL to the tiles parameter: http://{s}.yourtiles.com/{z}/{x}/{y}.png . To use the IGN beautiful map as a fallback and the OpenStreetMaps as default use the following snippet: app . layout = html . Div ( dl . Map ( [ dl . LayersControl ( [ dl . BaseLayer ( dl . TileLayer (), name = \"OpenStreetMaps\" , checked = True , ), dl . BaseLayer ( dl . TileLayer ( url = \"https://www.ign.es/wmts/mapa-raster?request=getTile&layer=MTN&TileMatrixSet=GoogleMapsCompatible&TileMatrix= {z} &TileCol= {x} &TileRow= {y} &format=image/jpeg\" , attribution = \"IGN\" , ), name = \"IGN\" , checked = False , ), ], ), get_data (), ], zoom = 7 , center = ( 40.0884 , - 3.68042 ), ), style = { \"height\" : \"100vh\" , }, )", "title": "Change tileset"}, {"location": "coding/python/dash_leaflet/#loading-the-data", "text": "", "title": "Loading the data"}, {"location": "coding/python/dash_leaflet/#using-markers", "text": "As with folium , loading different custom markers within the same geojson object is difficult, therefore we are again forced to use markers with cluster group. Assuming we've got a gpx file called data.gpx , we can use the following snippet to load all markers with a custom icon. import dash_leaflet as dl import gpxpy icon = { \"iconUrl\" : \"https://leafletjs.com/examples/custom-icons/leaf-green.png\" , \"shadowUrl\" : \"https://leafletjs.com/examples/custom-icons/leaf-shadow.png\" , \"iconSize\" : [ 38 , 95 ], # size of the icon \"shadowSize\" : [ 50 , 64 ], # size of the shadow \"iconAnchor\" : [ 22 , 94 , ], # point of the icon which will correspond to marker's location \"shadowAnchor\" : [ 4 , 62 ], # the same for the shadow \"popupAnchor\" : [ - 3 , - 76 , ], # point from which the popup should open relative to the iconAnchor } def get_data (): gpx_file = open ( \"data.gpx\" , \"r\" ) gpx = gpxpy . parse ( gpx_file ) markers = [] for waypoint in gpx . waypoints : markers . append ( dl . Marker ( title = waypoint . name , position = ( waypoint . latitude , waypoint . longitude ), icon = icon , children = [ dl . Tooltip ( waypoint . name ), dl . Popup ( waypoint . name ), ], ) ) cluster = dl . MarkerClusterGroup ( id = \"markers\" , children = markers ) return cluster app = dash . Dash ( __name__ ) app . layout = html . Div ( dl . Map ( [ dl . TileLayer (), get_data (), ], zoom = 7 , center = ( 40.0884 , - 3.68042 ), ), style = { \"height\" : \"100vh\" , }, ) Inside get_data you can add further logic to change the icon based on the data of the gpx.", "title": "Using Markers"}, {"location": "coding/python/dash_leaflet/#configurations", "text": "", "title": "Configurations"}, {"location": "coding/python/dash_leaflet/#add-custom-css-or-js", "text": "Including custom CSS or JavaScript in your Dash apps is simple. Just create a folder named assets in the root of your app directory and include your CSS and JavaScript files in that folder. Dash will automatically serve all of the files that are included in this folder.", "title": "Add custom css or js"}, {"location": "coding/python/dash_leaflet/#remove-the-border-around-the-map", "text": "Add a custom css file: File: assets/custom.css body { margin : 0 , }", "title": "Remove the border around the map"}, {"location": "coding/python/dash_leaflet/#references", "text": "Docs Git", "title": "References"}, {"location": "coding/python/data_classes/", "text": "A data class is a regular Python class that has basic data model methods like __init__() , __repr__() , and __eq__() implemented for you. Introduced in Python 3.7 , they typically containing mainly data, although there aren\u2019t really any restrictions. from dataclasses import dataclass @dataclass class DataClassCard : rank : str suit : str They behave similar to named tuples but come with many more features. At the same time, named tuples have some other features that are not necessarily desirable, such as: By design it's a regular tuple, which can lead to subtle and hard to find bugs. It's hard to add default values to some fields. It's by nature immutable. That being said, if you need your data structure to behave like a tuple, then a named tuple is a great alternative. Advantages over regular classes \u2691 Simplify the class definition @dataclass class DataClassCard : rank : str suit : str # Versus class RegularCard def __init__ ( self , rank , suit ): self . rank = rank self . suit = suit More descriptive object representation through a better default __repr__() method. >>> queen_of_hearts = DataClassCard ( 'Q' , 'Hearts' ) >>> queen_of_hearts DataClassCard ( rank = 'Q' , suit = 'Hearts' ) # Versus >>> queen_of_spades = RegularCard ( 'Q' , 'Spades' ) >>> queen_of_spades < __main__ . RegularCard object at 0x7fb6eee35d30 > * Instance comparison out of the box through a better default __eq__() method. >>> queen_of_hearts == DataClassCard ( 'Q' , 'Hearts' ) True # Versus >>> queen_of_spades == RegularCard ( 'Q' , 'Spades' ) False Usage \u2691 Definition \u2691 from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float What makes this a data class is the @dataclass decorator. Beneath the class Position: , simply list the fields you want in your data class. The data class decorator support the following parameters : init : Add .__init__() method? (Default is True). repr : Add .__repr__() method? (Default is True). eq : Add .__eq__() method? (Default is True). order : Add ordering methods? (Default is False). unsafe_hash : Force the addition of a .__hash__() method? (Default is False). frozen : If True , assigning to fields raise an exception. (Default is False). Default values \u2691 It's easy to add default values to the fields of your data class: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 More complex default values can be defined through the use of functions. For example, the next snippet builds a French deck: from dataclasses import dataclass , field from typing import List RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () def make_french_deck (): return [ PlayingCard ( r , s ) for s in SUITS for r in RANKS ] @dataclass class PlayingCard : rank : str suit : str @dataclass class Deck : cards : List [ PlayingCard ] = field ( default_factory = make_french_deck ) Using cards: List[PlayingCard] = make_french_deck() introduces the using mutable default arguments anti-pattern. Instead, data classes use the default_factory to handle mutable default values. To use it, you need to use the field() specifier which is used to customize each field of a data class individually. It supports the following parameters: default : Default value of the field. default_factory : Function that returns the initial value of the field. init : Use field in .__init__() method? (Default is True ). repr : Use field in repr of the object? (Default is True ). For example to hide a parameter from the repr , use lat: float = field(default=0.0, repr=False) . compare : Include the field in comparisons? (Default is True ). hash : Include the field when calculating hash() ? (Default is to use the same as compare ). metadata : A mapping with information about the field. It's not used by the data classes themselves but is available for you to attach information to fields. For example: from dataclasses import dataclass , field @dataclass class Position : name : str lon : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) lat : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) To retrieve the information use the fields() function. >>> from dataclasses import fields >>> fields ( Position ) ( Field ( name = 'name' , type =< class ' str '>,...,metadata= {} ), Field ( name = 'lon' , type =< class ' float '>,...,metadata={' unit ': ' degrees '}), Field ( name = 'lat' , type =< class ' float '>,...,metadata={' unit ': ' degrees '})) >>> lat_unit = fields ( Position )[ 2 ] . metadata [ 'unit' ] >>> lat_unit 'degrees' Type hints \u2691 They support typing out of the box. Without a type hint, the field will not be a part of the data class. While you need to add type hints in some form when using data classes, these types are not enforced at runtime. This is how typing in python usually works: Python is and will always be a dynamically typed language . Adding methods \u2691 Same as with a normal class. Adding complex order comparison logic \u2691 from dataclasses import dataclass @dataclass ( order = True ) class PlayingCard : rank : str suit : str def __str__ ( self ): return f ' { self . suit }{ self . rank } ' After setting order=True in the decorator definition the instances of PlayingCard can be compared. >>> queen_of_hearts = PlayingCard ( 'Q' , '\u2661' ) >>> ace_of_spades = PlayingCard ( 'A' , '\u2660' ) >>> ace_of_spades > queen_of_hearts False Data classes compare objects as if they were tuples of their fields. A Queen is higher than an Ace because Q comes after A in the alphabet. >>> ( 'A' , '\u2660' ) > ( 'Q' , '\u2661' ) False To use more complex comparisons, we need to add the field .sort_index to the class. However, this field should be calculated from the other fields automatically. That's what the special method .__post_init__() is for. It allows for special processing after the regular .__init__() method is called. from dataclasses import dataclass , field RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () @dataclass ( order = True ) class PlayingCard : sort_index : int = field ( init = False , repr = False ) rank : str suit : str def __post_init__ ( self ): self . sort_index = ( RANKS . index ( self . rank ) * len ( SUITS ) + SUITS . index ( self . suit )) def __str__ ( self ): return f ' { self . suit }{ self . rank } ' Note that .sort_index is added as the first field of the class. That way, the comparison is first done using .sort_index and only if there are ties are the other fields used. Using field() , you must also specify that .sort_index should not be included as a parameter in the .__init__() method (because it is calculated from the .rank and .suit fields). To avoid confusing the user about this implementation detail, it is probably also a good idea to remove .sort_index from the repr of the class. Immutable data classes \u2691 To make a data class immutable, set frozen=True when you create it. from dataclasses import dataclass @dataclass ( frozen = True ) class Position : name : str lon : float = 0.0 lat : float = 0.0 In a frozen data class, you can not assign values to the fields after creation: >>> pos = Position ( 'Oslo' , 10.8 , 59.9 ) >>> pos . name 'Oslo' >>> pos . name = 'Stockholm' dataclasses . FrozenInstanceError : cannot assign to field 'name' Be aware though that if your data class contains mutable fields, those might still change. This is true for all nested data structures in Python: from dataclasses import dataclass from typing import List @dataclass ( frozen = True ) class ImmutableCard : rank : str suit : str @dataclass ( frozen = True ) class ImmutableDeck : cards : List [ PlayingCard ] Even though both ImmutableCard and ImmutableDeck are immutable, the list holding cards is not. You can therefore still change the cards in the deck: >>> queen_of_hearts = ImmutableCard ( 'Q' , '\u2661' ) >>> ace_of_spades = ImmutableCard ( 'A' , '\u2660' ) >>> deck = ImmutableDeck ([ queen_of_hearts , ace_of_spades ]) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = 'Q' , suit = '\u2661' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) >>> deck . cards [ 0 ] = ImmutableCard ( '7' , '\u2662' ) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = '7' , suit = '\u2662' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) To avoid this, make sure all fields of an immutable data class use immutable types (but remember that types are not enforced at runtime). The ImmutableDeck should be implemented using a tuple instead of a list. Inheritance \u2691 You can subclass data classes quite freely. from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float @dataclass class Capital ( Position ): country : str >>> Capital ( 'Oslo' , 10.8 , 59.9 , 'Norway' ) Capital ( name = 'Oslo' , lon = 10.8 , lat = 59.9 , country = 'Norway' ) Warning This won't work if the base class have default values unless all the subclass parameters also have default values. Warning If you redefine a base class field, you need to keep the fields order after the subclass new fields: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 @dataclass class Capital ( Position ): country : str = 'Unknown' lat : float = 40.0 Optimizing Data Classes \u2691 Slots can be used to make classes faster and use less memory. from dataclasses import dataclass @dataclass class SimplePosition : name : str lon : float lat : float @dataclass class SlotPosition : __slots__ = [ 'name' , 'lon' , 'lat' ] name : str lon : float lat : float Essentially, slots are defined using .__slots__ to list the variables on a class. Variables or attributes not present in .__slots__ may not be defined. Furthermore, a slots class may not have default values . References \u2691 Real Python Data classes article", "title": "Data Classes"}, {"location": "coding/python/data_classes/#advantages-over-regular-classes", "text": "Simplify the class definition @dataclass class DataClassCard : rank : str suit : str # Versus class RegularCard def __init__ ( self , rank , suit ): self . rank = rank self . suit = suit More descriptive object representation through a better default __repr__() method. >>> queen_of_hearts = DataClassCard ( 'Q' , 'Hearts' ) >>> queen_of_hearts DataClassCard ( rank = 'Q' , suit = 'Hearts' ) # Versus >>> queen_of_spades = RegularCard ( 'Q' , 'Spades' ) >>> queen_of_spades < __main__ . RegularCard object at 0x7fb6eee35d30 > * Instance comparison out of the box through a better default __eq__() method. >>> queen_of_hearts == DataClassCard ( 'Q' , 'Hearts' ) True # Versus >>> queen_of_spades == RegularCard ( 'Q' , 'Spades' ) False", "title": "Advantages over regular classes"}, {"location": "coding/python/data_classes/#usage", "text": "", "title": "Usage"}, {"location": "coding/python/data_classes/#definition", "text": "from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float What makes this a data class is the @dataclass decorator. Beneath the class Position: , simply list the fields you want in your data class. The data class decorator support the following parameters : init : Add .__init__() method? (Default is True). repr : Add .__repr__() method? (Default is True). eq : Add .__eq__() method? (Default is True). order : Add ordering methods? (Default is False). unsafe_hash : Force the addition of a .__hash__() method? (Default is False). frozen : If True , assigning to fields raise an exception. (Default is False).", "title": "Definition"}, {"location": "coding/python/data_classes/#default-values", "text": "It's easy to add default values to the fields of your data class: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 More complex default values can be defined through the use of functions. For example, the next snippet builds a French deck: from dataclasses import dataclass , field from typing import List RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () def make_french_deck (): return [ PlayingCard ( r , s ) for s in SUITS for r in RANKS ] @dataclass class PlayingCard : rank : str suit : str @dataclass class Deck : cards : List [ PlayingCard ] = field ( default_factory = make_french_deck ) Using cards: List[PlayingCard] = make_french_deck() introduces the using mutable default arguments anti-pattern. Instead, data classes use the default_factory to handle mutable default values. To use it, you need to use the field() specifier which is used to customize each field of a data class individually. It supports the following parameters: default : Default value of the field. default_factory : Function that returns the initial value of the field. init : Use field in .__init__() method? (Default is True ). repr : Use field in repr of the object? (Default is True ). For example to hide a parameter from the repr , use lat: float = field(default=0.0, repr=False) . compare : Include the field in comparisons? (Default is True ). hash : Include the field when calculating hash() ? (Default is to use the same as compare ). metadata : A mapping with information about the field. It's not used by the data classes themselves but is available for you to attach information to fields. For example: from dataclasses import dataclass , field @dataclass class Position : name : str lon : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) lat : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) To retrieve the information use the fields() function. >>> from dataclasses import fields >>> fields ( Position ) ( Field ( name = 'name' , type =< class ' str '>,...,metadata= {} ), Field ( name = 'lon' , type =< class ' float '>,...,metadata={' unit ': ' degrees '}), Field ( name = 'lat' , type =< class ' float '>,...,metadata={' unit ': ' degrees '})) >>> lat_unit = fields ( Position )[ 2 ] . metadata [ 'unit' ] >>> lat_unit 'degrees'", "title": "Default values"}, {"location": "coding/python/data_classes/#type-hints", "text": "They support typing out of the box. Without a type hint, the field will not be a part of the data class. While you need to add type hints in some form when using data classes, these types are not enforced at runtime. This is how typing in python usually works: Python is and will always be a dynamically typed language .", "title": "Type hints"}, {"location": "coding/python/data_classes/#adding-methods", "text": "Same as with a normal class.", "title": "Adding methods"}, {"location": "coding/python/data_classes/#adding-complex-order-comparison-logic", "text": "from dataclasses import dataclass @dataclass ( order = True ) class PlayingCard : rank : str suit : str def __str__ ( self ): return f ' { self . suit }{ self . rank } ' After setting order=True in the decorator definition the instances of PlayingCard can be compared. >>> queen_of_hearts = PlayingCard ( 'Q' , '\u2661' ) >>> ace_of_spades = PlayingCard ( 'A' , '\u2660' ) >>> ace_of_spades > queen_of_hearts False Data classes compare objects as if they were tuples of their fields. A Queen is higher than an Ace because Q comes after A in the alphabet. >>> ( 'A' , '\u2660' ) > ( 'Q' , '\u2661' ) False To use more complex comparisons, we need to add the field .sort_index to the class. However, this field should be calculated from the other fields automatically. That's what the special method .__post_init__() is for. It allows for special processing after the regular .__init__() method is called. from dataclasses import dataclass , field RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () @dataclass ( order = True ) class PlayingCard : sort_index : int = field ( init = False , repr = False ) rank : str suit : str def __post_init__ ( self ): self . sort_index = ( RANKS . index ( self . rank ) * len ( SUITS ) + SUITS . index ( self . suit )) def __str__ ( self ): return f ' { self . suit }{ self . rank } ' Note that .sort_index is added as the first field of the class. That way, the comparison is first done using .sort_index and only if there are ties are the other fields used. Using field() , you must also specify that .sort_index should not be included as a parameter in the .__init__() method (because it is calculated from the .rank and .suit fields). To avoid confusing the user about this implementation detail, it is probably also a good idea to remove .sort_index from the repr of the class.", "title": "Adding complex order comparison logic"}, {"location": "coding/python/data_classes/#immutable-data-classes", "text": "To make a data class immutable, set frozen=True when you create it. from dataclasses import dataclass @dataclass ( frozen = True ) class Position : name : str lon : float = 0.0 lat : float = 0.0 In a frozen data class, you can not assign values to the fields after creation: >>> pos = Position ( 'Oslo' , 10.8 , 59.9 ) >>> pos . name 'Oslo' >>> pos . name = 'Stockholm' dataclasses . FrozenInstanceError : cannot assign to field 'name' Be aware though that if your data class contains mutable fields, those might still change. This is true for all nested data structures in Python: from dataclasses import dataclass from typing import List @dataclass ( frozen = True ) class ImmutableCard : rank : str suit : str @dataclass ( frozen = True ) class ImmutableDeck : cards : List [ PlayingCard ] Even though both ImmutableCard and ImmutableDeck are immutable, the list holding cards is not. You can therefore still change the cards in the deck: >>> queen_of_hearts = ImmutableCard ( 'Q' , '\u2661' ) >>> ace_of_spades = ImmutableCard ( 'A' , '\u2660' ) >>> deck = ImmutableDeck ([ queen_of_hearts , ace_of_spades ]) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = 'Q' , suit = '\u2661' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) >>> deck . cards [ 0 ] = ImmutableCard ( '7' , '\u2662' ) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = '7' , suit = '\u2662' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) To avoid this, make sure all fields of an immutable data class use immutable types (but remember that types are not enforced at runtime). The ImmutableDeck should be implemented using a tuple instead of a list.", "title": "Immutable data classes"}, {"location": "coding/python/data_classes/#inheritance", "text": "You can subclass data classes quite freely. from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float @dataclass class Capital ( Position ): country : str >>> Capital ( 'Oslo' , 10.8 , 59.9 , 'Norway' ) Capital ( name = 'Oslo' , lon = 10.8 , lat = 59.9 , country = 'Norway' ) Warning This won't work if the base class have default values unless all the subclass parameters also have default values. Warning If you redefine a base class field, you need to keep the fields order after the subclass new fields: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 @dataclass class Capital ( Position ): country : str = 'Unknown' lat : float = 40.0", "title": "Inheritance"}, {"location": "coding/python/data_classes/#optimizing-data-classes", "text": "Slots can be used to make classes faster and use less memory. from dataclasses import dataclass @dataclass class SimplePosition : name : str lon : float lat : float @dataclass class SlotPosition : __slots__ = [ 'name' , 'lon' , 'lat' ] name : str lon : float lat : float Essentially, slots are defined using .__slots__ to list the variables on a class. Variables or attributes not present in .__slots__ may not be defined. Furthermore, a slots class may not have default values .", "title": "Optimizing Data Classes"}, {"location": "coding/python/data_classes/#references", "text": "Real Python Data classes article", "title": "References"}, {"location": "coding/python/deepdiff/", "text": "The DeepDiff library is used to perform search and differences in Python objects. It comes with three operations: DeepDiff: Deep Difference of dictionaries, iterables, strings and other objects. It will recursively look for all the changes. DeepSearch: Search for objects within other objects. DeepHash: Hash any object based on their content even if they are not \u201chashable\u201d. Install \u2691 Install from PyPi: pip install deepdiff DeepSearch \u2691 Deep Search inside objects to find the item matching your criteria. Note that is searches for either the path to match your criteria or the word in an item. Examples: Importing from deepdiff import DeepSearch , grep DeepSearch comes with grep function which is easier to remember! Search in list for string >>> obj = [ \"long somewhere\" , \"string\" , 0 , \"somewhere great!\" ] >>> item = \"somewhere\" >>> ds = obj | grep ( item , verbose_level = 2 ) >>> print ( ds ) { 'matched_values' : { 'root[3]' : 'somewhere great!' , 'root[0]' : 'long somewhere' }} Search in nested data for string >>> obj = [ \"something somewhere\" , { \"long\" : \"somewhere\" , \"string\" : 2 , 0 : 0 , \"somewhere\" : \"around\" }] >>> item = \"somewhere\" >>> ds = obj | grep ( item , verbose_level = 2 ) >>> pprint ( ds , indent = 2 ) { 'matched_paths' : { \"root[1]['somewhere']\" : 'around' }, 'matched_values' : { 'root[0]' : 'something somewhere' , \"root[1]['long']\" : 'somewhere' }} To obtain the keys and values of the matched objects, you can use the Extract object. >>> from deepdiff import grep >>> obj = { 1 : [{ '2' : 'b' }, 3 ], 2 : [ 4 , 5 ]} >>> result = obj | grep ( 5 ) >>> result { 'matched_values' : OrderedSet ([ 'root[2][1]' ])} >>> result [ 'matched_values' ][ 0 ] 'root[2][1]' >>> path = result [ 'matched_values' ][ 0 ] >>> extract ( obj , path ) 5 References \u2691 Homepage/Docs Old Docs", "title": "DeepDiff"}, {"location": "coding/python/deepdiff/#install", "text": "Install from PyPi: pip install deepdiff", "title": "Install"}, {"location": "coding/python/deepdiff/#deepsearch", "text": "Deep Search inside objects to find the item matching your criteria. Note that is searches for either the path to match your criteria or the word in an item. Examples: Importing from deepdiff import DeepSearch , grep DeepSearch comes with grep function which is easier to remember! Search in list for string >>> obj = [ \"long somewhere\" , \"string\" , 0 , \"somewhere great!\" ] >>> item = \"somewhere\" >>> ds = obj | grep ( item , verbose_level = 2 ) >>> print ( ds ) { 'matched_values' : { 'root[3]' : 'somewhere great!' , 'root[0]' : 'long somewhere' }} Search in nested data for string >>> obj = [ \"something somewhere\" , { \"long\" : \"somewhere\" , \"string\" : 2 , 0 : 0 , \"somewhere\" : \"around\" }] >>> item = \"somewhere\" >>> ds = obj | grep ( item , verbose_level = 2 ) >>> pprint ( ds , indent = 2 ) { 'matched_paths' : { \"root[1]['somewhere']\" : 'around' }, 'matched_values' : { 'root[0]' : 'something somewhere' , \"root[1]['long']\" : 'somewhere' }} To obtain the keys and values of the matched objects, you can use the Extract object. >>> from deepdiff import grep >>> obj = { 1 : [{ '2' : 'b' }, 3 ], 2 : [ 4 , 5 ]} >>> result = obj | grep ( 5 ) >>> result { 'matched_values' : OrderedSet ([ 'root[2][1]' ])} >>> result [ 'matched_values' ][ 0 ] 'root[2][1]' >>> path = result [ 'matched_values' ][ 0 ] >>> extract ( obj , path ) 5", "title": "DeepSearch"}, {"location": "coding/python/deepdiff/#references", "text": "Homepage/Docs Old Docs", "title": "References"}, {"location": "coding/python/docstrings/", "text": "Docstrings are strings that define the purpose and use of a module, class, function or method. They are accessible from the doc attribute __doc__ and with the built-in help() function. Documenting your code is very important as it's more often read than written . Documenting vs commenting \u2691 Commenting is describing your code to/for developers. The intended audience is the maintainers and developers of the Python code. In conjuction with well-written code, comments help to guide the reader to better understand your code and its purpose and design. Documenting is describing its use and functionality to your users. While it may be helpful in the development process, the main intended audience are the users. Docstring format \u2691 Docstrings should use the triple-double quote ( \"\"\" ) string format. This should be done whether the docstring is multi-lined or not. At a bare minimum, a docstring should be a quick summary of whatever is it you\u2019re describing and should be contained within a single line. Multi-lined docstrings are used to further elaborate on the object beyond the summary. All multi-lined docstrings have the following parts: A one-line summary line A blank line proceeding the summary Any further elaboration for the docstring Another blank line To ensure your docstrings follow these practices, configure flakeheaven with the flake8-docstrings extension. Docstring types \u2691 Class docstrings \u2691 Class Docstrings are created for the class itself, as well as any class methods. The docstrings are placed immediately following the class or class method indented by one level: class SimpleClass: \"\"\"Class docstrings go here.\"\"\" def say_hello(self, name: str): \"\"\"Class method docstrings go here.\"\"\" print(f'Hello {name}') Class docstrings should contain the following information: A brief summary of its purpose and behavior Any public methods, along with a brief description Any class properties (attributes) Anything related to the interface for subclassers, if the class is intended to be subclassed The class constructor parameters should be documented within the init class method docstring. Individual methods should be documented using their individual docstrings. Class method docstrings should contain the following: A brief description of what the method is and what it\u2019s used for Any arguments (both required and optional) that are passed including keyword arguments Label any arguments that are considered optional or have a default value Any side effects that occur when executing the method Any exceptions that are raised Any restrictions on when the method can be called Package and module docstrings \u2691 Package docstrings should be placed at the top of the package\u2019s __init__.py file. This docstring should list the modules and sub-packages that are exported by the package. Module docstrings should include the following: A brief description of the module and its purpose A list of any classes, exception, functions, and any other objects exported by the module. Only needed if they are not defined in the same file, otherwise help() will get them automatically. The docstring for a module function should include the same items as a class method. Docstring formats \u2691 Google docstrings : the most user friendly. reStructured Text : The official ones, but super ugly to write. Numpy docstrings . Google Docstrings \u2691 Napoleon gathered a nice cheatsheet with examples . Functions and methods \u2691 A method that overrides a method from a base class may have a simple docstring sending the reader to its overridden method\u2019s docstring, such as \"\"\"See base class.\"\"\" . The rationale is that there is no need to repeat in many places documentation that is already present in the base method\u2019s docstring. However, if the overriding method\u2019s behavior is substantially different from the overridden method, or details need to be provided (e.g., documenting additional side effects), a docstring with at least those differences is required on the overriding method. Certain aspects of a function should be documented in special sections, listed below. Each section begins with a heading line, which ends with a colon. All sections other than the heading should maintain a hanging indent of two or four spaces (be consistent within a file). These sections can be omitted in cases where the function\u2019s name and signature are informative enough that it can be aptly described using a one-line docstring. Args List each parameter by name. A description should follow the name, and be separated by a colon followed by either a space or newline. If the description is too long to fit on a single 80-character line, use a hanging indent of 2 or 4 spaces more than the parameter name (be consistent with the rest of the docstrings in the file). The description should include required type(s) if the code does not contain a corresponding type annotation. If a function accepts *foo (variable length argument lists) and/or **bar (arbitrary keyword arguments), they should be listed as *foo and **bar . Returns (or Yields: for generators) Describe the type and semantics of the return value. If the function only returns None, this section is not required. It may also be omitted if the docstring starts with Returns or Yields (e.g. \"\"\"Returns row from API as a tuple of strings.\"\"\" ) and the opening sentence is sufficient to describe return value. Raises List all exceptions that are relevant to the interface followed by a description. Use a similar exception name + colon + space or newline and hanging indent style as described in Args:. You should not document exceptions that get raised if the API specified in the docstring is violated (because this would paradoxically make behavior under violation of the API part of the API). Tests docstrings \u2691 Without template \u2691 jml has very good tips on writing test's docstrings : If you\u2019re struggling to write docstrings for your tests, here\u2019s a handy five-step guide: Write the first docstring that comes to mind. It will almost certainly be: \"\"\"Test that input is parsed correctly.\"\"\" Get rid of \u201cTest that\u201d or \u201cCheck that\u201d. We know it\u2019s a test. \"\"\"Input should be parsed correctly.\"\"\" Seriously?! Why\u2019d you have to go and add \u201cshould\u201d? It\u2019s a test, it\u2019s all about \u201cshould\u201d. \"\"\"Input is parsed correctly.\"\"\" \u201cCorrectly\u201d, \u201cproperly\u201d, and \u201cas we expect\u201d are all redundant. Axe them too. \"\"\"Input is parsed.\"\"\" Look at what\u2019s left. Is it saying anything at all? If so, great. If not, consider adding something specific about the test behaviour and perhaps even why it\u2019s desirable behaviour to have. \"\"\" Input is parsed into an immutable dict according to the config schema, so we get config info without worrying about input validation all the time. \"\"\" Given When Then \u2691 Given-When-Then is a style of representing test. It's an approach developed as part of Behavior-Driven Development (BDD). It appears as a structuring approach for many testing frameworks such as Cucumber. You can also look at it as a reformulation of the Four-Phase Test pattern. The essential idea is to break down writing a scenario (or test) into three sections: The given part describes the state of the world before you begin the behavior you're specifying in this scenario. You can think of it as the pre-conditions to the test. The when section is that behavior that you're specifying. Finally the then section describes the changes you expect due to the specified behavior. I already implement this test structure with the Arrange, Act, Assert structure, so it made sense to use it in the docstring too. The side effects is that you repeat a lot of prose where a single line would suffice. Automatic documentation generation \u2691 Use the mkdocstrings plugin to automatically generate the documentation. References \u2691 Real Python post on docstrings by James Mertz", "title": "Docstrings"}, {"location": "coding/python/docstrings/#documenting-vs-commenting", "text": "Commenting is describing your code to/for developers. The intended audience is the maintainers and developers of the Python code. In conjuction with well-written code, comments help to guide the reader to better understand your code and its purpose and design. Documenting is describing its use and functionality to your users. While it may be helpful in the development process, the main intended audience are the users.", "title": "Documenting vs commenting"}, {"location": "coding/python/docstrings/#docstring-format", "text": "Docstrings should use the triple-double quote ( \"\"\" ) string format. This should be done whether the docstring is multi-lined or not. At a bare minimum, a docstring should be a quick summary of whatever is it you\u2019re describing and should be contained within a single line. Multi-lined docstrings are used to further elaborate on the object beyond the summary. All multi-lined docstrings have the following parts: A one-line summary line A blank line proceeding the summary Any further elaboration for the docstring Another blank line To ensure your docstrings follow these practices, configure flakeheaven with the flake8-docstrings extension.", "title": "Docstring format"}, {"location": "coding/python/docstrings/#docstring-types", "text": "", "title": "Docstring types"}, {"location": "coding/python/docstrings/#class-docstrings", "text": "Class Docstrings are created for the class itself, as well as any class methods. The docstrings are placed immediately following the class or class method indented by one level: class SimpleClass: \"\"\"Class docstrings go here.\"\"\" def say_hello(self, name: str): \"\"\"Class method docstrings go here.\"\"\" print(f'Hello {name}') Class docstrings should contain the following information: A brief summary of its purpose and behavior Any public methods, along with a brief description Any class properties (attributes) Anything related to the interface for subclassers, if the class is intended to be subclassed The class constructor parameters should be documented within the init class method docstring. Individual methods should be documented using their individual docstrings. Class method docstrings should contain the following: A brief description of what the method is and what it\u2019s used for Any arguments (both required and optional) that are passed including keyword arguments Label any arguments that are considered optional or have a default value Any side effects that occur when executing the method Any exceptions that are raised Any restrictions on when the method can be called", "title": "Class docstrings"}, {"location": "coding/python/docstrings/#package-and-module-docstrings", "text": "Package docstrings should be placed at the top of the package\u2019s __init__.py file. This docstring should list the modules and sub-packages that are exported by the package. Module docstrings should include the following: A brief description of the module and its purpose A list of any classes, exception, functions, and any other objects exported by the module. Only needed if they are not defined in the same file, otherwise help() will get them automatically. The docstring for a module function should include the same items as a class method.", "title": "Package and module docstrings"}, {"location": "coding/python/docstrings/#docstring-formats", "text": "Google docstrings : the most user friendly. reStructured Text : The official ones, but super ugly to write. Numpy docstrings .", "title": "Docstring formats"}, {"location": "coding/python/docstrings/#google-docstrings", "text": "Napoleon gathered a nice cheatsheet with examples .", "title": "Google Docstrings"}, {"location": "coding/python/docstrings/#functions-and-methods", "text": "A method that overrides a method from a base class may have a simple docstring sending the reader to its overridden method\u2019s docstring, such as \"\"\"See base class.\"\"\" . The rationale is that there is no need to repeat in many places documentation that is already present in the base method\u2019s docstring. However, if the overriding method\u2019s behavior is substantially different from the overridden method, or details need to be provided (e.g., documenting additional side effects), a docstring with at least those differences is required on the overriding method. Certain aspects of a function should be documented in special sections, listed below. Each section begins with a heading line, which ends with a colon. All sections other than the heading should maintain a hanging indent of two or four spaces (be consistent within a file). These sections can be omitted in cases where the function\u2019s name and signature are informative enough that it can be aptly described using a one-line docstring. Args List each parameter by name. A description should follow the name, and be separated by a colon followed by either a space or newline. If the description is too long to fit on a single 80-character line, use a hanging indent of 2 or 4 spaces more than the parameter name (be consistent with the rest of the docstrings in the file). The description should include required type(s) if the code does not contain a corresponding type annotation. If a function accepts *foo (variable length argument lists) and/or **bar (arbitrary keyword arguments), they should be listed as *foo and **bar . Returns (or Yields: for generators) Describe the type and semantics of the return value. If the function only returns None, this section is not required. It may also be omitted if the docstring starts with Returns or Yields (e.g. \"\"\"Returns row from API as a tuple of strings.\"\"\" ) and the opening sentence is sufficient to describe return value. Raises List all exceptions that are relevant to the interface followed by a description. Use a similar exception name + colon + space or newline and hanging indent style as described in Args:. You should not document exceptions that get raised if the API specified in the docstring is violated (because this would paradoxically make behavior under violation of the API part of the API).", "title": "Functions and methods"}, {"location": "coding/python/docstrings/#tests-docstrings", "text": "", "title": "Tests docstrings"}, {"location": "coding/python/docstrings/#without-template", "text": "jml has very good tips on writing test's docstrings : If you\u2019re struggling to write docstrings for your tests, here\u2019s a handy five-step guide: Write the first docstring that comes to mind. It will almost certainly be: \"\"\"Test that input is parsed correctly.\"\"\" Get rid of \u201cTest that\u201d or \u201cCheck that\u201d. We know it\u2019s a test. \"\"\"Input should be parsed correctly.\"\"\" Seriously?! Why\u2019d you have to go and add \u201cshould\u201d? It\u2019s a test, it\u2019s all about \u201cshould\u201d. \"\"\"Input is parsed correctly.\"\"\" \u201cCorrectly\u201d, \u201cproperly\u201d, and \u201cas we expect\u201d are all redundant. Axe them too. \"\"\"Input is parsed.\"\"\" Look at what\u2019s left. Is it saying anything at all? If so, great. If not, consider adding something specific about the test behaviour and perhaps even why it\u2019s desirable behaviour to have. \"\"\" Input is parsed into an immutable dict according to the config schema, so we get config info without worrying about input validation all the time. \"\"\"", "title": "Without template"}, {"location": "coding/python/docstrings/#given-when-then", "text": "Given-When-Then is a style of representing test. It's an approach developed as part of Behavior-Driven Development (BDD). It appears as a structuring approach for many testing frameworks such as Cucumber. You can also look at it as a reformulation of the Four-Phase Test pattern. The essential idea is to break down writing a scenario (or test) into three sections: The given part describes the state of the world before you begin the behavior you're specifying in this scenario. You can think of it as the pre-conditions to the test. The when section is that behavior that you're specifying. Finally the then section describes the changes you expect due to the specified behavior. I already implement this test structure with the Arrange, Act, Assert structure, so it made sense to use it in the docstring too. The side effects is that you repeat a lot of prose where a single line would suffice.", "title": "Given When Then"}, {"location": "coding/python/docstrings/#automatic-documentation-generation", "text": "Use the mkdocstrings plugin to automatically generate the documentation.", "title": "Automatic documentation generation"}, {"location": "coding/python/docstrings/#references", "text": "Real Python post on docstrings by James Mertz", "title": "References"}, {"location": "coding/python/factoryboy/", "text": "Factoryboy is a fixtures replacement library to generate fake data for your program. As it's designed to work well with different ORMs (Django, SQLAlchemy , Mongo) it serves the purpose of building real objects for your tests. If you use pydantic, pydantic-factories does all this automatically for you! Install \u2691 pip install factory_boy Or add it to the project requirements.txt . Define a factory class \u2691 Use the following code to generate a factory class for the User SQLAlchemy class. from {{ program_name }} import models import factory # XXX If you add new Factories remember to add the session in conftest.py class UserFactory ( factory . alchemy . SQLAlchemyModelFactory ): \"\"\" Class to generate a fake user element. \"\"\" id = factory . Sequence ( lambda n : n ) name = factory . Faker ( 'name' ) class Meta : model = models . User sqlalchemy_session_persistence = 'commit' As stated in the comment, and if you are using the proposed python project template , remember to add new Factories in conftest.py . Use the factory class \u2691 Create an instance. UserFactory . create () Create an instance with a defined attribute. UserFactory . create ( name = 'John' ) Create 100 instances of objects with an attribute defined. UserFactory . create_batch ( 100 , name = 'John' ) Define attributes \u2691 I like to use the faker integration of factory boy to generate most of the attributes. Generate numbers \u2691 Sequential numbers \u2691 Ideal for IDs id = factory . Sequence ( lambda n : n ) Random number or integer \u2691 author_id = factory . Faker ( 'random_number' ) If you want to limit the number of digits use factory.Faker('random_number', digits=3) Random float \u2691 score = factory . Faker ( 'pyfloat' ) Generate strings \u2691 Word \u2691 default = factory . Faker ( 'word' ) Word from a list \u2691 user = factory . Faker ( 'word' , ext_word_list = [ None , 'value_1' , 'value_2' ]) Word from Enum choices \u2691 First install the Enum provider factory . Faker . add_provider ( EnumProvider ) class EntityFactory ( factory . Factory ): # type: ignore state = factory . Faker ( \"enum\" , enum_cls = EntityState ) Sentences \u2691 description = factory . Faker ( 'sentence' ) Names \u2691 name = factory . Faker ( 'name' ) Urls \u2691 url = factory . Faker ( 'url' ) Files \u2691 file_path = factory . Faker ( 'file_path' ) Generate Datetime \u2691 factory . Faker ( 'date_time' ) Generate bool \u2691 factory . Faker ( 'pybool' ) Generate your own attributes \u2691 Using custom Faker providers \u2691 Using lazy_attributes \u2691 Use lazy_attribute decorator. If you want to use Faker inside a lazy_attribute use .generate({}) at the end of the attribute. In newer versions of Factoryboy you can't use Faker inside a lazy attribute As the Faker object doesn't have the generate method. @factory . lazy_attribute def due ( self ): if random . random () > 0.5 : return factory . Faker ( 'date_time' ) . generate ({}) Define relationships \u2691 Factory Inheritance \u2691 class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) title = factory . Faker ( 'sentence' ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' class ArticleFactory ( ContentFactory ): body = factory . Faker ( 'sentence' ) class Meta : model = models . Article sqlalchemy_session_persistence = 'commit' Dependent objects direct ForeignKey \u2691 When one attribute is actually a complex field (e.g a ForeignKey to another Model), use the SubFactory declaration. Assuming the following model definition: class Author ( Base ): id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'author' ) class Content ( Base ): id = Column ( Integer , primary_key = True , doc = 'Content ID' ) author_id = Column ( String , ForeignKey ( Author . id )) author = relationship ( Author , back_populates = 'contents' ) The related factories would be: class AuthorFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Faker ( 'word' ) class Meta : model = models . Author sqlalchemy_session_persistence = 'commit' class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) author = factory . SubFactory ( AuthorFactory ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' Automatically generate a factory from a pydantic model \u2691 Sadly it's not yet supported , it will at some point though . If you're interested in following this path, you can start with mgaitan snippet for dataclasses. References \u2691 Docs Git Common recipes", "title": "FactoryBoy"}, {"location": "coding/python/factoryboy/#install", "text": "pip install factory_boy Or add it to the project requirements.txt .", "title": "Install"}, {"location": "coding/python/factoryboy/#define-a-factory-class", "text": "Use the following code to generate a factory class for the User SQLAlchemy class. from {{ program_name }} import models import factory # XXX If you add new Factories remember to add the session in conftest.py class UserFactory ( factory . alchemy . SQLAlchemyModelFactory ): \"\"\" Class to generate a fake user element. \"\"\" id = factory . Sequence ( lambda n : n ) name = factory . Faker ( 'name' ) class Meta : model = models . User sqlalchemy_session_persistence = 'commit' As stated in the comment, and if you are using the proposed python project template , remember to add new Factories in conftest.py .", "title": "Define a factory class"}, {"location": "coding/python/factoryboy/#use-the-factory-class", "text": "Create an instance. UserFactory . create () Create an instance with a defined attribute. UserFactory . create ( name = 'John' ) Create 100 instances of objects with an attribute defined. UserFactory . create_batch ( 100 , name = 'John' )", "title": "Use the factory class"}, {"location": "coding/python/factoryboy/#define-attributes", "text": "I like to use the faker integration of factory boy to generate most of the attributes.", "title": "Define attributes"}, {"location": "coding/python/factoryboy/#generate-numbers", "text": "", "title": "Generate numbers"}, {"location": "coding/python/factoryboy/#sequential-numbers", "text": "Ideal for IDs id = factory . Sequence ( lambda n : n )", "title": "Sequential numbers"}, {"location": "coding/python/factoryboy/#random-number-or-integer", "text": "author_id = factory . Faker ( 'random_number' ) If you want to limit the number of digits use factory.Faker('random_number', digits=3)", "title": "Random number or integer"}, {"location": "coding/python/factoryboy/#random-float", "text": "score = factory . Faker ( 'pyfloat' )", "title": "Random float"}, {"location": "coding/python/factoryboy/#generate-strings", "text": "", "title": "Generate strings"}, {"location": "coding/python/factoryboy/#word", "text": "default = factory . Faker ( 'word' )", "title": "Word"}, {"location": "coding/python/factoryboy/#word-from-a-list", "text": "user = factory . Faker ( 'word' , ext_word_list = [ None , 'value_1' , 'value_2' ])", "title": "Word from a list"}, {"location": "coding/python/factoryboy/#word-from-enum-choices", "text": "First install the Enum provider factory . Faker . add_provider ( EnumProvider ) class EntityFactory ( factory . Factory ): # type: ignore state = factory . Faker ( \"enum\" , enum_cls = EntityState )", "title": "Word from Enum choices"}, {"location": "coding/python/factoryboy/#sentences", "text": "description = factory . Faker ( 'sentence' )", "title": "Sentences"}, {"location": "coding/python/factoryboy/#names", "text": "name = factory . Faker ( 'name' )", "title": "Names"}, {"location": "coding/python/factoryboy/#urls", "text": "url = factory . Faker ( 'url' )", "title": "Urls"}, {"location": "coding/python/factoryboy/#files", "text": "file_path = factory . Faker ( 'file_path' )", "title": "Files"}, {"location": "coding/python/factoryboy/#generate-datetime", "text": "factory . Faker ( 'date_time' )", "title": "Generate Datetime"}, {"location": "coding/python/factoryboy/#generate-bool", "text": "factory . Faker ( 'pybool' )", "title": "Generate bool"}, {"location": "coding/python/factoryboy/#generate-your-own-attributes", "text": "", "title": "Generate your own attributes"}, {"location": "coding/python/factoryboy/#using-custom-faker-providers", "text": "", "title": "Using custom Faker providers"}, {"location": "coding/python/factoryboy/#using-lazy_attributes", "text": "Use lazy_attribute decorator. If you want to use Faker inside a lazy_attribute use .generate({}) at the end of the attribute. In newer versions of Factoryboy you can't use Faker inside a lazy attribute As the Faker object doesn't have the generate method. @factory . lazy_attribute def due ( self ): if random . random () > 0.5 : return factory . Faker ( 'date_time' ) . generate ({})", "title": "Using lazy_attributes"}, {"location": "coding/python/factoryboy/#define-relationships", "text": "", "title": "Define relationships"}, {"location": "coding/python/factoryboy/#factory-inheritance", "text": "class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) title = factory . Faker ( 'sentence' ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' class ArticleFactory ( ContentFactory ): body = factory . Faker ( 'sentence' ) class Meta : model = models . Article sqlalchemy_session_persistence = 'commit'", "title": "Factory Inheritance"}, {"location": "coding/python/factoryboy/#dependent-objects-direct-foreignkey", "text": "When one attribute is actually a complex field (e.g a ForeignKey to another Model), use the SubFactory declaration. Assuming the following model definition: class Author ( Base ): id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'author' ) class Content ( Base ): id = Column ( Integer , primary_key = True , doc = 'Content ID' ) author_id = Column ( String , ForeignKey ( Author . id )) author = relationship ( Author , back_populates = 'contents' ) The related factories would be: class AuthorFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Faker ( 'word' ) class Meta : model = models . Author sqlalchemy_session_persistence = 'commit' class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) author = factory . SubFactory ( AuthorFactory ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit'", "title": "Dependent objects direct ForeignKey"}, {"location": "coding/python/factoryboy/#automatically-generate-a-factory-from-a-pydantic-model", "text": "Sadly it's not yet supported , it will at some point though . If you're interested in following this path, you can start with mgaitan snippet for dataclasses.", "title": "Automatically generate a factory from a pydantic model"}, {"location": "coding/python/factoryboy/#references", "text": "Docs Git Common recipes", "title": "References"}, {"location": "coding/python/faker/", "text": "Faker is a Python package that generates fake data for you. Whether you need to bootstrap your database, create good-looking XML documents, fill-in your persistence to stress test it, or anonymize data taken from a production service, Faker is for you. Install \u2691 If you use factoryboy you'd probably have it. If you don't use pip install faker Or add it to the project requirements.txt . Use \u2691 Faker includes a faker fixture for pytest. def test_faker ( faker ): assert isinstance ( faker . name (), str ) By default it's populated with a seed of 0 , to set a random seed add the following to your test configuration. File: conftest.py from random import SystemRandom @pytest . fixture ( scope = \"session\" , autouse = True ) def faker_seed () -> int : \"\"\"Create a random seed for the Faker library.\"\"\" return SystemRandom () . randint ( 0 , 999999 ) Generate fake number \u2691 fake . random_number () If you want to specify max and min values use: faker . pyint ( min_value = 0 , max_value = 99 ) Generate a fake dictionary \u2691 fake . pydict ( nb_elements = 5 , variable_nb_elements = True , * value_types ) Where *value_types can be 'str', 'list' Generate a fake date \u2691 fake . date_time () Generate a random string \u2691 faker . pystr () Create a random string with a defined format \u2691 faker . pystr_format ( \"id-#######{{random_letter}}\" ) 'id-6443059M' Create an IP address \u2691 faker . ipv4 () If you want a CIDR, use network=True . Create a random choice from an Enum \u2691 pydantic uses Enum objects to define the choices of fields , so we need them to create the factories of those objects. Sadly, there is no official provider for Enums , but NazarioJL made a custom provider. Install \u2691 pip install faker-enum Usage \u2691 from enum import Enum from faker import Faker from faker_enum import EnumProvider fake = Faker () fake . add_provider ( EnumProvider ) class Color ( Enum ): RED = 1 GREEN = 2 BLUE = 3 fake . enum ( Color ) # One of [Color.RED, Color.GREEN, Color.BLUE] If you're using factoryboy , check this instructions . Create Optional data \u2691 Install \u2691 pip install fake-optional Usage \u2691 from faker import Faker from faker_optional import OptionalProvider fake = Faker () fake . add_provider ( OptionalProvider ) fake . optional_int () # None fake . optional_int () # 1234 OptionalProvider uses existent faker providers to create the data, so you can use the provider method arguments. For example, optional_int uses the python provider pyint , so you can use the min_value , max_value , and step arguments. Every optional_ method accepts the float ratio argument between 0 and 1 , with a default value of 0.5 to define what percent of results should be None , a greater value will mean that less results will be None . Check the supported methods . Generate your own custom provider \u2691 Providers are just classes which define the methods we call on Faker objects to generate fake data. To define a provider, you need to create a class that inherits from the BaseProvider . That class can then define as many methods as you want. Once your provider is ready, add it to your Faker instance. import random from faker import Faker from faker.providers import BaseProvider fake = Faker () # Our custom provider inherits from the BaseProvider class TravelProvider ( BaseProvider ): def destination ( self ): destinations = [ 'NY' , 'CO' , 'CA' , 'TX' , 'RI' ] # We select a random destination from the list and return it return random . choice ( destinations ) # Add the TravelProvider to our faker object fake . add_provider ( TravelProvider ) # We can now use the destination method: print ( fake . destination ()) If you want to give arguments when calling the provider, add them to the provider method. References \u2691 Git Docs Faker python providers", "title": "Faker"}, {"location": "coding/python/faker/#install", "text": "If you use factoryboy you'd probably have it. If you don't use pip install faker Or add it to the project requirements.txt .", "title": "Install"}, {"location": "coding/python/faker/#use", "text": "Faker includes a faker fixture for pytest. def test_faker ( faker ): assert isinstance ( faker . name (), str ) By default it's populated with a seed of 0 , to set a random seed add the following to your test configuration. File: conftest.py from random import SystemRandom @pytest . fixture ( scope = \"session\" , autouse = True ) def faker_seed () -> int : \"\"\"Create a random seed for the Faker library.\"\"\" return SystemRandom () . randint ( 0 , 999999 )", "title": "Use"}, {"location": "coding/python/faker/#generate-fake-number", "text": "fake . random_number () If you want to specify max and min values use: faker . pyint ( min_value = 0 , max_value = 99 )", "title": "Generate fake number"}, {"location": "coding/python/faker/#generate-a-fake-dictionary", "text": "fake . pydict ( nb_elements = 5 , variable_nb_elements = True , * value_types ) Where *value_types can be 'str', 'list'", "title": "Generate a fake dictionary"}, {"location": "coding/python/faker/#generate-a-fake-date", "text": "fake . date_time ()", "title": "Generate a fake date"}, {"location": "coding/python/faker/#generate-a-random-string", "text": "faker . pystr ()", "title": "Generate a random string"}, {"location": "coding/python/faker/#create-a-random-string-with-a-defined-format", "text": "faker . pystr_format ( \"id-#######{{random_letter}}\" ) 'id-6443059M'", "title": "Create a random string with a defined format"}, {"location": "coding/python/faker/#create-an-ip-address", "text": "faker . ipv4 () If you want a CIDR, use network=True .", "title": "Create an IP address"}, {"location": "coding/python/faker/#create-a-random-choice-from-an-enum", "text": "pydantic uses Enum objects to define the choices of fields , so we need them to create the factories of those objects. Sadly, there is no official provider for Enums , but NazarioJL made a custom provider.", "title": "Create a random choice from an Enum"}, {"location": "coding/python/faker/#install_1", "text": "pip install faker-enum", "title": "Install"}, {"location": "coding/python/faker/#usage", "text": "from enum import Enum from faker import Faker from faker_enum import EnumProvider fake = Faker () fake . add_provider ( EnumProvider ) class Color ( Enum ): RED = 1 GREEN = 2 BLUE = 3 fake . enum ( Color ) # One of [Color.RED, Color.GREEN, Color.BLUE] If you're using factoryboy , check this instructions .", "title": "Usage"}, {"location": "coding/python/faker/#create-optional-data", "text": "", "title": "Create Optional data"}, {"location": "coding/python/faker/#install_2", "text": "pip install fake-optional", "title": "Install"}, {"location": "coding/python/faker/#usage_1", "text": "from faker import Faker from faker_optional import OptionalProvider fake = Faker () fake . add_provider ( OptionalProvider ) fake . optional_int () # None fake . optional_int () # 1234 OptionalProvider uses existent faker providers to create the data, so you can use the provider method arguments. For example, optional_int uses the python provider pyint , so you can use the min_value , max_value , and step arguments. Every optional_ method accepts the float ratio argument between 0 and 1 , with a default value of 0.5 to define what percent of results should be None , a greater value will mean that less results will be None . Check the supported methods .", "title": "Usage"}, {"location": "coding/python/faker/#generate-your-own-custom-provider", "text": "Providers are just classes which define the methods we call on Faker objects to generate fake data. To define a provider, you need to create a class that inherits from the BaseProvider . That class can then define as many methods as you want. Once your provider is ready, add it to your Faker instance. import random from faker import Faker from faker.providers import BaseProvider fake = Faker () # Our custom provider inherits from the BaseProvider class TravelProvider ( BaseProvider ): def destination ( self ): destinations = [ 'NY' , 'CO' , 'CA' , 'TX' , 'RI' ] # We select a random destination from the list and return it return random . choice ( destinations ) # Add the TravelProvider to our faker object fake . add_provider ( TravelProvider ) # We can now use the destination method: print ( fake . destination ()) If you want to give arguments when calling the provider, add them to the provider method.", "title": "Generate your own custom provider"}, {"location": "coding/python/faker/#references", "text": "Git Docs Faker python providers", "title": "References"}, {"location": "coding/python/feedparser/", "text": "Parse Atom and RSS feeds in Python. Install \u2691 pip install feedparser Basic Usage \u2691 Parsing content \u2691 Parse a feed from a remote URL \u2691 >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> d [ 'feed' ][ 'title' ] u 'Sample Feed' Parse a feed from a string \u2691 >>> import feedparser >>> rawdata = \"\"\"<rss version=\"2.0\"> <channel> <title>Sample Feed</title> </channel> </rss>\"\"\" >>> d = feedparser . parse ( rawdata ) >>> d [ 'feed' ][ 'title' ] u 'Sample Feed' Access common elements \u2691 The most commonly used elements in RSS feeds (regardless of version) are title, link, description, publication date, and entry ID. Channel elements \u2691 >>> d . feed . title u 'Sample Feed' >>> d . feed . link u 'http://example.org/' >>> d . feed . description u 'For documentation <em>only</em>' >>> d . feed . published u 'Sat, 07 Sep 2002 00:00:01 GMT' >>> d . feed . published_parsed ( 2002 , 9 , 7 , 0 , 0 , 1 , 5 , 250 , 0 ) All parsed dates can be converted to datetime with the following snippet: from time import mktime from datetime import datetime dt = datetime . fromtimestamp ( mktime ( item [ 'updated_parsed' ])) Item elements \u2691 >>> d . entries [ 0 ] . title u 'First item title' >>> d . entries [ 0 ] . link u 'http://example.org/item/1' >>> d . entries [ 0 ] . description u 'Watch out for <span>nasty tricks</span>' >>> d . entries [ 0 ] . published u 'Thu, 05 Sep 2002 00:00:01 GMT' >>> d . entries [ 0 ] . published_parsed ( 2002 , 9 , 5 , 0 , 0 , 1 , 3 , 248 , 0 ) >>> d . entries [ 0 ] . id u 'http://example.org/guid/1' An RSS feed can specify a small image which some aggregators display as a logo. >>> d . feed . image { 'title' : u 'Example banner' , 'href' : u 'http://example.org/banner.png' , 'width' : 80 , 'height' : 15 , 'link' : u 'http://example.org/' } Feeds and entries can be assigned to multiple categories , and in some versions of RSS, categories can be associated with a \u201cdomain\u201d. >>> d . feed . categories [( u 'Syndic8' , u '1024' ), ( u 'dmoz' , 'Top/Society/People/Personal_Homepages/P/' )] As feeds in the real world may be missing some elements, you may want to test for the existence of an element before getting its value. >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> 'title' in d . feed True >>> 'ttl' in d . feed False >>> d . feed . get ( 'title' , 'No title' ) u 'Sample feed' >>> d . feed . get ( 'ttl' , 60 ) 60 Advanced usage \u2691 It is possible to interact with feeds that are protected with credentials . Issues \u2691 Deprecation warning when using updated_parsed , once solved tweak the airss/adapters/extractor.py#RSS.get at updated_at . Links \u2691 Git Docs", "title": "Feedparser"}, {"location": "coding/python/feedparser/#install", "text": "pip install feedparser", "title": "Install"}, {"location": "coding/python/feedparser/#basic-usage", "text": "", "title": "Basic Usage"}, {"location": "coding/python/feedparser/#parsing-content", "text": "", "title": "Parsing content"}, {"location": "coding/python/feedparser/#parse-a-feed-from-a-remote-url", "text": ">>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> d [ 'feed' ][ 'title' ] u 'Sample Feed'", "title": "Parse a feed from a remote URL"}, {"location": "coding/python/feedparser/#parse-a-feed-from-a-string", "text": ">>> import feedparser >>> rawdata = \"\"\"<rss version=\"2.0\"> <channel> <title>Sample Feed</title> </channel> </rss>\"\"\" >>> d = feedparser . parse ( rawdata ) >>> d [ 'feed' ][ 'title' ] u 'Sample Feed'", "title": "Parse a feed from a string"}, {"location": "coding/python/feedparser/#access-common-elements", "text": "The most commonly used elements in RSS feeds (regardless of version) are title, link, description, publication date, and entry ID.", "title": "Access common elements"}, {"location": "coding/python/feedparser/#channel-elements", "text": ">>> d . feed . title u 'Sample Feed' >>> d . feed . link u 'http://example.org/' >>> d . feed . description u 'For documentation <em>only</em>' >>> d . feed . published u 'Sat, 07 Sep 2002 00:00:01 GMT' >>> d . feed . published_parsed ( 2002 , 9 , 7 , 0 , 0 , 1 , 5 , 250 , 0 ) All parsed dates can be converted to datetime with the following snippet: from time import mktime from datetime import datetime dt = datetime . fromtimestamp ( mktime ( item [ 'updated_parsed' ]))", "title": "Channel elements"}, {"location": "coding/python/feedparser/#item-elements", "text": ">>> d . entries [ 0 ] . title u 'First item title' >>> d . entries [ 0 ] . link u 'http://example.org/item/1' >>> d . entries [ 0 ] . description u 'Watch out for <span>nasty tricks</span>' >>> d . entries [ 0 ] . published u 'Thu, 05 Sep 2002 00:00:01 GMT' >>> d . entries [ 0 ] . published_parsed ( 2002 , 9 , 5 , 0 , 0 , 1 , 3 , 248 , 0 ) >>> d . entries [ 0 ] . id u 'http://example.org/guid/1' An RSS feed can specify a small image which some aggregators display as a logo. >>> d . feed . image { 'title' : u 'Example banner' , 'href' : u 'http://example.org/banner.png' , 'width' : 80 , 'height' : 15 , 'link' : u 'http://example.org/' } Feeds and entries can be assigned to multiple categories , and in some versions of RSS, categories can be associated with a \u201cdomain\u201d. >>> d . feed . categories [( u 'Syndic8' , u '1024' ), ( u 'dmoz' , 'Top/Society/People/Personal_Homepages/P/' )] As feeds in the real world may be missing some elements, you may want to test for the existence of an element before getting its value. >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> 'title' in d . feed True >>> 'ttl' in d . feed False >>> d . feed . get ( 'title' , 'No title' ) u 'Sample feed' >>> d . feed . get ( 'ttl' , 60 ) 60", "title": "Item elements"}, {"location": "coding/python/feedparser/#advanced-usage", "text": "It is possible to interact with feeds that are protected with credentials .", "title": "Advanced usage"}, {"location": "coding/python/feedparser/#issues", "text": "Deprecation warning when using updated_parsed , once solved tweak the airss/adapters/extractor.py#RSS.get at updated_at .", "title": "Issues"}, {"location": "coding/python/feedparser/#links", "text": "Git Docs", "title": "Links"}, {"location": "coding/python/flask/", "text": "Flask is a micro web framework written in Python. It has no database abstraction layer, form validation, or any other components where pre-existing third-party libraries provide common functions. However, Flask supports extensions that can add application features as if they were implemented in Flask itself. Extensions exist for object-relational mappers, form validation, upload handling, various open authentication technologies and several common framework related tools. Extensions are updated far more frequently than the core Flask program. Install \u2691 pip install flask How flask blueprints work \u2691 A blueprint is an object that defines a collection of views, templates, static files and other elements that can be applied to an application. The killer use-case for blueprints is to organize our application into distinct components. However, a Flask Blueprint needs to be registered in an application before you can run it. Once it's done, it extends the application with the contents of the Blueprint. Making a Flask Blueprint \u2691 References \u2691 Docs Tutorials \u2691 Miguel's Flask Mega-Tutorial Patrick's Software Flask Tutorial Blueprints \u2691 Flask docs on blueprints Explore flask article on Blueprints", "title": "Flask"}, {"location": "coding/python/flask/#install", "text": "pip install flask", "title": "Install"}, {"location": "coding/python/flask/#how-flask-blueprints-work", "text": "A blueprint is an object that defines a collection of views, templates, static files and other elements that can be applied to an application. The killer use-case for blueprints is to organize our application into distinct components. However, a Flask Blueprint needs to be registered in an application before you can run it. Once it's done, it extends the application with the contents of the Blueprint.", "title": "How flask blueprints work"}, {"location": "coding/python/flask/#making-a-flask-blueprint", "text": "", "title": "Making a Flask Blueprint"}, {"location": "coding/python/flask/#references", "text": "Docs", "title": "References"}, {"location": "coding/python/flask/#tutorials", "text": "Miguel's Flask Mega-Tutorial Patrick's Software Flask Tutorial", "title": "Tutorials"}, {"location": "coding/python/flask/#blueprints", "text": "Flask docs on blueprints Explore flask article on Blueprints", "title": "Blueprints"}, {"location": "coding/python/flask_restplus/", "text": "Use FastAPI instead Flask-RESTPlus is an extension for Flask that adds support for quickly building REST APIs. Flask-RESTPlus encourages best practices with minimal setup. If you are familiar with Flask, Flask-RESTPlus should be easy to pick up. It provides a coherent collection of decorators and tools to describe your API and expose its documentation properly using Swagger. Over plain flask it adds the following capabilities: It defines namespaces, which are ways of creating prefixes and structuring the code : This helps long-term maintenance and helps with the design when creating new endpoints. It has a full solution for parsing input parameters : This means that we have an easy way of dealing with endpoints that requires several parameters and validates them. It has a serialization framework for the resulting objects : This helps to define objects that can be reused, clarifying the interface and simplifying the development. It has full Swagger API documentation support . Install \u2691 pip install flask-restplus References \u2691 Docs Git", "title": "Flask Restplus"}, {"location": "coding/python/flask_restplus/#install", "text": "pip install flask-restplus", "title": "Install"}, {"location": "coding/python/flask_restplus/#references", "text": "Docs Git", "title": "References"}, {"location": "coding/python/folium/", "text": "folium makes it easy to visualize data that\u2019s been manipulated in Python on an interactive leaflet map. It enables both the binding of data to a map for choropleth visualizations as well as passing rich vector/raster/HTML visualizations as markers on the map. The library has a number of built-in tilesets from OpenStreetMap, Mapbox, and Stamen, and supports custom tilesets with Mapbox or Cloudmade API keys. folium supports both Image, Video, GeoJSON and TopoJSON overlays. Use dash-leaflet if you want to do complex stuff. If you want to do multiple filters on the plotted data or connect the map with other graphics, use dash-leaflet instead. Install \u2691 Although you can install it with pip install folium their release pace is slow, therefore I recommend pulling it directly from master pip install git+https://github.com/python-visualization/folium.git@master It's a heavy repo, so it might take some time. Usage \u2691 Use the following snippet to create an empty map centered in Spain import folium m = folium . Map ( location = [ 40.0884 , - 3.68042 ], zoom_start = 7 , ) # Map configuration goes here m . save ( \"map.html\" ) From now on, those lines are going to be assumed, and all the snippets are supposed to go in between the definition and the save. If you need to center the map elsewhere, I suggest that you configure the map to show the coordinates of the mouse with from folium.plugins import MousePosition MousePosition () . add_to ( m ) Change tileset \u2691 Folium Map object supports different tiles by default. It also supports any WMS or WMTS by passing a Leaflet-style URL to the tiles parameter: http://{s}.yourtiles.com/{z}/{x}/{y}.png . To use the IGN beautiful map as a fallback and the OpenStreetMaps as default use the following snippet: m = folium . Map ( location = [ 40.0884 , - 3.68042 ], zoom_start = 7 , tiles = None , ) folium . raster_layers . TileLayer ( name = \"OpenStreetMaps\" , tiles = \"OpenStreetMap\" , attr = \"OpenStreetMaps\" , ) . add_to ( m ) folium . raster_layers . TileLayer ( name = \"IGN\" , tiles = \"https://www.ign.es/wmts/mapa-raster?request=getTile&layer=MTN&TileMatrixSet=GoogleMapsCompatible&TileMatrix= {z} &TileCol= {x} &TileRow= {y} &format=image/jpeg\" , attr = \"IGN\" , ) . add_to ( m ) folium . LayerControl () . add_to ( m ) We need to set the tiles=None in the Map definition so both are shown in the LayerControl menu. Loading the data \u2691 Using geojson \u2691 folium . GeoJson ( \"my_map.geojson\" , name = \"geojson\" ) . add_to ( m ) If you don't want the data to be embedded in the html use embed=False , this can be handy if you don't want to redeploy the web application on each change of data. You'll need to supply a valid url to the data file. The downside (as of today) of using geojson is that you can't have different markers for the data . The solution is to load it and iterate over the elements. See the issue for more information. Using gpx \u2691 Another popular data format is gpx, which is the one that OsmAnd uses. To import it we'll use the gpxpy library. import gpxpy import gpxpy.gpx gpx_file = open ( 'map.gpx' , 'r' ) gpx = gpxpy . parse ( gpx_file ) References \u2691 Git Docs Quickstart Examples , more examples Use examples \u2691 Flask example Build Interactive GPS activity maps from GPX files Use Open Data to build interactive maps Search examples \u2691 Official docs Leafleft search control Leafleft search examples ( source code ) Searching in OSM data It seems that it doesn't yet support searching for multiple attributes in the geojson data", "title": "Folium"}, {"location": "coding/python/folium/#install", "text": "Although you can install it with pip install folium their release pace is slow, therefore I recommend pulling it directly from master pip install git+https://github.com/python-visualization/folium.git@master It's a heavy repo, so it might take some time.", "title": "Install"}, {"location": "coding/python/folium/#usage", "text": "Use the following snippet to create an empty map centered in Spain import folium m = folium . Map ( location = [ 40.0884 , - 3.68042 ], zoom_start = 7 , ) # Map configuration goes here m . save ( \"map.html\" ) From now on, those lines are going to be assumed, and all the snippets are supposed to go in between the definition and the save. If you need to center the map elsewhere, I suggest that you configure the map to show the coordinates of the mouse with from folium.plugins import MousePosition MousePosition () . add_to ( m )", "title": "Usage"}, {"location": "coding/python/folium/#change-tileset", "text": "Folium Map object supports different tiles by default. It also supports any WMS or WMTS by passing a Leaflet-style URL to the tiles parameter: http://{s}.yourtiles.com/{z}/{x}/{y}.png . To use the IGN beautiful map as a fallback and the OpenStreetMaps as default use the following snippet: m = folium . Map ( location = [ 40.0884 , - 3.68042 ], zoom_start = 7 , tiles = None , ) folium . raster_layers . TileLayer ( name = \"OpenStreetMaps\" , tiles = \"OpenStreetMap\" , attr = \"OpenStreetMaps\" , ) . add_to ( m ) folium . raster_layers . TileLayer ( name = \"IGN\" , tiles = \"https://www.ign.es/wmts/mapa-raster?request=getTile&layer=MTN&TileMatrixSet=GoogleMapsCompatible&TileMatrix= {z} &TileCol= {x} &TileRow= {y} &format=image/jpeg\" , attr = \"IGN\" , ) . add_to ( m ) folium . LayerControl () . add_to ( m ) We need to set the tiles=None in the Map definition so both are shown in the LayerControl menu.", "title": "Change tileset"}, {"location": "coding/python/folium/#loading-the-data", "text": "", "title": "Loading the data"}, {"location": "coding/python/folium/#using-geojson", "text": "folium . GeoJson ( \"my_map.geojson\" , name = \"geojson\" ) . add_to ( m ) If you don't want the data to be embedded in the html use embed=False , this can be handy if you don't want to redeploy the web application on each change of data. You'll need to supply a valid url to the data file. The downside (as of today) of using geojson is that you can't have different markers for the data . The solution is to load it and iterate over the elements. See the issue for more information.", "title": "Using geojson"}, {"location": "coding/python/folium/#using-gpx", "text": "Another popular data format is gpx, which is the one that OsmAnd uses. To import it we'll use the gpxpy library. import gpxpy import gpxpy.gpx gpx_file = open ( 'map.gpx' , 'r' ) gpx = gpxpy . parse ( gpx_file )", "title": "Using gpx"}, {"location": "coding/python/folium/#references", "text": "Git Docs Quickstart Examples , more examples", "title": "References"}, {"location": "coding/python/folium/#use-examples", "text": "Flask example Build Interactive GPS activity maps from GPX files Use Open Data to build interactive maps", "title": "Use examples"}, {"location": "coding/python/folium/#search-examples", "text": "Official docs Leafleft search control Leafleft search examples ( source code ) Searching in OSM data It seems that it doesn't yet support searching for multiple attributes in the geojson data", "title": "Search examples"}, {"location": "coding/python/gitpython/", "text": "GitPython is a python library used to interact with git repositories, high-level like git-porcelain, or low-level like git-plumbing. It provides abstractions of git objects for easy access of repository data, and additionally allows you to access the git repository more directly using either a pure python implementation, or the faster, but more resource intensive git command implementation. The object database implementation is optimized for handling large quantities of objects and large datasets, which is achieved by using low-level structures and data streaming. Installation \u2691 pip install GitPython Usage \u2691 Initialize a repository \u2691 from git import Repo repo = Repo . init ( \"path/to/initialize\" ) If you want to get the working directory of the Repo object use the working_dir attribute. Load a repository \u2691 from git import Repo repo = Repo ( \"existing/git/repo/path\" ) Clone a repository \u2691 from git import Repo Repo . clone_from ( git_url , repo_dir ) Make a commit \u2691 Given a repo object: index = repo . index # add the changes index . add ([ \"README.md\" ]) from git import Actor author = Actor ( \"An author\" , \"author@example.com\" ) committer = Actor ( \"A committer\" , \"committer@example.com\" ) # commit by commit message and author and committer index . commit ( \"my commit message\" , author = author , committer = committer ) Change commit date \u2691 When building fake data , creating commits in other points in time is useful. import datetime from dateutil import tz commit_date = ( datetime . datetime ( 2020 , 2 , 2 , tzinfo = tz . tzlocal ()),) index . commit ( \"my commit message\" , author = author , committer = committer , author_date = commit_date , commit_date = commit_date , ) Inspect the history \u2691 You first need to select the branch you want to inspect. To use the default repository branch use head.reference . repo . head . reference Then you can either get all the Commit objects of that reference, or inspect the log. Get all commits of a branch \u2691 [ commit for commit in repo . iter_commits ( rev = repo . head . reference )] It gives you a List of commits where the first element is the last commit in time. Inspect the log \u2691 Inspect it with the repo.head.reference.log() , which contains a list of RefLogEntry objects that have the interesting attributes: actor : Actor object of the author of the commit time : The commit timestamp, to load it as a datetime object use the datetime.datetime.fromtimestamp method message : Message as a string. Create a branch \u2691 new_branch = repo . create_head ( \"new_branch\" ) assert repo . active_branch != new_branch # It's not checked out yet repo . head . reference = new_branch assert not repo . head . is_detached Get the latest commit of a repository \u2691 repo . head . object . hexsha Testing \u2691 There is no testing functionality, you need to either Mock, build fake data or fake adapters. Build fake data \u2691 Create a test_data directory in your testing directory with the contents of the git repository you want to test. Don't initialize it, we'll create a repo fixture that does it. Assuming that the data is in tests/assets/test_data : File tests/conftest.py : import shutil import pytest from git import Repo @pytest . fixture ( name = \"repo\" ) def repo_ ( tmp_path : Path ) -> Repo : \"\"\"Create a git repository with fake data and history. Args: tmp_path: Pytest fixture that creates a temporal Path \"\"\" # Copy the content from `tests/assets/test_data`. repo_path = tmp_path / \"test_data\" shutil . copytree ( \"tests/assets/test_data\" , repo_path ) # Initializes the git repository. return Repo . init ( repo_path ) On each test you can add the commits that you need for your use case. author = Actor ( \"An author\" , \"author@example.com\" ) committer = Actor ( \"A committer\" , \"committer@example.com\" ) @pytest . mark . freeze_time ( \"2021-02-01T12:00:00\" ) def test_repo_is_not_empty ( repo : Repo ) -> None : commit_date = datetime . datetime ( 2021 , 2 , 1 , tzinfo = tz . tzlocal ()) repo . index . add ([ \"mkdocs.yml\" ]) repo . index . commit ( \"Initial skeleton\" , author = author , committer = committer , author_date = commit_date , commit_date = commit_date , ) assert not repo . bare If you feel that the tests are too verbose, you can create a fixture with all the commits done, and select each case with the freezegun pytest fixture . In my opinion, it will make the tests less clear though. The fixture can look like: File: tests/conftest.py : import datetime from dateutil import tz import shutil import textwrap import pytest from git import Actor , Repo @pytest . fixture ( name = \"full_repo\" ) def full_repo_ ( repo : Repo ) -> Repo : \"\"\"Create a git repository with fake data and history. Args: repo: an initialized Repo \"\"\" index = repo . index author = Actor ( \"An author\" , \"author@example.com\" ) committer = Actor ( \"A committer\" , \"committer@example.com\" ) # Add a commit in time commit_date = datetime . datetime ( 2021 , 2 , 1 , tzinfo = tz . tzlocal ()) index . add ([ \"mkdocs.yml\" ]) index . commit ( \"Initial skeleton\" , author = author , committer = committer , author_date = commit_date , commit_date = commit_date , ) Then you can use that fixture in any test: @pytest . mark . freeze_time ( \"2021-02-01T12:00:00\" ) def test_assert_true ( full_repo : Repo ) -> None : assert not repo . bare References \u2691 Docs Git Tutorial", "title": "GitPython"}, {"location": "coding/python/gitpython/#installation", "text": "pip install GitPython", "title": "Installation"}, {"location": "coding/python/gitpython/#usage", "text": "", "title": "Usage"}, {"location": "coding/python/gitpython/#initialize-a-repository", "text": "from git import Repo repo = Repo . init ( \"path/to/initialize\" ) If you want to get the working directory of the Repo object use the working_dir attribute.", "title": "Initialize a repository"}, {"location": "coding/python/gitpython/#load-a-repository", "text": "from git import Repo repo = Repo ( \"existing/git/repo/path\" )", "title": "Load a repository"}, {"location": "coding/python/gitpython/#clone-a-repository", "text": "from git import Repo Repo . clone_from ( git_url , repo_dir )", "title": "Clone a repository"}, {"location": "coding/python/gitpython/#make-a-commit", "text": "Given a repo object: index = repo . index # add the changes index . add ([ \"README.md\" ]) from git import Actor author = Actor ( \"An author\" , \"author@example.com\" ) committer = Actor ( \"A committer\" , \"committer@example.com\" ) # commit by commit message and author and committer index . commit ( \"my commit message\" , author = author , committer = committer )", "title": "Make a commit"}, {"location": "coding/python/gitpython/#change-commit-date", "text": "When building fake data , creating commits in other points in time is useful. import datetime from dateutil import tz commit_date = ( datetime . datetime ( 2020 , 2 , 2 , tzinfo = tz . tzlocal ()),) index . commit ( \"my commit message\" , author = author , committer = committer , author_date = commit_date , commit_date = commit_date , )", "title": "Change commit date"}, {"location": "coding/python/gitpython/#inspect-the-history", "text": "You first need to select the branch you want to inspect. To use the default repository branch use head.reference . repo . head . reference Then you can either get all the Commit objects of that reference, or inspect the log.", "title": "Inspect the history"}, {"location": "coding/python/gitpython/#get-all-commits-of-a-branch", "text": "[ commit for commit in repo . iter_commits ( rev = repo . head . reference )] It gives you a List of commits where the first element is the last commit in time.", "title": "Get all commits of a branch"}, {"location": "coding/python/gitpython/#inspect-the-log", "text": "Inspect it with the repo.head.reference.log() , which contains a list of RefLogEntry objects that have the interesting attributes: actor : Actor object of the author of the commit time : The commit timestamp, to load it as a datetime object use the datetime.datetime.fromtimestamp method message : Message as a string.", "title": "Inspect the log"}, {"location": "coding/python/gitpython/#create-a-branch", "text": "new_branch = repo . create_head ( \"new_branch\" ) assert repo . active_branch != new_branch # It's not checked out yet repo . head . reference = new_branch assert not repo . head . is_detached", "title": "Create a branch"}, {"location": "coding/python/gitpython/#get-the-latest-commit-of-a-repository", "text": "repo . head . object . hexsha", "title": "Get the latest commit of a repository"}, {"location": "coding/python/gitpython/#testing", "text": "There is no testing functionality, you need to either Mock, build fake data or fake adapters.", "title": "Testing"}, {"location": "coding/python/gitpython/#build-fake-data", "text": "Create a test_data directory in your testing directory with the contents of the git repository you want to test. Don't initialize it, we'll create a repo fixture that does it. Assuming that the data is in tests/assets/test_data : File tests/conftest.py : import shutil import pytest from git import Repo @pytest . fixture ( name = \"repo\" ) def repo_ ( tmp_path : Path ) -> Repo : \"\"\"Create a git repository with fake data and history. Args: tmp_path: Pytest fixture that creates a temporal Path \"\"\" # Copy the content from `tests/assets/test_data`. repo_path = tmp_path / \"test_data\" shutil . copytree ( \"tests/assets/test_data\" , repo_path ) # Initializes the git repository. return Repo . init ( repo_path ) On each test you can add the commits that you need for your use case. author = Actor ( \"An author\" , \"author@example.com\" ) committer = Actor ( \"A committer\" , \"committer@example.com\" ) @pytest . mark . freeze_time ( \"2021-02-01T12:00:00\" ) def test_repo_is_not_empty ( repo : Repo ) -> None : commit_date = datetime . datetime ( 2021 , 2 , 1 , tzinfo = tz . tzlocal ()) repo . index . add ([ \"mkdocs.yml\" ]) repo . index . commit ( \"Initial skeleton\" , author = author , committer = committer , author_date = commit_date , commit_date = commit_date , ) assert not repo . bare If you feel that the tests are too verbose, you can create a fixture with all the commits done, and select each case with the freezegun pytest fixture . In my opinion, it will make the tests less clear though. The fixture can look like: File: tests/conftest.py : import datetime from dateutil import tz import shutil import textwrap import pytest from git import Actor , Repo @pytest . fixture ( name = \"full_repo\" ) def full_repo_ ( repo : Repo ) -> Repo : \"\"\"Create a git repository with fake data and history. Args: repo: an initialized Repo \"\"\" index = repo . index author = Actor ( \"An author\" , \"author@example.com\" ) committer = Actor ( \"A committer\" , \"committer@example.com\" ) # Add a commit in time commit_date = datetime . datetime ( 2021 , 2 , 1 , tzinfo = tz . tzlocal ()) index . add ([ \"mkdocs.yml\" ]) index . commit ( \"Initial skeleton\" , author = author , committer = committer , author_date = commit_date , commit_date = commit_date , ) Then you can use that fixture in any test: @pytest . mark . freeze_time ( \"2021-02-01T12:00:00\" ) def test_assert_true ( full_repo : Repo ) -> None : assert not repo . bare", "title": "Build fake data"}, {"location": "coding/python/gitpython/#references", "text": "Docs Git Tutorial", "title": "References"}, {"location": "coding/python/mkdocstrings/", "text": "mkdocstrings is a library to automatically generate mkdocs pages from the code docstrings. Install \u2691 pip install mkdocstrings Activate the plugin by adding it to the plugin section in the mkdocs.yml configuration file: plugins : - mkdocstrings Usage \u2691 MkDocstrings works by processing special expressions in your Markdown files. The syntax is as follow: ::: identifier YAML block The identifier is a string identifying the object you want to document. The format of an identifier can vary from one handler to another. For example, the Python handler expects the full dotted-path to a Python object: my_package.my_module.MyClass.my_method . The YAML block is optional, and contains some configuration options: handler : the name of the handler to use to collect and render this object. selection : a dictionary of options passed to the handler's collector. The collector is responsible for collecting the documentation from the source code. Therefore, selection options change how the documentation is collected from the source code. rendering : a dictionary of options passed to the handler's renderer. The renderer is responsible for rendering the documentation with Jinja2 templates. Therefore, rendering options affect how the selected object's documentation is rendered. Example with the Python handler docs/my_page.md mkdocs.yml src/my_package/my_module.py Result # Documentation for `MyClass` ::: my_package.my_module.MyClass handler: python selection: members: - method_a - method_b rendering: show_root_heading: false show_source: false nav : - \"My page\" : my_page.md class MyClass : \"\"\"Print print print!\"\"\" def method_a ( self ): \"\"\"Print A!\"\"\" print ( \"A!\" ) def method_b ( self ): \"\"\"Print B!\"\"\" print ( \"B!\" ) def method_c ( self ): \"\"\"Print C!\"\"\" print ( \"C!\" ) Documentation for MyClass Print print print! method_a ( self ) Print A! method_b ( self ) Print B! Reference the objects in the documentation \u2691 With a custom title: [ `Object 1` ][full.path.object1] With the identifier as title: [ full.path.object2 ][] Global options \u2691 MkDocstrings accept a few top-level configuration options in mkdocs.yml : watch : a list of directories to watch while serving the documentation. So if any file is changed in those directories, the documentation is rebuilt. default_handler : the handler that is used by default when no handler is specified. custom_templates : the path to a directory containing custom templates. The path is relative to the docs directory. See Customization . handlers : the handlers global configuration. Example: plugins : - mkdocstrings : default_handler : python handlers : python : rendering : show_source : false custom_templates : templates watch : - src/my_package The handlers global configuration can then be overridden by local configurations: :: : my_package.my_module.MyClass rendering : show_source : true Check the Python handler options for more details. References \u2691 Docs", "title": "mkdocstrings"}, {"location": "coding/python/mkdocstrings/#install", "text": "pip install mkdocstrings Activate the plugin by adding it to the plugin section in the mkdocs.yml configuration file: plugins : - mkdocstrings", "title": "Install"}, {"location": "coding/python/mkdocstrings/#usage", "text": "MkDocstrings works by processing special expressions in your Markdown files. The syntax is as follow: ::: identifier YAML block The identifier is a string identifying the object you want to document. The format of an identifier can vary from one handler to another. For example, the Python handler expects the full dotted-path to a Python object: my_package.my_module.MyClass.my_method . The YAML block is optional, and contains some configuration options: handler : the name of the handler to use to collect and render this object. selection : a dictionary of options passed to the handler's collector. The collector is responsible for collecting the documentation from the source code. Therefore, selection options change how the documentation is collected from the source code. rendering : a dictionary of options passed to the handler's renderer. The renderer is responsible for rendering the documentation with Jinja2 templates. Therefore, rendering options affect how the selected object's documentation is rendered. Example with the Python handler docs/my_page.md mkdocs.yml src/my_package/my_module.py Result # Documentation for `MyClass` ::: my_package.my_module.MyClass handler: python selection: members: - method_a - method_b rendering: show_root_heading: false show_source: false nav : - \"My page\" : my_page.md class MyClass : \"\"\"Print print print!\"\"\" def method_a ( self ): \"\"\"Print A!\"\"\" print ( \"A!\" ) def method_b ( self ): \"\"\"Print B!\"\"\" print ( \"B!\" ) def method_c ( self ): \"\"\"Print C!\"\"\" print ( \"C!\" )", "title": "Usage"}, {"location": "coding/python/mkdocstrings/#reference-the-objects-in-the-documentation", "text": "With a custom title: [ `Object 1` ][full.path.object1] With the identifier as title: [ full.path.object2 ][]", "title": "Reference the objects in the documentation"}, {"location": "coding/python/mkdocstrings/#global-options", "text": "MkDocstrings accept a few top-level configuration options in mkdocs.yml : watch : a list of directories to watch while serving the documentation. So if any file is changed in those directories, the documentation is rebuilt. default_handler : the handler that is used by default when no handler is specified. custom_templates : the path to a directory containing custom templates. The path is relative to the docs directory. See Customization . handlers : the handlers global configuration. Example: plugins : - mkdocstrings : default_handler : python handlers : python : rendering : show_source : false custom_templates : templates watch : - src/my_package The handlers global configuration can then be overridden by local configurations: :: : my_package.my_module.MyClass rendering : show_source : true Check the Python handler options for more details.", "title": "Global options"}, {"location": "coding/python/mkdocstrings/#references", "text": "Docs", "title": "References"}, {"location": "coding/python/pandas/", "text": "Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. Install \u2691 pip3 install pandas Import import pandas as pd Snippets \u2691 Load csv \u2691 data = pd . read_csv ( \"filename.csv\" ) If you want to parse the dates of the start column give read_csv the argument parse_dates=['start'] . Do operation on column data and save it in other column \u2691 # make a simple dataframe df = pd . DataFrame ({ 'a' :[ 1 , 2 ], 'b' :[ 3 , 4 ]}) df # a b # 0 1 3 # 1 2 4 # create an unattached column with an index df . apply ( lambda row : row . a + row . b , axis = 1 ) # 0 4 # 1 6 # do same but attach it to the dataframe df [ 'c' ] = df . apply ( lambda row : row . a + row . b , axis = 1 ) df # a b c # 0 1 3 4 # 1 2 4 6 Get unique values of column \u2691 If we want to get the unique values of the name column: df . name . unique () Extract columns of dataframe \u2691 df1 = df [[ 'a' , 'b' ]] Remove dumplicate rows \u2691 df = df . drop_duplicates () Remove column from dataframe \u2691 del df [ 'name' ] Count unique combinations of values in selected columns \u2691 df1 . groupby ([ 'A' , 'B' ]) . size () . reset_index () . rename ( columns = { 0 : 'count' }) A B count 0 no no 1 1 no yes 2 2 yes no 4 3 yes yes 3 Get row that contains the maximum value of a column \u2691 df . loc [ df [ 'Value' ] . idxmax ()] References \u2691 Homepage", "title": "Pandas"}, {"location": "coding/python/pandas/#install", "text": "pip3 install pandas Import import pandas as pd", "title": "Install"}, {"location": "coding/python/pandas/#snippets", "text": "", "title": "Snippets"}, {"location": "coding/python/pandas/#load-csv", "text": "data = pd . read_csv ( \"filename.csv\" ) If you want to parse the dates of the start column give read_csv the argument parse_dates=['start'] .", "title": "Load csv"}, {"location": "coding/python/pandas/#do-operation-on-column-data-and-save-it-in-other-column", "text": "# make a simple dataframe df = pd . DataFrame ({ 'a' :[ 1 , 2 ], 'b' :[ 3 , 4 ]}) df # a b # 0 1 3 # 1 2 4 # create an unattached column with an index df . apply ( lambda row : row . a + row . b , axis = 1 ) # 0 4 # 1 6 # do same but attach it to the dataframe df [ 'c' ] = df . apply ( lambda row : row . a + row . b , axis = 1 ) df # a b c # 0 1 3 4 # 1 2 4 6", "title": "Do operation on column data and save it in other column"}, {"location": "coding/python/pandas/#get-unique-values-of-column", "text": "If we want to get the unique values of the name column: df . name . unique ()", "title": "Get unique values of column"}, {"location": "coding/python/pandas/#extract-columns-of-dataframe", "text": "df1 = df [[ 'a' , 'b' ]]", "title": "Extract columns of dataframe"}, {"location": "coding/python/pandas/#remove-dumplicate-rows", "text": "df = df . drop_duplicates ()", "title": "Remove dumplicate rows"}, {"location": "coding/python/pandas/#remove-column-from-dataframe", "text": "del df [ 'name' ]", "title": "Remove column from dataframe"}, {"location": "coding/python/pandas/#count-unique-combinations-of-values-in-selected-columns", "text": "df1 . groupby ([ 'A' , 'B' ]) . size () . reset_index () . rename ( columns = { 0 : 'count' }) A B count 0 no no 1 1 no yes 2 2 yes no 4 3 yes yes 3", "title": "Count unique combinations of values in selected columns"}, {"location": "coding/python/pandas/#get-row-that-contains-the-maximum-value-of-a-column", "text": "df . loc [ df [ 'Value' ] . idxmax ()]", "title": "Get row that contains the maximum value of a column"}, {"location": "coding/python/pandas/#references", "text": "Homepage", "title": "References"}, {"location": "coding/python/passpy/", "text": "passpy a platform independent library and cli that is compatible with ZX2C4's pass . Installation \u2691 pip install passpy Usage \u2691 To use passpy in your Python project, we will first have to create a new passpy.store.Store object. import passpy store = passpy . Store () If git or gpg2 are not in your PATH you will have to specify them via git_bin and gpg_bin when creating the store object. You can also create the store on a different folder, be passing store_dir along. To initialize the password store at store_dir , if it isn't already, use store . init_store ( 'store gpg id' ) Where store gpg id is the name of a GPG ID. Optionally, git can be initialized in very much the same way. store . init_git () You are now ready to interact with the password store. You can set and get keys using passpy.store.Store.set_key and passpy.store.Store.get_key . passpy.store.Store.gen_key generates a new password for a new or existing key. To delete a key or directory, use passpy.store.Store.remove_path . For a full overview over all available methods see store-module-label . References \u2691 Docs Git", "title": "Passpy"}, {"location": "coding/python/passpy/#installation", "text": "pip install passpy", "title": "Installation"}, {"location": "coding/python/passpy/#usage", "text": "To use passpy in your Python project, we will first have to create a new passpy.store.Store object. import passpy store = passpy . Store () If git or gpg2 are not in your PATH you will have to specify them via git_bin and gpg_bin when creating the store object. You can also create the store on a different folder, be passing store_dir along. To initialize the password store at store_dir , if it isn't already, use store . init_store ( 'store gpg id' ) Where store gpg id is the name of a GPG ID. Optionally, git can be initialized in very much the same way. store . init_git () You are now ready to interact with the password store. You can set and get keys using passpy.store.Store.set_key and passpy.store.Store.get_key . passpy.store.Store.gen_key generates a new password for a new or existing key. To delete a key or directory, use passpy.store.Store.remove_path . For a full overview over all available methods see store-module-label .", "title": "Usage"}, {"location": "coding/python/passpy/#references", "text": "Docs Git", "title": "References"}, {"location": "coding/python/plotly/", "text": "Plotly is a Python graphing library that makes interactive, publication-quality graphs. Install \u2691 pip3 install plotly Import import plotly.graph_objects as go Snippets \u2691 Select graph source using dropdown \u2691 # imports import plotly.graph_objects as go import numpy as np # data x = list ( np . linspace ( - np . pi , np . pi , 100 )) values_1 = list ( np . sin ( x )) values_1b = [ elem *- 1 for elem in values_1 ] values_2 = list ( np . tan ( x )) values_2b = [ elem *- 1 for elem in values_2 ] # plotly setup] fig = go . Figure () # Add one ore more traces fig . add_traces ( go . Scatter ( x = x , y = values_1 )) fig . add_traces ( go . Scatter ( x = x , y = values_1b )) # construct menus updatemenus = [{ 'buttons' : [{ 'method' : 'update' , 'label' : 'Val 1' , 'args' : [{ 'y' : [ values_1 , values_1b ]},] }, { 'method' : 'update' , 'label' : 'Val 2' , 'args' : [{ 'y' : [ values_2 , values_2b ]},]}], 'direction' : 'down' , 'showactive' : True ,}] # update layout with buttons, and show the figure fig . update_layout ( updatemenus = updatemenus ) fig . show () References \u2691 Homepage Git", "title": "Plotly"}, {"location": "coding/python/plotly/#install", "text": "pip3 install plotly Import import plotly.graph_objects as go", "title": "Install"}, {"location": "coding/python/plotly/#snippets", "text": "", "title": "Snippets"}, {"location": "coding/python/plotly/#select-graph-source-using-dropdown", "text": "# imports import plotly.graph_objects as go import numpy as np # data x = list ( np . linspace ( - np . pi , np . pi , 100 )) values_1 = list ( np . sin ( x )) values_1b = [ elem *- 1 for elem in values_1 ] values_2 = list ( np . tan ( x )) values_2b = [ elem *- 1 for elem in values_2 ] # plotly setup] fig = go . Figure () # Add one ore more traces fig . add_traces ( go . Scatter ( x = x , y = values_1 )) fig . add_traces ( go . Scatter ( x = x , y = values_1b )) # construct menus updatemenus = [{ 'buttons' : [{ 'method' : 'update' , 'label' : 'Val 1' , 'args' : [{ 'y' : [ values_1 , values_1b ]},] }, { 'method' : 'update' , 'label' : 'Val 2' , 'args' : [{ 'y' : [ values_2 , values_2b ]},]}], 'direction' : 'down' , 'showactive' : True ,}] # update layout with buttons, and show the figure fig . update_layout ( updatemenus = updatemenus ) fig . show ()", "title": "Select graph source using dropdown"}, {"location": "coding/python/plotly/#references", "text": "Homepage Git", "title": "References"}, {"location": "coding/python/prompt_toolkit/", "text": "Python Prompt Toolkit is a library for building powerful interactive command line and terminal applications in Python. Installation \u2691 pip install prompt_toolkit Usage \u2691 A simple prompt \u2691 The following snippet is the most simple example, it uses the prompt() function to asks the user for input and returns the text. Just like (raw_)input . from prompt_toolkit import prompt text = prompt ( 'Give me some input: ' ) print ( 'You said: %s ' % text ) It can be used to build REPL applications or full screen ones .", "title": "Prompt Toolkit"}, {"location": "coding/python/prompt_toolkit/#installation", "text": "pip install prompt_toolkit", "title": "Installation"}, {"location": "coding/python/prompt_toolkit/#usage", "text": "", "title": "Usage"}, {"location": "coding/python/prompt_toolkit/#a-simple-prompt", "text": "The following snippet is the most simple example, it uses the prompt() function to asks the user for input and returns the text. Just like (raw_)input . from prompt_toolkit import prompt text = prompt ( 'Give me some input: ' ) print ( 'You said: %s ' % text ) It can be used to build REPL applications or full screen ones .", "title": "A simple prompt"}, {"location": "coding/python/pydantic/", "text": "Pydantic is a data validation and settings management using python type annotations. pydantic enforces type hints at runtime, and provides user friendly errors when data is invalid. Define how data should be in pure, canonical python; check it with pydantic. Install \u2691 pip install pydantic If you use mypy I highly recommend you to activate the pydantic plugin by adding to your pyproject.toml : [tool.mypy] plugins = [ \"pydantic.mypy\" ] [tool.pydantic-mypy] init_forbid_extra = true init_typed = true warn_required_dynamic_aliases = true warn_untyped_fields = true Advantages and disadvantages \u2691 Advantages: Perform data validation in an easy and nice way. Seamless integration with FastAPI and Typer . Nice way to export the data and data schema. Disadvantages: You can't define cyclic relationships , therefore there is no way to simulate the backref SQLAlchemy function. Models \u2691 The primary means of defining objects in pydantic is via models (models are simply classes which inherit from BaseModel ). You can think of models as similar to types in strictly typed languages, or as the requirements of a single endpoint in an API. Untrusted data can be passed to a model, and after parsing and validation pydantic guarantees that the fields of the resultant model instance will conform to the field types defined on the model. Basic model usage \u2691 from pydantic import BaseModel class User ( BaseModel ): id : int name = 'Jane Doe' User here is a model with two fields id which is an integer and is required, and name which is a string and is not required (it has a default value). The type of name is inferred from the default value, and so a type annotation is not required. user = User ( id = '123' ) user here is an instance of User . Initialisation of the object will perform all parsing and validation, if no ValidationError is raised, you know the resulting model instance is valid. Model properties \u2691 Models possess the following methods and attributes: dict() returns a dictionary of the model's fields and values. json() returns a JSON string representation dict() . copy() returns a deep copy of the model. parse_obj() very similar to the __init__ method of the model, used to import objects from a dict rather than keyword arguments. If the object passed is not a dict a ValidationError will be raised. parse_raw() takes a str or bytes and parses it as json , then passes the result to parse_obj . parse_file() reads a file and passes the contents to parse_raw . If content_type is omitted, it is inferred from the file's extension. from_orm() loads data into a model from an arbitrary class. schema() returns a dictionary representing the model as JSON Schema. schema_json() returns a JSON string representation of schema() . Recursive Models \u2691 More complex hierarchical data structures can be defined using models themselves as types in annotations. from typing import List from pydantic import BaseModel class Foo ( BaseModel ): count : int size : float = None class Bar ( BaseModel ): apple = 'x' banana = 'y' class Spam ( BaseModel ): foo : Foo bars : List [ Bar ] m = Spam ( foo = { 'count' : 4 }, bars = [{ 'apple' : 'x1' }, { 'apple' : 'x2' }]) print ( m ) #> foo=Foo(count=4, size=None) bars=[Bar(apple='x1', banana='y'), #> Bar(apple='x2', banana='y')] print ( m . dict ()) \"\"\" { 'foo': {'count': 4, 'size': None}, 'bars': [ {'apple': 'x1', 'banana': 'y'}, {'apple': 'x2', 'banana': 'y'}, ], } \"\"\" For self-referencing models, use postponed annotations. Definition of two models that reference each other \u2691 class A ( BaseModel ): b : Optional [ \"B\" ] = None class B ( BaseModel ): a : Optional [ A ] = None A . update_forward_refs () Although it doesn't work as expected! Error Handling \u2691 pydantic will raise ValidationError whenever it finds an error in the data it's validating. Note Validation code should not raise ValidationError itself, but rather raise ValueError , TypeError or AssertionError (or subclasses of ValueError or TypeError ) which will be caught and used to populate ValidationError . One exception will be raised regardless of the number of errors found, that ValidationError will contain information about all the errors and how they happened. You can access these errors in a several ways: e.errors() method will return list of errors found in the input data. e.json() method will return a JSON representation of errors . str(e) method will return a human readable representation of the errors. Each error object contains: loc the error's location as a list. The first item in the list will be the field where the error occurred, and if the field is a sub-model, subsequent items will be present to indicate the nested location of the error. type a computer-readable identifier of the error type. msg a human readable explanation of the error. ctx an optional object which contains values required to render the error message. Custom Errors \u2691 You can also define your own error classes, which can specify a custom error code, message template, and context: from pydantic import BaseModel , PydanticValueError , ValidationError , validator class NotABarError ( PydanticValueError ): code = 'not_a_bar' msg_template = 'value is not \"bar\", got \" {wrong_value} \"' class Model ( BaseModel ): foo : str @validator ( 'foo' ) def name_must_contain_space ( cls , v ): if v != 'bar' : raise NotABarError ( wrong_value = v ) return v try : Model ( foo = 'ber' ) except ValidationError as e : print ( e . json ()) \"\"\" [ { \"loc\": [ \"foo\" ], \"msg\": \"value is not \\\"bar\\\", got \\\"ber\\\"\", \"type\": \"value_error.not_a_bar\", \"ctx\": { \"wrong_value\": \"ber\" } } ] \"\"\" Dynamic model creation \u2691 There are some occasions where the shape of a model is not known until runtime. For this pydantic provides the create_model method to allow models to be created on the fly. from pydantic import BaseModel , create_model DynamicFoobarModel = create_model ( 'DynamicFoobarModel' , foo = ( str , ... ), bar = 123 ) class StaticFoobarModel ( BaseModel ): foo : str bar : int = 123 Here StaticFoobarModel and DynamicFoobarModel are identical. Warning See the note in Required Optional Fields for the distinct between an ellipsis as a field default and annotation only fields. See samuelcolvin/pydantic#1047 for more details. Fields are defined by either a tuple of the form (<type>, <default value>) or just a default value. The special key word arguments __config__ and __base__ can be used to customize the new model. This includes extending a base model with extra fields. from pydantic import BaseModel , create_model class FooModel ( BaseModel ): foo : str bar : int = 123 BarModel = create_model ( 'BarModel' , apple = 'russet' , banana = 'yellow' , __base__ = FooModel , ) print ( BarModel ) #> <class 'BarModel'> print ( BarModel . __fields__ . keys ()) #> dict_keys(['foo', 'bar', 'apple', 'banana']) Abstract Base Classes \u2691 Pydantic models can be used alongside Python's Abstract Base Classes (ABCs). import abc from pydantic import BaseModel class FooBarModel ( BaseModel , abc . ABC ): a : str b : int @abc . abstractmethod def my_abstract_method ( self ): pass Field Ordering \u2691 Field order is important in models for the following reasons: Validation is performed in the order fields are defined; fields validators can access the values of earlier fields, but not later ones Field order is preserved in the model schema Field order is preserved in validation errors Field order is preserved by .dict() and .json() etc. As of v1.0 all fields with annotations (whether annotation-only or with a default value) will precede all fields without an annotation. Within their respective groups, fields remain in the order they were defined. Field with dynamic default value \u2691 When declaring a field with a default value, you may want it to be dynamic (i.e. different for each model). To do this, you may want to use a default_factory . In Beta The default_factory argument is in beta , it has been added to pydantic in v1.5 on a provisional basis . It may change significantly in future releases and its signature or behaviour will not be concrete until v2 . Feedback from the community while it's still provisional would be extremely useful; either comment on #866 or create a new issue. Example of usage: from datetime import datetime from uuid import UUID , uuid4 from pydantic import BaseModel , Field class Model ( BaseModel ): uid : UUID = Field ( default_factory = uuid4 ) updated : datetime = Field ( default_factory = datetime . utcnow ) m1 = Model () m2 = Model () print ( f ' { m1 . uid } != { m2 . uid } ' ) #> 3b187763-a19c-4ed8-9588-387e224e04f1 != 0c58f97b-c8a7-4fe8-8550-e9b2b8026574 print ( f ' { m1 . updated } != { m2 . updated } ' ) #> 2020-07-15 20:01:48.451066 != 2020-07-15 20:01:48.451083 Warning The default_factory expects the field type to be set. Moreover if you want to validate default values with validate_all , pydantic will need to call the default_factory , which could lead to side effects! Field customization \u2691 Optionally, the Field function can be used to provide extra information about the field and validations. It has the following arguments: default : (a positional argument) the default value of the field. Since the Field replaces the field's default, this first argument can be used to set the default. Use ellipsis ( ... ) to indicate the field is required. default_factory : a zero-argument callable that will be called when a default value is needed for this field. Among other purposes, this can be used to set dynamic default values. It is forbidden to set both default and default_factory . alias : the public name of the field. title : if omitted, field_name.title() is used. description : if omitted and the annotation is a sub-model, the docstring of the sub-model will be used. const : this argument must be the same as the field's default value if present. gt : for numeric values ( int , float , Decimal ), adds a validation of \"greater than\" and an annotation of exclusiveMinimum to the JSON Schema. ge : for numeric values, this adds a validation of \"greater than or equal\" and an annotation of minimum to the JSON Schema. lt : for numeric values, this adds a validation of \"less than\" and an annotation of exclusiveMaximum to the JSON Schema. le : for numeric values, this adds a validation of \"less than or equal\" and an annotation of maximum to the JSON Schema. multiple_of : for numeric values, this adds a validation of \"a multiple of\" and an annotation of multipleOf to the JSON Schema. min_items : for list values, this adds a corresponding validation and an annotation of minItems to the JSON Schema. max_items : for list values, this adds a corresponding validation and an annotation of maxItems to the JSON Schema. min_length : for string values, this adds a corresponding validation and an annotation of minLength to the JSON Schema. max_length : for string values, this adds a corresponding validation and an annotation of maxLength to the JSON Schema. allow_mutation : a boolean which defaults to True . When False , the field raises a TypeError if the field is assigned on an instance. The model config must set validate_assignment to True for this check to be performed. regex : for string values, this adds a Regular Expression validation generated from the passed string and an annotation of pattern to the JSON Schema. ** : any other keyword arguments (e.g. examples ) will be added verbatim to the field's schema. !!! note pydantic validates strings using re.match , which treats regular expressions as implicitly anchored at the beginning. On the contrary, JSON Schema validators treat the pattern keyword as implicitly unanchored, more like what re.search does. Instead of using Field , the fields property of the Config class can be used to set all of the arguments above except default. Parsing data into a specified type \u2691 Pydantic includes a standalone utility function parse_obj_as that can be used to apply the parsing logic used to populate pydantic models in a more ad-hoc way. This function behaves similarly to BaseModel.parse_obj , but works with arbitrary pydantic-compatible types. This is especially useful when you want to parse results into a type that is not a direct subclass of BaseModel . For example: from typing import List from pydantic import BaseModel , parse_obj_as class Item ( BaseModel ): id : int name : str # `item_data` could come from an API call, eg., via something like: # item_data = requests.get('https://my-api.com/items').json() item_data = [{ 'id' : 1 , 'name' : 'My Item' }] items = parse_obj_as ( List [ Item ], item_data ) print ( items ) #> [Item(id=1, name='My Item')] This function is capable of parsing data into any of the types pydantic can handle as fields of a BaseModel . Pydantic also includes a similar standalone function called parse_file_as , which is analogous to BaseModel.parse_file . Data Conversion \u2691 pydantic may cast input data to force it to conform to model field types, and in some cases this may result in a loss of information. For example: from pydantic import BaseModel class Model ( BaseModel ): a : int b : float c : str print ( Model ( a = 3.1415 , b = ' 2.72 ' , c = 123 ) . dict ()) #> {'a': 3, 'b': 2.72, 'c': '123'} This is a deliberate decision of pydantic , and in general it's the most useful approach. See here for a longer discussion on the subject. Initialize attributes at object creation \u2691 If you want to initialize attributes of the object automatically at object creation, similar of what you'd do with the __init__ method of the class, you need to use root_validators . from pydantic import root_validator class PypikaRepository ( BaseModel ): \"\"\"Implement the repository pattern using the Pypika query builder.\"\"\" connection : sqlite3 . Connection cursor : sqlite3 . Cursor class Config : \"\"\"Configure the pydantic model.\"\"\" arbitrary_types_allowed = True @root_validator ( pre = True ) @classmethod def set_connection ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Set the connection to the database. Raises: ConnectionError: If there is no database file. \"\"\" database_file = values [ \"database_url\" ] . replace ( \"sqlite:///\" , \"\" ) if not os . path . isfile ( database_file ): raise ConnectionError ( f \"There is no database file: { database_file } \" ) connection = sqlite3 . connect ( database_file ) values [ \"connection\" ] = connection values [ \"cursor\" ] = connection . cursor () return values I had to set the arbitrary_types_allowed because the sqlite3 objects are not between the pydantic object types. Set private attributes \u2691 If you want to define some attributes that are not part of the model use PrivateAttr : from datetime import datetime from random import randint from pydantic import BaseModel , PrivateAttr class TimeAwareModel ( BaseModel ): _processed_at : datetime = PrivateAttr ( default_factory = datetime . now ) _secret_value : str = PrivateAttr () def __init__ ( self , ** data : Any ) -> None : super () . __init__ ( ** data ) # this could also be done with default_factory self . _secret_value = randint ( 1 , 5 ) m = TimeAwareModel () print ( m . _processed_at ) #> 2021-03-03 17:30:04.030758 print ( m . _secret_value ) #> 5 Define fields to exclude from exporting at config level \u2691 This won't be necessary once they release the version 1.9 because you can define the fields to exclude in the Config of the model using something like: class User ( BaseModel ): id : int username : str password : str class Transaction ( BaseModel ): id : str user : User value : int class Config : fields = { 'value' : { 'alias' : 'Amount' , 'exclude' : ... , }, 'user' : { 'exclude' : { 'username' , 'password' } }, 'id' : { 'dump_alias' : 'external_id' } } The release it's taking its time because the developer's gremlin and salaried work are sucking his time off . Update entity attributes with a dictionary \u2691 To update a model with the data of a dictionary you can create a new object with the new data using the update argument of the copy method. class FooBarModel ( BaseModel ): banana : float foo : str m = FooBarModel ( banana = 3.14 , foo = 'hello' ) m . copy ( update = { 'banana' : 0 }) Lazy loading attributes \u2691 Currently there is no official support for lazy loading model attributes. You can define your own properties but when you export the schema they won't appear there. dgasmith has a workaround though. Troubleshooting \u2691 Ignore a field when representing an object \u2691 Use repr=False . This is useful for properties that don't return a value quickly, for example if you save an sh background process. class Temp ( BaseModel ): foo : typing . Any boo : typing . Any = Field ( ... , repr = False ) Copy produces copy that modifies the original \u2691 When copying a model, changing the value of an attribute on the copy updates the value of the attribute on the original. This only happens if deep != True . To fix it use: model.copy(deep=True) . E0611: No name 'BaseModel' in module 'pydantic' \u2691 Add to your pyproject.toml the following lines: # --------- Pylint ------------- [tool.pylint.'MESSAGES CONTROL'] extension-pkg-whitelist = \"pydantic\" Or if it fails, add to the line # pylint: extension-pkg-whitelist . References \u2691 Docs Git", "title": "Pydantic"}, {"location": "coding/python/pydantic/#install", "text": "pip install pydantic If you use mypy I highly recommend you to activate the pydantic plugin by adding to your pyproject.toml : [tool.mypy] plugins = [ \"pydantic.mypy\" ] [tool.pydantic-mypy] init_forbid_extra = true init_typed = true warn_required_dynamic_aliases = true warn_untyped_fields = true", "title": "Install"}, {"location": "coding/python/pydantic/#advantages-and-disadvantages", "text": "Advantages: Perform data validation in an easy and nice way. Seamless integration with FastAPI and Typer . Nice way to export the data and data schema. Disadvantages: You can't define cyclic relationships , therefore there is no way to simulate the backref SQLAlchemy function.", "title": "Advantages and disadvantages"}, {"location": "coding/python/pydantic/#models", "text": "The primary means of defining objects in pydantic is via models (models are simply classes which inherit from BaseModel ). You can think of models as similar to types in strictly typed languages, or as the requirements of a single endpoint in an API. Untrusted data can be passed to a model, and after parsing and validation pydantic guarantees that the fields of the resultant model instance will conform to the field types defined on the model.", "title": "Models"}, {"location": "coding/python/pydantic/#basic-model-usage", "text": "from pydantic import BaseModel class User ( BaseModel ): id : int name = 'Jane Doe' User here is a model with two fields id which is an integer and is required, and name which is a string and is not required (it has a default value). The type of name is inferred from the default value, and so a type annotation is not required. user = User ( id = '123' ) user here is an instance of User . Initialisation of the object will perform all parsing and validation, if no ValidationError is raised, you know the resulting model instance is valid.", "title": "Basic model usage"}, {"location": "coding/python/pydantic/#model-properties", "text": "Models possess the following methods and attributes: dict() returns a dictionary of the model's fields and values. json() returns a JSON string representation dict() . copy() returns a deep copy of the model. parse_obj() very similar to the __init__ method of the model, used to import objects from a dict rather than keyword arguments. If the object passed is not a dict a ValidationError will be raised. parse_raw() takes a str or bytes and parses it as json , then passes the result to parse_obj . parse_file() reads a file and passes the contents to parse_raw . If content_type is omitted, it is inferred from the file's extension. from_orm() loads data into a model from an arbitrary class. schema() returns a dictionary representing the model as JSON Schema. schema_json() returns a JSON string representation of schema() .", "title": "Model properties"}, {"location": "coding/python/pydantic/#recursive-models", "text": "More complex hierarchical data structures can be defined using models themselves as types in annotations. from typing import List from pydantic import BaseModel class Foo ( BaseModel ): count : int size : float = None class Bar ( BaseModel ): apple = 'x' banana = 'y' class Spam ( BaseModel ): foo : Foo bars : List [ Bar ] m = Spam ( foo = { 'count' : 4 }, bars = [{ 'apple' : 'x1' }, { 'apple' : 'x2' }]) print ( m ) #> foo=Foo(count=4, size=None) bars=[Bar(apple='x1', banana='y'), #> Bar(apple='x2', banana='y')] print ( m . dict ()) \"\"\" { 'foo': {'count': 4, 'size': None}, 'bars': [ {'apple': 'x1', 'banana': 'y'}, {'apple': 'x2', 'banana': 'y'}, ], } \"\"\" For self-referencing models, use postponed annotations.", "title": "Recursive Models"}, {"location": "coding/python/pydantic/#definition-of-two-models-that-reference-each-other", "text": "class A ( BaseModel ): b : Optional [ \"B\" ] = None class B ( BaseModel ): a : Optional [ A ] = None A . update_forward_refs () Although it doesn't work as expected!", "title": "Definition of two models that reference each other"}, {"location": "coding/python/pydantic/#error-handling", "text": "pydantic will raise ValidationError whenever it finds an error in the data it's validating. Note Validation code should not raise ValidationError itself, but rather raise ValueError , TypeError or AssertionError (or subclasses of ValueError or TypeError ) which will be caught and used to populate ValidationError . One exception will be raised regardless of the number of errors found, that ValidationError will contain information about all the errors and how they happened. You can access these errors in a several ways: e.errors() method will return list of errors found in the input data. e.json() method will return a JSON representation of errors . str(e) method will return a human readable representation of the errors. Each error object contains: loc the error's location as a list. The first item in the list will be the field where the error occurred, and if the field is a sub-model, subsequent items will be present to indicate the nested location of the error. type a computer-readable identifier of the error type. msg a human readable explanation of the error. ctx an optional object which contains values required to render the error message.", "title": "Error Handling"}, {"location": "coding/python/pydantic/#custom-errors", "text": "You can also define your own error classes, which can specify a custom error code, message template, and context: from pydantic import BaseModel , PydanticValueError , ValidationError , validator class NotABarError ( PydanticValueError ): code = 'not_a_bar' msg_template = 'value is not \"bar\", got \" {wrong_value} \"' class Model ( BaseModel ): foo : str @validator ( 'foo' ) def name_must_contain_space ( cls , v ): if v != 'bar' : raise NotABarError ( wrong_value = v ) return v try : Model ( foo = 'ber' ) except ValidationError as e : print ( e . json ()) \"\"\" [ { \"loc\": [ \"foo\" ], \"msg\": \"value is not \\\"bar\\\", got \\\"ber\\\"\", \"type\": \"value_error.not_a_bar\", \"ctx\": { \"wrong_value\": \"ber\" } } ] \"\"\"", "title": "Custom Errors"}, {"location": "coding/python/pydantic/#dynamic-model-creation", "text": "There are some occasions where the shape of a model is not known until runtime. For this pydantic provides the create_model method to allow models to be created on the fly. from pydantic import BaseModel , create_model DynamicFoobarModel = create_model ( 'DynamicFoobarModel' , foo = ( str , ... ), bar = 123 ) class StaticFoobarModel ( BaseModel ): foo : str bar : int = 123 Here StaticFoobarModel and DynamicFoobarModel are identical. Warning See the note in Required Optional Fields for the distinct between an ellipsis as a field default and annotation only fields. See samuelcolvin/pydantic#1047 for more details. Fields are defined by either a tuple of the form (<type>, <default value>) or just a default value. The special key word arguments __config__ and __base__ can be used to customize the new model. This includes extending a base model with extra fields. from pydantic import BaseModel , create_model class FooModel ( BaseModel ): foo : str bar : int = 123 BarModel = create_model ( 'BarModel' , apple = 'russet' , banana = 'yellow' , __base__ = FooModel , ) print ( BarModel ) #> <class 'BarModel'> print ( BarModel . __fields__ . keys ()) #> dict_keys(['foo', 'bar', 'apple', 'banana'])", "title": "Dynamic model creation"}, {"location": "coding/python/pydantic/#abstract-base-classes", "text": "Pydantic models can be used alongside Python's Abstract Base Classes (ABCs). import abc from pydantic import BaseModel class FooBarModel ( BaseModel , abc . ABC ): a : str b : int @abc . abstractmethod def my_abstract_method ( self ): pass", "title": "Abstract Base Classes"}, {"location": "coding/python/pydantic/#field-ordering", "text": "Field order is important in models for the following reasons: Validation is performed in the order fields are defined; fields validators can access the values of earlier fields, but not later ones Field order is preserved in the model schema Field order is preserved in validation errors Field order is preserved by .dict() and .json() etc. As of v1.0 all fields with annotations (whether annotation-only or with a default value) will precede all fields without an annotation. Within their respective groups, fields remain in the order they were defined.", "title": "Field Ordering"}, {"location": "coding/python/pydantic/#field-with-dynamic-default-value", "text": "When declaring a field with a default value, you may want it to be dynamic (i.e. different for each model). To do this, you may want to use a default_factory . In Beta The default_factory argument is in beta , it has been added to pydantic in v1.5 on a provisional basis . It may change significantly in future releases and its signature or behaviour will not be concrete until v2 . Feedback from the community while it's still provisional would be extremely useful; either comment on #866 or create a new issue. Example of usage: from datetime import datetime from uuid import UUID , uuid4 from pydantic import BaseModel , Field class Model ( BaseModel ): uid : UUID = Field ( default_factory = uuid4 ) updated : datetime = Field ( default_factory = datetime . utcnow ) m1 = Model () m2 = Model () print ( f ' { m1 . uid } != { m2 . uid } ' ) #> 3b187763-a19c-4ed8-9588-387e224e04f1 != 0c58f97b-c8a7-4fe8-8550-e9b2b8026574 print ( f ' { m1 . updated } != { m2 . updated } ' ) #> 2020-07-15 20:01:48.451066 != 2020-07-15 20:01:48.451083 Warning The default_factory expects the field type to be set. Moreover if you want to validate default values with validate_all , pydantic will need to call the default_factory , which could lead to side effects!", "title": "Field with dynamic default value"}, {"location": "coding/python/pydantic/#field-customization", "text": "Optionally, the Field function can be used to provide extra information about the field and validations. It has the following arguments: default : (a positional argument) the default value of the field. Since the Field replaces the field's default, this first argument can be used to set the default. Use ellipsis ( ... ) to indicate the field is required. default_factory : a zero-argument callable that will be called when a default value is needed for this field. Among other purposes, this can be used to set dynamic default values. It is forbidden to set both default and default_factory . alias : the public name of the field. title : if omitted, field_name.title() is used. description : if omitted and the annotation is a sub-model, the docstring of the sub-model will be used. const : this argument must be the same as the field's default value if present. gt : for numeric values ( int , float , Decimal ), adds a validation of \"greater than\" and an annotation of exclusiveMinimum to the JSON Schema. ge : for numeric values, this adds a validation of \"greater than or equal\" and an annotation of minimum to the JSON Schema. lt : for numeric values, this adds a validation of \"less than\" and an annotation of exclusiveMaximum to the JSON Schema. le : for numeric values, this adds a validation of \"less than or equal\" and an annotation of maximum to the JSON Schema. multiple_of : for numeric values, this adds a validation of \"a multiple of\" and an annotation of multipleOf to the JSON Schema. min_items : for list values, this adds a corresponding validation and an annotation of minItems to the JSON Schema. max_items : for list values, this adds a corresponding validation and an annotation of maxItems to the JSON Schema. min_length : for string values, this adds a corresponding validation and an annotation of minLength to the JSON Schema. max_length : for string values, this adds a corresponding validation and an annotation of maxLength to the JSON Schema. allow_mutation : a boolean which defaults to True . When False , the field raises a TypeError if the field is assigned on an instance. The model config must set validate_assignment to True for this check to be performed. regex : for string values, this adds a Regular Expression validation generated from the passed string and an annotation of pattern to the JSON Schema. ** : any other keyword arguments (e.g. examples ) will be added verbatim to the field's schema. !!! note pydantic validates strings using re.match , which treats regular expressions as implicitly anchored at the beginning. On the contrary, JSON Schema validators treat the pattern keyword as implicitly unanchored, more like what re.search does. Instead of using Field , the fields property of the Config class can be used to set all of the arguments above except default.", "title": "Field customization"}, {"location": "coding/python/pydantic/#parsing-data-into-a-specified-type", "text": "Pydantic includes a standalone utility function parse_obj_as that can be used to apply the parsing logic used to populate pydantic models in a more ad-hoc way. This function behaves similarly to BaseModel.parse_obj , but works with arbitrary pydantic-compatible types. This is especially useful when you want to parse results into a type that is not a direct subclass of BaseModel . For example: from typing import List from pydantic import BaseModel , parse_obj_as class Item ( BaseModel ): id : int name : str # `item_data` could come from an API call, eg., via something like: # item_data = requests.get('https://my-api.com/items').json() item_data = [{ 'id' : 1 , 'name' : 'My Item' }] items = parse_obj_as ( List [ Item ], item_data ) print ( items ) #> [Item(id=1, name='My Item')] This function is capable of parsing data into any of the types pydantic can handle as fields of a BaseModel . Pydantic also includes a similar standalone function called parse_file_as , which is analogous to BaseModel.parse_file .", "title": "Parsing data into a specified type"}, {"location": "coding/python/pydantic/#data-conversion", "text": "pydantic may cast input data to force it to conform to model field types, and in some cases this may result in a loss of information. For example: from pydantic import BaseModel class Model ( BaseModel ): a : int b : float c : str print ( Model ( a = 3.1415 , b = ' 2.72 ' , c = 123 ) . dict ()) #> {'a': 3, 'b': 2.72, 'c': '123'} This is a deliberate decision of pydantic , and in general it's the most useful approach. See here for a longer discussion on the subject.", "title": "Data Conversion"}, {"location": "coding/python/pydantic/#initialize-attributes-at-object-creation", "text": "If you want to initialize attributes of the object automatically at object creation, similar of what you'd do with the __init__ method of the class, you need to use root_validators . from pydantic import root_validator class PypikaRepository ( BaseModel ): \"\"\"Implement the repository pattern using the Pypika query builder.\"\"\" connection : sqlite3 . Connection cursor : sqlite3 . Cursor class Config : \"\"\"Configure the pydantic model.\"\"\" arbitrary_types_allowed = True @root_validator ( pre = True ) @classmethod def set_connection ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Set the connection to the database. Raises: ConnectionError: If there is no database file. \"\"\" database_file = values [ \"database_url\" ] . replace ( \"sqlite:///\" , \"\" ) if not os . path . isfile ( database_file ): raise ConnectionError ( f \"There is no database file: { database_file } \" ) connection = sqlite3 . connect ( database_file ) values [ \"connection\" ] = connection values [ \"cursor\" ] = connection . cursor () return values I had to set the arbitrary_types_allowed because the sqlite3 objects are not between the pydantic object types.", "title": "Initialize attributes at object creation"}, {"location": "coding/python/pydantic/#set-private-attributes", "text": "If you want to define some attributes that are not part of the model use PrivateAttr : from datetime import datetime from random import randint from pydantic import BaseModel , PrivateAttr class TimeAwareModel ( BaseModel ): _processed_at : datetime = PrivateAttr ( default_factory = datetime . now ) _secret_value : str = PrivateAttr () def __init__ ( self , ** data : Any ) -> None : super () . __init__ ( ** data ) # this could also be done with default_factory self . _secret_value = randint ( 1 , 5 ) m = TimeAwareModel () print ( m . _processed_at ) #> 2021-03-03 17:30:04.030758 print ( m . _secret_value ) #> 5", "title": "Set private attributes"}, {"location": "coding/python/pydantic/#define-fields-to-exclude-from-exporting-at-config-level", "text": "This won't be necessary once they release the version 1.9 because you can define the fields to exclude in the Config of the model using something like: class User ( BaseModel ): id : int username : str password : str class Transaction ( BaseModel ): id : str user : User value : int class Config : fields = { 'value' : { 'alias' : 'Amount' , 'exclude' : ... , }, 'user' : { 'exclude' : { 'username' , 'password' } }, 'id' : { 'dump_alias' : 'external_id' } } The release it's taking its time because the developer's gremlin and salaried work are sucking his time off .", "title": "Define fields to exclude from exporting at config level"}, {"location": "coding/python/pydantic/#update-entity-attributes-with-a-dictionary", "text": "To update a model with the data of a dictionary you can create a new object with the new data using the update argument of the copy method. class FooBarModel ( BaseModel ): banana : float foo : str m = FooBarModel ( banana = 3.14 , foo = 'hello' ) m . copy ( update = { 'banana' : 0 })", "title": "Update entity attributes with a dictionary"}, {"location": "coding/python/pydantic/#lazy-loading-attributes", "text": "Currently there is no official support for lazy loading model attributes. You can define your own properties but when you export the schema they won't appear there. dgasmith has a workaround though.", "title": "Lazy loading attributes"}, {"location": "coding/python/pydantic/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "coding/python/pydantic/#ignore-a-field-when-representing-an-object", "text": "Use repr=False . This is useful for properties that don't return a value quickly, for example if you save an sh background process. class Temp ( BaseModel ): foo : typing . Any boo : typing . Any = Field ( ... , repr = False )", "title": "Ignore a field when representing an object"}, {"location": "coding/python/pydantic/#copy-produces-copy-that-modifies-the-original", "text": "When copying a model, changing the value of an attribute on the copy updates the value of the attribute on the original. This only happens if deep != True . To fix it use: model.copy(deep=True) .", "title": "Copy produces copy that modifies the original"}, {"location": "coding/python/pydantic/#e0611-no-name-basemodel-in-module-pydantic", "text": "Add to your pyproject.toml the following lines: # --------- Pylint ------------- [tool.pylint.'MESSAGES CONTROL'] extension-pkg-whitelist = \"pydantic\" Or if it fails, add to the line # pylint: extension-pkg-whitelist .", "title": "E0611: No name 'BaseModel' in module 'pydantic'"}, {"location": "coding/python/pydantic/#references", "text": "Docs Git", "title": "References"}, {"location": "coding/python/pydantic_exporting/", "text": "As well as accessing model attributes directly via their names (e.g. model.foobar ), models can be converted and exported in a number of ways: model.dict(...) \u2691 This is the primary way of converting a model to a dictionary. Sub-models will be recursively converted to dictionaries. Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. by_alias : Whether field aliases should be used as keys in the returned dictionary; default False . exclude_unset : Whether fields which were not explicitly set when creating the model should be excluded from the returned dictionary; default False . exclude_defaults : Whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False . exclude_none : Whether fields which are equal to None should be excluded from the returned dictionary; default False . Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) # returns a dictionary: print ( m . dict ()) \"\"\" { 'banana': 3.14, 'foo': 'hello', 'bar': {'whatever': 123}, } \"\"\" print ( m . dict ( include = { 'foo' , 'bar' })) #> {'foo': 'hello', 'bar': {'whatever': 123}} print ( m . dict ( exclude = { 'foo' , 'bar' })) #> {'banana': 3.14} dict(model) and iteration \u2691 pydantic models can also be converted to dictionaries using dict(model) , and you can also iterate over a model's field using for field_name, value in model: . With this approach the raw field values are returned, so sub-models will not be converted to dictionaries. Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) print ( dict ( m )) \"\"\" { 'banana': 3.14, 'foo': 'hello', 'bar': BarModel( whatever=123, ), } \"\"\" for name , value in m : print ( f ' { name } : { value } ' ) #> banana: 3.14 #> foo: hello #> bar: whatever=123 model.copy(...) \u2691 copy() allows models to be duplicated, which is particularly useful for immutable models. Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. update : A dictionary of values to change when creating the copied model deep : whether to make a deep copy of the new model; default False Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) print ( m . copy ( include = { 'foo' , 'bar' })) #> foo='hello' bar=BarModel(whatever=123) print ( m . copy ( exclude = { 'foo' , 'bar' })) #> banana=3.14 print ( m . copy ( update = { 'banana' : 0 })) #> banana=0 foo='hello' bar=BarModel(whatever=123) print ( id ( m . bar ), id ( m . copy () . bar )) #> 139868119420992 139868119420992 # normal copy gives the same object reference for `bar` print ( id ( m . bar ), id ( m . copy ( deep = True ) . bar )) #> 139868119420992 139868119423296 # deep copy gives a new object reference for `bar` model.json(...) \u2691 The .json() method will serialise a model to JSON. Typically, .json() in turn calls .dict() and serialises its result. (For models with a custom root type, after calling .dict() , only the value for the __root__ key is serialised). Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. by_alias : Whether field aliases should be used as keys in the returned dictionary; default False . exclude_unset : Whether fields which were not set when creating the model and have their default values should be excluded from the returned dictionary; default False . exclude_defaults : Whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False . exclude_none : Whether fields which are equal to None should be excluded from the returned dictionary; default False . encoder : A custom encoder function passed to the default argument of json.dumps() ; defaults to a custom encoder designed to take care of all common types. **dumps_kwargs : Any other keyword arguments are passed to json.dumps() , e.g. indent . pydantic can serialise many commonly used types to JSON (e.g. datetime , date or UUID ) which would normally fail with a simple json.dumps(foobar) . from datetime import datetime from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): foo : datetime bar : BarModel m = FooBarModel ( foo = datetime ( 2032 , 6 , 1 , 12 , 13 , 14 ), bar = { 'whatever' : 123 }) print ( m . json ()) #> {\"foo\": \"2032-06-01T12:13:14\", \"bar\": {\"whatever\": 123}} Advanced include and exclude \u2691 The dict , json , and copy methods support include and exclude arguments which can either be sets or dictionaries. This allows nested selection of which fields to export: from pydantic import BaseModel , SecretStr class User ( BaseModel ): id : int username : str password : SecretStr class Transaction ( BaseModel ): id : str user : User value : int t = Transaction ( id = '1234567890' , user = User ( id = 42 , username = 'JohnDoe' , password = 'hashedpassword' ), value = 9876543210 , ) # using a set: print ( t . dict ( exclude = { 'user' , 'value' })) #> {'id': '1234567890'} # using a dict: print ( t . dict ( exclude = { 'user' : { 'username' , 'password' }, 'value' : ... })) #> {'id': '1234567890', 'user': {'id': 42}} print ( t . dict ( include = { 'id' : ... , 'user' : { 'id' }})) #> {'id': '1234567890', 'user': {'id': 42}} The ellipsis ( ... ) indicates that we want to exclude or include an entire key, just as if we included it in a set. Of course, the same can be done at any depth level. Special care must be taken when including or excluding fields from a list or tuple of submodels or dictionaries. In this scenario, dict and related methods expect integer keys for element-wise inclusion or exclusion. To exclude a field from every member of a list or tuple, the dictionary key '__all__' can be used as follows: import datetime from typing import List from pydantic import BaseModel , SecretStr class Country ( BaseModel ): name : str phone_code : int class Address ( BaseModel ): post_code : int country : Country class CardDetails ( BaseModel ): number : SecretStr expires : datetime . date class Hobby ( BaseModel ): name : str info : str class User ( BaseModel ): first_name : str second_name : str address : Address card_details : CardDetails hobbies : List [ Hobby ] user = User ( first_name = 'John' , second_name = 'Doe' , address = Address ( post_code = 123456 , country = Country ( name = 'USA' , phone_code = 1 ) ), card_details = CardDetails ( number = 4212934504460000 , expires = datetime . date ( 2020 , 5 , 1 ) ), hobbies = [ Hobby ( name = 'Programming' , info = 'Writing code and stuff' ), Hobby ( name = 'Gaming' , info = 'Hell Yeah!!!' ), ], ) exclude_keys = { 'second_name' : ... , 'address' : { 'post_code' : ... , 'country' : { 'phone_code' }}, 'card_details' : ... , # You can exclude fields from specific members of a tuple/list by index: 'hobbies' : { - 1 : { 'info' }}, } include_keys = { 'first_name' : ... , 'address' : { 'country' : { 'name' }}, 'hobbies' : { 0 : ... , - 1 : { 'name' }}, } # would be the same as user.dict(exclude=exclude_keys) in this case: print ( user . dict ( include = include_keys )) \"\"\" { 'first_name': 'John', 'address': {'country': {'name': 'USA'}}, 'hobbies': [ { 'name': 'Programming', 'info': 'Writing code and stuff', }, {'name': 'Gaming'}, ], } \"\"\" # To exclude a field from all members of a nested list or tuple, use \"__all__\": print ( user . dict ( exclude = { 'hobbies' : { '__all__' : { 'info' }}})) \"\"\" { 'first_name': 'John', 'second_name': 'Doe', 'address': { 'post_code': 123456, 'country': {'name': 'USA', 'phone_code': 1}, }, 'card_details': { 'number': SecretStr('**********'), 'expires': datetime.date(2020, 5, 1), }, 'hobbies': [{'name': 'Programming'}, {'name': 'Gaming'}], } \"\"\" The same holds for the json and copy methods. References \u2691 Pydantic exporting models", "title": "Pydantic Exporting Models"}, {"location": "coding/python/pydantic_exporting/#modeldict", "text": "This is the primary way of converting a model to a dictionary. Sub-models will be recursively converted to dictionaries. Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. by_alias : Whether field aliases should be used as keys in the returned dictionary; default False . exclude_unset : Whether fields which were not explicitly set when creating the model should be excluded from the returned dictionary; default False . exclude_defaults : Whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False . exclude_none : Whether fields which are equal to None should be excluded from the returned dictionary; default False . Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) # returns a dictionary: print ( m . dict ()) \"\"\" { 'banana': 3.14, 'foo': 'hello', 'bar': {'whatever': 123}, } \"\"\" print ( m . dict ( include = { 'foo' , 'bar' })) #> {'foo': 'hello', 'bar': {'whatever': 123}} print ( m . dict ( exclude = { 'foo' , 'bar' })) #> {'banana': 3.14}", "title": "model.dict(...)"}, {"location": "coding/python/pydantic_exporting/#dictmodel-and-iteration", "text": "pydantic models can also be converted to dictionaries using dict(model) , and you can also iterate over a model's field using for field_name, value in model: . With this approach the raw field values are returned, so sub-models will not be converted to dictionaries. Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) print ( dict ( m )) \"\"\" { 'banana': 3.14, 'foo': 'hello', 'bar': BarModel( whatever=123, ), } \"\"\" for name , value in m : print ( f ' { name } : { value } ' ) #> banana: 3.14 #> foo: hello #> bar: whatever=123", "title": "dict(model) and iteration"}, {"location": "coding/python/pydantic_exporting/#modelcopy", "text": "copy() allows models to be duplicated, which is particularly useful for immutable models. Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. update : A dictionary of values to change when creating the copied model deep : whether to make a deep copy of the new model; default False Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) print ( m . copy ( include = { 'foo' , 'bar' })) #> foo='hello' bar=BarModel(whatever=123) print ( m . copy ( exclude = { 'foo' , 'bar' })) #> banana=3.14 print ( m . copy ( update = { 'banana' : 0 })) #> banana=0 foo='hello' bar=BarModel(whatever=123) print ( id ( m . bar ), id ( m . copy () . bar )) #> 139868119420992 139868119420992 # normal copy gives the same object reference for `bar` print ( id ( m . bar ), id ( m . copy ( deep = True ) . bar )) #> 139868119420992 139868119423296 # deep copy gives a new object reference for `bar`", "title": "model.copy(...)"}, {"location": "coding/python/pydantic_exporting/#modeljson", "text": "The .json() method will serialise a model to JSON. Typically, .json() in turn calls .dict() and serialises its result. (For models with a custom root type, after calling .dict() , only the value for the __root__ key is serialised). Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. by_alias : Whether field aliases should be used as keys in the returned dictionary; default False . exclude_unset : Whether fields which were not set when creating the model and have their default values should be excluded from the returned dictionary; default False . exclude_defaults : Whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False . exclude_none : Whether fields which are equal to None should be excluded from the returned dictionary; default False . encoder : A custom encoder function passed to the default argument of json.dumps() ; defaults to a custom encoder designed to take care of all common types. **dumps_kwargs : Any other keyword arguments are passed to json.dumps() , e.g. indent . pydantic can serialise many commonly used types to JSON (e.g. datetime , date or UUID ) which would normally fail with a simple json.dumps(foobar) . from datetime import datetime from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): foo : datetime bar : BarModel m = FooBarModel ( foo = datetime ( 2032 , 6 , 1 , 12 , 13 , 14 ), bar = { 'whatever' : 123 }) print ( m . json ()) #> {\"foo\": \"2032-06-01T12:13:14\", \"bar\": {\"whatever\": 123}}", "title": "model.json(...)"}, {"location": "coding/python/pydantic_exporting/#advanced-include-and-exclude", "text": "The dict , json , and copy methods support include and exclude arguments which can either be sets or dictionaries. This allows nested selection of which fields to export: from pydantic import BaseModel , SecretStr class User ( BaseModel ): id : int username : str password : SecretStr class Transaction ( BaseModel ): id : str user : User value : int t = Transaction ( id = '1234567890' , user = User ( id = 42 , username = 'JohnDoe' , password = 'hashedpassword' ), value = 9876543210 , ) # using a set: print ( t . dict ( exclude = { 'user' , 'value' })) #> {'id': '1234567890'} # using a dict: print ( t . dict ( exclude = { 'user' : { 'username' , 'password' }, 'value' : ... })) #> {'id': '1234567890', 'user': {'id': 42}} print ( t . dict ( include = { 'id' : ... , 'user' : { 'id' }})) #> {'id': '1234567890', 'user': {'id': 42}} The ellipsis ( ... ) indicates that we want to exclude or include an entire key, just as if we included it in a set. Of course, the same can be done at any depth level. Special care must be taken when including or excluding fields from a list or tuple of submodels or dictionaries. In this scenario, dict and related methods expect integer keys for element-wise inclusion or exclusion. To exclude a field from every member of a list or tuple, the dictionary key '__all__' can be used as follows: import datetime from typing import List from pydantic import BaseModel , SecretStr class Country ( BaseModel ): name : str phone_code : int class Address ( BaseModel ): post_code : int country : Country class CardDetails ( BaseModel ): number : SecretStr expires : datetime . date class Hobby ( BaseModel ): name : str info : str class User ( BaseModel ): first_name : str second_name : str address : Address card_details : CardDetails hobbies : List [ Hobby ] user = User ( first_name = 'John' , second_name = 'Doe' , address = Address ( post_code = 123456 , country = Country ( name = 'USA' , phone_code = 1 ) ), card_details = CardDetails ( number = 4212934504460000 , expires = datetime . date ( 2020 , 5 , 1 ) ), hobbies = [ Hobby ( name = 'Programming' , info = 'Writing code and stuff' ), Hobby ( name = 'Gaming' , info = 'Hell Yeah!!!' ), ], ) exclude_keys = { 'second_name' : ... , 'address' : { 'post_code' : ... , 'country' : { 'phone_code' }}, 'card_details' : ... , # You can exclude fields from specific members of a tuple/list by index: 'hobbies' : { - 1 : { 'info' }}, } include_keys = { 'first_name' : ... , 'address' : { 'country' : { 'name' }}, 'hobbies' : { 0 : ... , - 1 : { 'name' }}, } # would be the same as user.dict(exclude=exclude_keys) in this case: print ( user . dict ( include = include_keys )) \"\"\" { 'first_name': 'John', 'address': {'country': {'name': 'USA'}}, 'hobbies': [ { 'name': 'Programming', 'info': 'Writing code and stuff', }, {'name': 'Gaming'}, ], } \"\"\" # To exclude a field from all members of a nested list or tuple, use \"__all__\": print ( user . dict ( exclude = { 'hobbies' : { '__all__' : { 'info' }}})) \"\"\" { 'first_name': 'John', 'second_name': 'Doe', 'address': { 'post_code': 123456, 'country': {'name': 'USA', 'phone_code': 1}, }, 'card_details': { 'number': SecretStr('**********'), 'expires': datetime.date(2020, 5, 1), }, 'hobbies': [{'name': 'Programming'}, {'name': 'Gaming'}], } \"\"\" The same holds for the json and copy methods.", "title": "Advanced include and exclude"}, {"location": "coding/python/pydantic_exporting/#references", "text": "Pydantic exporting models", "title": "References"}, {"location": "coding/python/pydantic_functions/", "text": "The validate_arguments decorator allows the arguments passed to a function to be parsed and validated using the function's annotations before the function is called. While under the hood this uses the same approach of model creation and initialisation; it provides an extremely easy way to apply validation to your code with minimal boilerplate. In Beta The validate_arguments decorator is in beta , it has been added to pydantic in v1.5 on a provisional basis . It may change significantly in future releases and its interface will not be concrete until v2 . Feedback from the community while it's still provisional would be extremely useful; either comment on #1205 or create a new issue. Be sure you understand it's limitations . Example of usage: from pydantic import validate_arguments , ValidationError @validate_arguments def repeat ( s : str , count : int , * , separator : bytes = b '' ) -> bytes : b = s . encode () return separator . join ( b for _ in range ( count )) a = repeat ( 'hello' , 3 ) print ( a ) #> b'hellohellohello' b = repeat ( 'x' , '4' , separator = ' ' ) print ( b ) #> b'x x x x' try : c = repeat ( 'hello' , 'wrong' ) except ValidationError as exc : print ( exc ) \"\"\" 1 validation error for Repeat count value is not a valid integer (type=type_error.integer) \"\"\" Usage with mypy \u2691 The validate_arguments decorator should work \"out of the box\" with mypy since it's defined to return a function with the same signature as the function it decorates. The only limitation is that since we trick mypy into thinking the function returned by the decorator is the same as the function being decorated; access to the raw function or other attributes will require type: ignore . References \u2691 Pydantic validation decorator docs", "title": "Pydantic Validating Functions"}, {"location": "coding/python/pydantic_functions/#usage-with-mypy", "text": "The validate_arguments decorator should work \"out of the box\" with mypy since it's defined to return a function with the same signature as the function it decorates. The only limitation is that since we trick mypy into thinking the function returned by the decorator is the same as the function being decorated; access to the raw function or other attributes will require type: ignore .", "title": "Usage with mypy"}, {"location": "coding/python/pydantic_functions/#references", "text": "Pydantic validation decorator docs", "title": "References"}, {"location": "coding/python/pydantic_mypy_plugin/", "text": "Pydantic works well with mypy right out of the box. However, Pydantic also ships with a mypy plugin that adds a number of important pydantic-specific features to mypy that improve its ability to type-check your code. Enabling the Plugin \u2691 To enable the plugin, just add pydantic.mypy to the list of plugins in your mypy config file (this could be mypy.ini or setup.cfg ). To get started, all you need to do is create a mypy.ini file with following contents: [mypy] plugins = pydantic.mypy See the mypy usage and plugin configuration docs for more details. References \u2691 Pydantic mypy plugin docs", "title": "Pydantic Mypy Plugin"}, {"location": "coding/python/pydantic_mypy_plugin/#enabling-the-plugin", "text": "To enable the plugin, just add pydantic.mypy to the list of plugins in your mypy config file (this could be mypy.ini or setup.cfg ). To get started, all you need to do is create a mypy.ini file with following contents: [mypy] plugins = pydantic.mypy See the mypy usage and plugin configuration docs for more details.", "title": "Enabling the Plugin"}, {"location": "coding/python/pydantic_mypy_plugin/#references", "text": "Pydantic mypy plugin docs", "title": "References"}, {"location": "coding/python/pydantic_types/", "text": "Where possible pydantic uses standard library types to define fields, thus smoothing the learning curve. For many useful applications, however, no standard library type exists, so pydantic implements many commonly used types . If no existing type suits your purpose you can also implement your own pydantic-compatible types with custom properties and validation. Standard Library Types \u2691 pydantic supports many common types from the python standard library. If you need stricter processing see Strict Types ; if you need to constrain the values allowed (e.g. to require a positive int) see Constrained Types . bool see Booleans for details on how bools are validated and what values are permitted. int pydantic uses int(v) to coerce types to an int ; see this warning on loss of information during data conversion. float similarly, float(v) is used to coerce values to floats. str strings are accepted as-is, int float and Decimal are coerced using str(v) , bytes and bytearray are converted using v.decode() , enums inheriting from str are converted using v.value , and all other types cause an error. list allows list , tuple , set , frozenset , or generators and casts to a list. tuple allows list , tuple , set , frozenset , or generators and casts to a tuple. dict dict(v) is used to attempt to convert a dictionary. set allows list , tuple , set , frozenset , or generators and casts to a set. frozenset allows list , tuple , set , frozenset , or generators and casts to a frozen set. datetime.date see Datetime Types below for more detail on parsing and validation. datetime.time see Datetime Types below for more detail on parsing and validation. datetime.datetime see Datetime Types below for more detail on parsing and validation. datetime.timedelta see Datetime Types below for more detail on parsing and validation. typing.Any allows any value include None , thus an Any field is optional. typing.TypeVar constrains the values allowed based on constraints or bound , see TypeVar . typing.Union see Unions below for more detail on parsing and validation. typing.Optional Optional[x] is simply short hand for Union[x, None] ; see Unions below for more detail on parsing and validation. typing.List : typing.Tuple : typing.Dict : typing.Set : typing.FrozenSet : typing.Sequence : typing.Iterable this is reserved for iterables that shouldn't be consumed. See Infinite Generators below for more detail on parsing and validation. typing.Type see Type below for more detail on parsing and validation. typing.Callable see Callable for more detail on parsing and validation. typing.Pattern will cause the input value to be passed to re.compile(v) to create a regex pattern. ipaddress.IPv4Address simply uses the type itself for validation by passing the value to IPv4Address(v) . ipaddress.IPv4Interface simply uses the type itself for validation by passing the value to IPv4Address(v) . ipaddress.IPv4Network simply uses the type itself for validation by passing the value to IPv4Network(v) . enum.Enum checks that the value is a valid member of the enum; see Enums and Choices for more details. enum.IntEnum checks that the value is a valid member of the integer enum; see Enums and Choices for more details. decimal.Decimal pydantic attempts to convert the value to a string, then passes the string to Decimal(v) . pathlib.Path simply uses the type itself for validation by passing the value to Path(v) . Iterables \u2691 Define default value for an iterable \u2691 If you want to define an empty list, dictionary, set or other iterable as a model attribute, you can use the default_factory . from typing import Sequence from pydantic import BaseModel , Field class Foo ( BaseModel ): defaulted_list_field : Sequence [ str ] = Field ( default_factory = list ) It might be tempting to do class Foo ( BaseModel ): defaulted_list_field : Sequence [ str ] = [] # Bad! But you'll follow the mutable default argument anti-pattern. Unions \u2691 The Union type allows a model attribute to accept different types, e.g.: from uuid import UUID from typing import Union from pydantic import BaseModel class User ( BaseModel ): id : Union [ int , str , UUID ] name : str user_01 = User ( id = 123 , name = 'John Doe' ) print ( user_01 ) #> id=123 name='John Doe' print ( user_01 . id ) #> 123 user_02 = User ( id = '1234' , name = 'John Doe' ) print ( user_02 ) #> id=1234 name='John Doe' print ( user_02 . id ) #> 1234 user_03_uuid = UUID ( 'cf57432e-809e-4353-adbd-9d5c0d733868' ) user_03 = User ( id = user_03_uuid , name = 'John Doe' ) print ( user_03 ) #> id=275603287559914445491632874575877060712 name='John Doe' print ( user_03 . id ) #> 275603287559914445491632874575877060712 print ( user_03_uuid . int ) #> 275603287559914445491632874575877060712 However, as can be seen above, pydantic will attempt to 'match' any of the types defined under Union and will use the first one that matches. In the above example the id of user_03 was defined as a uuid.UUID class (which is defined under the attribute's Union annotation) but as the uuid.UUID can be marshalled into an int it chose to match against the int type and disregarded the other types. As such, it is recommended that, when defining Union annotations, the most specific type is included first and followed by less specific types. In the above example, the UUID class should precede the int and str classes to preclude the unexpected representation as such: from uuid import UUID from typing import Union from pydantic import BaseModel class User ( BaseModel ): id : Union [ UUID , int , str ] name : str user_03_uuid = UUID ( 'cf57432e-809e-4353-adbd-9d5c0d733868' ) user_03 = User ( id = user_03_uuid , name = 'John Doe' ) print ( user_03 ) #> id=UUID('cf57432e-809e-4353-adbd-9d5c0d733868') name='John Doe' print ( user_03 . id ) #> cf57432e-809e-4353-adbd-9d5c0d733868 print ( user_03_uuid . int ) #> 275603287559914445491632874575877060712 Enums and Choices \u2691 pydantic uses python's standard enum classes to define choices. from enum import Enum , IntEnum from pydantic import BaseModel , ValidationError class FruitEnum ( str , Enum ): pear = 'pear' banana = 'banana' class ToolEnum ( IntEnum ): spanner = 1 wrench = 2 class CookingModel ( BaseModel ): fruit : FruitEnum = FruitEnum . pear tool : ToolEnum = ToolEnum . spanner print ( CookingModel ()) #> fruit=<FruitEnum.pear: 'pear'> tool=<ToolEnum.spanner: 1> print ( CookingModel ( tool = 2 , fruit = 'banana' )) #> fruit=<FruitEnum.banana: 'banana'> tool=<ToolEnum.wrench: 2> try : CookingModel ( fruit = 'other' ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for CookingModel fruit value is not a valid enumeration member; permitted: 'pear', 'banana' (type=type_error.enum; enum_values=[<FruitEnum.pear: 'pear'>, <FruitEnum.banana: 'banana'>]) \"\"\" Datetime Types \u2691 Pydantic supports the following datetime types: datetime fields can be: datetime , existing datetime object int or float , assumed as Unix time, i.e. seconds (if >= -2e10 or <= 2e10 ) or milliseconds (if < -2e10 or > 2e10 ) since 1 January 1970 str , following formats work: YYYY-MM-DD[T]HH:MM[:SS[.ffffff]][Z[\u00b1]HH[:]MM]]] int or float as a string (assumed as Unix time) date fields can be: date , existing date object int or float , see datetime str , following formats work: YYYY-MM-DD int or float , see datetime time fields can be: time , existing time object str , following formats work: HH:MM[:SS[.ffffff]] timedelta fields can be: timedelta , existing timedelta object int or float , assumed as seconds str , following formats work: [-][DD ][HH:MM]SS[.ffffff] [\u00b1]P[DD]DT[HH]H[MM]M[SS]S (ISO 8601 format for timedelta) Type \u2691 pydantic supports the use of Type[T] to specify that a field may only accept classes (not instances) that are subclasses of T . from typing import Type from pydantic import BaseModel from pydantic import ValidationError class Foo : pass class Bar ( Foo ): pass class Other : pass class SimpleModel ( BaseModel ): just_subclasses : Type [ Foo ] SimpleModel ( just_subclasses = Foo ) SimpleModel ( just_subclasses = Bar ) try : SimpleModel ( just_subclasses = Other ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for SimpleModel just_subclasses subclass of Foo expected (type=type_error.subclass; expected_class=Foo) \"\"\" TypeVar \u2691 TypeVar is supported either unconstrained, constrained or with a bound. from typing import TypeVar from pydantic import BaseModel Foobar = TypeVar ( 'Foobar' ) BoundFloat = TypeVar ( 'BoundFloat' , bound = float ) IntStr = TypeVar ( 'IntStr' , int , str ) class Model ( BaseModel ): a : Foobar # equivalent of \": Any\" b : BoundFloat # equivalent of \": float\" c : IntStr # equivalent of \": Union[int, str]\" print ( Model ( a = [ 1 ], b = 4.2 , c = 'x' )) #> a=[1] b=4.2 c='x' # a may be None and is therefore optional print ( Model ( b = 1 , c = 1 )) #> a=None b=1.0 c=1 Pydantic Types \u2691 pydantic also provides a variety of other useful types: FilePath like Path , but the path must exist and be a file. DirectoryPath like Path , but the path must exist and be a directory. Color for parsing HTML and CSS colors; see Color Type . Json a special type wrapper which loads JSON before parsing; see JSON Type . AnyUrl any URL; see URLs . AnyHttpUrl an HTTP URL; see URLs . HttpUrl a stricter HTTP URL; see URLs . PostgresDsn a postgres DSN style URL; see URLs . RedisDsn a redis DSN style URL; see URLs . SecretStr string where the value is kept partially secret; see Secrets . IPvAnyAddress allows either an IPv4Address or an IPv6Address . IPvAnyInterface allows either an IPv4Interface or an IPv6Interface . IPvAnyNetwork allows either an IPv4Network or an IPv6Network . NegativeFloat allows a float which is negative; uses standard float parsing then checks the value is less than 0; see Constrained Types . NegativeInt allows an int which is negative; uses standard int parsing then checks the value is less than 0; see Constrained Types . PositiveFloat allows a float which is positive; uses standard float parsing then checks the value is greater than 0; see Constrained Types . PositiveInt allows an int which is positive; uses standard int parsing then checks the value is greater than 0; see Constrained Types . condecimal type method for constraining Decimals; see Constrained Types . confloat type method for constraining floats; see Constrained Types . conint type method for constraining ints; see Constrained Types . conlist type method for constraining lists; see Constrained Types . conset type method for constraining sets; see Constrained Types . constr type method for constraining strs; see Constrained Types . Custom Data Types \u2691 You can also define your own custom data types. There are several ways to achieve it. Classes with __get_validators__ \u2691 You use a custom class with a classmethod __get_validators__ . It will be called to get validators to parse and validate the input data. Tip These validators have the same semantics as in Validators , you can declare a parameter config , field , etc. import re from pydantic import BaseModel # https://en.wikipedia.org/wiki/Postcodes_in_the_United_Kingdom#Validation post_code_regex = re . compile ( r '(?:' r '([A-Z]{1,2}[0-9][A-Z0-9]?|ASCN|STHL|TDCU|BBND|[BFS]IQQ|PCRN|TKCA) ?' r '([0-9][A-Z] {2} )|' r '(BFPO) ?([0-9]{1,4})|' r '(KY[0-9]|MSR|VG|AI)[ -]?[0-9] {4} |' r '([A-Z] {2} ) ?([0-9] {2} )|' r '(GE) ?(CX)|' r '(GIR) ?(0A {2} )|' r '(SAN) ?(TA1)' r ')' ) class PostCode ( str ): \"\"\" Partial UK postcode validation. Note: this is just an example, and is not intended for use in production; in particular this does NOT guarantee a postcode exists, just that it has a valid format. \"\"\" @classmethod def __get_validators__ ( cls ): # one or more validators may be yielded which will be called in the # order to validate the input, each validator will receive as an input # the value returned from the previous validator yield cls . validate @classmethod def __modify_schema__ ( cls , field_schema ): # __modify_schema__ should mutate the dict it receives in place, # the returned value will be ignored field_schema . update ( # simplified regex here for brevity, see the wikipedia link above pattern = '^[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z] {2} $' , # some example postcodes examples = [ 'SP11 9DG' , 'w1j7bu' ], ) @classmethod def validate ( cls , v ): if not isinstance ( v , str ): raise TypeError ( 'string required' ) m = post_code_regex . fullmatch ( v . upper ()) if not m : raise ValueError ( 'invalid postcode format' ) # you could also return a string here which would mean model.post_code # would be a string, pydantic won't care but you could end up with some # confusion since the value's type won't match the type annotation # exactly return cls ( f ' { m . group ( 1 ) } { m . group ( 2 ) } ' ) def __repr__ ( self ): return f 'PostCode( { super () . __repr__ () } )' class Model ( BaseModel ): post_code : PostCode model = Model ( post_code = 'sw8 5el' ) print ( model ) #> post_code=PostCode('SW8 5EL') print ( model . post_code ) #> SW8 5EL print ( Model . schema ()) \"\"\" { 'title': 'Model', 'type': 'object', 'properties': { 'post_code': { 'title': 'Post Code', 'pattern': '^[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z]{2}$', 'examples': ['SP11 9DG', 'w1j7bu'], 'type': 'string', }, }, 'required': ['post_code'], } \"\"\" Generic Classes as Types \u2691 Warning This is an advanced technique that you might not need in the beginning. In most of the cases you will probably be fine with standard pydantic models. You can use Generic Classes as field types and perform custom validation based on the \"type parameters\" (or sub-types) with __get_validators__ . If the Generic class that you are using as a sub-type has a classmethod __get_validators__ you don't need to use arbitrary_types_allowed for it to work. Because you can declare validators that receive the current field , you can extract the sub_fields (from the generic class type parameters) and validate data with them. from pydantic import BaseModel , ValidationError from pydantic.fields import ModelField from typing import TypeVar , Generic AgedType = TypeVar ( 'AgedType' ) QualityType = TypeVar ( 'QualityType' ) # This is not a pydantic model, it's an arbitrary generic class class TastingModel ( Generic [ AgedType , QualityType ]): def __init__ ( self , name : str , aged : AgedType , quality : QualityType ): self . name = name self . aged = aged self . quality = quality @classmethod def __get_validators__ ( cls ): yield cls . validate @classmethod # You don't need to add the \"ModelField\", but it will help your # editor give you completion and catch errors def validate ( cls , v , field : ModelField ): if not isinstance ( v , cls ): # The value is not even a TastingModel raise TypeError ( 'Invalid value' ) if not field . sub_fields : # Generic parameters were not provided so we don't try to validate # them and just return the value as is return v aged_f = field . sub_fields [ 0 ] quality_f = field . sub_fields [ 1 ] errors = [] # Here we don't need the validated value, but we want the errors valid_value , error = aged_f . validate ( v . aged , {}, loc = 'aged' ) if error : errors . append ( error ) # Here we don't need the validated value, but we want the errors valid_value , error = quality_f . validate ( v . quality , {}, loc = 'quality' ) if error : errors . append ( error ) if errors : raise ValidationError ( errors , cls ) # Validation passed without errors, return the same instance received return v class Model ( BaseModel ): # for wine, \"aged\" is an int with years, \"quality\" is a float wine : TastingModel [ int , float ] # for cheese, \"aged\" is a bool, \"quality\" is a str cheese : TastingModel [ bool , str ] # for thing, \"aged\" is a Any, \"quality\" is Any thing : TastingModel model = Model ( # This wine was aged for 20 years and has a quality of 85.6 wine = TastingModel ( name = 'Cabernet Sauvignon' , aged = 20 , quality = 85.6 ), # This cheese is aged (is mature) and has \"Good\" quality cheese = TastingModel ( name = 'Gouda' , aged = True , quality = 'Good' ), # This Python thing has aged \"Not much\" and has a quality \"Awesome\" thing = TastingModel ( name = 'Python' , aged = 'Not much' , quality = 'Awesome' ), ) print ( model ) \"\"\" wine=<types_generics.TastingModel object at 0x7f3593a4eee0> cheese=<types_generics.TastingModel object at 0x7f3593a46100> thing=<types_generics.TastingModel object at 0x7f3593a464c0> \"\"\" print ( model . wine . aged ) #> 20 print ( model . wine . quality ) #> 85.6 print ( model . cheese . aged ) #> True print ( model . cheese . quality ) #> Good print ( model . thing . aged ) #> Not much try : # If the values of the sub-types are invalid, we get an error Model ( # For wine, aged should be an int with the years, and quality a float wine = TastingModel ( name = 'Merlot' , aged = True , quality = 'Kinda good' ), # For cheese, aged should be a bool, and quality a str cheese = TastingModel ( name = 'Gouda' , aged = 'yeah' , quality = 5 ), # For thing, no type parameters are declared, and we skipped validation # in those cases in the Assessment.validate() function thing = TastingModel ( name = 'Python' , aged = 'Not much' , quality = 'Awesome' ), ) except ValidationError as e : print ( e ) \"\"\" 2 validation errors for Model wine -> quality value is not a valid float (type=type_error.float) cheese -> aged value could not be parsed to a boolean (type=type_error.bool) \"\"\" Using constrained strings in list attributes \u2691 If you try to use: from pydantic import constr Regexp = constr ( regex = \"^i-.*\" ) class Data ( pydantic . BaseModel ): regex : List [ Regex ] You'll encounter the Variable \"Regexp\" is not valid as a type [valid-type] mypy error. There are a few ways to achieve this: Using typing.Annotated with pydantic.Field \u2691 Instead of using constr to specify the regex constraint, you can specify it as an argument to Field and then use it in combination with typing.Annotated : !!! warning \"Until this open issue is not solved, this won't work.\" !!! note \" typing.Annotated is only available since Python 3.9. For older Python versions typing_extensions.Annotated can be used.\" import pydantic from pydantic import Field from typing import Annotated Regex = Annotated [ str , Field ( regex = \"^[0-9a-z_]*$\" )] class DataNotList ( pydantic . BaseModel ): regex : Regex data = DataNotList ( ** { \"regex\" : \"abc\" }) print ( data ) # regex='abc' print ( data . json ()) # {\"regex\": \"abc\"} Mypy treats Annotated[str, Field(regex=\"^[0-9a-z_]*$\")] as a type alias of str . But it also tells pydantic to do validation. This is described in the pydantic docs . Unfortunately it does not currently work with the following: class Data ( pydantic . BaseModel ): regex : List [ Regex ] Inheriting from pydantic.ConstrainedStr \u2691 Instead of using constr to specify the regex constraint (which uses pydantic.ConstrainedStr internally), you can inherit from pydantic.ConstrainedStr directly: import re import pydantic from pydantic import Field from typing import List class Regex ( pydantic . ConstrainedStr ): regex = re . compile ( \"^[0-9a-z_]*$\" ) class Data ( pydantic . BaseModel ): regex : List [ Regex ] data = Data ( ** { \"regex\" : [ \"abc\" , \"123\" , \"asdf\" ]}) print ( data ) # regex=['abc', '123', 'asdf'] print ( data . json ()) # {\"regex\": [\"abc\", \"123\", \"asdf\"]} Mypy accepts this happily and pydantic does correct validation. The type of data.regex[i] is Regex , but as pydantic.ConstrainedStr itself inherits from str , it can be used as a string in most places. References \u2691 Field types", "title": "Pydantic Field Types"}, {"location": "coding/python/pydantic_types/#standard-library-types", "text": "pydantic supports many common types from the python standard library. If you need stricter processing see Strict Types ; if you need to constrain the values allowed (e.g. to require a positive int) see Constrained Types . bool see Booleans for details on how bools are validated and what values are permitted. int pydantic uses int(v) to coerce types to an int ; see this warning on loss of information during data conversion. float similarly, float(v) is used to coerce values to floats. str strings are accepted as-is, int float and Decimal are coerced using str(v) , bytes and bytearray are converted using v.decode() , enums inheriting from str are converted using v.value , and all other types cause an error. list allows list , tuple , set , frozenset , or generators and casts to a list. tuple allows list , tuple , set , frozenset , or generators and casts to a tuple. dict dict(v) is used to attempt to convert a dictionary. set allows list , tuple , set , frozenset , or generators and casts to a set. frozenset allows list , tuple , set , frozenset , or generators and casts to a frozen set. datetime.date see Datetime Types below for more detail on parsing and validation. datetime.time see Datetime Types below for more detail on parsing and validation. datetime.datetime see Datetime Types below for more detail on parsing and validation. datetime.timedelta see Datetime Types below for more detail on parsing and validation. typing.Any allows any value include None , thus an Any field is optional. typing.TypeVar constrains the values allowed based on constraints or bound , see TypeVar . typing.Union see Unions below for more detail on parsing and validation. typing.Optional Optional[x] is simply short hand for Union[x, None] ; see Unions below for more detail on parsing and validation. typing.List : typing.Tuple : typing.Dict : typing.Set : typing.FrozenSet : typing.Sequence : typing.Iterable this is reserved for iterables that shouldn't be consumed. See Infinite Generators below for more detail on parsing and validation. typing.Type see Type below for more detail on parsing and validation. typing.Callable see Callable for more detail on parsing and validation. typing.Pattern will cause the input value to be passed to re.compile(v) to create a regex pattern. ipaddress.IPv4Address simply uses the type itself for validation by passing the value to IPv4Address(v) . ipaddress.IPv4Interface simply uses the type itself for validation by passing the value to IPv4Address(v) . ipaddress.IPv4Network simply uses the type itself for validation by passing the value to IPv4Network(v) . enum.Enum checks that the value is a valid member of the enum; see Enums and Choices for more details. enum.IntEnum checks that the value is a valid member of the integer enum; see Enums and Choices for more details. decimal.Decimal pydantic attempts to convert the value to a string, then passes the string to Decimal(v) . pathlib.Path simply uses the type itself for validation by passing the value to Path(v) .", "title": "Standard Library Types"}, {"location": "coding/python/pydantic_types/#iterables", "text": "", "title": "Iterables"}, {"location": "coding/python/pydantic_types/#define-default-value-for-an-iterable", "text": "If you want to define an empty list, dictionary, set or other iterable as a model attribute, you can use the default_factory . from typing import Sequence from pydantic import BaseModel , Field class Foo ( BaseModel ): defaulted_list_field : Sequence [ str ] = Field ( default_factory = list ) It might be tempting to do class Foo ( BaseModel ): defaulted_list_field : Sequence [ str ] = [] # Bad! But you'll follow the mutable default argument anti-pattern.", "title": "Define default value for an iterable"}, {"location": "coding/python/pydantic_types/#unions", "text": "The Union type allows a model attribute to accept different types, e.g.: from uuid import UUID from typing import Union from pydantic import BaseModel class User ( BaseModel ): id : Union [ int , str , UUID ] name : str user_01 = User ( id = 123 , name = 'John Doe' ) print ( user_01 ) #> id=123 name='John Doe' print ( user_01 . id ) #> 123 user_02 = User ( id = '1234' , name = 'John Doe' ) print ( user_02 ) #> id=1234 name='John Doe' print ( user_02 . id ) #> 1234 user_03_uuid = UUID ( 'cf57432e-809e-4353-adbd-9d5c0d733868' ) user_03 = User ( id = user_03_uuid , name = 'John Doe' ) print ( user_03 ) #> id=275603287559914445491632874575877060712 name='John Doe' print ( user_03 . id ) #> 275603287559914445491632874575877060712 print ( user_03_uuid . int ) #> 275603287559914445491632874575877060712 However, as can be seen above, pydantic will attempt to 'match' any of the types defined under Union and will use the first one that matches. In the above example the id of user_03 was defined as a uuid.UUID class (which is defined under the attribute's Union annotation) but as the uuid.UUID can be marshalled into an int it chose to match against the int type and disregarded the other types. As such, it is recommended that, when defining Union annotations, the most specific type is included first and followed by less specific types. In the above example, the UUID class should precede the int and str classes to preclude the unexpected representation as such: from uuid import UUID from typing import Union from pydantic import BaseModel class User ( BaseModel ): id : Union [ UUID , int , str ] name : str user_03_uuid = UUID ( 'cf57432e-809e-4353-adbd-9d5c0d733868' ) user_03 = User ( id = user_03_uuid , name = 'John Doe' ) print ( user_03 ) #> id=UUID('cf57432e-809e-4353-adbd-9d5c0d733868') name='John Doe' print ( user_03 . id ) #> cf57432e-809e-4353-adbd-9d5c0d733868 print ( user_03_uuid . int ) #> 275603287559914445491632874575877060712", "title": "Unions"}, {"location": "coding/python/pydantic_types/#enums-and-choices", "text": "pydantic uses python's standard enum classes to define choices. from enum import Enum , IntEnum from pydantic import BaseModel , ValidationError class FruitEnum ( str , Enum ): pear = 'pear' banana = 'banana' class ToolEnum ( IntEnum ): spanner = 1 wrench = 2 class CookingModel ( BaseModel ): fruit : FruitEnum = FruitEnum . pear tool : ToolEnum = ToolEnum . spanner print ( CookingModel ()) #> fruit=<FruitEnum.pear: 'pear'> tool=<ToolEnum.spanner: 1> print ( CookingModel ( tool = 2 , fruit = 'banana' )) #> fruit=<FruitEnum.banana: 'banana'> tool=<ToolEnum.wrench: 2> try : CookingModel ( fruit = 'other' ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for CookingModel fruit value is not a valid enumeration member; permitted: 'pear', 'banana' (type=type_error.enum; enum_values=[<FruitEnum.pear: 'pear'>, <FruitEnum.banana: 'banana'>]) \"\"\"", "title": "Enums and Choices"}, {"location": "coding/python/pydantic_types/#datetime-types", "text": "Pydantic supports the following datetime types: datetime fields can be: datetime , existing datetime object int or float , assumed as Unix time, i.e. seconds (if >= -2e10 or <= 2e10 ) or milliseconds (if < -2e10 or > 2e10 ) since 1 January 1970 str , following formats work: YYYY-MM-DD[T]HH:MM[:SS[.ffffff]][Z[\u00b1]HH[:]MM]]] int or float as a string (assumed as Unix time) date fields can be: date , existing date object int or float , see datetime str , following formats work: YYYY-MM-DD int or float , see datetime time fields can be: time , existing time object str , following formats work: HH:MM[:SS[.ffffff]] timedelta fields can be: timedelta , existing timedelta object int or float , assumed as seconds str , following formats work: [-][DD ][HH:MM]SS[.ffffff] [\u00b1]P[DD]DT[HH]H[MM]M[SS]S (ISO 8601 format for timedelta)", "title": "Datetime Types"}, {"location": "coding/python/pydantic_types/#type", "text": "pydantic supports the use of Type[T] to specify that a field may only accept classes (not instances) that are subclasses of T . from typing import Type from pydantic import BaseModel from pydantic import ValidationError class Foo : pass class Bar ( Foo ): pass class Other : pass class SimpleModel ( BaseModel ): just_subclasses : Type [ Foo ] SimpleModel ( just_subclasses = Foo ) SimpleModel ( just_subclasses = Bar ) try : SimpleModel ( just_subclasses = Other ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for SimpleModel just_subclasses subclass of Foo expected (type=type_error.subclass; expected_class=Foo) \"\"\"", "title": "Type"}, {"location": "coding/python/pydantic_types/#typevar", "text": "TypeVar is supported either unconstrained, constrained or with a bound. from typing import TypeVar from pydantic import BaseModel Foobar = TypeVar ( 'Foobar' ) BoundFloat = TypeVar ( 'BoundFloat' , bound = float ) IntStr = TypeVar ( 'IntStr' , int , str ) class Model ( BaseModel ): a : Foobar # equivalent of \": Any\" b : BoundFloat # equivalent of \": float\" c : IntStr # equivalent of \": Union[int, str]\" print ( Model ( a = [ 1 ], b = 4.2 , c = 'x' )) #> a=[1] b=4.2 c='x' # a may be None and is therefore optional print ( Model ( b = 1 , c = 1 )) #> a=None b=1.0 c=1", "title": "TypeVar"}, {"location": "coding/python/pydantic_types/#pydantic-types", "text": "pydantic also provides a variety of other useful types: FilePath like Path , but the path must exist and be a file. DirectoryPath like Path , but the path must exist and be a directory. Color for parsing HTML and CSS colors; see Color Type . Json a special type wrapper which loads JSON before parsing; see JSON Type . AnyUrl any URL; see URLs . AnyHttpUrl an HTTP URL; see URLs . HttpUrl a stricter HTTP URL; see URLs . PostgresDsn a postgres DSN style URL; see URLs . RedisDsn a redis DSN style URL; see URLs . SecretStr string where the value is kept partially secret; see Secrets . IPvAnyAddress allows either an IPv4Address or an IPv6Address . IPvAnyInterface allows either an IPv4Interface or an IPv6Interface . IPvAnyNetwork allows either an IPv4Network or an IPv6Network . NegativeFloat allows a float which is negative; uses standard float parsing then checks the value is less than 0; see Constrained Types . NegativeInt allows an int which is negative; uses standard int parsing then checks the value is less than 0; see Constrained Types . PositiveFloat allows a float which is positive; uses standard float parsing then checks the value is greater than 0; see Constrained Types . PositiveInt allows an int which is positive; uses standard int parsing then checks the value is greater than 0; see Constrained Types . condecimal type method for constraining Decimals; see Constrained Types . confloat type method for constraining floats; see Constrained Types . conint type method for constraining ints; see Constrained Types . conlist type method for constraining lists; see Constrained Types . conset type method for constraining sets; see Constrained Types . constr type method for constraining strs; see Constrained Types .", "title": "Pydantic Types"}, {"location": "coding/python/pydantic_types/#custom-data-types", "text": "You can also define your own custom data types. There are several ways to achieve it.", "title": "Custom Data Types"}, {"location": "coding/python/pydantic_types/#classes-with-__get_validators__", "text": "You use a custom class with a classmethod __get_validators__ . It will be called to get validators to parse and validate the input data. Tip These validators have the same semantics as in Validators , you can declare a parameter config , field , etc. import re from pydantic import BaseModel # https://en.wikipedia.org/wiki/Postcodes_in_the_United_Kingdom#Validation post_code_regex = re . compile ( r '(?:' r '([A-Z]{1,2}[0-9][A-Z0-9]?|ASCN|STHL|TDCU|BBND|[BFS]IQQ|PCRN|TKCA) ?' r '([0-9][A-Z] {2} )|' r '(BFPO) ?([0-9]{1,4})|' r '(KY[0-9]|MSR|VG|AI)[ -]?[0-9] {4} |' r '([A-Z] {2} ) ?([0-9] {2} )|' r '(GE) ?(CX)|' r '(GIR) ?(0A {2} )|' r '(SAN) ?(TA1)' r ')' ) class PostCode ( str ): \"\"\" Partial UK postcode validation. Note: this is just an example, and is not intended for use in production; in particular this does NOT guarantee a postcode exists, just that it has a valid format. \"\"\" @classmethod def __get_validators__ ( cls ): # one or more validators may be yielded which will be called in the # order to validate the input, each validator will receive as an input # the value returned from the previous validator yield cls . validate @classmethod def __modify_schema__ ( cls , field_schema ): # __modify_schema__ should mutate the dict it receives in place, # the returned value will be ignored field_schema . update ( # simplified regex here for brevity, see the wikipedia link above pattern = '^[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z] {2} $' , # some example postcodes examples = [ 'SP11 9DG' , 'w1j7bu' ], ) @classmethod def validate ( cls , v ): if not isinstance ( v , str ): raise TypeError ( 'string required' ) m = post_code_regex . fullmatch ( v . upper ()) if not m : raise ValueError ( 'invalid postcode format' ) # you could also return a string here which would mean model.post_code # would be a string, pydantic won't care but you could end up with some # confusion since the value's type won't match the type annotation # exactly return cls ( f ' { m . group ( 1 ) } { m . group ( 2 ) } ' ) def __repr__ ( self ): return f 'PostCode( { super () . __repr__ () } )' class Model ( BaseModel ): post_code : PostCode model = Model ( post_code = 'sw8 5el' ) print ( model ) #> post_code=PostCode('SW8 5EL') print ( model . post_code ) #> SW8 5EL print ( Model . schema ()) \"\"\" { 'title': 'Model', 'type': 'object', 'properties': { 'post_code': { 'title': 'Post Code', 'pattern': '^[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z]{2}$', 'examples': ['SP11 9DG', 'w1j7bu'], 'type': 'string', }, }, 'required': ['post_code'], } \"\"\"", "title": "Classes with __get_validators__"}, {"location": "coding/python/pydantic_types/#generic-classes-as-types", "text": "Warning This is an advanced technique that you might not need in the beginning. In most of the cases you will probably be fine with standard pydantic models. You can use Generic Classes as field types and perform custom validation based on the \"type parameters\" (or sub-types) with __get_validators__ . If the Generic class that you are using as a sub-type has a classmethod __get_validators__ you don't need to use arbitrary_types_allowed for it to work. Because you can declare validators that receive the current field , you can extract the sub_fields (from the generic class type parameters) and validate data with them. from pydantic import BaseModel , ValidationError from pydantic.fields import ModelField from typing import TypeVar , Generic AgedType = TypeVar ( 'AgedType' ) QualityType = TypeVar ( 'QualityType' ) # This is not a pydantic model, it's an arbitrary generic class class TastingModel ( Generic [ AgedType , QualityType ]): def __init__ ( self , name : str , aged : AgedType , quality : QualityType ): self . name = name self . aged = aged self . quality = quality @classmethod def __get_validators__ ( cls ): yield cls . validate @classmethod # You don't need to add the \"ModelField\", but it will help your # editor give you completion and catch errors def validate ( cls , v , field : ModelField ): if not isinstance ( v , cls ): # The value is not even a TastingModel raise TypeError ( 'Invalid value' ) if not field . sub_fields : # Generic parameters were not provided so we don't try to validate # them and just return the value as is return v aged_f = field . sub_fields [ 0 ] quality_f = field . sub_fields [ 1 ] errors = [] # Here we don't need the validated value, but we want the errors valid_value , error = aged_f . validate ( v . aged , {}, loc = 'aged' ) if error : errors . append ( error ) # Here we don't need the validated value, but we want the errors valid_value , error = quality_f . validate ( v . quality , {}, loc = 'quality' ) if error : errors . append ( error ) if errors : raise ValidationError ( errors , cls ) # Validation passed without errors, return the same instance received return v class Model ( BaseModel ): # for wine, \"aged\" is an int with years, \"quality\" is a float wine : TastingModel [ int , float ] # for cheese, \"aged\" is a bool, \"quality\" is a str cheese : TastingModel [ bool , str ] # for thing, \"aged\" is a Any, \"quality\" is Any thing : TastingModel model = Model ( # This wine was aged for 20 years and has a quality of 85.6 wine = TastingModel ( name = 'Cabernet Sauvignon' , aged = 20 , quality = 85.6 ), # This cheese is aged (is mature) and has \"Good\" quality cheese = TastingModel ( name = 'Gouda' , aged = True , quality = 'Good' ), # This Python thing has aged \"Not much\" and has a quality \"Awesome\" thing = TastingModel ( name = 'Python' , aged = 'Not much' , quality = 'Awesome' ), ) print ( model ) \"\"\" wine=<types_generics.TastingModel object at 0x7f3593a4eee0> cheese=<types_generics.TastingModel object at 0x7f3593a46100> thing=<types_generics.TastingModel object at 0x7f3593a464c0> \"\"\" print ( model . wine . aged ) #> 20 print ( model . wine . quality ) #> 85.6 print ( model . cheese . aged ) #> True print ( model . cheese . quality ) #> Good print ( model . thing . aged ) #> Not much try : # If the values of the sub-types are invalid, we get an error Model ( # For wine, aged should be an int with the years, and quality a float wine = TastingModel ( name = 'Merlot' , aged = True , quality = 'Kinda good' ), # For cheese, aged should be a bool, and quality a str cheese = TastingModel ( name = 'Gouda' , aged = 'yeah' , quality = 5 ), # For thing, no type parameters are declared, and we skipped validation # in those cases in the Assessment.validate() function thing = TastingModel ( name = 'Python' , aged = 'Not much' , quality = 'Awesome' ), ) except ValidationError as e : print ( e ) \"\"\" 2 validation errors for Model wine -> quality value is not a valid float (type=type_error.float) cheese -> aged value could not be parsed to a boolean (type=type_error.bool) \"\"\"", "title": "Generic Classes as Types"}, {"location": "coding/python/pydantic_types/#using-constrained-strings-in-list-attributes", "text": "If you try to use: from pydantic import constr Regexp = constr ( regex = \"^i-.*\" ) class Data ( pydantic . BaseModel ): regex : List [ Regex ] You'll encounter the Variable \"Regexp\" is not valid as a type [valid-type] mypy error. There are a few ways to achieve this:", "title": "Using constrained strings in list attributes"}, {"location": "coding/python/pydantic_types/#using-typingannotated-with-pydanticfield", "text": "Instead of using constr to specify the regex constraint, you can specify it as an argument to Field and then use it in combination with typing.Annotated : !!! warning \"Until this open issue is not solved, this won't work.\" !!! note \" typing.Annotated is only available since Python 3.9. For older Python versions typing_extensions.Annotated can be used.\" import pydantic from pydantic import Field from typing import Annotated Regex = Annotated [ str , Field ( regex = \"^[0-9a-z_]*$\" )] class DataNotList ( pydantic . BaseModel ): regex : Regex data = DataNotList ( ** { \"regex\" : \"abc\" }) print ( data ) # regex='abc' print ( data . json ()) # {\"regex\": \"abc\"} Mypy treats Annotated[str, Field(regex=\"^[0-9a-z_]*$\")] as a type alias of str . But it also tells pydantic to do validation. This is described in the pydantic docs . Unfortunately it does not currently work with the following: class Data ( pydantic . BaseModel ): regex : List [ Regex ]", "title": "Using typing.Annotated with pydantic.Field"}, {"location": "coding/python/pydantic_types/#inheriting-from-pydanticconstrainedstr", "text": "Instead of using constr to specify the regex constraint (which uses pydantic.ConstrainedStr internally), you can inherit from pydantic.ConstrainedStr directly: import re import pydantic from pydantic import Field from typing import List class Regex ( pydantic . ConstrainedStr ): regex = re . compile ( \"^[0-9a-z_]*$\" ) class Data ( pydantic . BaseModel ): regex : List [ Regex ] data = Data ( ** { \"regex\" : [ \"abc\" , \"123\" , \"asdf\" ]}) print ( data ) # regex=['abc', '123', 'asdf'] print ( data . json ()) # {\"regex\": [\"abc\", \"123\", \"asdf\"]} Mypy accepts this happily and pydantic does correct validation. The type of data.regex[i] is Regex , but as pydantic.ConstrainedStr itself inherits from str , it can be used as a string in most places.", "title": "Inheriting from pydantic.ConstrainedStr"}, {"location": "coding/python/pydantic_types/#references", "text": "Field types", "title": "References"}, {"location": "coding/python/pydantic_validators/", "text": "Custom validation and complex relationships between objects can be achieved using the validator decorator. from pydantic import BaseModel , ValidationError , validator class UserModel ( BaseModel ): name : str username : str password1 : str password2 : str @validator ( 'name' ) def name_must_contain_space ( cls , v ): if ' ' not in v : raise ValueError ( 'must contain a space' ) return v . title () @validator ( 'password2' ) def passwords_match ( cls , v , values , ** kwargs ): if 'password1' in values and v != values [ 'password1' ]: raise ValueError ( 'passwords do not match' ) return v @validator ( 'username' ) def username_alphanumeric ( cls , v ): assert v . isalnum (), 'must be alphanumeric' return v user = UserModel ( name = 'samuel colvin' , username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' , ) print ( user ) #> name='Samuel Colvin' username='scolvin' password1='zxcvbn' password2='zxcvbn' try : UserModel ( name = 'samuel' , username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn2' , ) except ValidationError as e : print ( e ) \"\"\" 2 validation errors for UserModel name must contain a space (type=value_error) password2 passwords do not match (type=value_error) \"\"\" You need to be aware of these validator behaviours. Validators are \"class methods\", so the first argument value they receive is the UserModel class, not an instance of UserModel . The second argument is always the field value to validate; it can be named as you please. You can also add any subset of the following arguments to the signature (the names must match): values : a dict containing the name-to-value mapping of any previously-validated fields. config : the model config. field : the field being validated. **kwargs : if provided, this will include the arguments above not explicitly listed in the signature. Validators should either return the parsed value or raise a ValueError , TypeError , or AssertionError ( assert statements may be used). Where validators rely on other values, you should be aware that: Validation is done in the order fields are defined. If validation fails on another field (or that field is missing) it will not be included in values , hence if 'password1' in values and ... in this example. Pre and per-item validators \u2691 Validators can do a few more complex things: A single validator can be applied to multiple fields by passing it multiple field names. A single validator can also be called on all fields by passing the special value '*' . The keyword argument pre will cause the validator to be called prior to other validation. Passing each_item=True will result in the validator being applied to individual values (e.g. of List , Dict , Set , etc.), rather than the whole object. from typing import List from pydantic import BaseModel , ValidationError , validator class DemoModel ( BaseModel ): square_numbers : List [ int ] = [] cube_numbers : List [ int ] = [] # '*' is the same as 'cube_numbers', 'square_numbers' here: @validator ( '*' , pre = True ) def split_str ( cls , v ): if isinstance ( v , str ): return v . split ( '|' ) return v @validator ( 'cube_numbers' , 'square_numbers' ) def check_sum ( cls , v ): if sum ( v ) > 42 : raise ValueError ( 'sum of numbers greater than 42' ) return v @validator ( 'square_numbers' , each_item = True ) def check_squares ( cls , v ): assert v ** 0.5 % 1 == 0 , f ' { v } is not a square number' return v @validator ( 'cube_numbers' , each_item = True ) def check_cubes ( cls , v ): # 64 ** (1 / 3) == 3.9999999999999996 (!) # this is not a good way of checking cubes assert v ** ( 1 / 3 ) % 1 == 0 , f ' { v } is not a cubed number' return v print ( DemoModel ( square_numbers = [ 1 , 4 , 9 ])) #> square_numbers=[1, 4, 9] cube_numbers=[] print ( DemoModel ( square_numbers = '1|4|16' )) #> square_numbers=[1, 4, 16] cube_numbers=[] print ( DemoModel ( square_numbers = [ 16 ], cube_numbers = [ 8 , 27 ])) #> square_numbers=[16] cube_numbers=[8, 27] try : DemoModel ( square_numbers = [ 1 , 4 , 2 ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for DemoModel square_numbers -> 2 2 is not a square number (type=assertion_error) \"\"\" try : DemoModel ( cube_numbers = [ 27 , 27 ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for DemoModel cube_numbers sum of numbers greater than 42 (type=value_error) \"\"\" Subclass Validators and each_item \u2691 If using a validator with a subclass that references a List type field on a parent class, using each_item=True will cause the validator not to run; instead, the list must be iterated over programatically. from typing import List from pydantic import BaseModel , ValidationError , validator class ParentModel ( BaseModel ): names : List [ str ] class ChildModel ( ParentModel ): @validator ( 'names' , each_item = True ) def check_names_not_empty ( cls , v ): assert v != '' , 'Empty strings are not allowed.' return v # This will NOT raise a ValidationError because the validator was not called try : child = ChildModel ( names = [ 'Alice' , 'Bob' , 'Eve' , '' ]) except ValidationError as e : print ( e ) else : print ( 'No ValidationError caught.' ) #> No ValidationError caught. class ChildModel2 ( ParentModel ): @validator ( 'names' ) def check_names_not_empty ( cls , v ): for name in v : assert name != '' , 'Empty strings are not allowed.' return v try : child = ChildModel2 ( names = [ 'Alice' , 'Bob' , 'Eve' , '' ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for ChildModel2 names Empty strings are not allowed. (type=assertion_error) \"\"\" Validate Always \u2691 For performance reasons, by default validators are not called for fields when a value is not supplied. However there are situations where it may be useful or required to always call the validator, e.g. to set a dynamic default value. from datetime import datetime from pydantic import BaseModel , validator class DemoModel ( BaseModel ): ts : datetime = None @validator ( 'ts' , pre = True , always = True ) def set_ts_now ( cls , v ): return v or datetime . now () print ( DemoModel ()) #> ts=datetime.datetime(2020, 7, 15, 20, 1, 48, 966302) print ( DemoModel ( ts = '2017-11-08T14:00' )) #> ts=datetime.datetime(2017, 11, 8, 14, 0) You'll often want to use this together with pre , since otherwise with always=True pydantic would try to validate the default None which would cause an error. Reuse validators \u2691 Occasionally, you will want to use the same validator on multiple fields/models (e.g. to normalize some input data). The \"naive\" approach would be to write a separate function, then call it from multiple decorators. Obviously, this entails a lot of repetition and boiler plate code. To circumvent this, the allow_reuse parameter has been added to pydantic.validator in v1.2 ( False by default): from pydantic import BaseModel , validator def normalize ( name : str ) -> str : return ' ' . join (( word . capitalize ()) for word in name . split ( ' ' )) class Producer ( BaseModel ): name : str # validators _normalize_name = validator ( 'name' , allow_reuse = True )( normalize ) class Consumer ( BaseModel ): name : str # validators _normalize_name = validator ( 'name' , allow_reuse = True )( normalize ) jane_doe = Producer ( name = 'JaNe DOE' ) john_doe = Consumer ( name = 'joHN dOe' ) assert jane_doe . name == 'Jane Doe' assert john_doe . name == 'John Doe' As it is obvious, repetition has been reduced and the models become again almost declarative. Tip If you have a lot of fields that you want to validate, it usually makes sense to define a help function with which you will avoid setting allow_reuse=True over and over again. Root Validators \u2691 Validation can also be performed on the entire model's data. from pydantic import BaseModel , ValidationError , root_validator class UserModel ( BaseModel ): username : str password1 : str password2 : str @root_validator ( pre = True ) def check_card_number_omitted ( cls , values ): assert 'card_number' not in values , 'card_number should not be included' return values @root_validator def check_passwords_match ( cls , values ): pw1 , pw2 = values . get ( 'password1' ), values . get ( 'password2' ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( 'passwords do not match' ) return values print ( UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' )) #> username='scolvin' password1='zxcvbn' password2='zxcvbn' try : UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn2' ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for UserModel __root__ passwords do not match (type=value_error) \"\"\" try : UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' , card_number = '1234' , ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for UserModel __root__ card_number should not be included (type=assertion_error) \"\"\" As with field validators, root validators can have pre=True , in which case they're called before field validation occurs (and are provided with the raw input data), or pre=False (the default), in which case they're called after field validation. Field validation will not occur if pre=True root validators raise an error. As with field validators, \"post\" (i.e. pre=False ) root validators by default will be called even if prior validators fail; this behaviour can be changed by setting the skip_on_failure=True keyword argument to the validator. The values argument will be a dict containing the values which passed field validation and field defaults where applicable. Field Checks \u2691 On class creation, validators are checked to confirm that the fields they specify actually exist on the model. Occasionally however this is undesirable: e.g. if you define a validator to validate fields on inheriting models. In this case you should set check_fields=False on the validator. Dataclass Validators \u2691 Validators also work with pydantic dataclasses. from datetime import datetime from pydantic import validator from pydantic.dataclasses import dataclass @dataclass class DemoDataclass : ts : datetime = None @validator ( 'ts' , pre = True , always = True ) def set_ts_now ( cls , v ): return v or datetime . now () print ( DemoDataclass ()) #> DemoDataclass(ts=datetime.datetime(2020, 7, 15, 20, 1, 48, 969037)) print ( DemoDataclass ( ts = '2017-11-08T14:00' )) #> DemoDataclass(ts=datetime.datetime(2017, 11, 8, 14, 0)) Troubleshooting validators \u2691 pylint complains on the validators \u2691 Pylint complains that R0201: Method could be a function and N805: first argument of a method should be named 'self' . Seems to be an error of pylint, people have solved it by specifying @classmethod between the definition and the validator decorator. References \u2691 Pydantic validators", "title": "Pydantic Validators"}, {"location": "coding/python/pydantic_validators/#pre-and-per-item-validators", "text": "Validators can do a few more complex things: A single validator can be applied to multiple fields by passing it multiple field names. A single validator can also be called on all fields by passing the special value '*' . The keyword argument pre will cause the validator to be called prior to other validation. Passing each_item=True will result in the validator being applied to individual values (e.g. of List , Dict , Set , etc.), rather than the whole object. from typing import List from pydantic import BaseModel , ValidationError , validator class DemoModel ( BaseModel ): square_numbers : List [ int ] = [] cube_numbers : List [ int ] = [] # '*' is the same as 'cube_numbers', 'square_numbers' here: @validator ( '*' , pre = True ) def split_str ( cls , v ): if isinstance ( v , str ): return v . split ( '|' ) return v @validator ( 'cube_numbers' , 'square_numbers' ) def check_sum ( cls , v ): if sum ( v ) > 42 : raise ValueError ( 'sum of numbers greater than 42' ) return v @validator ( 'square_numbers' , each_item = True ) def check_squares ( cls , v ): assert v ** 0.5 % 1 == 0 , f ' { v } is not a square number' return v @validator ( 'cube_numbers' , each_item = True ) def check_cubes ( cls , v ): # 64 ** (1 / 3) == 3.9999999999999996 (!) # this is not a good way of checking cubes assert v ** ( 1 / 3 ) % 1 == 0 , f ' { v } is not a cubed number' return v print ( DemoModel ( square_numbers = [ 1 , 4 , 9 ])) #> square_numbers=[1, 4, 9] cube_numbers=[] print ( DemoModel ( square_numbers = '1|4|16' )) #> square_numbers=[1, 4, 16] cube_numbers=[] print ( DemoModel ( square_numbers = [ 16 ], cube_numbers = [ 8 , 27 ])) #> square_numbers=[16] cube_numbers=[8, 27] try : DemoModel ( square_numbers = [ 1 , 4 , 2 ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for DemoModel square_numbers -> 2 2 is not a square number (type=assertion_error) \"\"\" try : DemoModel ( cube_numbers = [ 27 , 27 ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for DemoModel cube_numbers sum of numbers greater than 42 (type=value_error) \"\"\"", "title": "Pre and per-item validators"}, {"location": "coding/python/pydantic_validators/#subclass-validators-and-each_item", "text": "If using a validator with a subclass that references a List type field on a parent class, using each_item=True will cause the validator not to run; instead, the list must be iterated over programatically. from typing import List from pydantic import BaseModel , ValidationError , validator class ParentModel ( BaseModel ): names : List [ str ] class ChildModel ( ParentModel ): @validator ( 'names' , each_item = True ) def check_names_not_empty ( cls , v ): assert v != '' , 'Empty strings are not allowed.' return v # This will NOT raise a ValidationError because the validator was not called try : child = ChildModel ( names = [ 'Alice' , 'Bob' , 'Eve' , '' ]) except ValidationError as e : print ( e ) else : print ( 'No ValidationError caught.' ) #> No ValidationError caught. class ChildModel2 ( ParentModel ): @validator ( 'names' ) def check_names_not_empty ( cls , v ): for name in v : assert name != '' , 'Empty strings are not allowed.' return v try : child = ChildModel2 ( names = [ 'Alice' , 'Bob' , 'Eve' , '' ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for ChildModel2 names Empty strings are not allowed. (type=assertion_error) \"\"\"", "title": "Subclass Validators and each_item"}, {"location": "coding/python/pydantic_validators/#validate-always", "text": "For performance reasons, by default validators are not called for fields when a value is not supplied. However there are situations where it may be useful or required to always call the validator, e.g. to set a dynamic default value. from datetime import datetime from pydantic import BaseModel , validator class DemoModel ( BaseModel ): ts : datetime = None @validator ( 'ts' , pre = True , always = True ) def set_ts_now ( cls , v ): return v or datetime . now () print ( DemoModel ()) #> ts=datetime.datetime(2020, 7, 15, 20, 1, 48, 966302) print ( DemoModel ( ts = '2017-11-08T14:00' )) #> ts=datetime.datetime(2017, 11, 8, 14, 0) You'll often want to use this together with pre , since otherwise with always=True pydantic would try to validate the default None which would cause an error.", "title": "Validate Always"}, {"location": "coding/python/pydantic_validators/#reuse-validators", "text": "Occasionally, you will want to use the same validator on multiple fields/models (e.g. to normalize some input data). The \"naive\" approach would be to write a separate function, then call it from multiple decorators. Obviously, this entails a lot of repetition and boiler plate code. To circumvent this, the allow_reuse parameter has been added to pydantic.validator in v1.2 ( False by default): from pydantic import BaseModel , validator def normalize ( name : str ) -> str : return ' ' . join (( word . capitalize ()) for word in name . split ( ' ' )) class Producer ( BaseModel ): name : str # validators _normalize_name = validator ( 'name' , allow_reuse = True )( normalize ) class Consumer ( BaseModel ): name : str # validators _normalize_name = validator ( 'name' , allow_reuse = True )( normalize ) jane_doe = Producer ( name = 'JaNe DOE' ) john_doe = Consumer ( name = 'joHN dOe' ) assert jane_doe . name == 'Jane Doe' assert john_doe . name == 'John Doe' As it is obvious, repetition has been reduced and the models become again almost declarative. Tip If you have a lot of fields that you want to validate, it usually makes sense to define a help function with which you will avoid setting allow_reuse=True over and over again.", "title": "Reuse validators"}, {"location": "coding/python/pydantic_validators/#root-validators", "text": "Validation can also be performed on the entire model's data. from pydantic import BaseModel , ValidationError , root_validator class UserModel ( BaseModel ): username : str password1 : str password2 : str @root_validator ( pre = True ) def check_card_number_omitted ( cls , values ): assert 'card_number' not in values , 'card_number should not be included' return values @root_validator def check_passwords_match ( cls , values ): pw1 , pw2 = values . get ( 'password1' ), values . get ( 'password2' ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( 'passwords do not match' ) return values print ( UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' )) #> username='scolvin' password1='zxcvbn' password2='zxcvbn' try : UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn2' ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for UserModel __root__ passwords do not match (type=value_error) \"\"\" try : UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' , card_number = '1234' , ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for UserModel __root__ card_number should not be included (type=assertion_error) \"\"\" As with field validators, root validators can have pre=True , in which case they're called before field validation occurs (and are provided with the raw input data), or pre=False (the default), in which case they're called after field validation. Field validation will not occur if pre=True root validators raise an error. As with field validators, \"post\" (i.e. pre=False ) root validators by default will be called even if prior validators fail; this behaviour can be changed by setting the skip_on_failure=True keyword argument to the validator. The values argument will be a dict containing the values which passed field validation and field defaults where applicable.", "title": "Root Validators"}, {"location": "coding/python/pydantic_validators/#field-checks", "text": "On class creation, validators are checked to confirm that the fields they specify actually exist on the model. Occasionally however this is undesirable: e.g. if you define a validator to validate fields on inheriting models. In this case you should set check_fields=False on the validator.", "title": "Field Checks"}, {"location": "coding/python/pydantic_validators/#dataclass-validators", "text": "Validators also work with pydantic dataclasses. from datetime import datetime from pydantic import validator from pydantic.dataclasses import dataclass @dataclass class DemoDataclass : ts : datetime = None @validator ( 'ts' , pre = True , always = True ) def set_ts_now ( cls , v ): return v or datetime . now () print ( DemoDataclass ()) #> DemoDataclass(ts=datetime.datetime(2020, 7, 15, 20, 1, 48, 969037)) print ( DemoDataclass ( ts = '2017-11-08T14:00' )) #> DemoDataclass(ts=datetime.datetime(2017, 11, 8, 14, 0))", "title": "Dataclass Validators"}, {"location": "coding/python/pydantic_validators/#troubleshooting-validators", "text": "", "title": "Troubleshooting validators"}, {"location": "coding/python/pydantic_validators/#pylint-complains-on-the-validators", "text": "Pylint complains that R0201: Method could be a function and N805: first argument of a method should be named 'self' . Seems to be an error of pylint, people have solved it by specifying @classmethod between the definition and the validator decorator.", "title": "pylint complains on the validators"}, {"location": "coding/python/pydantic_validators/#references", "text": "Pydantic validators", "title": "References"}, {"location": "coding/python/pyenv/", "text": "pyenv pyenv lets you easily switch between multiple versions of Python. It's simple, unobtrusive, and follows the UNIX tradition of single-purpose tools. Installation \u2691 pyenv builds Python from source, which means you\u2019ll need build dependencies to actually use pyenv. The build dependencies vary by platform. If you are on Ubuntu/Debian and want to install the build dependencies, you could use the following: sudo apt-get install -y make build-essential libssl-dev zlib1g-dev \\ libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev \\ libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl curl https://pyenv.run | bash Das in die .bashrc kopieren: export PATH = \" $HOME /.pyenv/bin: $PATH \" eval \" $( pyenv init - ) \" eval \" $( pyenv virtualenv-init - ) \" Bash restarten. Basic commands \u2691 Liste alle verf\u00fcgbaren Linux Versionen pyenv install --list falls eine Version nicht in der Liste ist, pyenv updaten . pyenv update Eine bestimmte Version installieren pyenv install 3 .11.0 und dann global verf\u00fcgbar machen: pyenv global 3 .11.0 It will create several files and directories under the selected path, the mos References \u2691 Git Realpython", "title": "Pyenv"}, {"location": "coding/python/pyenv/#installation", "text": "pyenv builds Python from source, which means you\u2019ll need build dependencies to actually use pyenv. The build dependencies vary by platform. If you are on Ubuntu/Debian and want to install the build dependencies, you could use the following: sudo apt-get install -y make build-essential libssl-dev zlib1g-dev \\ libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev \\ libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl curl https://pyenv.run | bash Das in die .bashrc kopieren: export PATH = \" $HOME /.pyenv/bin: $PATH \" eval \" $( pyenv init - ) \" eval \" $( pyenv virtualenv-init - ) \" Bash restarten.", "title": "Installation"}, {"location": "coding/python/pyenv/#basic-commands", "text": "Liste alle verf\u00fcgbaren Linux Versionen pyenv install --list falls eine Version nicht in der Liste ist, pyenv updaten . pyenv update Eine bestimmte Version installieren pyenv install 3 .11.0 und dann global verf\u00fcgbar machen: pyenv global 3 .11.0 It will create several files and directories under the selected path, the mos", "title": "Basic commands"}, {"location": "coding/python/pyenv/#references", "text": "Git Realpython", "title": "References"}, {"location": "coding/python/pypika/", "text": "Pypika is a Python API for building SQL queries. The motivation behind PyPika is to provide a simple interface for building SQL queries without limiting the flexibility of handwritten SQL. PyPika is a fast, expressive and flexible way to replace handwritten SQL. Validation of SQL correctness is not an explicit goal of the project. Instead you are encouraged to check inputs you provide to PyPika or appropriately handle errors raised from your SQL database. After the queries have been built you need to interact with the database with other libraries. Installation \u2691 pip install pypika Usage \u2691 The main classes in pypika are pypika.Query , pypika.Table , and pypika.Field . from pypika import Query , Table , Field Creating Tables \u2691 The entry point for creating tables is pypika.Query.create_table , which is used with the class pypika.Column . As with selecting data, first the table should be specified. This can be either a string or a pypika.Table . Then the columns, and constraints.. stmt = Query \\ . create_table ( \"person\" ) \\ . columns ( Column ( \"id\" , \"INT\" , nullable = False ), Column ( \"first_name\" , \"VARCHAR(100)\" , nullable = False ), Column ( \"last_name\" , \"VARCHAR(100)\" , nullable = False ), Column ( \"phone_number\" , \"VARCHAR(20)\" , nullable = True ), Column ( \"status\" , \"VARCHAR(20)\" , nullable = False , default = ValueWrapper ( \"NEW\" )), Column ( \"date_of_birth\" , \"DATETIME\" )) \\ . unique ( \"last_name\" , \"first_name\" ) \\ . primary_key ( \"id\" ) This produces: CREATE TABLE \"person\" ( \"id\" INT NOT NULL , \"first_name\" VARCHAR ( 100 ) NOT NULL , \"last_name\" VARCHAR ( 100 ) NOT NULL , \"phone_number\" VARCHAR ( 20 ) NULL , \"status\" VARCHAR ( 20 ) NOT NULL DEFAULT 'NEW' , \"date_of_birth\" DATETIME , UNIQUE ( \"last_name\" , \"first_name\" ), PRIMARY KEY ( \"id\" ) ) It seems that they don't yet support the definition of FOREIGN KEYS when creating a new table. Inserting data \u2691 Data can be inserted into tables either by providing the values in the query or by selecting them through another query. By default, data can be inserted by providing values for all columns in the order that they are defined in the table. Insert with values \u2691 customers = Table ( 'customers' ) q = Query . into ( customers ) . insert ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) customers = Table ( 'customers' ) q = customers . insert ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) Multiple rows of data can be inserted either by chaining the insert function or passing multiple tuples as args. customers = Table ( 'customers' ) q = ( Query . into ( customers ) . insert ( 1 , \"Jane\" , \"Doe\" , \"jane@example.com\" ) . insert ( 2 , \"John\" , \"Doe\" , \"john@example.com\" ) ) customers = Table ( 'customers' ) q = Query . into ( customers ) . insert ( ( 1 , \"Jane\" , \"Doe\" , \"jane@example.com\" ), ( 2 , \"John\" , \"Doe\" , \"john@example.com\" ) ) INSERT INTO \"customers\" VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ),( 2 , 'John' , 'Doe' , 'john@example.com' ) Insert with on Duplicate Key Update \u2691 customers = Table ( 'customers' ) q = Query . into ( customers ) \\ . insert ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) \\ . on_duplicate_key_update ( customers . email , Values ( customers . email )) INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) ON DUPLICATE KEY UPDATE ` email `= VALUES ( ` email ` ) .on_duplicate_key_update works similar to .set for updating rows, additionally it provides the Values wrapper to update to the value specified in the INSERT clause. Insert from a SELECT Sub-query \u2691 INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ),( 2 , 'John' , 'Doe' , 'john@example.com' ) To specify the columns and the order, use the columns function. customers = Table ( 'customers' ) q = Query . into ( customers ) . columns ( 'id' , 'fname' , 'lname' ) . insert ( 1 , 'Jane' , 'Doe' ) INSERT INTO customers ( id , fname , lname ) VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) Inserting data with a query works the same as querying data with the additional call to the into method in the builder chain. customers , customers_backup = Tables ( 'customers' , 'customers_backup' ) q = Query . into ( customers_backup ) . from_ ( customers ) . select ( '*' ) INSERT INTO customers_backup SELECT * FROM customers customers , customers_backup = Tables ( 'customers' , 'customers_backup' ) q = Query . into ( customers_backup ) . columns ( 'id' , 'fname' , 'lname' ) . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname ) INSERT INTO customers_backup SELECT \"id\" , \"fname\" , \"lname\" FROM customers The syntax for joining tables is the same as when selecting data customers , orders , orders_backup = Tables ( 'customers' , 'orders' , 'orders_backup' ) q = Query . into ( orders_backup ) . columns ( 'id' , 'address' , 'customer_fname' , 'customer_lname' ) . from_ ( customers ) . join ( orders ) . on ( orders . customer_id == customers . id ) . select ( orders . id , customers . fname , customers . lname ) INSERT INTO \"orders_backup\" ( \"id\" , \"address\" , \"customer_fname\" , \"customer_lname\" ) SELECT \"orders\" . \"id\" , \"customers\" . \"fname\" , \"customers\" . \"lname\" FROM \"customers\" JOIN \"orders\" ON \"orders\" . \"customer_id\" = \"customers\" . \"id\" [Updating \u2691 data]( https://pypika.readthedocs.io/en/latest/2_tutorial.html#updating-data ) PyPika allows update queries to be constructed with or without where clauses. customers = Table ( 'customers' ) Query . update ( customers ) . set ( customers . last_login , '2017-01-01 10:00:00' ) Query . update ( customers ) . set ( customers . lname , 'smith' ) . where ( customers . id == 10 ) UPDATE \"customers\" SET \"last_login\" = '2017-01-01 10:00:00' UPDATE \"customers\" SET \"lname\" = 'smith' WHERE \"id\" = 10 The syntax for joining tables is the same as when selecting data customers , profiles = Tables ( 'customers' , 'profiles' ) Query . update ( customers ) . join ( profiles ) . on ( profiles . customer_id == customers . id ) . set ( customers . lname , profiles . lname ) UPDATE \"customers\" JOIN \"profiles\" ON \"profiles\" . \"customer_id\" = \"customers\" . \"id\" SET \"customers\" . \"lname\" = \"profiles\" . \"lname\" Using pypika.Table alias to perform the update customers = Table ( 'customers' ) customers . update () . set ( customers . lname , 'smith' ) . where ( customers . id == 10 ) UPDATE \"customers\" SET \"lname\" = 'smith' WHERE \"id\" = 10 Using limit for performing update customers = Table ( 'customers' ) customers . update () . set ( customers . lname , 'smith' ) . limit ( 2 ) UPDATE \"customers\" SET \"lname\" = 'smith' LIMIT 2 Selecting Data \u2691 The entry point for building queries is pypika.Query . In order to select columns from a table, the table must first be added to the query. For simple queries with only one table, tables and columns can be references using strings. For more sophisticated queries a pypika.Table must be used. q = Query . from_ ( 'customers' ) . select ( 'id' , 'fname' , 'lname' , 'phone' ) To convert the query into raw SQL, it can be cast to a string. str ( q ) Alternatively, you can use the Query.get_sql() function: q . get_sql () The .select statement doesn't need to be after the .from_ statement. This is useful when composing a query in multiple steps, where you can do the .join before the .select . In simple queries like the above example, columns in the \u201cfrom\u201d table can be referenced by passing string names into the select query builder function. In more complex examples, the pypika.Table class should be used. Columns can be referenced as attributes on instances of pypika.Table . from pypika import Table , Query customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) Both of the above examples result in the following SQL: SELECT id , fname , lname , phone FROM customers An alias for the table can be given using the .as_ function on pypika.Table . Table ( 'x_view_customers' ) . as_ ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . phone ) SELECT id , phone FROM x_view_customers customers An alias for the columns can also be given using the .as_ function on the columns. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id . as_ ( \"customer.id\" ), customers . fname . as_ ( \"customer.name\" ) ) SELECT \"id\" \"customer.id\" , \"fname\" \"customer.name\" FROM customers A schema can also be specified. Tables can be referenced as attributes on the schema. from pypika import Table , Query , Schema views = Schema ( 'views' ) q = Query . from_ ( views . customers ) . select ( customers . id , customers . phone ) SELECT id , phone FROM views . customers Also references to databases can be used. Schemas can be referenced as attributes on the database. from pypika import Table , Query , Database my_db = Database ( 'my_db' ) q = Query . from_ ( my_db . analytics . customers ) . select ( customers . id , customers . phone ) SELECT id , phone FROM my_db . analytics . customers Results can be ordered by using the following syntax: from pypika import Order Query . from_ ( 'customers' ) . select ( 'id' , 'fname' , 'lname' , 'phone' ) . orderby ( 'id' , order = Order . desc ) This results in the following SQL: SELECT \"id\" , \"fname\" , \"lname\" , \"phone\" FROM \"customers\" ORDER BY \"id\" DESC Filtering \u2691 Queries can be filtered with pypika.Criterion by using equality or inequality operators. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( customers . lname == 'Mustermann' ) SELECT id , fname , lname , phone FROM customers WHERE lname = 'Mustermann' Query methods such as select, where, groupby, and orderby can be called multiple times. Multiple calls to the where method will add additional conditions as: customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( customers . fname == 'Max' ) . where ( customers . lname == 'Mustermann' ) SELECT id , fname , lname , phone FROM customers WHERE fname = 'Max' AND lname = 'Mustermann' Filters such as IN and BETWEEN are also supported. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname ) . where ( customers . age [ 18 : 65 ] & customers . status . isin ([ 'new' , 'active' ]) ) SELECT id , fname FROM customers WHERE age BETWEEN 18 AND 65 AND status IN ( 'new' , 'active' ) Filtering with complex criteria can be created using boolean symbols & , | , and ^ . AND customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) & ( customers . lname == 'Mustermann' ) ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 AND lname = 'Mustermann' OR customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) | ( customers . lname == 'Mustermann' ) ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 OR lname = 'Mustermann' XOR customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) ^ customers . is_registered ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 XOR is_registered Using the REGEXP filter Pypika supports regex, but if you're using sqlite3 you need to configure the connection to the database . Joining tables and subqueries \u2691 Tables and subqueries can be joined to any query using the Query.join() method. Joins can be performed with either a USING or ON clauses. The USING clause can be used when both tables/subqueries contain the same field and the ON clause can be used with a criterion. To perform a join, ...join() can be chained but then must be followed immediately by ...on(<criterion>) or ...using(*field) . Join Types \u2691 All join types are supported by PyPika. Query \\ . from_ ( base_table ) ... . join ( join_table , JoinType . left ) ... Query \\ . from_ ( base_table ) ... . left_join ( join_table ) \\ . right_join ( join_table ) \\ . inner_join ( join_table ) \\ . outer_join ( join_table ) \\ . cross_join ( join_table ) \\ ... Example of a join using ON \u2691 history , customers = Tables ( 'history' , 'customers' ) q = Query \\ . from_ ( history ) \\ . join ( customers ) \\ . on ( history . customer_id == customers . id ) \\ . select ( history . star ) \\ . where ( customers . id == 5 ) SELECT \"history\" . * FROM \"history\" JOIN \"customers\" ON \"history\" . \"customer_id\" = \"customers\" . \"id\" WHERE \"customers\" . \"id\" = 5 Example of a join using ON_FIELD \u2691 As a shortcut, the Query.join().on_field() function is provided for joining the (first) table in the FROM clause with the joined table when the field name(s) are the same in both tables. history , customers = Tables ( 'history' , 'customers' ) q = Query \\ . from_ ( history ) \\ . join ( customers ) \\ . on_field ( 'customer_id' , 'group' ) \\ . select ( history . star ) \\ . where ( customers . group == 'A' ) SELECT \"history\" . * FROM \"history\" JOIN \"customers\" ON \"history\" . \"customer_id\" = \"customers\" . \"customer_id\" AND \"history\" . \"group\" = \"customers\" . \"group\" WHERE \"customers\" . \"group\" = 'A' Example of a join using USING \u2691 history , customers = Tables ( 'history' , 'customers' ) q = Query \\ . from_ ( history ) \\ . join ( customers ) \\ . using ( 'customer_id' ) \\ . select ( history . star ) \\ . where ( customers . id == 5 ) SELECT \"history\" . * FROM \"history\" JOIN \"customers\" USING \"customer_id\" WHERE \"customers\" . \"id\" = 5 Example of a correlated subquery in the SELECT \u2691 history , customers = Tables ( 'history' , 'customers' ) last_purchase_at = Query . from_ ( history ) . select ( history . purchase_at ) . where ( history . customer_id == customers . customer_id ) . orderby ( history . purchase_at , order = Order . desc ) . limit ( 1 ) q = Query . from_ ( customers ) . select ( customers . id , last_purchase_at . _as ( 'last_purchase_at' ) ) SELECT \"id\" , ( SELECT \"history\" . \"purchase_at\" FROM \"history\" WHERE \"history\" . \"customer_id\" = \"customers\" . \"customer_id\" ORDER BY \"history\" . \"purchase_at\" DESC LIMIT 1 ) \"last_purchase_at\" FROM \"customers\" Deleting data \u2691 Query . from_ ( table ) . delete () . where ( table . id == id ) References \u2691 Docs Source", "title": "Pypika"}, {"location": "coding/python/pypika/#installation", "text": "pip install pypika", "title": "Installation"}, {"location": "coding/python/pypika/#usage", "text": "The main classes in pypika are pypika.Query , pypika.Table , and pypika.Field . from pypika import Query , Table , Field", "title": "Usage"}, {"location": "coding/python/pypika/#creating-tables", "text": "The entry point for creating tables is pypika.Query.create_table , which is used with the class pypika.Column . As with selecting data, first the table should be specified. This can be either a string or a pypika.Table . Then the columns, and constraints.. stmt = Query \\ . create_table ( \"person\" ) \\ . columns ( Column ( \"id\" , \"INT\" , nullable = False ), Column ( \"first_name\" , \"VARCHAR(100)\" , nullable = False ), Column ( \"last_name\" , \"VARCHAR(100)\" , nullable = False ), Column ( \"phone_number\" , \"VARCHAR(20)\" , nullable = True ), Column ( \"status\" , \"VARCHAR(20)\" , nullable = False , default = ValueWrapper ( \"NEW\" )), Column ( \"date_of_birth\" , \"DATETIME\" )) \\ . unique ( \"last_name\" , \"first_name\" ) \\ . primary_key ( \"id\" ) This produces: CREATE TABLE \"person\" ( \"id\" INT NOT NULL , \"first_name\" VARCHAR ( 100 ) NOT NULL , \"last_name\" VARCHAR ( 100 ) NOT NULL , \"phone_number\" VARCHAR ( 20 ) NULL , \"status\" VARCHAR ( 20 ) NOT NULL DEFAULT 'NEW' , \"date_of_birth\" DATETIME , UNIQUE ( \"last_name\" , \"first_name\" ), PRIMARY KEY ( \"id\" ) ) It seems that they don't yet support the definition of FOREIGN KEYS when creating a new table.", "title": "Creating Tables"}, {"location": "coding/python/pypika/#inserting-data", "text": "Data can be inserted into tables either by providing the values in the query or by selecting them through another query. By default, data can be inserted by providing values for all columns in the order that they are defined in the table.", "title": "Inserting data"}, {"location": "coding/python/pypika/#insert-with-values", "text": "customers = Table ( 'customers' ) q = Query . into ( customers ) . insert ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) customers = Table ( 'customers' ) q = customers . insert ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) Multiple rows of data can be inserted either by chaining the insert function or passing multiple tuples as args. customers = Table ( 'customers' ) q = ( Query . into ( customers ) . insert ( 1 , \"Jane\" , \"Doe\" , \"jane@example.com\" ) . insert ( 2 , \"John\" , \"Doe\" , \"john@example.com\" ) ) customers = Table ( 'customers' ) q = Query . into ( customers ) . insert ( ( 1 , \"Jane\" , \"Doe\" , \"jane@example.com\" ), ( 2 , \"John\" , \"Doe\" , \"john@example.com\" ) ) INSERT INTO \"customers\" VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ),( 2 , 'John' , 'Doe' , 'john@example.com' )", "title": "Insert with values"}, {"location": "coding/python/pypika/#insert-with-on-duplicate-key-update", "text": "customers = Table ( 'customers' ) q = Query . into ( customers ) \\ . insert ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) \\ . on_duplicate_key_update ( customers . email , Values ( customers . email )) INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) ON DUPLICATE KEY UPDATE ` email `= VALUES ( ` email ` ) .on_duplicate_key_update works similar to .set for updating rows, additionally it provides the Values wrapper to update to the value specified in the INSERT clause.", "title": "Insert with on Duplicate Key Update"}, {"location": "coding/python/pypika/#insert-from-a-select-sub-query", "text": "INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ),( 2 , 'John' , 'Doe' , 'john@example.com' ) To specify the columns and the order, use the columns function. customers = Table ( 'customers' ) q = Query . into ( customers ) . columns ( 'id' , 'fname' , 'lname' ) . insert ( 1 , 'Jane' , 'Doe' ) INSERT INTO customers ( id , fname , lname ) VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) Inserting data with a query works the same as querying data with the additional call to the into method in the builder chain. customers , customers_backup = Tables ( 'customers' , 'customers_backup' ) q = Query . into ( customers_backup ) . from_ ( customers ) . select ( '*' ) INSERT INTO customers_backup SELECT * FROM customers customers , customers_backup = Tables ( 'customers' , 'customers_backup' ) q = Query . into ( customers_backup ) . columns ( 'id' , 'fname' , 'lname' ) . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname ) INSERT INTO customers_backup SELECT \"id\" , \"fname\" , \"lname\" FROM customers The syntax for joining tables is the same as when selecting data customers , orders , orders_backup = Tables ( 'customers' , 'orders' , 'orders_backup' ) q = Query . into ( orders_backup ) . columns ( 'id' , 'address' , 'customer_fname' , 'customer_lname' ) . from_ ( customers ) . join ( orders ) . on ( orders . customer_id == customers . id ) . select ( orders . id , customers . fname , customers . lname ) INSERT INTO \"orders_backup\" ( \"id\" , \"address\" , \"customer_fname\" , \"customer_lname\" ) SELECT \"orders\" . \"id\" , \"customers\" . \"fname\" , \"customers\" . \"lname\" FROM \"customers\" JOIN \"orders\" ON \"orders\" . \"customer_id\" = \"customers\" . \"id\"", "title": "Insert from a SELECT Sub-query"}, {"location": "coding/python/pypika/#updating", "text": "data]( https://pypika.readthedocs.io/en/latest/2_tutorial.html#updating-data ) PyPika allows update queries to be constructed with or without where clauses. customers = Table ( 'customers' ) Query . update ( customers ) . set ( customers . last_login , '2017-01-01 10:00:00' ) Query . update ( customers ) . set ( customers . lname , 'smith' ) . where ( customers . id == 10 ) UPDATE \"customers\" SET \"last_login\" = '2017-01-01 10:00:00' UPDATE \"customers\" SET \"lname\" = 'smith' WHERE \"id\" = 10 The syntax for joining tables is the same as when selecting data customers , profiles = Tables ( 'customers' , 'profiles' ) Query . update ( customers ) . join ( profiles ) . on ( profiles . customer_id == customers . id ) . set ( customers . lname , profiles . lname ) UPDATE \"customers\" JOIN \"profiles\" ON \"profiles\" . \"customer_id\" = \"customers\" . \"id\" SET \"customers\" . \"lname\" = \"profiles\" . \"lname\" Using pypika.Table alias to perform the update customers = Table ( 'customers' ) customers . update () . set ( customers . lname , 'smith' ) . where ( customers . id == 10 ) UPDATE \"customers\" SET \"lname\" = 'smith' WHERE \"id\" = 10 Using limit for performing update customers = Table ( 'customers' ) customers . update () . set ( customers . lname , 'smith' ) . limit ( 2 ) UPDATE \"customers\" SET \"lname\" = 'smith' LIMIT 2", "title": "[Updating"}, {"location": "coding/python/pypika/#selecting-data", "text": "The entry point for building queries is pypika.Query . In order to select columns from a table, the table must first be added to the query. For simple queries with only one table, tables and columns can be references using strings. For more sophisticated queries a pypika.Table must be used. q = Query . from_ ( 'customers' ) . select ( 'id' , 'fname' , 'lname' , 'phone' ) To convert the query into raw SQL, it can be cast to a string. str ( q ) Alternatively, you can use the Query.get_sql() function: q . get_sql () The .select statement doesn't need to be after the .from_ statement. This is useful when composing a query in multiple steps, where you can do the .join before the .select . In simple queries like the above example, columns in the \u201cfrom\u201d table can be referenced by passing string names into the select query builder function. In more complex examples, the pypika.Table class should be used. Columns can be referenced as attributes on instances of pypika.Table . from pypika import Table , Query customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) Both of the above examples result in the following SQL: SELECT id , fname , lname , phone FROM customers An alias for the table can be given using the .as_ function on pypika.Table . Table ( 'x_view_customers' ) . as_ ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . phone ) SELECT id , phone FROM x_view_customers customers An alias for the columns can also be given using the .as_ function on the columns. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id . as_ ( \"customer.id\" ), customers . fname . as_ ( \"customer.name\" ) ) SELECT \"id\" \"customer.id\" , \"fname\" \"customer.name\" FROM customers A schema can also be specified. Tables can be referenced as attributes on the schema. from pypika import Table , Query , Schema views = Schema ( 'views' ) q = Query . from_ ( views . customers ) . select ( customers . id , customers . phone ) SELECT id , phone FROM views . customers Also references to databases can be used. Schemas can be referenced as attributes on the database. from pypika import Table , Query , Database my_db = Database ( 'my_db' ) q = Query . from_ ( my_db . analytics . customers ) . select ( customers . id , customers . phone ) SELECT id , phone FROM my_db . analytics . customers Results can be ordered by using the following syntax: from pypika import Order Query . from_ ( 'customers' ) . select ( 'id' , 'fname' , 'lname' , 'phone' ) . orderby ( 'id' , order = Order . desc ) This results in the following SQL: SELECT \"id\" , \"fname\" , \"lname\" , \"phone\" FROM \"customers\" ORDER BY \"id\" DESC", "title": "Selecting Data"}, {"location": "coding/python/pypika/#filtering", "text": "Queries can be filtered with pypika.Criterion by using equality or inequality operators. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( customers . lname == 'Mustermann' ) SELECT id , fname , lname , phone FROM customers WHERE lname = 'Mustermann' Query methods such as select, where, groupby, and orderby can be called multiple times. Multiple calls to the where method will add additional conditions as: customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( customers . fname == 'Max' ) . where ( customers . lname == 'Mustermann' ) SELECT id , fname , lname , phone FROM customers WHERE fname = 'Max' AND lname = 'Mustermann' Filters such as IN and BETWEEN are also supported. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname ) . where ( customers . age [ 18 : 65 ] & customers . status . isin ([ 'new' , 'active' ]) ) SELECT id , fname FROM customers WHERE age BETWEEN 18 AND 65 AND status IN ( 'new' , 'active' ) Filtering with complex criteria can be created using boolean symbols & , | , and ^ . AND customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) & ( customers . lname == 'Mustermann' ) ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 AND lname = 'Mustermann' OR customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) | ( customers . lname == 'Mustermann' ) ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 OR lname = 'Mustermann' XOR customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) ^ customers . is_registered ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 XOR is_registered Using the REGEXP filter Pypika supports regex, but if you're using sqlite3 you need to configure the connection to the database .", "title": "Filtering"}, {"location": "coding/python/pypika/#joining-tables-and-subqueries", "text": "Tables and subqueries can be joined to any query using the Query.join() method. Joins can be performed with either a USING or ON clauses. The USING clause can be used when both tables/subqueries contain the same field and the ON clause can be used with a criterion. To perform a join, ...join() can be chained but then must be followed immediately by ...on(<criterion>) or ...using(*field) .", "title": "Joining tables and subqueries"}, {"location": "coding/python/pypika/#join-types", "text": "All join types are supported by PyPika. Query \\ . from_ ( base_table ) ... . join ( join_table , JoinType . left ) ... Query \\ . from_ ( base_table ) ... . left_join ( join_table ) \\ . right_join ( join_table ) \\ . inner_join ( join_table ) \\ . outer_join ( join_table ) \\ . cross_join ( join_table ) \\ ...", "title": "Join Types"}, {"location": "coding/python/pypika/#example-of-a-join-using-on", "text": "history , customers = Tables ( 'history' , 'customers' ) q = Query \\ . from_ ( history ) \\ . join ( customers ) \\ . on ( history . customer_id == customers . id ) \\ . select ( history . star ) \\ . where ( customers . id == 5 ) SELECT \"history\" . * FROM \"history\" JOIN \"customers\" ON \"history\" . \"customer_id\" = \"customers\" . \"id\" WHERE \"customers\" . \"id\" = 5", "title": "Example of a join using ON"}, {"location": "coding/python/pypika/#example-of-a-join-using-on_field", "text": "As a shortcut, the Query.join().on_field() function is provided for joining the (first) table in the FROM clause with the joined table when the field name(s) are the same in both tables. history , customers = Tables ( 'history' , 'customers' ) q = Query \\ . from_ ( history ) \\ . join ( customers ) \\ . on_field ( 'customer_id' , 'group' ) \\ . select ( history . star ) \\ . where ( customers . group == 'A' ) SELECT \"history\" . * FROM \"history\" JOIN \"customers\" ON \"history\" . \"customer_id\" = \"customers\" . \"customer_id\" AND \"history\" . \"group\" = \"customers\" . \"group\" WHERE \"customers\" . \"group\" = 'A'", "title": "Example of a join using ON_FIELD"}, {"location": "coding/python/pypika/#example-of-a-join-using-using", "text": "history , customers = Tables ( 'history' , 'customers' ) q = Query \\ . from_ ( history ) \\ . join ( customers ) \\ . using ( 'customer_id' ) \\ . select ( history . star ) \\ . where ( customers . id == 5 ) SELECT \"history\" . * FROM \"history\" JOIN \"customers\" USING \"customer_id\" WHERE \"customers\" . \"id\" = 5", "title": "Example of a join using USING"}, {"location": "coding/python/pypika/#example-of-a-correlated-subquery-in-the-select", "text": "history , customers = Tables ( 'history' , 'customers' ) last_purchase_at = Query . from_ ( history ) . select ( history . purchase_at ) . where ( history . customer_id == customers . customer_id ) . orderby ( history . purchase_at , order = Order . desc ) . limit ( 1 ) q = Query . from_ ( customers ) . select ( customers . id , last_purchase_at . _as ( 'last_purchase_at' ) ) SELECT \"id\" , ( SELECT \"history\" . \"purchase_at\" FROM \"history\" WHERE \"history\" . \"customer_id\" = \"customers\" . \"customer_id\" ORDER BY \"history\" . \"purchase_at\" DESC LIMIT 1 ) \"last_purchase_at\" FROM \"customers\"", "title": "Example of a correlated subquery in the SELECT"}, {"location": "coding/python/pypika/#deleting-data", "text": "Query . from_ ( table ) . delete () . where ( table . id == id )", "title": "Deleting data"}, {"location": "coding/python/pypika/#references", "text": "Docs Source", "title": "References"}, {"location": "coding/python/pytest/", "text": "pytest is a Python framework to makes it easy to write small tests, yet scales to support complex functional testing for applications and libraries. Pytest stands out over other test frameworks in: Simple tests are simple to write in pytest. Complex tests are still simple to write. Tests are easy to read. You can get started in seconds. You use assert to fail a test, not things like self.assertEqual() or self.assertLessThan() . Just assert . You can use pytest to run tests written for unittest or nose. Note: You can use this cookiecutter template to create a python project with pytest already configured. Install \u2691 pip install pytest Usage \u2691 Run in the project directory. pytest If you need more information run it with -v . Pytest automatically finds which tests to run in a phase called test discovery . It will get the tests that match one of the following conditions: Test files that are named test_{{ something }}.py or {{ something }}_test.py . Test methods and functions named test_{{ something }} . Test classes named Test{{ Something }} . There are several possible outcomes of a test function: PASSED (.) : The test ran successfully. FAILED (F) : The test did not run usccessfully (or XPASS + strict). SKIPPED (s) : The test was skipped. You can tell pytest to skip a test by using enter the @pytest.mark.skip() or pytest.mark.skipif() decorators. xfail (x) : The test was not supposed to pass, ran, and failed. You can tell pytest that a test is expected to fail by using the @pytest.mark.xfail() decorator. XPASS (X) : The tests was not supposed to pass, ran, and passed. ERROR (E) : An exception happened outside of the test function, in either a fixture or a hook function. Pytest supports several cool flags like: -k EXPRESSION : Used to select a subset of tests to run. For example pytest -k \"asdict or defaults\" will run both test_asdict() and test_defaults() . --lf or --last-failed : Just run the tests that have failed in the previous run. -x , or --exitfirst : Exit on first failed test. -l or --showlocals : Print out the local variables in a test if the test fails. -s Allows any output that normally would be printed to stdout to actually be printed to stdout . It's an alias of --capture=no , so the output is not captured when the tests are run, which is the default behavior. This is useful to debug with print() statements. --durations=N : It reports the slowest N number of tests/setups/teardowns after the test run. If you pass in --durations=0 , it reports everything in order of slowest to fastest. --setup-show : Show the fixtures in use. Fixtures \u2691 Fixtures are functions that are run by pytest before (and sometimes after) the actual test functions. You can use fixtures to get a data set for the tests to work on, or use them to get a system into a known state before running a test. They are also used to get data ready for multiple tests. Here's a simple fixture that returns a number: import pytest @pytest . fixture () def some_data () \"\"\" Return answer to the ultimate question \"\"\" return 42 def test_some_data ( some_data ): \"\"\" Use fixture return value in a test\"\"\" assert some_data == 42 The @pytest.fixture() decorator is used to tell pytest that a function is a fixture.When you include the fixture name in the parameter list of a test function,pytest knows to run it before running the test. Fixtures can do work, and can also return data to the test function. The test test_some_data() has the name of the fixture, some_data, as a parameter.pytest will see this and look for a fixture with this name. Naming is significant in pytest. pytest will look in the module of the test for a fixture of that name. If the function is defined in the same file as where it's being used pylint will raise an W0621: Redefining name %r from outer scope (line %s) error. To solve it either move the fixture to other file or name the decorated function fixture_<fixturename> and then use @pytest.fixture(name='<fixturename>') . Sharing fixtures through conftest.py \u2691 You can put fixtures into individual test files, but to share fixtures among multiple test files, you need to use a conftest.py file somewhere centrally located for all of the tests. Additionally you can have conftest.py files in subdirectories of the top tests directory. If you do, fixtures defined in these lower level conftest.py files will be available to tests in that directory and subdirectories. Although conftest.py is a Python module, it should not be imported by test files. The file gets read by pytest, and is considered a local plugin . Another option is to save the fixtures in a file by creating a local pytest plugin . File: tests/unit/conftest.py pytest_plugins = [ \"tests.unit.fixtures.some_stuff\" , ] File: tests/unit/fixtures/some_stuff.py : import pytest @pytest . fixture def foo (): return \"foobar\" Specifying fixture scope \u2691 Fixtures include an optional parameter called scope, which controls how often a fixture gets set up and torn down. The scope parameter to @pytest.fixture() can have the values of function, class, module, or session. Here\u2019s a rundown of each scope value: scope='function' : Run once per test function. The setup portion is run before each test using the fixture. The teardown portion is run after each test using the fixture. This is the default scope used when no scope parameter is specified. scope='class' : Run once per test class, regardless of how many test methods are in the class. scope='module' : Run once per module, regardless of how many test functions or methods or other fixtures in the module use it. scope='session' Run once per session. All test methods and functions using a fixture of session scope share one setup and teardown call. Using fixtures at class level \u2691 Sometimes test functions do not directly need access to a fixture object. For example, tests may require to operate with an empty directory as the current working directory but otherwise do not care for the concrete directory. @pytest . mark . usefixtures ( \"cleandir\" ) class TestDirectoryInit : ... Due to the usefixtures marker, the cleandir fixture will be required for the execution of each test method, just as if you specified a cleandir function argument to each of them. You can specify multiple fixtures like this: @pytest . mark . usefixtures ( \"cleandir\" , \"anotherfixture\" ) Useful Fixtures \u2691 The tmp_path fixture \u2691 You can use the tmp_path fixture which will provide a temporary directory unique to the test invocation, created in the base temporary directory. tmp_path is a pathlib.Path object. Here is an example test usage: def test_create_file ( tmp_path ): d = tmp_path / \"sub\" d . mkdir () p = d / \"hello.txt\" p . write_text ( CONTENT ) assert p . read_text () == CONTENT assert len ( list ( tmp_path . iterdir ())) == 1 assert 0 The tmpdir fixture \u2691 Warning: Don't use tmpdir use tmp_path instead because tmpdir uses py which is unmaintained and has unpatched vulnerabilities. You can use the tmpdir fixture which will provide a temporary directory unique to the test invocation, created in the base temporary directory. tmpdir is a py.path.local object which offers os.path methods and more. Here is an example test usage: File: test_tmpdir.py : from py._path.local import LocalPath def test_create_file ( tmpdir : LocalPath ): p = tmpdir . mkdir ( \"sub\" ) . join ( \"hello.txt\" ) p . write ( \"content\" ) assert p . read () == \"content\" assert len ( tmpdir . listdir ()) == 1 assert 0 The tmpdir fixture has a scope of function so you can't make a session directory. Instead use the tmpdir_factory fixture. from _pytest.tmpdir import TempPathFactory @pytest . fixture ( scope = \"session\" ) def image_file ( tmpdir_factory : TempPathFactory ): img = compute_expensive_image () fn = tmpdir_factory . mktemp ( \"data\" ) . join ( \"img.png\" ) img . save ( str ( fn )) return fn def test_histogram ( image_file ): img = load_image ( image_file ) # compute and test histogram Make a subdirectory \u2691 p = tmpdir . mkdir ( \"sub\" ) . join ( \"hello.txt\" ) The caplog fixture \u2691 pytest captures log messages of level WARNING or above automatically and displays them in their own section for each failed test in the same manner as captured stdout and stderr. You can change the default logging level in the pytest configuration: File: pytest.ini : [pytest] log_level = debug Although it may not be a good idea in most cases. It's better to change the log level in the tests that need a lower level. All the logs sent to the logger during the test run are available on the fixture in the form of both the logging.LogRecord instances and the final log text. This is useful for when you want to assert on the contents of a message: from _pytest.logging import LogCaptureFixture def test_baz ( caplog : LogCaptureFixture ): func_under_test () for record in caplog . records : assert record . levelname != \"CRITICAL\" assert \"wally\" not in caplog . text You can also resort to record_tuples if all you want to do is to ensure that certain messages have been logged under a given logger name with a given severity and message: def test_foo ( caplog : LogCaptureFixture ): logging . getLogger () . info ( \"boo %s \" , \"arg\" ) assert ( \"root\" , logging . INFO , \"boo arg\" ) in caplog . record_tuples You can call caplog.clear() to reset the captured log records in a test. Change the log level \u2691 Inside tests it's possible to change the log level for the captured log messages. def test_foo ( caplog : LogCaptureFixture ): caplog . set_level ( logging . INFO ) pass If you just want to change the log level of a dependency you can use: caplog . set_level ( logging . WARNING , logger = \"urllib3\" ) The capsys fixture \u2691 The capsys builtin fixture provides two bits of functionality: it allows you to retrieve stdout and stderr from some code, and it disables output capture temporarily. Suppose you have a function to print a greeting to stdout: def greeting ( name ): print ( f \"Hi, { name } \" ) You can test the output by using capsys . from _pytest.capture import CaptureFixture def test_greeting ( capsys : CaptureFixture [ Any ]): greeting ( \"Earthling\" ) out , err = capsys . readouterr () assert out == \"Hi, Earthling \\n \" assert err == \"\" The return value is whatever has been captured since the beginning of the function, or from the last time it was called. freezegun \u2691 freezegun lets you freeze time in both the test and fixtures. Install \u2691 pip install pytest-freezegun Usage \u2691 Global usage \u2691 Most of the tests work with frozen time, so it's better to freeze it by default and unfreeze it on the ones that actually need time to move. To do that set in your tests/conftest.py a globally used fixture: if TYPE_CHECKING : from freezegun.api import FrozenDateTimeFactory @pytest . fixture ( autouse = True ) def frozen_time () -> Generator [ FrozenDateTimeFactory , None , None ]: \"\"\"Freeze all tests time\"\"\" with freezegun . freeze_time () as freeze : yield freeze Freeze time by using the freezer fixture: Manual use \u2691 if TYPE_CHECKING : from freezegun.api import FrozenDateTimeFactory def test_frozen_date ( freezer : FrozenDateTimeFactory ): now = datetime . now () time . sleep ( 1 ) later = datetime . now () assert now == later This can then be used to move time: def test_moving_date ( freezer ): now = datetime . now () freezer . move_to ( \"2017-05-20\" ) later = datetime . now () assert now != later You can also pass arguments to freezegun by using the freeze_time mark: @pytest . mark . freeze_time ( \"2017-05-21\" ) def test_current_date (): assert date . today () == date ( 2017 , 5 , 21 ) The freezer fixture and freeze_time mark can be used together, and they work with other fixtures: @pytest . fixture def current_date (): return date . today () @pytest . mark . freeze_time () def test_changing_date ( current_date , freezer ): freezer . move_to ( \"2017-05-20\" ) assert current_date == date ( 2017 , 5 , 20 ) freezer . move_to ( \"2017-05-21\" ) assert current_date == date ( 2017 , 5 , 21 ) They can also be used in class-based tests: class TestDate : @pytest . mark . freeze_time def test_changing_date ( self , current_date , freezer ): freezer . move_to ( \"2017-05-20\" ) assert current_date == date ( 2017 , 5 , 20 ) freezer . move_to ( \"2017-05-21\" ) assert current_date == date ( 2017 , 5 , 21 ) Customize nested fixtures \u2691 Sometimes you need to tweak your fixtures so they can be used in different tests. As usual, there are different solutions to the same problem. Note: \"TL;DR: For simple cases parametrize your fixtures or use parametrization to override the default valued fixture . As your test suite get's more complex migrate to pytest-case .\" Let's say you're running along merrily with some fixtures that create database objects for you: @pytest . fixture def supplier ( db ): s = Supplier ( ref = random_ref (), name = random_name (), country = \"US\" , ) db . add ( s ) yield s db . remove ( s ) @pytest . fixture () def product ( db , supplier ): p = Product ( ref = random_ref (), name = random_name (), supplier = supplier , net_price = 9.99 , ) db . add ( p ) yield p db . remove ( p ) And now you're writing a new test and you suddenly realize you need to customize your default \"supplier\" fixture: def test_US_supplier_has_total_price_equal_net_price ( product ): assert product . total_price == product . net_price def test_EU_supplier_has_total_price_including_VAT ( supplier , product ): supplier . country = \"FR\" # oh, this doesn't work assert product . total_price == product . net_price * 1.2 There are different ways to modify your fixtures Add more fixtures \u2691 We can just create more fixtures, and try to do a bit of DRY by extracting out common logic: def _default_supplier (): return Supplier ( ref = random_ref (), name = random_name (), ) @pytest . fixture def us_supplier ( db ): s = _default_supplier () s . country = \"US\" db . add ( s ) yield s db . remove ( s ) @pytest . fixture def eu_supplier ( db ): s = _default_supplier () s . country = \"FR\" db . add ( s ) yield s db . remove ( s ) That's just one way you could do it, maybe you can figure out ways to reduce the duplication of the db.add() stuff as well, but you are going to have a different, named fixture for each customization of Supplier, and eventually you may decide that doesn't scale. Use factory fixtures \u2691 Instead of a fixture returning an object directly, it can return a function that creates an object, and that function can take arguments: @pytest . fixture def make_supplier ( db ): s = Supplier ( ref = random_ref (), name = random_name (), ) def _make_supplier ( country ): s . country = country db . add ( s ) return s yield _make_supplier db . remove ( s ) The problem with this is that, once you start, you tend to have to go all the way, and make all of your fixture hierarchy into factory functions: def test_EU_supplier_has_total_price_including_VAT ( make_supplier , product ): supplier = make_supplier ( country = \"FR\" ) product . supplier = ( supplier # OH, now this doesn't work, because it's too late again ) assert product . total_price == product . net_price * 1.2 And so... @pytest . fixture def make_product ( db ): p = Product ( ref = random_ref (), name = random_name (), ) def _make_product ( supplier ): p . supplier = supplier db . add ( p ) return p yield _make_product db . remove ( p ) def test_EU_supplier_has_total_price_including_VAT ( make_supplier , make_product ): supplier = make_supplier ( country = \"FR\" ) product = make_product ( supplier = supplier ) assert product . total_price == product . net_price * 1.2 That works, but firstly now everything is a factory-fixture, which makes them more convoluted, and secondly, your tests are filling up with extra calls to make_things , and you're having to embed some of the domain knowledge of what-depends-on-what into your tests as well as your fixtures. Ugly! Parametrize your fixtures \u2691 You can also parametrize your fixtures . @pytest . fixture ( params = [ \"US\" , \"FR\" ]) def supplier ( db , request ): s = Supplier ( ref = random_ref (), name = random_name (), country = request . param ) db . add ( s ) yield s db . remove ( s ) Now any test that depends on supplier, directly or indirectly, will be run twice, once with supplier.country = US and once with FR . That's really cool for checking that a given piece of logic works in a variety of different cases, but it's not really ideal in our case. We have to build a bunch of if logic into our tests: def test_US_supplier_has_no_VAT_but_EU_supplier_has_total_price_including_VAT ( product ): # this test is magically run twice, but: if product . supplier . country == \"US\" : assert product . total_price == product . net_price if product . supplier . country == \"FR\" : assert product . total_price == product . net_price * 1.2 So that's ugly, and on top of that, now every single test that depends (indirectly) on supplier gets run twice, and some of those extra test runs may be totally irrelevant to what the country is. Use pytest parametrization to override the default valued fixtures \u2691 We introduce an extra fixture that holds a default value for the country field: @pytest . fixture () def country (): return \"US\" @pytest . fixture def supplier ( db , country ): s = Supplier ( ref = random_ref (), name = random_name (), country = country , ) db . add ( s ) yield s db . remove ( s ) And then in the tests that need to change it, we can use parametrize to override the default value of country, even though the country fixture isn't explicitly named in that test: @pytest . mark . parametrize ( \"country\" , [ \"US\" ]) def test_US_supplier_has_total_price_equal_net_price ( product ): assert product . total_price == product . net_price @pytest . mark . parametrize ( \"country\" , [ \"EU\" ]) def test_EU_supplier_has_total_price_including_VAT ( product ): assert product . total_price == product . net_price * 1.2 The only problem is that you're now likely to build a implicit dependencies where the only way to find out what's actually happening is to spend ages spelunking in conftest.py. Use pytest-case \u2691 pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations. Check that file for further information. Use a fixture more than once in a function \u2691 One solution is to make your fixture return a factory instead of the resource directly: @pytest . fixture ( name = 'make_user' ) def make_user_ (): created = [] def make_user (): u = models . User () u . commit () created . append ( u ) return u yield make_user for u in created : u . delete () def test_two_users ( make_user ): user1 = make_user () user2 = make_user () # test them # you can even have the normal fixture when you only need a single user @pytest . fixture def user ( make_user ): return make_user () def test_one_user ( user ): # test him/her Marks \u2691 Pytest marks can be used to group tests. It can be useful to: slow : Mark the tests that are slow. secondary : Mart the tests that use functionality that is being tested in the same file. To mark a test, use the @pytest.mark decorator. For example: @pytest . mark . slow def test_really_slow_test (): pass Pytest requires you to register your marks, do so in the pytest.ini file [pytest] markers = slow: marks tests as slow (deselect with '-m \"not slow\"') secondary: mark tests that use functionality tested in the same file (deselect with '-m \"not secondary\"') Snippets \u2691 Mocking sys.exit \u2691 with pytest . raises ( SystemExit ): # Code to test Testing exceptions with pytest \u2691 def test_value_error_is_raised (): with pytest . raises ( ValueError , match = \"invalid literal for int() with base 10: 'a'\" ): int ( \"a\" ) Excluding code from coverage \u2691 You may have code in your project that you know won't be executed, and you want to tell coverage.py to ignore it. For example, if you have some code in abstract classes that is going to be tested on the subclasses, you can ignore it with # pragma: no cover . If you want other code to be excluded , for example the statements inside the if TYPE_CHECKING: add to your pyproject.toml : [tool.coverage.report] exclude_lines = [ \"pragma: no cover\" , \"if TYPE_CHECKING:\" ,] Running tests in parallel \u2691 pytest-xdist makes it possible to run the tests in parallel, useful when the test suit is large or when the tests are slow. Installation \u2691 pip install pytest-xdist Usage \u2691 pytest -n 4 It will run the tests with 4 workers. If you use auto it will adapt the number of workers to the number of CPUS, or 1 if you use --pdb . To configure it in the pyproject.toml use the addopts [tool.pytest.ini_options] minversion = \"6.0\" addopts = \"-vv --tb=short -n auto\" Enforce serial execution of related tests \u2691 Use a lock \u2691 Implement a serial fixture with a session-scoped file lock fixture using the filelock package. You can add this to your conftest.py : pip install filelock import contextlib import os import filelock import pytest from filelock import BaseFileLock @pytest . fixture ( name = \"lock\" , scope = \"session\" ) def lock_ ( tmp_path_factory : pytest . TempPathFactory , ) -> Generator [ BaseFileLock , None , None ]: \"\"\"Create lock file.\"\"\" base_temp = tmp_path_factory . getbasetemp () lock_file = base_temp . parent / \"serial.lock\" yield FileLock ( lock_file = str ( lock_file )) with contextlib . suppress ( OSError ): os . remove ( path = lock_file ) @pytest . fixture ( name = \"serial\" ) def _serial ( lock : BaseFileLock ) -> Generator [ None , None , None ]: \"\"\"Fixture to run tests in serial.\"\"\" with lock . acquire ( poll_interval = 0.1 ): yield Then inject the serial fixture in any test that requires serial execution. All tests that use the serial fixture are executed serially while any tests that do not use the fixture are executed in parallel. Mark them and run separately \u2691 Mark the tests you want to execute serially with a special mark, say serial: @pytest . mark . serial class Test : ... @pytest . mark . serial def test_foo (): ... Execute your parallel tests, excluding those with the serial mark: $ py.test -n auto -m \"not serial\" Next, execute your serial tests in a separate session: $ py.test -n0 -m \"serial\" Setting a timeout for your tests \u2691 To make your tests fail if they don't end in less than X seconds, use pytest-timeout . Install it with: pip install pytest-timeout You can set a global timeout in your pyproject.toml : [pytest] timeout = 300 Or define it for each test with: @pytest . mark . timeout ( 60 ) def test_foo (): pass Rerun tests that fail sometimes \u2691 pytest-rerunfailures is a plugin for pytest that re-runs tests to eliminate intermittent failures. Using this plugin is generally a bad idea, it would be best to solve the reason why your code is not reliable. It's useful when you rely on non robust third party software in a way that you can't solve, or if the error is not in your code but in the testing code, and again you are not able to fix it. Install it with: pip install pytest-rerunfailures To re-run all test failures, use the --reruns command line option with the maximum number of times you\u2019d like the tests to run: pytest --reruns 5 Failed fixture or setup_class will also be re-executed. To add a delay time between re-runs use the --reruns-delay command line option with the amount of seconds that you would like wait before the next test re-run is launched: pytest --reruns 5 --reruns-delay 1 To mark individual tests as flaky, and have them automatically re-run when they fail, add the flaky mark with the maximum number of times you\u2019d like the test to run: @pytest . mark . flaky ( reruns = 5 ) def test_example (): import random assert random . choice ([ True , False ]) Run tests in a random order \u2691 pytest-random-order is a pytest plugin that randomises the order of tests. This can be useful to detect a test that passes just because it happens to run after an unrelated test that leaves the system in a favourable state. To use it add the --random-order to your pytest run. It can't yet be used with pytest-xdist though :(. Capture deprecation warnings \u2691 Python and its ecosystem does not have an assumption of strict SemVer, and has a tradition of providing deprecation warnings. If you have good CI, you should be able to catch warnings even before your users see them. Try the following pytest configuration: [tool.pytest.ini_options] filterwarnings = [ \"error\" ,] This will turn warnings into errors and allow your CI to break before users break. You can ignore specific warnings as well. For example, the configuration below will ignore all user warnings and specific deprecation warnings matching a regex, but will transform all other warnings into errors. [tool.pytest.ini_options] filterwarnings = [ \"error\" , \"ignore::UserWarning\" , \"ignore:function ham\\\\(\\\\) is deprecated:DeprecationWarning\" ,] When a warning matches more than one option in the list, the action for the last matching option is performed. If you want to ignore the warning of a specific package use: filterwarnings = [ \"error\" , \"ignore::DeprecationWarning:pytest_freezegun.*\" ,] Note: It's better to suppress a warning instead of disabling it for the whole code, check how here . Ensuring code triggers a deprecation warning \u2691 You can also use pytest.deprecated_call() for checking that a certain function call triggers a DeprecationWarning or PendingDeprecationWarning : import pytest def test_myfunction_deprecated (): with pytest . deprecated_call (): myfunction ( 17 ) Asserting warnings with the warns function \u2691 You can check that code raises a particular warning using pytest.warns(), which works in a similar manner to raises: import warnings import pytest def test_warning (): with pytest . warns ( UserWarning ): warnings . warn ( \"my warning\" , UserWarning ) The test will fail if the warning in question is not raised. The keyword argument match to assert that the exception matches a text or regex: >>> with pytest . warns ( UserWarning , match = 'must be 0 or None' ): ... warnings . warn ( \"value must be 0 or None\" , UserWarning ) Recording warnings \u2691 You can record raised warnings either using pytest.warns() or with the recwarn fixture. To record with pytest.warns() without asserting anything about the warnings, pass no arguments as the expected warning type and it will default to a generic Warning: with pytest . warns () as record : warnings . warn ( \"user\" , UserWarning ) warnings . warn ( \"runtime\" , RuntimeWarning ) assert len ( record ) == 2 assert str ( record [ 0 ] . message ) == \"user\" assert str ( record [ 1 ] . message ) == \"runtime\" The recwarn fixture will record warnings for the whole function: import warnings def test_hello ( recwarn ): warnings . warn ( \"hello\" , UserWarning ) assert len ( recwarn ) == 1 w = recwarn . pop ( UserWarning ) assert issubclass ( w . category , UserWarning ) assert str ( w . message ) == \"hello\" assert w . filename assert w . lineno Both recwarn and pytest.warns() return the same interface for recorded warnings: a WarningsRecorder instance. To view the recorded warnings, you can iterate over this instance, call len on it to get the number of recorded warnings, or index into it to get a particular recorded warning. Show logging messages on the test run \u2691 Add to your pyproject.toml : [tool.pytest.ini_options] log_cli = true log_cli_level = 10 Or run it in the command itself pytest -o log_cli=true --log-cli-level=10 func.py . Remember you can change the log level of the different components in case it's too verbose. Pytest integration with Vim \u2691 Integrating pytest into your Vim workflow enhances your productivity while writing code, thus making it easier to code using TDD. I use Janko-m's Vim-test plugin (which can be installed through Vundle ) with the following configuration. nmap < silent > t :TestNearest -- pdb < CR > nmap < silent > < leader > t :TestSuite tests/unit < CR > nmap < silent > < leader > i :TestSuite tests/integration < CR > nmap < silent > < leader > T :TestFile < CR > let test#python#runner = 'pytest' let test#strategy = \"neovim\" I often open Vim with a vertical split ( :vs ), in the left window I have the tests and in the right the code. Whenever I want to run a single test I press t when the cursor is inside that test. If you need to make changes in the code, you can press t again while the cursor is at the code you are testing and it will run the last test. Once the unit test has passed, I run the whole unit tests with ;t (as ; is my <leader> ). And finally I use ;i to run the integration tests. Finally, if the test suite is huge, I use ;T to run only the tests of a single file. As you can see only the t has the --pdb flag, so the rest of them will run en parallel and any pdb trace will fail. Reference \u2691 Book Python Testing with pytest by Brian Okken . Docs Vim-test plugin", "title": "Pytest"}, {"location": "coding/python/pytest/#install", "text": "pip install pytest", "title": "Install"}, {"location": "coding/python/pytest/#usage", "text": "Run in the project directory. pytest If you need more information run it with -v . Pytest automatically finds which tests to run in a phase called test discovery . It will get the tests that match one of the following conditions: Test files that are named test_{{ something }}.py or {{ something }}_test.py . Test methods and functions named test_{{ something }} . Test classes named Test{{ Something }} . There are several possible outcomes of a test function: PASSED (.) : The test ran successfully. FAILED (F) : The test did not run usccessfully (or XPASS + strict). SKIPPED (s) : The test was skipped. You can tell pytest to skip a test by using enter the @pytest.mark.skip() or pytest.mark.skipif() decorators. xfail (x) : The test was not supposed to pass, ran, and failed. You can tell pytest that a test is expected to fail by using the @pytest.mark.xfail() decorator. XPASS (X) : The tests was not supposed to pass, ran, and passed. ERROR (E) : An exception happened outside of the test function, in either a fixture or a hook function. Pytest supports several cool flags like: -k EXPRESSION : Used to select a subset of tests to run. For example pytest -k \"asdict or defaults\" will run both test_asdict() and test_defaults() . --lf or --last-failed : Just run the tests that have failed in the previous run. -x , or --exitfirst : Exit on first failed test. -l or --showlocals : Print out the local variables in a test if the test fails. -s Allows any output that normally would be printed to stdout to actually be printed to stdout . It's an alias of --capture=no , so the output is not captured when the tests are run, which is the default behavior. This is useful to debug with print() statements. --durations=N : It reports the slowest N number of tests/setups/teardowns after the test run. If you pass in --durations=0 , it reports everything in order of slowest to fastest. --setup-show : Show the fixtures in use.", "title": "Usage"}, {"location": "coding/python/pytest/#fixtures", "text": "Fixtures are functions that are run by pytest before (and sometimes after) the actual test functions. You can use fixtures to get a data set for the tests to work on, or use them to get a system into a known state before running a test. They are also used to get data ready for multiple tests. Here's a simple fixture that returns a number: import pytest @pytest . fixture () def some_data () \"\"\" Return answer to the ultimate question \"\"\" return 42 def test_some_data ( some_data ): \"\"\" Use fixture return value in a test\"\"\" assert some_data == 42 The @pytest.fixture() decorator is used to tell pytest that a function is a fixture.When you include the fixture name in the parameter list of a test function,pytest knows to run it before running the test. Fixtures can do work, and can also return data to the test function. The test test_some_data() has the name of the fixture, some_data, as a parameter.pytest will see this and look for a fixture with this name. Naming is significant in pytest. pytest will look in the module of the test for a fixture of that name. If the function is defined in the same file as where it's being used pylint will raise an W0621: Redefining name %r from outer scope (line %s) error. To solve it either move the fixture to other file or name the decorated function fixture_<fixturename> and then use @pytest.fixture(name='<fixturename>') .", "title": "Fixtures"}, {"location": "coding/python/pytest/#sharing-fixtures-through-conftestpy", "text": "You can put fixtures into individual test files, but to share fixtures among multiple test files, you need to use a conftest.py file somewhere centrally located for all of the tests. Additionally you can have conftest.py files in subdirectories of the top tests directory. If you do, fixtures defined in these lower level conftest.py files will be available to tests in that directory and subdirectories. Although conftest.py is a Python module, it should not be imported by test files. The file gets read by pytest, and is considered a local plugin . Another option is to save the fixtures in a file by creating a local pytest plugin . File: tests/unit/conftest.py pytest_plugins = [ \"tests.unit.fixtures.some_stuff\" , ] File: tests/unit/fixtures/some_stuff.py : import pytest @pytest . fixture def foo (): return \"foobar\"", "title": "Sharing fixtures through conftest.py"}, {"location": "coding/python/pytest/#specifying-fixture-scope", "text": "Fixtures include an optional parameter called scope, which controls how often a fixture gets set up and torn down. The scope parameter to @pytest.fixture() can have the values of function, class, module, or session. Here\u2019s a rundown of each scope value: scope='function' : Run once per test function. The setup portion is run before each test using the fixture. The teardown portion is run after each test using the fixture. This is the default scope used when no scope parameter is specified. scope='class' : Run once per test class, regardless of how many test methods are in the class. scope='module' : Run once per module, regardless of how many test functions or methods or other fixtures in the module use it. scope='session' Run once per session. All test methods and functions using a fixture of session scope share one setup and teardown call.", "title": "Specifying fixture scope"}, {"location": "coding/python/pytest/#using-fixtures-at-class-level", "text": "Sometimes test functions do not directly need access to a fixture object. For example, tests may require to operate with an empty directory as the current working directory but otherwise do not care for the concrete directory. @pytest . mark . usefixtures ( \"cleandir\" ) class TestDirectoryInit : ... Due to the usefixtures marker, the cleandir fixture will be required for the execution of each test method, just as if you specified a cleandir function argument to each of them. You can specify multiple fixtures like this: @pytest . mark . usefixtures ( \"cleandir\" , \"anotherfixture\" )", "title": "Using fixtures at class level"}, {"location": "coding/python/pytest/#useful-fixtures", "text": "", "title": "Useful Fixtures"}, {"location": "coding/python/pytest/#the-tmp_path-fixture", "text": "You can use the tmp_path fixture which will provide a temporary directory unique to the test invocation, created in the base temporary directory. tmp_path is a pathlib.Path object. Here is an example test usage: def test_create_file ( tmp_path ): d = tmp_path / \"sub\" d . mkdir () p = d / \"hello.txt\" p . write_text ( CONTENT ) assert p . read_text () == CONTENT assert len ( list ( tmp_path . iterdir ())) == 1 assert 0", "title": "The tmp_path fixture"}, {"location": "coding/python/pytest/#the-tmpdir-fixture", "text": "Warning: Don't use tmpdir use tmp_path instead because tmpdir uses py which is unmaintained and has unpatched vulnerabilities. You can use the tmpdir fixture which will provide a temporary directory unique to the test invocation, created in the base temporary directory. tmpdir is a py.path.local object which offers os.path methods and more. Here is an example test usage: File: test_tmpdir.py : from py._path.local import LocalPath def test_create_file ( tmpdir : LocalPath ): p = tmpdir . mkdir ( \"sub\" ) . join ( \"hello.txt\" ) p . write ( \"content\" ) assert p . read () == \"content\" assert len ( tmpdir . listdir ()) == 1 assert 0 The tmpdir fixture has a scope of function so you can't make a session directory. Instead use the tmpdir_factory fixture. from _pytest.tmpdir import TempPathFactory @pytest . fixture ( scope = \"session\" ) def image_file ( tmpdir_factory : TempPathFactory ): img = compute_expensive_image () fn = tmpdir_factory . mktemp ( \"data\" ) . join ( \"img.png\" ) img . save ( str ( fn )) return fn def test_histogram ( image_file ): img = load_image ( image_file ) # compute and test histogram", "title": "The tmpdir fixture"}, {"location": "coding/python/pytest/#make-a-subdirectory", "text": "p = tmpdir . mkdir ( \"sub\" ) . join ( \"hello.txt\" )", "title": "Make a subdirectory"}, {"location": "coding/python/pytest/#the-caplog-fixture", "text": "pytest captures log messages of level WARNING or above automatically and displays them in their own section for each failed test in the same manner as captured stdout and stderr. You can change the default logging level in the pytest configuration: File: pytest.ini : [pytest] log_level = debug Although it may not be a good idea in most cases. It's better to change the log level in the tests that need a lower level. All the logs sent to the logger during the test run are available on the fixture in the form of both the logging.LogRecord instances and the final log text. This is useful for when you want to assert on the contents of a message: from _pytest.logging import LogCaptureFixture def test_baz ( caplog : LogCaptureFixture ): func_under_test () for record in caplog . records : assert record . levelname != \"CRITICAL\" assert \"wally\" not in caplog . text You can also resort to record_tuples if all you want to do is to ensure that certain messages have been logged under a given logger name with a given severity and message: def test_foo ( caplog : LogCaptureFixture ): logging . getLogger () . info ( \"boo %s \" , \"arg\" ) assert ( \"root\" , logging . INFO , \"boo arg\" ) in caplog . record_tuples You can call caplog.clear() to reset the captured log records in a test.", "title": "The caplog fixture"}, {"location": "coding/python/pytest/#change-the-log-level", "text": "Inside tests it's possible to change the log level for the captured log messages. def test_foo ( caplog : LogCaptureFixture ): caplog . set_level ( logging . INFO ) pass If you just want to change the log level of a dependency you can use: caplog . set_level ( logging . WARNING , logger = \"urllib3\" )", "title": "Change the log level"}, {"location": "coding/python/pytest/#the-capsys-fixture", "text": "The capsys builtin fixture provides two bits of functionality: it allows you to retrieve stdout and stderr from some code, and it disables output capture temporarily. Suppose you have a function to print a greeting to stdout: def greeting ( name ): print ( f \"Hi, { name } \" ) You can test the output by using capsys . from _pytest.capture import CaptureFixture def test_greeting ( capsys : CaptureFixture [ Any ]): greeting ( \"Earthling\" ) out , err = capsys . readouterr () assert out == \"Hi, Earthling \\n \" assert err == \"\" The return value is whatever has been captured since the beginning of the function, or from the last time it was called.", "title": "The capsys fixture"}, {"location": "coding/python/pytest/#freezegun", "text": "freezegun lets you freeze time in both the test and fixtures.", "title": "freezegun"}, {"location": "coding/python/pytest/#install_1", "text": "pip install pytest-freezegun", "title": "Install"}, {"location": "coding/python/pytest/#usage_1", "text": "", "title": "Usage"}, {"location": "coding/python/pytest/#global-usage", "text": "Most of the tests work with frozen time, so it's better to freeze it by default and unfreeze it on the ones that actually need time to move. To do that set in your tests/conftest.py a globally used fixture: if TYPE_CHECKING : from freezegun.api import FrozenDateTimeFactory @pytest . fixture ( autouse = True ) def frozen_time () -> Generator [ FrozenDateTimeFactory , None , None ]: \"\"\"Freeze all tests time\"\"\" with freezegun . freeze_time () as freeze : yield freeze Freeze time by using the freezer fixture:", "title": "Global usage"}, {"location": "coding/python/pytest/#manual-use", "text": "if TYPE_CHECKING : from freezegun.api import FrozenDateTimeFactory def test_frozen_date ( freezer : FrozenDateTimeFactory ): now = datetime . now () time . sleep ( 1 ) later = datetime . now () assert now == later This can then be used to move time: def test_moving_date ( freezer ): now = datetime . now () freezer . move_to ( \"2017-05-20\" ) later = datetime . now () assert now != later You can also pass arguments to freezegun by using the freeze_time mark: @pytest . mark . freeze_time ( \"2017-05-21\" ) def test_current_date (): assert date . today () == date ( 2017 , 5 , 21 ) The freezer fixture and freeze_time mark can be used together, and they work with other fixtures: @pytest . fixture def current_date (): return date . today () @pytest . mark . freeze_time () def test_changing_date ( current_date , freezer ): freezer . move_to ( \"2017-05-20\" ) assert current_date == date ( 2017 , 5 , 20 ) freezer . move_to ( \"2017-05-21\" ) assert current_date == date ( 2017 , 5 , 21 ) They can also be used in class-based tests: class TestDate : @pytest . mark . freeze_time def test_changing_date ( self , current_date , freezer ): freezer . move_to ( \"2017-05-20\" ) assert current_date == date ( 2017 , 5 , 20 ) freezer . move_to ( \"2017-05-21\" ) assert current_date == date ( 2017 , 5 , 21 )", "title": "Manual use"}, {"location": "coding/python/pytest/#customize-nested-fixtures", "text": "Sometimes you need to tweak your fixtures so they can be used in different tests. As usual, there are different solutions to the same problem. Note: \"TL;DR: For simple cases parametrize your fixtures or use parametrization to override the default valued fixture . As your test suite get's more complex migrate to pytest-case .\" Let's say you're running along merrily with some fixtures that create database objects for you: @pytest . fixture def supplier ( db ): s = Supplier ( ref = random_ref (), name = random_name (), country = \"US\" , ) db . add ( s ) yield s db . remove ( s ) @pytest . fixture () def product ( db , supplier ): p = Product ( ref = random_ref (), name = random_name (), supplier = supplier , net_price = 9.99 , ) db . add ( p ) yield p db . remove ( p ) And now you're writing a new test and you suddenly realize you need to customize your default \"supplier\" fixture: def test_US_supplier_has_total_price_equal_net_price ( product ): assert product . total_price == product . net_price def test_EU_supplier_has_total_price_including_VAT ( supplier , product ): supplier . country = \"FR\" # oh, this doesn't work assert product . total_price == product . net_price * 1.2 There are different ways to modify your fixtures", "title": "Customize nested fixtures"}, {"location": "coding/python/pytest/#add-more-fixtures", "text": "We can just create more fixtures, and try to do a bit of DRY by extracting out common logic: def _default_supplier (): return Supplier ( ref = random_ref (), name = random_name (), ) @pytest . fixture def us_supplier ( db ): s = _default_supplier () s . country = \"US\" db . add ( s ) yield s db . remove ( s ) @pytest . fixture def eu_supplier ( db ): s = _default_supplier () s . country = \"FR\" db . add ( s ) yield s db . remove ( s ) That's just one way you could do it, maybe you can figure out ways to reduce the duplication of the db.add() stuff as well, but you are going to have a different, named fixture for each customization of Supplier, and eventually you may decide that doesn't scale.", "title": "Add more fixtures"}, {"location": "coding/python/pytest/#use-factory-fixtures", "text": "Instead of a fixture returning an object directly, it can return a function that creates an object, and that function can take arguments: @pytest . fixture def make_supplier ( db ): s = Supplier ( ref = random_ref (), name = random_name (), ) def _make_supplier ( country ): s . country = country db . add ( s ) return s yield _make_supplier db . remove ( s ) The problem with this is that, once you start, you tend to have to go all the way, and make all of your fixture hierarchy into factory functions: def test_EU_supplier_has_total_price_including_VAT ( make_supplier , product ): supplier = make_supplier ( country = \"FR\" ) product . supplier = ( supplier # OH, now this doesn't work, because it's too late again ) assert product . total_price == product . net_price * 1.2 And so... @pytest . fixture def make_product ( db ): p = Product ( ref = random_ref (), name = random_name (), ) def _make_product ( supplier ): p . supplier = supplier db . add ( p ) return p yield _make_product db . remove ( p ) def test_EU_supplier_has_total_price_including_VAT ( make_supplier , make_product ): supplier = make_supplier ( country = \"FR\" ) product = make_product ( supplier = supplier ) assert product . total_price == product . net_price * 1.2 That works, but firstly now everything is a factory-fixture, which makes them more convoluted, and secondly, your tests are filling up with extra calls to make_things , and you're having to embed some of the domain knowledge of what-depends-on-what into your tests as well as your fixtures. Ugly!", "title": "Use factory fixtures"}, {"location": "coding/python/pytest/#parametrize-your-fixtures", "text": "You can also parametrize your fixtures . @pytest . fixture ( params = [ \"US\" , \"FR\" ]) def supplier ( db , request ): s = Supplier ( ref = random_ref (), name = random_name (), country = request . param ) db . add ( s ) yield s db . remove ( s ) Now any test that depends on supplier, directly or indirectly, will be run twice, once with supplier.country = US and once with FR . That's really cool for checking that a given piece of logic works in a variety of different cases, but it's not really ideal in our case. We have to build a bunch of if logic into our tests: def test_US_supplier_has_no_VAT_but_EU_supplier_has_total_price_including_VAT ( product ): # this test is magically run twice, but: if product . supplier . country == \"US\" : assert product . total_price == product . net_price if product . supplier . country == \"FR\" : assert product . total_price == product . net_price * 1.2 So that's ugly, and on top of that, now every single test that depends (indirectly) on supplier gets run twice, and some of those extra test runs may be totally irrelevant to what the country is.", "title": "Parametrize your fixtures"}, {"location": "coding/python/pytest/#use-pytest-parametrization-to-override-the-default-valued-fixtures", "text": "We introduce an extra fixture that holds a default value for the country field: @pytest . fixture () def country (): return \"US\" @pytest . fixture def supplier ( db , country ): s = Supplier ( ref = random_ref (), name = random_name (), country = country , ) db . add ( s ) yield s db . remove ( s ) And then in the tests that need to change it, we can use parametrize to override the default value of country, even though the country fixture isn't explicitly named in that test: @pytest . mark . parametrize ( \"country\" , [ \"US\" ]) def test_US_supplier_has_total_price_equal_net_price ( product ): assert product . total_price == product . net_price @pytest . mark . parametrize ( \"country\" , [ \"EU\" ]) def test_EU_supplier_has_total_price_including_VAT ( product ): assert product . total_price == product . net_price * 1.2 The only problem is that you're now likely to build a implicit dependencies where the only way to find out what's actually happening is to spend ages spelunking in conftest.py.", "title": "Use pytest parametrization to override the default valued fixtures"}, {"location": "coding/python/pytest/#use-pytest-case", "text": "pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations. Check that file for further information.", "title": "Use pytest-case"}, {"location": "coding/python/pytest/#use-a-fixture-more-than-once-in-a-function", "text": "One solution is to make your fixture return a factory instead of the resource directly: @pytest . fixture ( name = 'make_user' ) def make_user_ (): created = [] def make_user (): u = models . User () u . commit () created . append ( u ) return u yield make_user for u in created : u . delete () def test_two_users ( make_user ): user1 = make_user () user2 = make_user () # test them # you can even have the normal fixture when you only need a single user @pytest . fixture def user ( make_user ): return make_user () def test_one_user ( user ): # test him/her", "title": "Use a fixture more than once in a function"}, {"location": "coding/python/pytest/#marks", "text": "Pytest marks can be used to group tests. It can be useful to: slow : Mark the tests that are slow. secondary : Mart the tests that use functionality that is being tested in the same file. To mark a test, use the @pytest.mark decorator. For example: @pytest . mark . slow def test_really_slow_test (): pass Pytest requires you to register your marks, do so in the pytest.ini file [pytest] markers = slow: marks tests as slow (deselect with '-m \"not slow\"') secondary: mark tests that use functionality tested in the same file (deselect with '-m \"not secondary\"')", "title": "Marks"}, {"location": "coding/python/pytest/#snippets", "text": "", "title": "Snippets"}, {"location": "coding/python/pytest/#mocking-sysexit", "text": "with pytest . raises ( SystemExit ): # Code to test", "title": "Mocking sys.exit"}, {"location": "coding/python/pytest/#testing-exceptions-with-pytest", "text": "def test_value_error_is_raised (): with pytest . raises ( ValueError , match = \"invalid literal for int() with base 10: 'a'\" ): int ( \"a\" )", "title": "Testing exceptions with pytest"}, {"location": "coding/python/pytest/#excluding-code-from-coverage", "text": "You may have code in your project that you know won't be executed, and you want to tell coverage.py to ignore it. For example, if you have some code in abstract classes that is going to be tested on the subclasses, you can ignore it with # pragma: no cover . If you want other code to be excluded , for example the statements inside the if TYPE_CHECKING: add to your pyproject.toml : [tool.coverage.report] exclude_lines = [ \"pragma: no cover\" , \"if TYPE_CHECKING:\" ,]", "title": "Excluding code from coverage"}, {"location": "coding/python/pytest/#running-tests-in-parallel", "text": "pytest-xdist makes it possible to run the tests in parallel, useful when the test suit is large or when the tests are slow.", "title": "Running tests in parallel"}, {"location": "coding/python/pytest/#installation", "text": "pip install pytest-xdist", "title": "Installation"}, {"location": "coding/python/pytest/#usage_2", "text": "pytest -n 4 It will run the tests with 4 workers. If you use auto it will adapt the number of workers to the number of CPUS, or 1 if you use --pdb . To configure it in the pyproject.toml use the addopts [tool.pytest.ini_options] minversion = \"6.0\" addopts = \"-vv --tb=short -n auto\"", "title": "Usage"}, {"location": "coding/python/pytest/#enforce-serial-execution-of-related-tests", "text": "", "title": "Enforce serial execution of related tests"}, {"location": "coding/python/pytest/#use-a-lock", "text": "Implement a serial fixture with a session-scoped file lock fixture using the filelock package. You can add this to your conftest.py : pip install filelock import contextlib import os import filelock import pytest from filelock import BaseFileLock @pytest . fixture ( name = \"lock\" , scope = \"session\" ) def lock_ ( tmp_path_factory : pytest . TempPathFactory , ) -> Generator [ BaseFileLock , None , None ]: \"\"\"Create lock file.\"\"\" base_temp = tmp_path_factory . getbasetemp () lock_file = base_temp . parent / \"serial.lock\" yield FileLock ( lock_file = str ( lock_file )) with contextlib . suppress ( OSError ): os . remove ( path = lock_file ) @pytest . fixture ( name = \"serial\" ) def _serial ( lock : BaseFileLock ) -> Generator [ None , None , None ]: \"\"\"Fixture to run tests in serial.\"\"\" with lock . acquire ( poll_interval = 0.1 ): yield Then inject the serial fixture in any test that requires serial execution. All tests that use the serial fixture are executed serially while any tests that do not use the fixture are executed in parallel.", "title": "Use a lock"}, {"location": "coding/python/pytest/#mark-them-and-run-separately", "text": "Mark the tests you want to execute serially with a special mark, say serial: @pytest . mark . serial class Test : ... @pytest . mark . serial def test_foo (): ... Execute your parallel tests, excluding those with the serial mark: $ py.test -n auto -m \"not serial\" Next, execute your serial tests in a separate session: $ py.test -n0 -m \"serial\"", "title": "Mark them and run separately"}, {"location": "coding/python/pytest/#setting-a-timeout-for-your-tests", "text": "To make your tests fail if they don't end in less than X seconds, use pytest-timeout . Install it with: pip install pytest-timeout You can set a global timeout in your pyproject.toml : [pytest] timeout = 300 Or define it for each test with: @pytest . mark . timeout ( 60 ) def test_foo (): pass", "title": "Setting a timeout for your tests"}, {"location": "coding/python/pytest/#rerun-tests-that-fail-sometimes", "text": "pytest-rerunfailures is a plugin for pytest that re-runs tests to eliminate intermittent failures. Using this plugin is generally a bad idea, it would be best to solve the reason why your code is not reliable. It's useful when you rely on non robust third party software in a way that you can't solve, or if the error is not in your code but in the testing code, and again you are not able to fix it. Install it with: pip install pytest-rerunfailures To re-run all test failures, use the --reruns command line option with the maximum number of times you\u2019d like the tests to run: pytest --reruns 5 Failed fixture or setup_class will also be re-executed. To add a delay time between re-runs use the --reruns-delay command line option with the amount of seconds that you would like wait before the next test re-run is launched: pytest --reruns 5 --reruns-delay 1 To mark individual tests as flaky, and have them automatically re-run when they fail, add the flaky mark with the maximum number of times you\u2019d like the test to run: @pytest . mark . flaky ( reruns = 5 ) def test_example (): import random assert random . choice ([ True , False ])", "title": "Rerun tests that fail sometimes"}, {"location": "coding/python/pytest/#run-tests-in-a-random-order", "text": "pytest-random-order is a pytest plugin that randomises the order of tests. This can be useful to detect a test that passes just because it happens to run after an unrelated test that leaves the system in a favourable state. To use it add the --random-order to your pytest run. It can't yet be used with pytest-xdist though :(.", "title": "Run tests in a random order"}, {"location": "coding/python/pytest/#capture-deprecation-warnings", "text": "Python and its ecosystem does not have an assumption of strict SemVer, and has a tradition of providing deprecation warnings. If you have good CI, you should be able to catch warnings even before your users see them. Try the following pytest configuration: [tool.pytest.ini_options] filterwarnings = [ \"error\" ,] This will turn warnings into errors and allow your CI to break before users break. You can ignore specific warnings as well. For example, the configuration below will ignore all user warnings and specific deprecation warnings matching a regex, but will transform all other warnings into errors. [tool.pytest.ini_options] filterwarnings = [ \"error\" , \"ignore::UserWarning\" , \"ignore:function ham\\\\(\\\\) is deprecated:DeprecationWarning\" ,] When a warning matches more than one option in the list, the action for the last matching option is performed. If you want to ignore the warning of a specific package use: filterwarnings = [ \"error\" , \"ignore::DeprecationWarning:pytest_freezegun.*\" ,] Note: It's better to suppress a warning instead of disabling it for the whole code, check how here .", "title": "Capture deprecation warnings"}, {"location": "coding/python/pytest/#ensuring-code-triggers-a-deprecation-warning", "text": "You can also use pytest.deprecated_call() for checking that a certain function call triggers a DeprecationWarning or PendingDeprecationWarning : import pytest def test_myfunction_deprecated (): with pytest . deprecated_call (): myfunction ( 17 )", "title": "Ensuring code triggers a deprecation warning"}, {"location": "coding/python/pytest/#asserting-warnings-with-the-warns-function", "text": "You can check that code raises a particular warning using pytest.warns(), which works in a similar manner to raises: import warnings import pytest def test_warning (): with pytest . warns ( UserWarning ): warnings . warn ( \"my warning\" , UserWarning ) The test will fail if the warning in question is not raised. The keyword argument match to assert that the exception matches a text or regex: >>> with pytest . warns ( UserWarning , match = 'must be 0 or None' ): ... warnings . warn ( \"value must be 0 or None\" , UserWarning )", "title": "Asserting warnings with the warns function"}, {"location": "coding/python/pytest/#recording-warnings", "text": "You can record raised warnings either using pytest.warns() or with the recwarn fixture. To record with pytest.warns() without asserting anything about the warnings, pass no arguments as the expected warning type and it will default to a generic Warning: with pytest . warns () as record : warnings . warn ( \"user\" , UserWarning ) warnings . warn ( \"runtime\" , RuntimeWarning ) assert len ( record ) == 2 assert str ( record [ 0 ] . message ) == \"user\" assert str ( record [ 1 ] . message ) == \"runtime\" The recwarn fixture will record warnings for the whole function: import warnings def test_hello ( recwarn ): warnings . warn ( \"hello\" , UserWarning ) assert len ( recwarn ) == 1 w = recwarn . pop ( UserWarning ) assert issubclass ( w . category , UserWarning ) assert str ( w . message ) == \"hello\" assert w . filename assert w . lineno Both recwarn and pytest.warns() return the same interface for recorded warnings: a WarningsRecorder instance. To view the recorded warnings, you can iterate over this instance, call len on it to get the number of recorded warnings, or index into it to get a particular recorded warning.", "title": "Recording warnings"}, {"location": "coding/python/pytest/#show-logging-messages-on-the-test-run", "text": "Add to your pyproject.toml : [tool.pytest.ini_options] log_cli = true log_cli_level = 10 Or run it in the command itself pytest -o log_cli=true --log-cli-level=10 func.py . Remember you can change the log level of the different components in case it's too verbose.", "title": "Show logging messages on the test run"}, {"location": "coding/python/pytest/#pytest-integration-with-vim", "text": "Integrating pytest into your Vim workflow enhances your productivity while writing code, thus making it easier to code using TDD. I use Janko-m's Vim-test plugin (which can be installed through Vundle ) with the following configuration. nmap < silent > t :TestNearest -- pdb < CR > nmap < silent > < leader > t :TestSuite tests/unit < CR > nmap < silent > < leader > i :TestSuite tests/integration < CR > nmap < silent > < leader > T :TestFile < CR > let test#python#runner = 'pytest' let test#strategy = \"neovim\" I often open Vim with a vertical split ( :vs ), in the left window I have the tests and in the right the code. Whenever I want to run a single test I press t when the cursor is inside that test. If you need to make changes in the code, you can press t again while the cursor is at the code you are testing and it will run the last test. Once the unit test has passed, I run the whole unit tests with ;t (as ; is my <leader> ). And finally I use ;i to run the integration tests. Finally, if the test suite is huge, I use ;T to run only the tests of a single file. As you can see only the t has the --pdb flag, so the rest of them will run en parallel and any pdb trace will fail.", "title": "Pytest integration with Vim"}, {"location": "coding/python/pytest/#reference", "text": "Book Python Testing with pytest by Brian Okken . Docs Vim-test plugin", "title": "Reference"}, {"location": "coding/python/pytest_cases/", "text": "pytest-cases is a pytest plugin that allows you to separate your test cases from your test functions . In addition, pytest-cases provides several useful goodies to empower pytest . In particular it improves the fixture mechanism to support \"fixture unions\". This is a major change in the internal pytest engine, unlocking many possibilities such as using fixture references as parameter values in a test function. Installing \u2691 pip install pytest_cases Installing pytest-cases has effects on the order of pytest tests execution, even if you do not use its features. One positive side effect is that it fixed pytest#5054 . But if you see less desirable ordering please report it . Why pytest-cases ? \u2691 Let's consider the following foo function under test, located in example.py : def foo ( a , b ): return a + 1 , b + 1 If we were using plain pytest to test it with various inputs, we would create a test_foo.py file and use @pytest.mark.parametrize : import pytest from example import foo @pytest . mark . parametrize ( \"a,b\" , [( 1 , 2 ), ( - 1 , - 2 )]) def test_foo ( a , b ): # check that foo runs correctly and that the result is a tuple. assert isinstance ( foo ( a , b ), tuple ) This is the fastest and most compact thing to do when you have a few number of test cases, that do not require code to generate each test case. Now imagine that instead of (1, 2) and (-1, -2) each of our test cases: Requires a few lines of code to be generated. Requires documentation to explain the other developers the intent of that precise test case. Requires external resources (data files on the filesystem, databases...), with a variable number of cases depending on what is available on the resource. Requires a readable id , such as 'uniformly_sampled_nonsorted_with_holes' for the above example. Of course we could use pytest.param or ids=<list> but that is \"a pain to maintain\" according to pytest doc. Such a design does not feel right as the id is detached from the case. With standard pytest there is no particular pattern to simplify your life here. Investigating a little bit, people usually end up trying to mix parameters and fixtures and asking this kind of question: so1 , so2 . But by design it is not possible to solve this problem using fixtures, because pytest does not handle \"unions\" of fixtures . There is also an example in pytest doc with a metafunc hook . The issue with such workarounds is that you can do anything . And anything is a bit too much: this does not provide any convention / \"good practice\" on how to organize test cases, which is an open door to developing ad-hoc unreadable or unmaintainable solutions. pytest_cases was created to provide an answer to this precise situation. It proposes a simple framework to separate test cases from test functions. The test cases are typically located in a separate \"companion\" file: test_foo.py is your usual test file containing the test functions (named test_<id> ). test_foo_cases.py contains the test cases , that are also functions. Note: an alternate file naming style cases_foo.py is also available if you prefer it. Basic usage \u2691 Case functions \u2691 Let's create a test_foo_cases.py file. This file will contain test cases generator functions , that we will call case functions for brevity. In these functions, you will typically either parse some test data files, generate some simulated test data, expected results, etc. File: test_foo_cases.py def case_two_positive_ints (): \"\"\" Inputs are two positive integers \"\"\" return 1 , 2 def case_two_negative_ints (): \"\"\" Inputs are two negative integers \"\"\" return - 1 , - 2 Case functions can return anything that is considered useful to run the associated test. You can use all classic pytest mechanism on case functions (id customization, skip/fail marks, parametrization or fixtures injection). Test functions \u2691 As usual we write our pytest test functions starting with test_ , in a test_foo.py file. The only difference is that we now decorate it with @parametrize_with_cases instead of @pytest.mark.parametrize as we were doing previously: File: test_foo.py from example import foo from pytest_cases import parametrize_with_cases @parametrize_with_cases ( \"a,b\" ) def test_foo ( a , b ): # check that foo runs correctly and that the result is a tuple. assert isinstance ( foo ( a , b ), tuple ) Executing pytest will now run our test function once for every case function: >>> pytest -s -v ============================= test session starts ============================= ( ... ) <your_project>/tests/test_foo.py::test_foo [ two_positive_ints ] PASSED [ 50 % ] <your_project>/tests/test_foo.py::test_foo [ two_negative_ints ] PASSED [ 100 % ] ========================== 2 passed in 0 .24 seconds ========================== Usage \u2691 Cases collection \u2691 Alternate source(s) \u2691 It is not mandatory that case functions should be in a different file than the test functions: both can be in the same file. For this you can use cases='.' or cases=THIS_MODULE to refer to the module in which the test function is located: from pytest_cases import parametrize_with_cases def case_one_positive_int (): return 1 def case_one_negative_int (): return - 1 @parametrize_with_cases ( \"i\" , cases = '.' ) def test_with_this_module ( i ): assert i == int ( i ) Only the case functions defined BEFORE the test function in the module file will be taken into account. @parametrize_with_cases(cases=...) also accepts explicit list of case functions, classes containing case functions, and modules. See API Reference for details. A typical way to organize cases is to use classes for example: from pytest_cases import parametrize_with_cases class Foo : def case_a_positive_int ( self ): return 1 def case_another_positive_int ( self ): return 2 @parametrize_with_cases ( \"a\" , cases = Foo ) def test_foo ( a ): assert a > 0 Note that as for pytest , self is recreated for every test and therefore should not be used to store any useful information. Alternate prefix \u2691 case_ might not be your preferred prefix, especially if you wish to store in the same module or class various kind of case data. @parametrize_with_cases offers a prefix=... argument to select an alternate prefix for your case functions. That way, you can store in the same module or class case functions as diverse as datasets (e.g. data_ ), user descriptions (e.g. user_ ), algorithms or machine learning models (e.g. model_ or algo_ ), etc. from pytest_cases import parametrize_with_cases , parametrize def data_a (): return 'a' @parametrize ( \"hello\" , [ True , False ]) def data_b ( hello ): return \"hello\" if hello else \"world\" def case_c (): return dict ( name = \"hi i'm not used\" ) def user_bob (): return \"bob\" @parametrize_with_cases ( \"data\" , cases = '.' , prefix = \"data_\" ) @parametrize_with_cases ( \"user\" , cases = '.' , prefix = \"user_\" ) def test_with_data ( data , user ): assert data in ( 'a' , \"hello\" , \"world\" ) assert user == 'bob' Yields test_doc_filters_n_tags.py::test_with_data[bob-a] PASSED [ 33%] test_doc_filters_n_tags.py::test_with_data[bob-b-True] PASSED [ 66%] test_doc_filters_n_tags.py::test_with_data[bob-b-False] PASSED [ 100%] Filters and tags \u2691 The easiest way to select only a subset of case functions in a module or a class, is to specify a custom prefix instead of the default one ( 'case_' ). However sometimes more advanced filtering is required. In that case, you can also rely on three additional mechanisms provided in @parametrize_with_cases : The glob argument can contain a glob-like pattern for case ids. This can become handy to separate for example good or bad cases, the latter returning an expected error type and/or message for use with pytest.raises or with our alternative assert_exception . from math import sqrt import pytest from pytest_cases import parametrize_with_cases def case_int_success (): return 1 def case_negative_int_failure (): # note that we decide to return the expected type of failure to check it return - 1 , ValueError , \"math domain error\" @parametrize_with_cases ( \"data\" , cases = '.' , glob = \"*success\" ) def test_good_datasets ( data ): assert sqrt ( data ) > 0 @parametrize_with_cases ( \"data, err_type, err_msg\" , cases = '.' , glob = \"*failure\" ) def test_bad_datasets ( data , err_type , err_msg ): with pytest . raises ( err_type , match = err_msg ): sqrt ( data ) The has_tag argument allows you to filter cases based on tags set on case functions using the @case decorator. See API reference of @case and @parametrize_with_cases . from pytest_cases import parametrize_with_cases , case class FooCases : def case_two_positive_ints ( self ): return 1 , 2 @case ( tags = 'foo' ) def case_one_positive_int ( self ): return 1 @parametrize_with_cases ( \"a\" , cases = FooCases , has_tag = 'foo' ) def test_foo ( a ): assert a > 0 Finally if none of the above matches your expectations, you can provide a callable to filter . This callable will receive each collected case function and should return True in case of success. Note that your function can leverage the _pytestcase attribute available on the case function to read the tags, marks and id found on it. @parametrize_with_cases ( \"data\" , cases = '.' , filter = lambda cf : \"success\" in cf . _pytestcase . id ) def test_good_datasets2 ( data ): assert sqrt ( data ) > 0 Pytest marks ( skip , xfail ...) on cases \u2691 pytest marks such as @pytest.mark.skipif can be applied on case functions the same way as with test functions . import sys import pytest @pytest . mark . skipif ( sys . version_info < ( 3 , 0 ), reason = \"Not useful on python 2\" ) def case_two_positive_ints (): return 1 , 2 Case generators \u2691 In many real-world usage we want to generate one test case per <something> . The most intuitive way would be to use a for loop to create the case functions, and to use the @case decorator to set their names ; however this would not be very readable. Instead, case functions can be parametrized the same way as with test functions : simply add the parameter names as arguments in their signature and decorate with @pytest.mark.parametrize . Even better, you can use the enhanced @parametrize from pytest-cases so as to benefit from its additional usability features: from pytest_cases import parametrize , parametrize_with_cases class CasesFoo : def case_hello ( self ): return \"hello world\" @parametrize ( who = ( 'you' , 'there' )) def case_simple_generator ( self , who ): return \"hello %s \" % who @parametrize_with_cases ( \"msg\" , cases = CasesFoo ) def test_foo ( msg ): assert isinstance ( msg , str ) and msg . startswith ( \"hello\" ) Yields test_generators.py::test_foo[hello] PASSED [ 33%] test_generators.py::test_foo[simple_generator-who=you] PASSED [ 66%] test_generators.py::test_foo[simple_generator-who=there] PASSED [100%] Cases requiring fixtures \u2691 Cases can use fixtures the same way as test functions do : simply add the fixture names as arguments in their signature and make sure the fixture exists either in the same module, or in a conftest.py file in one of the parent packages. See pytest documentation on sharing fixtures . Use @fixture instead of @pytest.fixture If a fixture is used by some of your cases only, then you should use the @fixture decorator from pytest-cases instead of the standard @pytest.fixture . Otherwise you fixture will be setup/teardown for all cases even those not requiring it. See @fixture doc . from pytest_cases import parametrize_with_cases , fixture , parametrize @fixture ( scope = 'session' ) def db (): return { 0 : 'louise' , 1 : 'bob' } def user_bob ( db ): return db [ 1 ] @parametrize ( id = range ( 2 )) def user_from_db ( db , id ): return db [ id ] @parametrize_with_cases ( \"a\" , cases = '.' , prefix = 'user_' ) def test_users ( a , db , request ): print ( \"this is test %r \" % request . node . nodeid ) assert a in db . values () Yields test_fixtures.py::test_users[a_is_bob] test_fixtures.py::test_users[a_is_from_db-id=0] test_fixtures.py::test_users[a_is_from_db-id=1] Parametrize fixtures with cases \u2691 In some scenarios you might wish to parametrize a fixture with the cases, rather than the test function. For example: To inject the same test cases in several test functions without duplicating the @parametrize_with_cases decorator on each of them. To generate the test cases once for the whole session, using a scope='session' fixture or another scope . To modify the test cases, log some message, or perform some other action before injecting them into the test functions, and/or after executing the test function (thanks to yield fixtures ). For this, simply use @fixture from pytest_cases instead of @pytest.fixture to define your fixture. That allows your fixtures to be easily parametrized with @parametrize_with_cases , @parametrize , and even @pytest.mark.parametrize . from pytest_cases import fixture , parametrize_with_cases @fixture @parametrize_with_cases ( \"a,b\" ) def c ( a , b ): return a + b def test_foo ( c ): assert isinstance ( c , int ) Pytest-cases internals \u2691 @fixture \u2691 @fixture is similar to pytest.fixture but without its param and ids arguments. Instead, it is able to pick the parametrization from @pytest.mark.parametrize marks applied on fixtures. This makes it very intuitive for users to parametrize both their tests and fixtures. Finally it now supports unpacking, see unpacking feature . @fixture deprecation if/when @pytest.fixture supports @pytest.mark.parametrize The ability for pytest fixtures to support the @pytest.mark.parametrize annotation is a feature that clearly belongs to pytest scope, and has been requested already . It is therefore expected that @fixture will be deprecated in favor of @pytest_fixture if/when the pytest team decides to add the proposed feature. As always, deprecation will happen slowly across versions (at least two minor, or one major version update) so as for users to have the time to update their code bases. unpack_fixture / unpack_into \u2691 In some cases fixtures return a tuple or a list of items. It is not easy to refer to a single of these items in a test or another fixture. With unpack_fixture you can easily do it: import pytest from pytest_cases import unpack_fixture , fixture @fixture @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] a , b = unpack_fixture ( \"a,b\" , c ) def test_function ( a , b ): assert a [ 0 ] == b Note that you can also use the unpack_into= argument of @fixture to do the same thing: import pytest from pytest_cases import fixture @fixture ( unpack_into = \"a,b\" ) @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] def test_function ( a , b ): assert a [ 0 ] == b And it is also available in fixture_union : import pytest from pytest_cases import fixture , fixture_union @fixture @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] @fixture @pytest . mark . parametrize ( \"o\" , [ 'yeepee' , 'yay' ]) def d ( o ): return o , o [ 0 ] fixture_union ( \"c_or_d\" , [ c , d ], unpack_into = \"a, b\" ) def test_function ( a , b ): assert a [ 0 ] == b param_fixture[s] \u2691 If you wish to share some parameters across several fixtures and tests, it might be convenient to have a fixture representing this parameter. This is relatively easy for single parameters, but a bit harder for parameter tuples. The two utilities functions param_fixture (for a single parameter name) and param_fixtures (for a tuple of parameter names) handle the difficulty for you: import pytest from pytest_cases import param_fixtures , param_fixture # create a single parameter fixture my_parameter = param_fixture ( \"my_parameter\" , [ 1 , 2 , 3 , 4 ]) @pytest . fixture def fixture_uses_param ( my_parameter ): ... def test_uses_param ( my_parameter , fixture_uses_param ): ... # ----- # create a 2-tuple parameter fixture arg1 , arg2 = param_fixtures ( \"arg1, arg2\" , [( 1 , 2 ), ( 3 , 4 )]) @pytest . fixture def fixture_uses_param2 ( arg2 ): ... def test_uses_param2 ( arg1 , arg2 , fixture_uses_param2 ): ... fixture_union \u2691 As of pytest 5, it is not possible to create a \"union\" fixture, i.e. a parametrized fixture that would first take all the possible values of fixture A, then all possible values of fixture B, etc. Indeed all fixture dependencies of each test node are grouped together, and if they have parameters a big \"cross-product\" of the parameters is done by pytest . from pytest_cases import fixture , fixture_union @fixture def first (): return 'hello' @fixture ( params = [ 'a' , 'b' ]) def second ( request ): return request . param # c will first take all the values of 'first', then all of 'second' c = fixture_union ( 'c' , [ first , second ]) def test_basic_union ( c ): print ( c ) yields <...>::test_basic_union[c_is_first] hello PASSED <...>::test_basic_union[c_is_second-a] a PASSED <...>::test_basic_union[c_is_second-b] b PASSED References \u2691 Docs Git", "title": "Pytest-cases"}, {"location": "coding/python/pytest_cases/#installing", "text": "pip install pytest_cases Installing pytest-cases has effects on the order of pytest tests execution, even if you do not use its features. One positive side effect is that it fixed pytest#5054 . But if you see less desirable ordering please report it .", "title": "Installing"}, {"location": "coding/python/pytest_cases/#why-pytest-cases", "text": "Let's consider the following foo function under test, located in example.py : def foo ( a , b ): return a + 1 , b + 1 If we were using plain pytest to test it with various inputs, we would create a test_foo.py file and use @pytest.mark.parametrize : import pytest from example import foo @pytest . mark . parametrize ( \"a,b\" , [( 1 , 2 ), ( - 1 , - 2 )]) def test_foo ( a , b ): # check that foo runs correctly and that the result is a tuple. assert isinstance ( foo ( a , b ), tuple ) This is the fastest and most compact thing to do when you have a few number of test cases, that do not require code to generate each test case. Now imagine that instead of (1, 2) and (-1, -2) each of our test cases: Requires a few lines of code to be generated. Requires documentation to explain the other developers the intent of that precise test case. Requires external resources (data files on the filesystem, databases...), with a variable number of cases depending on what is available on the resource. Requires a readable id , such as 'uniformly_sampled_nonsorted_with_holes' for the above example. Of course we could use pytest.param or ids=<list> but that is \"a pain to maintain\" according to pytest doc. Such a design does not feel right as the id is detached from the case. With standard pytest there is no particular pattern to simplify your life here. Investigating a little bit, people usually end up trying to mix parameters and fixtures and asking this kind of question: so1 , so2 . But by design it is not possible to solve this problem using fixtures, because pytest does not handle \"unions\" of fixtures . There is also an example in pytest doc with a metafunc hook . The issue with such workarounds is that you can do anything . And anything is a bit too much: this does not provide any convention / \"good practice\" on how to organize test cases, which is an open door to developing ad-hoc unreadable or unmaintainable solutions. pytest_cases was created to provide an answer to this precise situation. It proposes a simple framework to separate test cases from test functions. The test cases are typically located in a separate \"companion\" file: test_foo.py is your usual test file containing the test functions (named test_<id> ). test_foo_cases.py contains the test cases , that are also functions. Note: an alternate file naming style cases_foo.py is also available if you prefer it.", "title": "Why pytest-cases?"}, {"location": "coding/python/pytest_cases/#basic-usage", "text": "", "title": "Basic usage"}, {"location": "coding/python/pytest_cases/#case-functions", "text": "Let's create a test_foo_cases.py file. This file will contain test cases generator functions , that we will call case functions for brevity. In these functions, you will typically either parse some test data files, generate some simulated test data, expected results, etc. File: test_foo_cases.py def case_two_positive_ints (): \"\"\" Inputs are two positive integers \"\"\" return 1 , 2 def case_two_negative_ints (): \"\"\" Inputs are two negative integers \"\"\" return - 1 , - 2 Case functions can return anything that is considered useful to run the associated test. You can use all classic pytest mechanism on case functions (id customization, skip/fail marks, parametrization or fixtures injection).", "title": "Case functions"}, {"location": "coding/python/pytest_cases/#test-functions", "text": "As usual we write our pytest test functions starting with test_ , in a test_foo.py file. The only difference is that we now decorate it with @parametrize_with_cases instead of @pytest.mark.parametrize as we were doing previously: File: test_foo.py from example import foo from pytest_cases import parametrize_with_cases @parametrize_with_cases ( \"a,b\" ) def test_foo ( a , b ): # check that foo runs correctly and that the result is a tuple. assert isinstance ( foo ( a , b ), tuple ) Executing pytest will now run our test function once for every case function: >>> pytest -s -v ============================= test session starts ============================= ( ... ) <your_project>/tests/test_foo.py::test_foo [ two_positive_ints ] PASSED [ 50 % ] <your_project>/tests/test_foo.py::test_foo [ two_negative_ints ] PASSED [ 100 % ] ========================== 2 passed in 0 .24 seconds ==========================", "title": "Test functions"}, {"location": "coding/python/pytest_cases/#usage", "text": "", "title": "Usage"}, {"location": "coding/python/pytest_cases/#cases-collection", "text": "", "title": "Cases collection"}, {"location": "coding/python/pytest_cases/#alternate-sources", "text": "It is not mandatory that case functions should be in a different file than the test functions: both can be in the same file. For this you can use cases='.' or cases=THIS_MODULE to refer to the module in which the test function is located: from pytest_cases import parametrize_with_cases def case_one_positive_int (): return 1 def case_one_negative_int (): return - 1 @parametrize_with_cases ( \"i\" , cases = '.' ) def test_with_this_module ( i ): assert i == int ( i ) Only the case functions defined BEFORE the test function in the module file will be taken into account. @parametrize_with_cases(cases=...) also accepts explicit list of case functions, classes containing case functions, and modules. See API Reference for details. A typical way to organize cases is to use classes for example: from pytest_cases import parametrize_with_cases class Foo : def case_a_positive_int ( self ): return 1 def case_another_positive_int ( self ): return 2 @parametrize_with_cases ( \"a\" , cases = Foo ) def test_foo ( a ): assert a > 0 Note that as for pytest , self is recreated for every test and therefore should not be used to store any useful information.", "title": "Alternate source(s)"}, {"location": "coding/python/pytest_cases/#alternate-prefix", "text": "case_ might not be your preferred prefix, especially if you wish to store in the same module or class various kind of case data. @parametrize_with_cases offers a prefix=... argument to select an alternate prefix for your case functions. That way, you can store in the same module or class case functions as diverse as datasets (e.g. data_ ), user descriptions (e.g. user_ ), algorithms or machine learning models (e.g. model_ or algo_ ), etc. from pytest_cases import parametrize_with_cases , parametrize def data_a (): return 'a' @parametrize ( \"hello\" , [ True , False ]) def data_b ( hello ): return \"hello\" if hello else \"world\" def case_c (): return dict ( name = \"hi i'm not used\" ) def user_bob (): return \"bob\" @parametrize_with_cases ( \"data\" , cases = '.' , prefix = \"data_\" ) @parametrize_with_cases ( \"user\" , cases = '.' , prefix = \"user_\" ) def test_with_data ( data , user ): assert data in ( 'a' , \"hello\" , \"world\" ) assert user == 'bob' Yields test_doc_filters_n_tags.py::test_with_data[bob-a] PASSED [ 33%] test_doc_filters_n_tags.py::test_with_data[bob-b-True] PASSED [ 66%] test_doc_filters_n_tags.py::test_with_data[bob-b-False] PASSED [ 100%]", "title": "Alternate prefix"}, {"location": "coding/python/pytest_cases/#filters-and-tags", "text": "The easiest way to select only a subset of case functions in a module or a class, is to specify a custom prefix instead of the default one ( 'case_' ). However sometimes more advanced filtering is required. In that case, you can also rely on three additional mechanisms provided in @parametrize_with_cases : The glob argument can contain a glob-like pattern for case ids. This can become handy to separate for example good or bad cases, the latter returning an expected error type and/or message for use with pytest.raises or with our alternative assert_exception . from math import sqrt import pytest from pytest_cases import parametrize_with_cases def case_int_success (): return 1 def case_negative_int_failure (): # note that we decide to return the expected type of failure to check it return - 1 , ValueError , \"math domain error\" @parametrize_with_cases ( \"data\" , cases = '.' , glob = \"*success\" ) def test_good_datasets ( data ): assert sqrt ( data ) > 0 @parametrize_with_cases ( \"data, err_type, err_msg\" , cases = '.' , glob = \"*failure\" ) def test_bad_datasets ( data , err_type , err_msg ): with pytest . raises ( err_type , match = err_msg ): sqrt ( data ) The has_tag argument allows you to filter cases based on tags set on case functions using the @case decorator. See API reference of @case and @parametrize_with_cases . from pytest_cases import parametrize_with_cases , case class FooCases : def case_two_positive_ints ( self ): return 1 , 2 @case ( tags = 'foo' ) def case_one_positive_int ( self ): return 1 @parametrize_with_cases ( \"a\" , cases = FooCases , has_tag = 'foo' ) def test_foo ( a ): assert a > 0 Finally if none of the above matches your expectations, you can provide a callable to filter . This callable will receive each collected case function and should return True in case of success. Note that your function can leverage the _pytestcase attribute available on the case function to read the tags, marks and id found on it. @parametrize_with_cases ( \"data\" , cases = '.' , filter = lambda cf : \"success\" in cf . _pytestcase . id ) def test_good_datasets2 ( data ): assert sqrt ( data ) > 0", "title": "Filters and tags"}, {"location": "coding/python/pytest_cases/#pytest-marks-skip-xfail-on-cases", "text": "pytest marks such as @pytest.mark.skipif can be applied on case functions the same way as with test functions . import sys import pytest @pytest . mark . skipif ( sys . version_info < ( 3 , 0 ), reason = \"Not useful on python 2\" ) def case_two_positive_ints (): return 1 , 2", "title": "Pytest marks (skip, xfail...) on cases"}, {"location": "coding/python/pytest_cases/#case-generators", "text": "In many real-world usage we want to generate one test case per <something> . The most intuitive way would be to use a for loop to create the case functions, and to use the @case decorator to set their names ; however this would not be very readable. Instead, case functions can be parametrized the same way as with test functions : simply add the parameter names as arguments in their signature and decorate with @pytest.mark.parametrize . Even better, you can use the enhanced @parametrize from pytest-cases so as to benefit from its additional usability features: from pytest_cases import parametrize , parametrize_with_cases class CasesFoo : def case_hello ( self ): return \"hello world\" @parametrize ( who = ( 'you' , 'there' )) def case_simple_generator ( self , who ): return \"hello %s \" % who @parametrize_with_cases ( \"msg\" , cases = CasesFoo ) def test_foo ( msg ): assert isinstance ( msg , str ) and msg . startswith ( \"hello\" ) Yields test_generators.py::test_foo[hello] PASSED [ 33%] test_generators.py::test_foo[simple_generator-who=you] PASSED [ 66%] test_generators.py::test_foo[simple_generator-who=there] PASSED [100%]", "title": "Case generators"}, {"location": "coding/python/pytest_cases/#cases-requiring-fixtures", "text": "Cases can use fixtures the same way as test functions do : simply add the fixture names as arguments in their signature and make sure the fixture exists either in the same module, or in a conftest.py file in one of the parent packages. See pytest documentation on sharing fixtures . Use @fixture instead of @pytest.fixture If a fixture is used by some of your cases only, then you should use the @fixture decorator from pytest-cases instead of the standard @pytest.fixture . Otherwise you fixture will be setup/teardown for all cases even those not requiring it. See @fixture doc . from pytest_cases import parametrize_with_cases , fixture , parametrize @fixture ( scope = 'session' ) def db (): return { 0 : 'louise' , 1 : 'bob' } def user_bob ( db ): return db [ 1 ] @parametrize ( id = range ( 2 )) def user_from_db ( db , id ): return db [ id ] @parametrize_with_cases ( \"a\" , cases = '.' , prefix = 'user_' ) def test_users ( a , db , request ): print ( \"this is test %r \" % request . node . nodeid ) assert a in db . values () Yields test_fixtures.py::test_users[a_is_bob] test_fixtures.py::test_users[a_is_from_db-id=0] test_fixtures.py::test_users[a_is_from_db-id=1]", "title": "Cases requiring fixtures"}, {"location": "coding/python/pytest_cases/#parametrize-fixtures-with-cases", "text": "In some scenarios you might wish to parametrize a fixture with the cases, rather than the test function. For example: To inject the same test cases in several test functions without duplicating the @parametrize_with_cases decorator on each of them. To generate the test cases once for the whole session, using a scope='session' fixture or another scope . To modify the test cases, log some message, or perform some other action before injecting them into the test functions, and/or after executing the test function (thanks to yield fixtures ). For this, simply use @fixture from pytest_cases instead of @pytest.fixture to define your fixture. That allows your fixtures to be easily parametrized with @parametrize_with_cases , @parametrize , and even @pytest.mark.parametrize . from pytest_cases import fixture , parametrize_with_cases @fixture @parametrize_with_cases ( \"a,b\" ) def c ( a , b ): return a + b def test_foo ( c ): assert isinstance ( c , int )", "title": "Parametrize fixtures with cases"}, {"location": "coding/python/pytest_cases/#pytest-cases-internals", "text": "", "title": "Pytest-cases internals"}, {"location": "coding/python/pytest_cases/#fixture", "text": "@fixture is similar to pytest.fixture but without its param and ids arguments. Instead, it is able to pick the parametrization from @pytest.mark.parametrize marks applied on fixtures. This makes it very intuitive for users to parametrize both their tests and fixtures. Finally it now supports unpacking, see unpacking feature . @fixture deprecation if/when @pytest.fixture supports @pytest.mark.parametrize The ability for pytest fixtures to support the @pytest.mark.parametrize annotation is a feature that clearly belongs to pytest scope, and has been requested already . It is therefore expected that @fixture will be deprecated in favor of @pytest_fixture if/when the pytest team decides to add the proposed feature. As always, deprecation will happen slowly across versions (at least two minor, or one major version update) so as for users to have the time to update their code bases.", "title": "@fixture"}, {"location": "coding/python/pytest_cases/#unpack_fixture-unpack_into", "text": "In some cases fixtures return a tuple or a list of items. It is not easy to refer to a single of these items in a test or another fixture. With unpack_fixture you can easily do it: import pytest from pytest_cases import unpack_fixture , fixture @fixture @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] a , b = unpack_fixture ( \"a,b\" , c ) def test_function ( a , b ): assert a [ 0 ] == b Note that you can also use the unpack_into= argument of @fixture to do the same thing: import pytest from pytest_cases import fixture @fixture ( unpack_into = \"a,b\" ) @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] def test_function ( a , b ): assert a [ 0 ] == b And it is also available in fixture_union : import pytest from pytest_cases import fixture , fixture_union @fixture @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] @fixture @pytest . mark . parametrize ( \"o\" , [ 'yeepee' , 'yay' ]) def d ( o ): return o , o [ 0 ] fixture_union ( \"c_or_d\" , [ c , d ], unpack_into = \"a, b\" ) def test_function ( a , b ): assert a [ 0 ] == b", "title": "unpack_fixture / unpack_into"}, {"location": "coding/python/pytest_cases/#param_fixtures", "text": "If you wish to share some parameters across several fixtures and tests, it might be convenient to have a fixture representing this parameter. This is relatively easy for single parameters, but a bit harder for parameter tuples. The two utilities functions param_fixture (for a single parameter name) and param_fixtures (for a tuple of parameter names) handle the difficulty for you: import pytest from pytest_cases import param_fixtures , param_fixture # create a single parameter fixture my_parameter = param_fixture ( \"my_parameter\" , [ 1 , 2 , 3 , 4 ]) @pytest . fixture def fixture_uses_param ( my_parameter ): ... def test_uses_param ( my_parameter , fixture_uses_param ): ... # ----- # create a 2-tuple parameter fixture arg1 , arg2 = param_fixtures ( \"arg1, arg2\" , [( 1 , 2 ), ( 3 , 4 )]) @pytest . fixture def fixture_uses_param2 ( arg2 ): ... def test_uses_param2 ( arg1 , arg2 , fixture_uses_param2 ): ...", "title": "param_fixture[s]"}, {"location": "coding/python/pytest_cases/#fixture_union", "text": "As of pytest 5, it is not possible to create a \"union\" fixture, i.e. a parametrized fixture that would first take all the possible values of fixture A, then all possible values of fixture B, etc. Indeed all fixture dependencies of each test node are grouped together, and if they have parameters a big \"cross-product\" of the parameters is done by pytest . from pytest_cases import fixture , fixture_union @fixture def first (): return 'hello' @fixture ( params = [ 'a' , 'b' ]) def second ( request ): return request . param # c will first take all the values of 'first', then all of 'second' c = fixture_union ( 'c' , [ first , second ]) def test_basic_union ( c ): print ( c ) yields <...>::test_basic_union[c_is_first] hello PASSED <...>::test_basic_union[c_is_second-a] a PASSED <...>::test_basic_union[c_is_second-b] b PASSED", "title": "fixture_union"}, {"location": "coding/python/pytest_cases/#references", "text": "Docs Git", "title": "References"}, {"location": "coding/python/pytest_parametrized_testing/", "text": "Parametrization is a process of running the same test with varying sets of data. Each combination of a test and data is counted as a new test case. There are multiple ways to parametrize your tests, each differs in complexity and flexibility. Parametrize the test \u2691 The most simple form of parametrization is at test level: @pytest . mark . parametrize ( \"number\" , [ 1 , 2 , 3 , 0 , 42 ]) def test_foo ( number ): assert number > 0 In this case we are getting five tests: for number 1, 2, 3, 0 and 42. Each of those tests can fail independently of one another (if in this example the test with 0 will fail, and four others will pass). Parametrize the fixtures \u2691 Fixtures may have parameters. Those parameters are passed as a list to the argument params of @pytest.fixture() decorator. Those parameters must be iterables, such as lists. Each parameter to a fixture is applied to each function using this fixture. If a few fixtures are used in one test function, pytest generates a Cartesian product of parameters of those fixtures. To use those parameters, a fixture must consume a special fixture named request . It provides the special (built-in) fixture with some information on the function it deals with. request also contains request.param which contains one element from params . The fixture called as many times as the number of elements in the iterable of params argument, and the test function is called with values of fixtures the same number of times. (basically, the fixture is called len(iterable) times with each next element of iterable in the request.param ). @pytest . fixture ( params = [ \"one\" , \"uno\" ]) def fixture1 ( request ): return request . param @pytest . fixture ( params = [ \"two\" , \"duo\" ]) def fixture2 ( request ): return request . paramdef test_foobar ( fixture1 , fixture2 ): assert type ( fixture1 ) == type ( fixture2 ) The output is: #OUTPUT 3 collected 4 itemstest_3.py::test_foobar[one-two] PASSED [ 25%] test_3.py::test_foobar[one-duo] PASSED [ 50%] test_3.py::test_foobar[uno-two] PASSED [ 75%] test_3.py::test_foobar[uno-duo] PASSED [100%] Parametrization with pytest_generate_tests \u2691 There is an another way to generate arbitrary parametrization at collection time. It\u2019s a bit more direct and verbose, but it provides introspection of test functions, including the ability to see all other fixture names. At collection time Pytest looks up for and calls (if found) a special function in each module, named pytest_generate_tests . This function is not a fixture, but just a regular function. It receives the argument metafunc , which itself is not a fixture, but a special object. pytest_generate_tests is called for each test function in the module to give a chance to parametrize it. Parametrization may happen only through fixtures that test function requests. There is no way to parametrize a test function like this: def test_simple(): assert 2+2 == 4 You need some variables to be used as parameters, and those variables should be arguments to the test function. Pytest will replace those arguments with values from fixtures, and if there are a few values for a fixture, then this is parametrization at work. metafunc argument to pytest_generate_tests provides some useful information on a test function: Ability to see all fixture names that function requests. Ability to see the name of the function. Ability to see code of the function. Finally, metafunc has a parametrize function, which is the way to provide multiple variants of values for fixtures. The same case as before written with the pytest_generate_tests function is: def pytest_generate_tests ( metafunc ): if \"fixture1\" in metafunc . fixturenames : metafunc . parametrize ( \"fixture1\" , [ \"one\" , \"uno\" ]) if \"fixture2\" in metafunc . fixturenames : metafunc . parametrize ( \"fixture2\" , [ \"two\" , \"duo\" ]) def test_foobar ( fixture1 , fixture2 ): assert type ( fixture1 ) == type ( fixture2 ) This solution is a little bit magical, so I'd avoid it in favor of pytest-cases. Use pytest-cases \u2691 pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations. Check that file for further information. Customizations \u2691 Change the tests name \u2691 Sometimes you want to change how the tests are shown so you can understand better what the test is doing. You can use the ids argument to pytest.mark.parametrize . File tests/unit/test_func.py tasks_to_try = ( Task ( 'sleep' , done = True ), Task ( 'wake' , 'brian' ), Task ( 'wake' , 'brian' ), Task ( 'breathe' , 'BRIAN' , True ), Task ( 'exercise' , 'BrIaN' , False ), ) task_ids = [ f 'Task( { task . summary } , { task . owner } , { task . done } )' for task in tasks_to_try ] @pytest . mark . parametrize ( 'task' , tasks_to_try , ids = task_ids ) def test_add_4 ( task ): task_id = tasks . add ( task ) t_from_db = tasks . get ( task_id ) assert equivalent ( t_from_db , task ) $ pytest -v test_func.py::test_add_4 ===================== test session starts ====================== collected 5 items test_add_variety.py::test_add_4 [ Task ( sleep,None,True )] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 0 ] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 1 ] PASSED test_add_variety.py::test_add_4 [ Task ( breathe,BRIAN,True )] PASSED test_add_variety.py::test_add_4 [ Task ( exercise,BrIaN,False )] PASSED =================== 5 passed in 0 .04 seconds =================== Those identifiers can be used to run that specific test. For example pytest -v \"test_func.py::test_add_4[Task(breathe,BRIAN,True)]\" . parametrize() can be applied to classes as well. If the test id can't be derived from the parameter value, use the id argument for the pytest.param : @pytest . mark . parametrize ( 'task' , [ pytest . param ( Task ( 'create' ), id = 'just summary' ), pytest . param ( Task ( 'inspire' , 'Michelle' ), id = 'summary/owner' ), ]) def test_add_6 ( task ): ... Will yield: $ pytest-v test_add_variety.py::test_add_6 =================== test session starts ==================== collected 2 items test_add_variety.py::test_add_6 [ justsummary ] PASSED test_add_variety.py::test_add_6 [ summary/owner ] PASSED ================= 2 passed in 0 .05 seconds ================= References \u2691 A deep dive into Pytest parametrization by George Shulkin Book Python Testing with pytest by Brian Okken .", "title": "Parametrized testing"}, {"location": "coding/python/pytest_parametrized_testing/#parametrize-the-test", "text": "The most simple form of parametrization is at test level: @pytest . mark . parametrize ( \"number\" , [ 1 , 2 , 3 , 0 , 42 ]) def test_foo ( number ): assert number > 0 In this case we are getting five tests: for number 1, 2, 3, 0 and 42. Each of those tests can fail independently of one another (if in this example the test with 0 will fail, and four others will pass).", "title": "Parametrize the test"}, {"location": "coding/python/pytest_parametrized_testing/#parametrize-the-fixtures", "text": "Fixtures may have parameters. Those parameters are passed as a list to the argument params of @pytest.fixture() decorator. Those parameters must be iterables, such as lists. Each parameter to a fixture is applied to each function using this fixture. If a few fixtures are used in one test function, pytest generates a Cartesian product of parameters of those fixtures. To use those parameters, a fixture must consume a special fixture named request . It provides the special (built-in) fixture with some information on the function it deals with. request also contains request.param which contains one element from params . The fixture called as many times as the number of elements in the iterable of params argument, and the test function is called with values of fixtures the same number of times. (basically, the fixture is called len(iterable) times with each next element of iterable in the request.param ). @pytest . fixture ( params = [ \"one\" , \"uno\" ]) def fixture1 ( request ): return request . param @pytest . fixture ( params = [ \"two\" , \"duo\" ]) def fixture2 ( request ): return request . paramdef test_foobar ( fixture1 , fixture2 ): assert type ( fixture1 ) == type ( fixture2 ) The output is: #OUTPUT 3 collected 4 itemstest_3.py::test_foobar[one-two] PASSED [ 25%] test_3.py::test_foobar[one-duo] PASSED [ 50%] test_3.py::test_foobar[uno-two] PASSED [ 75%] test_3.py::test_foobar[uno-duo] PASSED [100%]", "title": "Parametrize the fixtures"}, {"location": "coding/python/pytest_parametrized_testing/#parametrization-with-pytest_generate_tests", "text": "There is an another way to generate arbitrary parametrization at collection time. It\u2019s a bit more direct and verbose, but it provides introspection of test functions, including the ability to see all other fixture names. At collection time Pytest looks up for and calls (if found) a special function in each module, named pytest_generate_tests . This function is not a fixture, but just a regular function. It receives the argument metafunc , which itself is not a fixture, but a special object. pytest_generate_tests is called for each test function in the module to give a chance to parametrize it. Parametrization may happen only through fixtures that test function requests. There is no way to parametrize a test function like this: def test_simple(): assert 2+2 == 4 You need some variables to be used as parameters, and those variables should be arguments to the test function. Pytest will replace those arguments with values from fixtures, and if there are a few values for a fixture, then this is parametrization at work. metafunc argument to pytest_generate_tests provides some useful information on a test function: Ability to see all fixture names that function requests. Ability to see the name of the function. Ability to see code of the function. Finally, metafunc has a parametrize function, which is the way to provide multiple variants of values for fixtures. The same case as before written with the pytest_generate_tests function is: def pytest_generate_tests ( metafunc ): if \"fixture1\" in metafunc . fixturenames : metafunc . parametrize ( \"fixture1\" , [ \"one\" , \"uno\" ]) if \"fixture2\" in metafunc . fixturenames : metafunc . parametrize ( \"fixture2\" , [ \"two\" , \"duo\" ]) def test_foobar ( fixture1 , fixture2 ): assert type ( fixture1 ) == type ( fixture2 ) This solution is a little bit magical, so I'd avoid it in favor of pytest-cases.", "title": "Parametrization with pytest_generate_tests"}, {"location": "coding/python/pytest_parametrized_testing/#use-pytest-cases", "text": "pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations. Check that file for further information.", "title": "Use pytest-cases"}, {"location": "coding/python/pytest_parametrized_testing/#customizations", "text": "", "title": "Customizations"}, {"location": "coding/python/pytest_parametrized_testing/#change-the-tests-name", "text": "Sometimes you want to change how the tests are shown so you can understand better what the test is doing. You can use the ids argument to pytest.mark.parametrize . File tests/unit/test_func.py tasks_to_try = ( Task ( 'sleep' , done = True ), Task ( 'wake' , 'brian' ), Task ( 'wake' , 'brian' ), Task ( 'breathe' , 'BRIAN' , True ), Task ( 'exercise' , 'BrIaN' , False ), ) task_ids = [ f 'Task( { task . summary } , { task . owner } , { task . done } )' for task in tasks_to_try ] @pytest . mark . parametrize ( 'task' , tasks_to_try , ids = task_ids ) def test_add_4 ( task ): task_id = tasks . add ( task ) t_from_db = tasks . get ( task_id ) assert equivalent ( t_from_db , task ) $ pytest -v test_func.py::test_add_4 ===================== test session starts ====================== collected 5 items test_add_variety.py::test_add_4 [ Task ( sleep,None,True )] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 0 ] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 1 ] PASSED test_add_variety.py::test_add_4 [ Task ( breathe,BRIAN,True )] PASSED test_add_variety.py::test_add_4 [ Task ( exercise,BrIaN,False )] PASSED =================== 5 passed in 0 .04 seconds =================== Those identifiers can be used to run that specific test. For example pytest -v \"test_func.py::test_add_4[Task(breathe,BRIAN,True)]\" . parametrize() can be applied to classes as well. If the test id can't be derived from the parameter value, use the id argument for the pytest.param : @pytest . mark . parametrize ( 'task' , [ pytest . param ( Task ( 'create' ), id = 'just summary' ), pytest . param ( Task ( 'inspire' , 'Michelle' ), id = 'summary/owner' ), ]) def test_add_6 ( task ): ... Will yield: $ pytest-v test_add_variety.py::test_add_6 =================== test session starts ==================== collected 2 items test_add_variety.py::test_add_6 [ justsummary ] PASSED test_add_variety.py::test_add_6 [ summary/owner ] PASSED ================= 2 passed in 0 .05 seconds =================", "title": "Change the tests name"}, {"location": "coding/python/pytest_parametrized_testing/#references", "text": "A deep dive into Pytest parametrization by George Shulkin Book Python Testing with pytest by Brian Okken .", "title": "References"}, {"location": "coding/python/python_anti_patterns/", "text": "Mutable default arguments \u2691 What You Wrote \u2691 def append_to ( element , to = []): to . append ( element ) return to What You Might Have Expected to Happen \u2691 my_list = append_to ( 12 ) print ( my_list ) my_other_list = append_to ( 42 ) print ( my_other_list ) A new list is created each time the function is called if a second argument isn\u2019t provided, so that the output is: [ 12 ] [ 42 ] What Does Happen [ 12 ] [ 12 , 42 ] A new list is created once when the function is defined, and the same list is used in each successive call. Python\u2019s default arguments are evaluated once when the function is defined, not each time the function is called. This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well. What You Should Do Instead \u2691 Create a new object each time the function is called, by using a default arg to signal that no argument was provided (None is often a good choice). def append_to ( element , to = None ): if to is None : to = [] to . append ( element ) return to Do not forget, you are passing a list object as the second argument. When the Gotcha Isn\u2019t a Gotcha \u2691 Sometimes you can specifically \u201cexploit\u201d this behavior to maintain state between calls of a function. This is often done when writing a caching function.", "title": "Anti-Patterns"}, {"location": "coding/python/python_anti_patterns/#mutable-default-arguments", "text": "", "title": "Mutable default arguments"}, {"location": "coding/python/python_anti_patterns/#what-you-wrote", "text": "def append_to ( element , to = []): to . append ( element ) return to", "title": "What You Wrote"}, {"location": "coding/python/python_anti_patterns/#what-you-might-have-expected-to-happen", "text": "my_list = append_to ( 12 ) print ( my_list ) my_other_list = append_to ( 42 ) print ( my_other_list ) A new list is created each time the function is called if a second argument isn\u2019t provided, so that the output is: [ 12 ] [ 42 ] What Does Happen [ 12 ] [ 12 , 42 ] A new list is created once when the function is defined, and the same list is used in each successive call. Python\u2019s default arguments are evaluated once when the function is defined, not each time the function is called. This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well.", "title": "What You Might Have Expected to Happen"}, {"location": "coding/python/python_anti_patterns/#what-you-should-do-instead", "text": "Create a new object each time the function is called, by using a default arg to signal that no argument was provided (None is often a good choice). def append_to ( element , to = None ): if to is None : to = [] to . append ( element ) return to Do not forget, you are passing a list object as the second argument.", "title": "What You Should Do Instead"}, {"location": "coding/python/python_anti_patterns/#when-the-gotcha-isnt-a-gotcha", "text": "Sometimes you can specifically \u201cexploit\u201d this behavior to maintain state between calls of a function. This is often done when writing a caching function.", "title": "When the Gotcha Isn\u2019t a Gotcha"}, {"location": "coding/python/python_code_styling/", "text": "Not using setdefault() to initialize a dictionary \u2691 When initializing a dictionary, it is common to see a code check for the existence of a key and then create the key if it does not exist. Given a dictionary = {} , if you want to create a key if it doesn't exist, instead of doing: try : dictionary [ 'key' ] except KeyError : dictionary [ 'key' ] = {} You can use: dictionary . setdefault ( 'key' , {}) Black code style \u2691 Black is a style guide enforcement tool. Flake8 \u2691 Flake8 is another style guide enforcement tool. f-strings \u2691 f-strings , also known as formatted string literals , are strings that have an f at the beginning and curly braces containing expressions that will be replaced with their values. Introduced in Python 3.6, they are more readable, concise, and less prone to error than other ways of formatting, as well as faster. >>> name = \"Eric\" >>> age = 74 >>> f \"Hello, { name } . You are { age } .\" 'Hello, Eric. You are 74.' Arbitrary expressions \u2691 Because f-strings are evaluated at runtime, you can put any valid Python expressions in them. For example, calling a function or method from within. >>> f \" { name . lower () } is funny.\" 'eric idle is funny.' Multiline f-strings \u2691 >>> name = \"Eric\" >>> profession = \"comedian\" >>> affiliation = \"Monty Python\" >>> message = ( ... f \"Hi { name } . \" ... f \"You are a { profession } . \" ... f \"You were in { affiliation } .\" ... ) >>> message 'Hi Eric. You are a comedian. You were in Monty Python.' Lint error fixes and ignores \u2691 Fix Pylint R0201 error \u2691 The error shows Method could be a function , it is used when there is no reference to the class, suggesting that the method could be used as a static function instead. Attempt using either of the decorators @classmethod or @staticmethod . If you don't need to change or use the class methods, use staticmethod . Example: Class Foo ( object ): ... def bar ( self , baz ): ... return llama Try instead to use: Class Foo ( object ): ... @classmethod def bar ( cls , baz ): ... return llama Or Class Foo ( object ): ... @staticmethod def bar ( baz ): ... return llama W1203 with F-strings \u2691 This rule suggest you to use the % interpolation in the logging methods because it might save some interpolation time when a logging statement is not run. Nevertheless the performance improvement is negligible and the advantages of using f-strings far outweigh them. W0106 in list comprehension \u2691 They just don't support it they suggest to use normal for loops. W1514 set encoding on open \u2691 with open ( 'file.txt' , 'r' , encoding = 'utf-8' ): [SIM105 Use \u2691 'contextlib.suppress(Exception)']( https://docs.python.org/3/library/contextlib.html#contextlib.suppress ) To bypass exceptions, it's better to use: from contextlib import suppress with suppress ( FileNotFoundError ): os . remove ( 'somefile.tmp' ) Instead of: try : os . remove ( 'somefile.tmp' ) except FileNotFoundError : pass", "title": "Code Styling"}, {"location": "coding/python/python_code_styling/#not-using-setdefault-to-initialize-a-dictionary", "text": "When initializing a dictionary, it is common to see a code check for the existence of a key and then create the key if it does not exist. Given a dictionary = {} , if you want to create a key if it doesn't exist, instead of doing: try : dictionary [ 'key' ] except KeyError : dictionary [ 'key' ] = {} You can use: dictionary . setdefault ( 'key' , {})", "title": "Not using setdefault() to initialize a dictionary"}, {"location": "coding/python/python_code_styling/#black-code-style", "text": "Black is a style guide enforcement tool.", "title": "Black code style"}, {"location": "coding/python/python_code_styling/#flake8", "text": "Flake8 is another style guide enforcement tool.", "title": "Flake8"}, {"location": "coding/python/python_code_styling/#f-strings", "text": "f-strings , also known as formatted string literals , are strings that have an f at the beginning and curly braces containing expressions that will be replaced with their values. Introduced in Python 3.6, they are more readable, concise, and less prone to error than other ways of formatting, as well as faster. >>> name = \"Eric\" >>> age = 74 >>> f \"Hello, { name } . You are { age } .\" 'Hello, Eric. You are 74.'", "title": "f-strings"}, {"location": "coding/python/python_code_styling/#arbitrary-expressions", "text": "Because f-strings are evaluated at runtime, you can put any valid Python expressions in them. For example, calling a function or method from within. >>> f \" { name . lower () } is funny.\" 'eric idle is funny.'", "title": "Arbitrary expressions"}, {"location": "coding/python/python_code_styling/#multiline-f-strings", "text": ">>> name = \"Eric\" >>> profession = \"comedian\" >>> affiliation = \"Monty Python\" >>> message = ( ... f \"Hi { name } . \" ... f \"You are a { profession } . \" ... f \"You were in { affiliation } .\" ... ) >>> message 'Hi Eric. You are a comedian. You were in Monty Python.'", "title": "Multiline f-strings"}, {"location": "coding/python/python_code_styling/#lint-error-fixes-and-ignores", "text": "", "title": "Lint error fixes and ignores"}, {"location": "coding/python/python_code_styling/#fix-pylint-r0201-error", "text": "The error shows Method could be a function , it is used when there is no reference to the class, suggesting that the method could be used as a static function instead. Attempt using either of the decorators @classmethod or @staticmethod . If you don't need to change or use the class methods, use staticmethod . Example: Class Foo ( object ): ... def bar ( self , baz ): ... return llama Try instead to use: Class Foo ( object ): ... @classmethod def bar ( cls , baz ): ... return llama Or Class Foo ( object ): ... @staticmethod def bar ( baz ): ... return llama", "title": "Fix Pylint R0201 error"}, {"location": "coding/python/python_code_styling/#w1203-with-f-strings", "text": "This rule suggest you to use the % interpolation in the logging methods because it might save some interpolation time when a logging statement is not run. Nevertheless the performance improvement is negligible and the advantages of using f-strings far outweigh them.", "title": "W1203 with F-strings"}, {"location": "coding/python/python_code_styling/#w0106-in-list-comprehension", "text": "They just don't support it they suggest to use normal for loops.", "title": "W0106 in list comprehension"}, {"location": "coding/python/python_code_styling/#w1514-set-encoding-on-open", "text": "with open ( 'file.txt' , 'r' , encoding = 'utf-8' ):", "title": "W1514 set encoding on open"}, {"location": "coding/python/python_code_styling/#sim105-use", "text": "'contextlib.suppress(Exception)']( https://docs.python.org/3/library/contextlib.html#contextlib.suppress ) To bypass exceptions, it's better to use: from contextlib import suppress with suppress ( FileNotFoundError ): os . remove ( 'somefile.tmp' ) Instead of: try : os . remove ( 'somefile.tmp' ) except FileNotFoundError : pass", "title": "[SIM105 Use"}, {"location": "coding/python/python_config_yaml/", "text": "Several programs load the configuration from file. After trying ini, json and yaml I've seen that the last one is the most comfortable. So here are the templates for the tests and class that loads the data from a yaml file and exposes it as a dictionary. In the following sections Jinja templating is used, so substitute everything between {{ }} to their correct values. It's assumed that: The root directory of the project has the same name as the program. A file with a valid config exists in assets/config.yaml . We'll use this file in the documentation, so comment it through. Code \u2691 The class code below is expected to introduced in the file configuration.py . Variables to substitute: program_name File {{ program_name }}/configuration.py \"\"\" Module to define the configuration of the main program. Classes: Config: Class to manipulate the configuration of the program. \"\"\" from collections import UserDict from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError import logging import os import sys log = logging . getLogger ( __name__ ) class Config ( UserDict ): \"\"\" Class to manipulate the configuration of the program. Arguments: config_path (str): Path to the configuration file. Default: ~/.local/share/{{ program_name }}/config.yaml Public methods: get: Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. load: Loads configuration from configuration YAML file. save: Saves configuration in the configuration YAML file. Attributes and properties: config_path (str): Path to the configuration file. data(dict): Program configuration. \"\"\" def __init__ ( self , config_path = '~/.local/share/{{ program_name }}/config.yaml' ): self . config_path = os . path . expanduser ( config_path ) self . load () def get ( self , key ): \"\"\" Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. So if the configuration contents are: self.data = { 'first': { 'second': 'value' }, } self.data.get('first.second') == 'value' Arguments: key(str): Configuration key to fetch \"\"\" keys = key . split ( '.' ) value = self . data . copy () for key in keys : value = value [ key ] return value def load ( self ): \"\"\" Loads configuration from configuration YAML file. \"\"\" try : with open ( os . path . expanduser ( self . config_path ), 'r' ) as f : try : self . data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( self . config_path ) ) sys . exit ( 1 ) def save ( self ): \"\"\" Saves configuration in the configuration YAML file. \"\"\" with open ( os . path . expanduser ( self . config_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( self . data , f ) We use ruamel PyYAML implementation to preserve the file comments. That class is meant to be loaded in the main __init__.py file, below the logging configuration (if there is any). Variables to substitute: program_name config_environmental_variable : The optional environmental variable where the path to the configuration is set. For example PYDO_CONFIG . This will be used in the tests to override the default path. We don't load this configuration from the program argument parser because it's definition often depends on the config file. File {{ program_name}}/ init .py # ... # (optional logging definition) import os from {{ program_name }} . configuration import Config config = Config ( os . getenv ( '{{ config_environmental_variable }}' , '~/.local/share/{{ program_name }}/config.yaml' )) # (Rest of the program) # ... If you want to use the config object in a module of your program, import them from the above file like this: File {{ program_name }}/cli.py from {{ program_name }} import config Tests \u2691 I feel that the tests should use the default configuration, therefore we're setting the environmental variable in the conftest.py file that gets executed by pytest in the tests setup. Variables to substitute: config_environmental_variable : Same as the one defined in the last section. File tests/conftest.py import os os . environ [{{ config_environmental_variable }}] = 'assets/config.yaml' Variables to substitute: program_name As it's really dependent in the config structure, you can improve the test_config_load test to make it more meaningful. File tests/unit/test_configuration.py from {{ program_name }} . configuration import Config from unittest.mock import patch from ruamel.yaml.scanner import ScannerError import os import pytest import shutil import tempfile class TestConfig : \"\"\" Class to test the Config object. Public attributes: config (Config object): Config object to test \"\"\" @pytest . fixture ( autouse = True ) def setup ( self ): self . config_path = 'assets/config.yaml' self . log_patch = patch ( '{{ program_name }}.configuration.log' , autospect = True ) self . log = self . log_patch . start () self . sys_patch = patch ( '{{ program_name }}.configuration.sys' , autospect = True ) self . sys = self . sys_patch . start () self . config = Config ( self . config_path ) yield 'setup' self . log_patch . stop () self . sys_patch . stop () def test_get_can_fetch_nested_items_with_dots ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config . get ( 'first.second' ) == 'value' def test_config_can_fetch_nested_items_with_dictionary_notation ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config [ 'first' ][ 'second' ] == 'value' def test_config_load ( self ): self . config . load () assert len ( self . config . data ) > 0 @patch ( '{{ program_name }}.configuration.YAML' ) def test_load_handles_wrong_file_format ( self , yamlMock ): yamlMock . return_value . load . side_effect = ScannerError ( 'error' , '' , 'problem' , 'mark' , ) self . config . load () self . log . error . assert_called_once_with ( 'Error parsing yaml of configuration file mark: problem' ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.open' ) def test_load_handles_file_not_found ( self , openMock ): openMock . side_effect = FileNotFoundError () self . config . load () self . log . error . assert_called_once_with ( 'Error opening configuration file {} ' . format ( self . config_path ) ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.Config.load' ) def test_init_calls_config_load ( self , loadMock ): Config () loadMock . assert_called_once_with () def test_save_config ( self ): tmp = tempfile . mkdtemp () save_file = os . path . join ( tmp , 'yaml_save_test.yaml' ) self . config = Config ( save_file ) self . config . data = { 'a' : 'b' } self . config . save () with open ( save_file , 'r' ) as f : assert \"a:\" in f . read () shutil . rmtree ( tmp ) Installation \u2691 It's always nice to have the default configuration template (with it's documentation) when configuring your use case. Therefore we're going to add a step in the installation process to copy the file. Variables to substitute in both files: program_name File setup.py import shutil ... Class PostInstallCommand ( install ): ... def run ( self ): install . run ( self ) try : data_directory = os . path . expanduser ( \"~/.local/share/{{ program_name }}\" ) os . makedirs ( data_directory ) log . info ( \"Data directory created\" ) except FileExistsError : log . info ( \"Data directory already exits\" ) config_path = os . path . join ( data_directory , 'config.yaml' ) if os . path . isfile ( config_path ) and os . access ( config_path , os . R_OK ): log . info ( \"Configuration file already exists, check the documentation \" \"for the new version changes.\" ) else : shutil . copyfile ( 'assets/config.yaml' , config_path ) log . info ( \"Copied default configuration template\" ) assets_url : Url pointing to the assets file, similar to https://github.com/lyz-code/pydo/blob/master/assets/config.yaml . README.md ... {{ program_name }} configuration is done through the yaml file located at ~/.local/share/{{ program_name }}/config.yaml . The default template is provided at installation time. ... It's also necessary to add the ruamel.yaml pip package to your setup.py and requirements.txt files.", "title": "Load config from YAML"}, {"location": "coding/python/python_config_yaml/#code", "text": "The class code below is expected to introduced in the file configuration.py . Variables to substitute: program_name File {{ program_name }}/configuration.py \"\"\" Module to define the configuration of the main program. Classes: Config: Class to manipulate the configuration of the program. \"\"\" from collections import UserDict from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError import logging import os import sys log = logging . getLogger ( __name__ ) class Config ( UserDict ): \"\"\" Class to manipulate the configuration of the program. Arguments: config_path (str): Path to the configuration file. Default: ~/.local/share/{{ program_name }}/config.yaml Public methods: get: Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. load: Loads configuration from configuration YAML file. save: Saves configuration in the configuration YAML file. Attributes and properties: config_path (str): Path to the configuration file. data(dict): Program configuration. \"\"\" def __init__ ( self , config_path = '~/.local/share/{{ program_name }}/config.yaml' ): self . config_path = os . path . expanduser ( config_path ) self . load () def get ( self , key ): \"\"\" Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. So if the configuration contents are: self.data = { 'first': { 'second': 'value' }, } self.data.get('first.second') == 'value' Arguments: key(str): Configuration key to fetch \"\"\" keys = key . split ( '.' ) value = self . data . copy () for key in keys : value = value [ key ] return value def load ( self ): \"\"\" Loads configuration from configuration YAML file. \"\"\" try : with open ( os . path . expanduser ( self . config_path ), 'r' ) as f : try : self . data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( self . config_path ) ) sys . exit ( 1 ) def save ( self ): \"\"\" Saves configuration in the configuration YAML file. \"\"\" with open ( os . path . expanduser ( self . config_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( self . data , f ) We use ruamel PyYAML implementation to preserve the file comments. That class is meant to be loaded in the main __init__.py file, below the logging configuration (if there is any). Variables to substitute: program_name config_environmental_variable : The optional environmental variable where the path to the configuration is set. For example PYDO_CONFIG . This will be used in the tests to override the default path. We don't load this configuration from the program argument parser because it's definition often depends on the config file. File {{ program_name}}/ init .py # ... # (optional logging definition) import os from {{ program_name }} . configuration import Config config = Config ( os . getenv ( '{{ config_environmental_variable }}' , '~/.local/share/{{ program_name }}/config.yaml' )) # (Rest of the program) # ... If you want to use the config object in a module of your program, import them from the above file like this: File {{ program_name }}/cli.py from {{ program_name }} import config", "title": "Code"}, {"location": "coding/python/python_config_yaml/#tests", "text": "I feel that the tests should use the default configuration, therefore we're setting the environmental variable in the conftest.py file that gets executed by pytest in the tests setup. Variables to substitute: config_environmental_variable : Same as the one defined in the last section. File tests/conftest.py import os os . environ [{{ config_environmental_variable }}] = 'assets/config.yaml' Variables to substitute: program_name As it's really dependent in the config structure, you can improve the test_config_load test to make it more meaningful. File tests/unit/test_configuration.py from {{ program_name }} . configuration import Config from unittest.mock import patch from ruamel.yaml.scanner import ScannerError import os import pytest import shutil import tempfile class TestConfig : \"\"\" Class to test the Config object. Public attributes: config (Config object): Config object to test \"\"\" @pytest . fixture ( autouse = True ) def setup ( self ): self . config_path = 'assets/config.yaml' self . log_patch = patch ( '{{ program_name }}.configuration.log' , autospect = True ) self . log = self . log_patch . start () self . sys_patch = patch ( '{{ program_name }}.configuration.sys' , autospect = True ) self . sys = self . sys_patch . start () self . config = Config ( self . config_path ) yield 'setup' self . log_patch . stop () self . sys_patch . stop () def test_get_can_fetch_nested_items_with_dots ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config . get ( 'first.second' ) == 'value' def test_config_can_fetch_nested_items_with_dictionary_notation ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config [ 'first' ][ 'second' ] == 'value' def test_config_load ( self ): self . config . load () assert len ( self . config . data ) > 0 @patch ( '{{ program_name }}.configuration.YAML' ) def test_load_handles_wrong_file_format ( self , yamlMock ): yamlMock . return_value . load . side_effect = ScannerError ( 'error' , '' , 'problem' , 'mark' , ) self . config . load () self . log . error . assert_called_once_with ( 'Error parsing yaml of configuration file mark: problem' ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.open' ) def test_load_handles_file_not_found ( self , openMock ): openMock . side_effect = FileNotFoundError () self . config . load () self . log . error . assert_called_once_with ( 'Error opening configuration file {} ' . format ( self . config_path ) ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.Config.load' ) def test_init_calls_config_load ( self , loadMock ): Config () loadMock . assert_called_once_with () def test_save_config ( self ): tmp = tempfile . mkdtemp () save_file = os . path . join ( tmp , 'yaml_save_test.yaml' ) self . config = Config ( save_file ) self . config . data = { 'a' : 'b' } self . config . save () with open ( save_file , 'r' ) as f : assert \"a:\" in f . read () shutil . rmtree ( tmp )", "title": "Tests"}, {"location": "coding/python/python_config_yaml/#installation", "text": "It's always nice to have the default configuration template (with it's documentation) when configuring your use case. Therefore we're going to add a step in the installation process to copy the file. Variables to substitute in both files: program_name File setup.py import shutil ... Class PostInstallCommand ( install ): ... def run ( self ): install . run ( self ) try : data_directory = os . path . expanduser ( \"~/.local/share/{{ program_name }}\" ) os . makedirs ( data_directory ) log . info ( \"Data directory created\" ) except FileExistsError : log . info ( \"Data directory already exits\" ) config_path = os . path . join ( data_directory , 'config.yaml' ) if os . path . isfile ( config_path ) and os . access ( config_path , os . R_OK ): log . info ( \"Configuration file already exists, check the documentation \" \"for the new version changes.\" ) else : shutil . copyfile ( 'assets/config.yaml' , config_path ) log . info ( \"Copied default configuration template\" ) assets_url : Url pointing to the assets file, similar to https://github.com/lyz-code/pydo/blob/master/assets/config.yaml . README.md ... {{ program_name }} configuration is done through the yaml file located at ~/.local/share/{{ program_name }}/config.yaml . The default template is provided at installation time. ... It's also necessary to add the ruamel.yaml pip package to your setup.py and requirements.txt files.", "title": "Installation"}, {"location": "coding/python/python_project_template/", "text": "It's hard to correctly define the directory structure to make python programs work as expected. Even more if testing, documentation or databases are involved. I've automated the creation of the python project skeleton following most of these section guidelines with this cookiecutter template . cruft https://github.com/lyz-code/cookiecutter-python-project If you don't know cruft, take a look here . Basic Python project \u2691 Create virtualenv mkdir {{ project_directory }} mkvirtualenv --python = python3 -a {{ project_directory }} {{ project_name }} Create git repository workon {{ project_name }} git init . git ignore-io python > .gitignore git add . git commit -m \"Added gitignore\" git checkout -b 'feat/initial_iteration' Project structure \u2691 After using different project structures, I've ended up using the following: . \u251c\u2500\u2500\u2500 docs \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 index.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 mkdocs.yml \u251c\u2500\u2500 mypy.ini \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements-dev.in \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 package_name \u2502 \u251c\u2500\u2500 adapters \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 entrypoints \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 model \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 py.typed \u2502 \u251c\u2500\u2500 services.py \u2502 \u2514\u2500\u2500 version.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 conftest.py \u251c\u2500\u2500 e2e \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 integration \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 unit \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 test_services.py Heavily inspired by ionel packaging python library post and the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory. Project types \u2691 Depending on the type of project you want to build there are different layouts: Command-line program . A single Flask web application . Multiple interconnected Flask microservices . Additional configurations \u2691 Once the basic project structure is defined, there are several common enhancements to be applied: Manage dependencies with pip-tools Create the documentation repository Continuous integration pipelines Configure SQLAlchemy to use the MariaDB/Mysql backend Configure Docker and Docker compose to host the application Load config from YAML Configure a Flask project Code tests \u2691 Unit, integration, end-to-end, edge-to-edge tests define the behaviour of the application. Trigger hooks: Github Actions: To run the tests each time a push or pull request is created in Github, create the .github/workflows/test.yml file with the following Jinja template. Make sure to check: The correct Python versions are configured. The steps make sense to your case scenario. Variables to substitute: program_name : your program name --- name : Python package on : [ push , pull_request ] jobs : build : runs-on : ubuntu-latest strategy : max-parallel : 3 matrix : python-version : [ 3.6 , 3.7 , 3.8 ] steps : - uses : actions/checkout@v1 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v1 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r requirements.txt - name : Lint with flake8 run : | pip install flake8 # stop the build if there are Python syntax errors or undefined names flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics - name : Test with pytest run : | pip install pytest pytest-cov python -m pytest --cov-report term-missing --cov {{ program_name }} tests If you want to add a badge stating the last build status in your readme, use the following template. Variables to substitute: repository_url : Github repository url, like https://github.com/lyz-code/pydo . [ ![Actions Status ]( {{ repository_url }}/workflows/Python%20package/badge.svg )]({{ repository_url }}/actions) References \u2691 ionel packaging a python library post and he's cookiecutter template", "title": "Project Template"}, {"location": "coding/python/python_project_template/#basic-python-project", "text": "Create virtualenv mkdir {{ project_directory }} mkvirtualenv --python = python3 -a {{ project_directory }} {{ project_name }} Create git repository workon {{ project_name }} git init . git ignore-io python > .gitignore git add . git commit -m \"Added gitignore\" git checkout -b 'feat/initial_iteration'", "title": "Basic Python project"}, {"location": "coding/python/python_project_template/#project-structure", "text": "After using different project structures, I've ended up using the following: . \u251c\u2500\u2500\u2500 docs \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 index.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 mkdocs.yml \u251c\u2500\u2500 mypy.ini \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements-dev.in \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 package_name \u2502 \u251c\u2500\u2500 adapters \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 entrypoints \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 model \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 py.typed \u2502 \u251c\u2500\u2500 services.py \u2502 \u2514\u2500\u2500 version.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 conftest.py \u251c\u2500\u2500 e2e \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 integration \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 unit \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 test_services.py Heavily inspired by ionel packaging python library post and the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.", "title": "Project structure"}, {"location": "coding/python/python_project_template/#project-types", "text": "Depending on the type of project you want to build there are different layouts: Command-line program . A single Flask web application . Multiple interconnected Flask microservices .", "title": "Project types"}, {"location": "coding/python/python_project_template/#additional-configurations", "text": "Once the basic project structure is defined, there are several common enhancements to be applied: Manage dependencies with pip-tools Create the documentation repository Continuous integration pipelines Configure SQLAlchemy to use the MariaDB/Mysql backend Configure Docker and Docker compose to host the application Load config from YAML Configure a Flask project", "title": "Additional configurations"}, {"location": "coding/python/python_project_template/#code-tests", "text": "Unit, integration, end-to-end, edge-to-edge tests define the behaviour of the application. Trigger hooks: Github Actions: To run the tests each time a push or pull request is created in Github, create the .github/workflows/test.yml file with the following Jinja template. Make sure to check: The correct Python versions are configured. The steps make sense to your case scenario. Variables to substitute: program_name : your program name --- name : Python package on : [ push , pull_request ] jobs : build : runs-on : ubuntu-latest strategy : max-parallel : 3 matrix : python-version : [ 3.6 , 3.7 , 3.8 ] steps : - uses : actions/checkout@v1 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v1 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r requirements.txt - name : Lint with flake8 run : | pip install flake8 # stop the build if there are Python syntax errors or undefined names flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics - name : Test with pytest run : | pip install pytest pytest-cov python -m pytest --cov-report term-missing --cov {{ program_name }} tests If you want to add a badge stating the last build status in your readme, use the following template. Variables to substitute: repository_url : Github repository url, like https://github.com/lyz-code/pydo . [ ![Actions Status ]( {{ repository_url }}/workflows/Python%20package/badge.svg )]({{ repository_url }}/actions)", "title": "Code tests"}, {"location": "coding/python/python_project_template/#references", "text": "ionel packaging a python library post and he's cookiecutter template", "title": "References"}, {"location": "coding/python/python_snippets/", "text": "Print an exception \u2691 Using the logging module \u2691 Logging an exception can be done with the module-level function logging.exception() like so: import logging try : 1 / 0 except BaseException : logging . exception ( \"An exception was thrown!\" ) ERROR:root:An exception was thrown! Traceback (most recent call last): File \".../Desktop/test.py\", line 4, in <module> 1/0 ZeroDivisionError: division by zero Notes The function logging.exception() should only be called from an exception handler. The logging module should not be used inside a logging handler to avoid a RecursionError . It's also possible to log the exception with another log level but still show the exception details by using the keyword argument exc_info=True , like so: logging . critical ( \"An exception was thrown!\" , exc_info = True ) logging . error ( \"An exception was thrown!\" , exc_info = True ) logging . warning ( \"An exception was thrown!\" , exc_info = True ) logging . info ( \"An exception was thrown!\" , exc_info = True ) logging . debug ( \"An exception was thrown!\" , exc_info = True ) # or the general form logging . log ( level , \"An exception was thrown!\" , exc_info = True ) With the traceback module \u2691 The traceback module provides methods for formatting and printing exceptions and their tracebacks, e.g. this would print exception like the default handler does: import traceback try : 1 / 0 except Exception : traceback . print_exc () Traceback ( most recent call last ): File \"C:\\scripts\\divide_by_zero.py\" , line 4 , in < module > 1 / 0 ZeroDivisionError : division by zero Get common elements of two lists \u2691 >>> a = [ 'a' , 'b' ] >>> b = [ 'c' , 'd' , 'b' ] >>> set ( a ) & set ( b ) { 'b' } Recursively find files \u2691 Using pathlib.Path.rglob \u2691 from pathlib import Path for path in Path ( \"src\" ) . rglob ( \"*.c\" ): print ( path . name ) If you don't want to use pathlib , use can use glob.glob('**/*.c') , but don't forget to pass in the recursive keyword parameter and it will use inordinate amount of time on large directories. os.walk \u2691 For older Python versions, use os.walk to recursively walk a directory and fnmatch.filter to match against a simple expression: import fnmatch import os matches = [] for root , dirnames , filenames in os . walk ( \"src\" ): for filename in fnmatch . filter ( filenames , \"*.c\" ): matches . append ( os . path . join ( root , filename )) Pad a string with spaces \u2691 >>> name = 'John' >>> name . ljust ( 15 ) 'John ' Get hostname of the machine \u2691 Any of the next three options: import os os . uname ()[ 1 ] import platform platform . node () import socket socket . gethostname () Pathlib make parent directories if they don't exist \u2691 pathlib . Path ( \"/tmp/sub1/sub2\" ) . mkdir ( parents = True , exist_ok = True ) From the docs : If parents is true , any missing parents of this path are created as needed; they are created with the default permissions without taking mode into account (mimicking the POSIX mkdir -p command). If parents is false (the default), a missing parent raises FileNotFoundError . If exist_ok is false (the default), FileExistsError is raised if the target directory already exists. If exist_ok is true , FileExistsError exceptions will be ignored (same behavior as the POSIX mkdir -p command), but only if the last path component is not an existing non-directory file. Pathlib touch a file \u2691 Create a file at this given path. pathlib . Path ( \"/tmp/file.txt\" ) . touch ( exist_ok = True ) If the file already exists, the function succeeds if exist_ok is true (and its modification time is updated to the current time), otherwise FileExistsError is raised. If the parent directory doesn't exist you need to create it first. global_conf_path = xdg_home / \"autoimport\" / \"config.toml\" global_conf_path . parent . mkdir ( parents = True ) global_conf_path . touch ( exist_ok = True ) Pad integer with zeros \u2691 >>> length = 1 >>> print ( f 'length = { length : 03 } ' ) length = 001 Print datetime with a defined format \u2691 now = datetime . now () today . strftime ( \"We are the %d , %b %Y\" ) Where the datetime format is a string built from these directives . Print string with asciiart \u2691 pip install pyfiglet from pyfiglet import figlet_format print ( figlet_format ( \"09 : 30\" )) If you want to change the default width of 80 caracteres use: from pyfiglet import Figlet f = Figlet ( font = \"standard\" , width = 100 ) print ( f . renderText ( \"aaaaaaaaaaaaaaaaa\" )) Print specific time format \u2691 datetime . datetime . now () . strftime ( \"%Y-%m- %d T%H:%M:%S\" ) Code Meaning Example %a Weekday as locale\u2019s abbreviated name. Mon %A Weekday as locale\u2019s full name. Monday %w Weekday as a decimal number, where 0 is Sunday and 6 is Saturday. 1 %d Day of the month as a zero-padded decimal number. 30 %-d Day of the month as a decimal number. (Platform specific) 30 %b Month as locale\u2019s abbreviated name. Sep %B Month as locale\u2019s full name. September %m Month as a zero-padded decimal number. 09 %-m Month as a decimal number. (Platform specific) 9 %y Year without century as a zero-padded decimal number. 13 %Y Year with century as a decimal number. 2013 %H Hour (24-hour clock) as a zero-padded decimal number. 07 %-H Hour (24-hour clock) as a decimal number. (Platform specific) 7 %I Hour (12-hour clock) as a zero-padded decimal number. 07 %-I Hour (12-hour clock) as a decimal number. (Platform specific) 7 %p Locale\u2019s equivalent of either AM or PM. AM %M Minute as a zero-padded decimal number. 06 %-M Minute as a decimal number. (Platform specific) 6 %S Second as a zero-padded decimal number. 05 %-S Second as a decimal number. (Platform specific) 5 %f Microsecond as a decimal number, zero-padded on the left. 000000 %z UTC offset in the form +HHMM or -HHMM (empty string if the the object is naive). %Z Time zone name (empty string if the object is naive). %j Day of the year as a zero-padded decimal number. 273 %-j Day of the year as a decimal number. (Platform specific) 273 %U Week number of the year (Sunday as the first day of the week) as a zero padded decimal number. All days in a new year preceding the first Sunday are considered to be in week 0. 39 %W Week number of the year (Monday as the first day of the week) as a decimal number. All days in a new year preceding the first Monday are considered to be in week 0. %c Locale\u2019s appropriate date and time representation. Mon Sep 30 07:06:05 2013 %x Locale\u2019s appropriate date representation. 09/30/13 %X Locale\u2019s appropriate time representation. 07:06:05 %% A literal '%' character. % Get an instance of an Enum by value \u2691 If you want to initialize a pydantic model with an Enum but all you have is the value of the Enum then you need to create a method to get the correct Enum. Otherwise mypy will complain that the type of the assignation is str and not Enum . So if the model is the next one: class ServiceStatus ( BaseModel ): \"\"\"Model the docker status of a service.\"\"\" name : str environment : Environment You can't do ServiceStatus(name='test', environment='production') . you need to add the get_by_value method to the Enum class: class Environment ( str , Enum ): \"\"\"Set the possible environments.\"\"\" STAGING = \"staging\" PRODUCTION = \"production\" @classmethod def get_by_value ( cls , value : str ) -> Enum : \"\"\"Return the Enum element that meets a value\"\"\" return [ member for member in cls if member . value == value ][ 0 ] Now you can do: ServiceStatus ( name = \"test\" , environment = Environment . get_by_value ( \"production\" )) Fix R1728: Consider using a generator \u2691 Removing [] inside calls that can use containers or generators should be considered for performance reasons since a generator will have an upfront cost to pay. The performance will be better if you are working with long lists or sets. Problematic code: list ([ 0 for y in list ( range ( 10 ))]) # [consider-using-generator] tuple ([ 0 for y in list ( range ( 10 ))]) # [consider-using-generator] sum ([ y ** 2 for y in list ( range ( 10 ))]) # [consider-using-generator] max ([ y ** 2 for y in list ( range ( 10 ))]) # [consider-using-generator] min ([ y ** 2 for y in list ( range ( 10 ))]) # [consider-using-generator] Correct code: list ( 0 for y in list ( range ( 10 ))) tuple ( 0 for y in list ( range ( 10 ))) sum ( y ** 2 for y in list ( range ( 10 ))) max ( y ** 2 for y in list ( range ( 10 ))) min ( y ** 2 for y in list ( range ( 10 ))) Fix W1510: Using subprocess.run without explicitly set check is not recommended \u2691 The run call in the example will succeed whether the command is successful or not. This is a problem because we silently ignore errors. import subprocess def example (): proc = subprocess . run ( \"ls\" ) return proc . stdout When we pass check=True , the behavior changes towards raising an exception when the return code of the command is non-zero. Convert bytes to string \u2691 byte_var . decode ( \"utf-8\" ) Use pipes with subprocess \u2691 To use pipes with subprocess you need to use the flag shell=True which is a bad idea . Instead you should use two processes and link them together in python: ps = subprocess . Popen (( \"ps\" , \"-A\" ), stdout = subprocess . PIPE ) output = subprocess . check_output (( \"grep\" , \"process_name\" ), stdin = ps . stdout ) ps . wait () Pass input to the stdin of a subprocess \u2691 import subprocess p = subprocess . run ([ \"myapp\" ], input = \"data_to_write\" , text = True ) Copy and paste from clipboard \u2691 You can use many libraries to do it, but if you don't want to add any other dependencies you can use subprocess run . To copy from the selection clipboard, assuming you've got xclip installed, you could do: subprocess . run ( [ \"xclip\" , \"-selection\" , \"clipboard\" , \"-i\" ], input = \"text to be copied\" , text = True , check = True , ) To paste it: subprocess . check_output ([ \"xclip\" , \"-o\" , \"-selection\" , \"clipboard\" ]) . decode ( \"utf-8\" ) Create random number \u2691 import random a = random . randint ( 1 , 10 ) Check if local port is available or in use \u2691 Create a temporary socket and then try to bind to the port to see if it's available. Close the socket after validating that the port is available. def port_in_use ( port ): \"\"\"Test if a local port is used.\"\"\" sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) with suppress ( OSError ): sock . bind (( \"0.0.0.0\" , port )) return True sock . close () return False Initialize a dataclass with kwargs \u2691 If you care about accessing attributes by name, or if you can't distinguish between known and unknown arguments during initialisation, then your last resort without rewriting __init__ (which pretty much defeats the purpose of using dataclasses in the first place) is writing a @classmethod : from dataclasses import dataclass from inspect import signature @dataclass class Container : user_id : int body : str @classmethod def from_kwargs ( cls , ** kwargs ): # fetch the constructor's signature cls_fields = { field for field in signature ( cls ) . parameters } # split the kwargs into native ones and new ones native_args , new_args = {}, {} for key , value in kwargs . items (): if key in cls_fields : native_args [ key ] = value else : new_args [ key ] = value # use the native ones to create the class ... ret = cls ( ** native_args ) # ... and add the new ones by hand for new_key , new_value in new_args . items (): setattr ( ret , new_key , new_value ) return ret Usage: params = { \"user_id\" : 1 , \"body\" : \"foo\" , \"bar\" : \"baz\" , \"amount\" : 10 } Container ( ** params ) # still doesn't work, raises a TypeError c = Container . from_kwargs ( ** params ) print ( c . bar ) # prints: 'baz' Replace a substring of a string \u2691 txt = \"I like bananas\" x = txt . replace ( \"bananas\" , \"apples\" ) Parse an RFC2822 date \u2691 Interesting to test the accepted format of RSS dates . >>> from email.utils import parsedate_to_datetime >>> datestr = 'Sun, 09 Mar 1997 13:45:00 -0500' >>> parsedate_to_datetime ( datestr ) datetime . datetime ( 1997 , 3 , 9 , 13 , 45 , tzinfo = datetime . timezone ( datetime . timedelta ( - 1 , 68400 ))) Convert a datetime to RFC2822 \u2691 Interesting as it's the accepted format of RSS dates . >>> import datetime >>> from email import utils >>> nowdt = datetime . datetime . now () >>> utils . format_datetime ( nowdt ) 'Tue, 10 Feb 2020 10:06:53 -0000' Encode url \u2691 import urllib.parse from pydantic import AnyHttpUrl def _normalize_url ( url : str ) -> AnyHttpUrl : \"\"\"Encode url to make it compatible with AnyHttpUrl.\"\"\" return typing . cast ( AnyHttpUrl , urllib . parse . quote ( url , \":/\" ), ) The :/ is needed when you try to parse urls that have the protocol, otherwise https://www. gets transformed into https%3A//www. . Fix SIM113 Use enumerate \u2691 Use enumerate to get a running number over an iterable. # Bad idx = 0 for el in iterable : ... idx += 1 # Good for idx , el in enumerate ( iterable ): ... Define a property of a class \u2691 If you're using Python 3.9 or above you can directly use the decorators: class G : @classmethod @property def __doc__ ( cls ): return f \"A doc for { cls . __name__ !r} \" If you're not, you can define the decorator classproperty : # N801: class name 'classproperty' should use CapWords convention, but it's a decorator. # C0103: Class name \"classproperty\" doesn't conform to PascalCase naming style but it's # a decorator. class classproperty : # noqa: N801, C0103 \"\"\"Define a class property. From Python 3.9 you can directly use the decorators directly. class G: @classmethod @property def __doc__(cls): return f'A doc for {cls.__name__!r}' \"\"\" def __init__ ( self , function : Callable [ ... , Any ]) -> None : \"\"\"Initialize the decorator.\"\"\" self . function = function # ANN401: Any not allowed in typings, but I don't know how to narrow the hints in # this case. def __get__ ( self , owner_self : Any , owner_cls : Any ) -> Any : # noqa: ANN401 \"\"\"Return the desired value.\"\"\" return self . function ( owner_self ) But you'll run into the W0143: Comparing against a callable, did you omit the parenthesis? (comparison-with-callable) mypy error when using it to compare the result of the property with anything, as it doesn't detect it's a property instead of a method. How to close a subprocess process \u2691 subprocess . terminate () How to extend a dictionary \u2691 a . update ( b ) How to Find Duplicates in a List in Python \u2691 numbers = [ 1 , 2 , 3 , 2 , 5 , 3 , 3 , 5 , 6 , 3 , 4 , 5 , 7 ] duplicates = [ number for number in numbers if numbers . count ( number ) > 1 ] unique_duplicates = list ( set ( duplicates )) # Returns: [2, 3, 5] If you want to count the number of occurrences of each duplicate, you can use: from collections import Counter numbers = [ 1 , 2 , 3 , 2 , 5 , 3 , 3 , 5 , 6 , 3 , 4 , 5 , 7 ] counts = dict ( Counter ( numbers )) duplicates = { key : value for key , value in counts . items () if value > 1 } # Returns: {2: 2, 3: 4, 5: 3} To remove the duplicates use a combination of list and set : unique = list ( set ( numbers )) # Returns: [1, 2, 3, 4, 5, 6, 7] How to decompress a gz file \u2691 import gzip import shutil with gzip . open ( \"file.txt.gz\" , \"rb\" ) as f_in : with open ( \"file.txt\" , \"wb\" ) as f_out : shutil . copyfileobj ( f_in , f_out ) How to compress/decompress a tar file \u2691 def compress ( tar_file , members ): \"\"\" Adds files (`members`) to a tar_file and compress it \"\"\" tar = tarfile . open ( tar_file , mode = \"w:gz\" ) for member in members : tar . add ( member ) tar . close () def decompress ( tar_file , path , members = None ): \"\"\" Extracts `tar_file` and puts the `members` to `path`. If members is None, all members on `tar_file` will be extracted. \"\"\" tar = tarfile . open ( tar_file , mode = \"r:gz\" ) if members is None : members = tar . getmembers () for member in members : tar . extract ( member , path = path ) tar . close () Parse XML file with beautifulsoup \u2691 You need both beautifulsoup4 and lxml : bs = BeautifulSoup ( requests . get ( url ), \"lxml\" ) Get a traceback from an exception \u2691 import traceback # `e` is an exception object that you get from somewhere traceback_str = \"\" . join ( traceback . format_tb ( e . __traceback__ )) Change the logging level of a library \u2691 For example to change the logging level of the library sh use: sh_logger = logging . getLogger ( \"sh\" ) sh_logger . setLevel ( logging . WARN ) Get all subdirectories of a directory \u2691 [ x [ 0 ] for x in os . walk ( directory )] Move a file \u2691 import os os . rename ( \"path/to/current/file.foo\" , \"path/to/new/destination/for/file.foo\" ) IPv4 regular expression \u2691 regex = re . compile ( r \"(?<![-\\.\\d])(?:0{0,2}?[0-9]\\.|1\\d?\\d?\\.|2[0-5]?[0-5]?\\.) {3} \" r '(?:0{0,2}?[0-9]|1\\d?\\d?|2[0-5]?[0-5]?)(?![\\.\\d])\"^[0-9]{1,3}*$' ) Remove the elements of a list from another \u2691 >>> set ([ 1 , 2 , 6 , 8 ]) - set ([ 2 , 3 , 5 , 8 ]) set ([ 1 , 6 ]) Note, however, that sets do not preserve the order of elements, and cause any duplicated elements to be removed. The elements also need to be hashable. If these restrictions are tolerable, this may often be the simplest and highest performance option. Copy a directory \u2691 import shutil shutil . copytree ( \"bar\" , \"foo\" ) Copy a file \u2691 import shutil shutil . copyfile ( src_file , dest_file ) Capture the stdout of a function \u2691 import io from contextlib import redirect_stdout f = io . StringIO () with redirect_stdout ( f ): do_something ( my_object ) out = f . getvalue () Make temporal directory \u2691 import tempfile dirpath = tempfile . mkdtemp () Change the working directory of a test \u2691 The following function-level fixture will change to the test case directory, run the test ( yield ), then change back to the calling directory to avoid side-effects. @pytest . fixture ( name = \"change_test_dir\" ) def change_test_dir_ ( request : SubRequest ) -> Any : os . chdir ( request . fspath . dirname ) yield os . chdir ( request . config . invocation_dir ) request is a built-in pytest fixture fspath is the LocalPath to the test module being executed dirname is the directory of the test module request.config.invocationdir is the folder from which pytest was executed request.config.rootdir is the pytest root, doesn't change based on where you run pytest. Not used here, but could be useful. Any processes that are kicked off by the test will use the test case folder as their working directory and copy their logs, outputs, etc. there, regardless of where the test suite was executed. Remove a substring from the end of a string \u2691 On Python 3.9 and newer you can use the removeprefix and removesuffix methods to remove an entire substring from either side of the string: url = \"abcdc.com\" url . removesuffix ( \".com\" ) # Returns 'abcdc' url . removeprefix ( \"abcdc.\" ) # Returns 'com' On Python 3.8 and older you can use endswith and slicing: url = \"abcdc.com\" if url . endswith ( \".com\" ): url = url [: - 4 ] Or a regular expression: import re url = \"abcdc.com\" url = re . sub ( \"\\.com$\" , \"\" , url ) Make a flat list of lists with a list comprehension \u2691 There is no nice way to do it :(. The best I've found is: t = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 ], [ 8 , 9 ]] flat_list = [ item for sublist in t for item in sublist ] Replace all characters of a string with another character \u2691 mystring = \"_\" * len ( mystring ) Locate element in list \u2691 a = [ \"a\" , \"b\" ] index = a . index ( \"b\" ) Transpose a list of lists \u2691 >>> l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] >>> [ list ( i ) for i in zip ( * l )] ... [[ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ]] Check the type of a list of strings \u2691 def _is_list_of_lists ( data : Any ) -> bool : \"\"\"Check if data is a list of strings.\"\"\" if data and isinstance ( data , list ): return all ( isinstance ( elem , list ) for elem in data ) else : return False Install default directories and files for a command line program \u2691 I've been trying for a long time to configure setup.py to run the required steps to configure the required directories and files when doing pip install without success. Finally, I decided that the program itself should create the data once the FileNotFoundError exception is found. That way, you don't penalize the load time because if the file or directory exists, that code is not run. Check if a dictionary is a subset of another \u2691 If you have two dictionaries big = {'a': 1, 'b': 2, 'c':3} and small = {'c': 3, 'a': 1} , and want to check whether small is a subset of big , use the next snippet: >>> small . items () <= big . items () True As the code is not very common or intuitive, I'd add a comment to explain what you're doing. When to use isinstance and when to use type \u2691 isinstance takes into account inheritance, while type doesn't. So if we have the next code: class Shape : pass class Rectangle ( Shape ): def __init__ ( self , length , width ): self . length = length self . width = width self . area = length * width def get_area ( self ): return self . length * self . width class Square ( Rectangle ): def __init__ ( self , length ): Rectangle . __init__ ( self , length , length ) And we want to check if an object a = Square(5) is of type Rectangle , we could not use isinstance because it'll return True as it's a subclass of Rectangle : >>> isinstance ( a , Rectangle ) True Instead, use a comparison with type : >>> type ( a ) == Rectangle False Find a static file of a python module \u2691 Useful when you want to initialize a configuration file of a cli program when it's not present. Imagine you have a setup.py with the next contents: setup ( name = \"pynbox\" , packages = find_packages ( \"src\" ), package_dir = { \"\" : \"src\" }, package_data = { \"pynbox\" : [ \"py.typed\" , \"assets/config.yaml\" ]}, Then you could import the data with: import pkg_resources file_path = ( pkg_resources . resource_filename ( \"pynbox\" , \"assets/config.yaml\" ),) Delete a file \u2691 import os os . remove ( \"demofile.txt\" ) Measure elapsed time between lines of code \u2691 import time start = time . time () print ( \"hello\" ) end = time . time () print ( end - start ) Create combination of elements in groups of two \u2691 Using the combinations function in Python's itertools module: >>> list ( itertools . combinations ( 'ABC' , 2 )) [( 'A' , 'B' ), ( 'A' , 'C' ), ( 'B' , 'C' )] If you want the permutations use itertools.permutations . Convert html to readable plaintext \u2691 pip install html2text import html2text html = open ( \"foobar.html\" ) . read () print ( html2text . html2text ( html )) Parse a datetime from a string \u2691 from dateutil import parser parser . parse ( \"Aug 28 1999 12:00AM\" ) # datetime.datetime(1999, 8, 28, 0, 0) If you don't want to use dateutil use datetime datetime . datetime . strptime ( \"2013-W26\" , \"%Y-W%W-%w\" ) Where the datetime format is a string built from the next directives: Directive Meaning Example %a Abbreviated weekday name. Sun, Mon, ... %A Full weekday name. Sunday, Monday, ... %w Weekday as a decimal number. 0, 1, ..., 6 %d Day of the month as a zero-padded decimal. 01, 02, ..., 31 %-d Day of the month as a decimal number. 1, 2, ..., 30 %b Abbreviated month name. Jan, Feb, ..., Dec %B Full month name. January, February, ... %m Month as a zero-padded decimal number. 01, 02, ..., 12 %-m Month as a decimal number. 1, 2, ..., 12 %y Year without century as a zero-padded decimal number. 00, 01, ..., 99 %-y Year without century as a decimal number. 0, 1, ..., 99 %Y Year with century as a decimal number. 2013, 2019 etc. %H Hour (24-hour clock) as a zero-padded decimal number. 00, 01, ..., 23 %-H Hour (24-hour clock) as a decimal number. 0, 1, ..., 23 %I Hour (12-hour clock) as a zero-padded decimal number. 01, 02, ..., 12 %-I Hour (12-hour clock) as a decimal number. 1, 2, ... 12 %p Locale\u2019s AM or PM. AM, PM %M Minute as a zero-padded decimal number. 00, 01, ..., 59 %-M Minute as a decimal number. 0, 1, ..., 59 %S Second as a zero-padded decimal number. 00, 01, ..., 59 %-S Second as a decimal number. 0, 1, ..., 59 %f Microsecond as a decimal number, zero-padded on the left. 000000 - 999999 %z UTC offset in the form +HHMM or -HHMM. %Z Time zone name. %j Day of the year as a zero-padded decimal number. 001, 002, ..., 366 %-j Day of the year as a decimal number. 1, 2, ..., 366 %U Week number of the year (Sunday as the first day of the week). 00, 01, ..., 53 %W Week number of the year (Monday as the first day of the week). 00, 01, ..., 53 %c Locale\u2019s appropriate date and time representation. Mon Sep 30 07:06:05 2013 %x Locale\u2019s appropriate date representation. 09/30/13 %X Locale\u2019s appropriate time representation. 07:06:05 %% A literal '%' character. % Install a python dependency from a git repository \u2691 With pip you can : pip install git+git://github.com/path/to/repository@master If you want to hard code it in your setup.py , you need to: install_requires = [ \"some-pkg @ git+ssh://git@github.com/someorgname/pkg-repo-name@v1.1#egg=some-pkg\" , ] But Pypi won't allow you to upload the package , as it will give you an error: HTTPError: 400 Bad Request from https://test.pypi.org/legacy/ Invalid value for requires_dist. Error: Can't have direct dependency: 'deepdiff @ git+git://github.com/lyz-code/deepdiff@master' It looks like this is a conscious decision on the PyPI side. Basically, they don't want pip to reach out to URLs outside their site when installing from PyPI. An ugly patch is to install the dependencies in a PostInstall custom script in the setup.py of your program: from setuptools.command.install import install from subprocess import getoutput # ignore: cannot subclass install, has type Any. And what would you do? class PostInstall ( install ): # type: ignore \"\"\"Install direct dependency. Pypi doesn't allow uploading packages with direct dependencies, so we need to install them manually. \"\"\" def run ( self ) -> None : \"\"\"Install dependencies.\"\"\" install . run ( self ) print ( getoutput ( \"pip install git+git://github.com/lyz-code/deepdiff@master\" )) setup ( cmdclass = { \"install\" : PostInstall }) Warning: It may not work! Last time I used this solution, when I added the library on a setup.py the direct dependencies weren't installed :S Check or test directories and files \u2691 def test_dir ( directory ): from os.path import exists from os import makedirs if not exists ( directory ): makedirs ( directory ) def test_file ( filepath , mode ): \"\"\"Check if a file exist and is accessible.\"\"\" def check_mode ( os_mode , mode ): if os . path . isfile ( filepath ) and os . access ( filepath , os_mode ): return else : raise IOError ( \"Can't access the file with mode \" + mode ) if mode is \"r\" : check_mode ( os . R_OK , mode ) elif mode is \"w\" : check_mode ( os . W_OK , mode ) elif mode is \"a\" : check_mode ( os . R_OK , mode ) check_mode ( os . W_OK , mode ) Remove the extension of a file \u2691 os . path . splitext ( \"/path/to/some/file.txt\" )[ 0 ] Iterate over the files of a directory \u2691 import os directory = \"/path/to/directory\" for entry in os . scandir ( directory ): if ( entry . path . endswith ( \".jpg\" ) or entry . path . endswith ( \".png\" )) and entry . is_file (): print ( entry . path ) Create directory \u2691 if not os . path . exists ( directory ): os . makedirs ( directory ) Touch a file \u2691 from pathlib import Path Path ( \"path/to/file.txt\" ) . touch () Get the first day of next month \u2691 current = datetime . datetime ( mydate . year , mydate . month , 1 ) next_month = datetime . datetime ( mydate . year + int ( mydate . month / 12 ), (( mydate . month % 12 ) + 1 ), 1 ) Get the week number of a datetime \u2691 datetime.datetime has a isocalendar() method, which returns a tuple containing the calendar week: >>> import datetime >>> datetime . datetime ( 2010 , 6 , 16 ) . isocalendar ()[ 1 ] 24 datetime.date.isocalendar() is an instance-method returning a tuple containing year, weeknumber and weekday in respective order for the given date instance. Get the monday of a week number \u2691 A week number is not enough to generate a date; you need a day of the week as well. Add a default: import datetime d = \"2013-W26\" r = datetime . datetime . strptime ( d + \"-1\" , \"%Y-W%W-%w\" ) The -1 and -%w pattern tells the parser to pick the Monday in that week. Get the month name from a number \u2691 import calendar >> calendar . month_name [ 3 ] 'March' Get ordinal from number \u2691 def int_to_ordinal ( number : int ) -> str : \"\"\"Convert an integer into its ordinal representation. make_ordinal(0) => '0th' make_ordinal(3) => '3rd' make_ordinal(122) => '122nd' make_ordinal(213) => '213th' Args: number: Number to convert Returns: ordinal representation of the number \"\"\" suffix = [ \"th\" , \"st\" , \"nd\" , \"rd\" , \"th\" ][ min ( number % 10 , 4 )] if 11 <= ( number % 100 ) <= 13 : suffix = \"th\" return f \" { number }{ suffix } \" Group or sort a list of dictionaries or objects by a specific key \u2691 Python lists have a built-in list.sort() method that modifies the list in-place. There is also a sorted() built-in function that builds a new sorted list from an iterable. Sorting basics \u2691 A simple ascending sort is very easy: just call the sorted() function. It returns a new sorted list: >>> sorted ([ 5 , 2 , 3 , 1 , 4 ]) [ 1 , 2 , 3 , 4 , 5 ] Key functions \u2691 Both list.sort() and sorted() have a key parameter to specify a function (or other callable) to be called on each list element prior to making comparisons. For example, here\u2019s a case-insensitive string comparison: >>> sorted ( \"This is a test string from Andrew\" . split (), key = str . lower ) [ 'a' , 'Andrew' , 'from' , 'is' , 'string' , 'test' , 'This' ] The value of the key parameter should be a function (or other callable) that takes a single argument and returns a key to use for sorting purposes. This technique is fast because the key function is called exactly once for each input record. A common pattern is to sort complex objects using some of the object\u2019s indices as keys. For example: >>> from operator import itemgetter >>> student_tuples = [ ( 'john' , 'A' , 15 ), ( 'jane' , 'B' , 12 ), ( 'dave' , 'B' , 10 ), ] >>> sorted ( student_tuples , key = itemgetter ( 2 )) # sort by age [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] The same technique works for objects with named attributes. For example: >>> from operator import attrgetter >>> class Student : def __init__ ( self , name , grade , age ): self . name = name self . grade = grade self . age = age def __repr__ ( self ): return repr (( self . name , self . grade , self . age )) >>> student_objects = [ Student ( 'john' , 'A' , 15 ), Student ( 'jane' , 'B' , 12 ), Student ( 'dave' , 'B' , 10 ), ] >>> sorted ( student_objects , key = attrgetter ( 'age' )) # sort by age [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] The operator module functions allow multiple levels of sorting. For example, to sort by grade then by age: >>> sorted ( student_tuples , key = itemgetter ( 1 , 2 )) [( 'john' , 'A' , 15 ), ( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 )] >>> sorted ( student_objects , key = attrgetter ( 'grade' , 'age' )) [( 'john' , 'A' , 15 ), ( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 )] Sorts stability and complex sorts \u2691 Sorts are guaranteed to be stable. That means that when multiple records have the same key, their original order is preserved. >>> data = [( 'red' , 1 ), ( 'blue' , 1 ), ( 'red' , 2 ), ( 'blue' , 2 )] >>> sorted ( data , key = itemgetter ( 0 )) [( 'blue' , 1 ), ( 'blue' , 2 ), ( 'red' , 1 ), ( 'red' , 2 )] Notice how the two records for blue retain their original order so that ('blue', 1) is guaranteed to precede ('blue', 2) . This wonderful property lets you build complex sorts in a series of sorting steps. For example, to sort the student data by descending grade and then ascending age, do the age sort first and then sort again using grade: >>> s = sorted ( student_objects , key = attrgetter ( 'age' )) # sort on secondary key >>> sorted ( s , key = attrgetter ( 'grade' ), reverse = True ) # now sort on primary key, descending [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] This can be abstracted out into a wrapper function that can take a list and tuples of field and order to sort them on multiple passes. >>> def multisort ( xs , specs ): for key , reverse in reversed ( specs ): xs . sort ( key = attrgetter ( key ), reverse = reverse ) return xs >>> multisort ( list ( student_objects ), (( 'grade' , True ), ( 'age' , False ))) [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] Get the attribute of an attribute \u2691 To sort the list in place: ut . sort ( key = lambda x : x . count , reverse = True ) To return a new list, use the sorted() built-in function: newlist = sorted ( ut , key = lambda x : x . body . id_ , reverse = True ) Iterate over an instance object's data attributes in Python \u2691 @dataclass ( frozen = True ) class Search : center : str distance : str se = Search ( \"a\" , \"b\" ) for key , value in se . __dict__ . items (): print ( key , value ) Generate ssh key \u2691 pip install cryptography from os import chmod from cryptography.hazmat.primitives import serialization from cryptography.hazmat.primitives.asymmetric import rsa from cryptography.hazmat.backends import default_backend as crypto_default_backend private_key = rsa . generate_private_key ( backend = crypto_default_backend (), public_exponent = 65537 , key_size = 4096 ) pem = private_key . private_bytes ( encoding = serialization . Encoding . PEM , format = serialization . PrivateFormat . TraditionalOpenSSL , encryption_algorithm = serialization . NoEncryption (), ) with open ( \"/tmp/private.key\" , \"wb\" ) as content_file : chmod ( \"/tmp/private.key\" , 0600 ) content_file . write ( pem ) public_key = ( private_key . public_key () . public_bytes ( encoding = serialization . Encoding . OpenSSH , format = serialization . PublicFormat . OpenSSH , ) + b \" user@email.org\" ) with open ( \"/tmp/public.key\" , \"wb\" ) as content_file : content_file . write ( public_key ) Make multiline code look clean \u2691 If you need variables that contain multiline strings inside functions or methods you need to remove the indentation def test (): # end first line with \\ to avoid the empty line! s = \"\"\" \\ hello world \"\"\" Which is inconvenient as it breaks some editor source code folding and it's ugly for the eye. The solution is to use textwrap.dedent() import textwrap def test (): # end first line with \\ to avoid the empty line! s = \"\"\" \\ hello world \"\"\" print ( repr ( s )) # prints ' hello\\n world\\n ' print ( repr ( textwrap . dedent ( s ))) # prints 'hello\\n world\\n' If you forget to add the trailing \\ character of s = '''\\ or use s = '''hello , you're going to have a bad time with black . Play a sound \u2691 pip install playsound from playsound import playsound playsound ( \"path/to/file.wav\" ) Deep copy a dictionary \u2691 import copy d = { ... } d2 = copy . deepcopy ( d ) Find the root directory of a package \u2691 pyprojroot finds the root working directory for your project as a pathlib object. You can now use the here function to pass in a relative path from the project root directory (no matter what working directory you are in the project), and you will get a full path to the specified file. Installation \u2691 pip install pyprojroot Usage \u2691 from pyprojroot import here here () Check if an object has an attribute \u2691 if hasattr ( a , \"property\" ): a . property Check if a loop ends completely \u2691 for loops can take an else block which is not run if the loop has ended with a break statement. for i in [ 1 , 2 , 3 ]: print ( i ) if i == 3 : break else : print ( \"for loop was not broken\" ) Merge two lists \u2691 z = x + y Merge two dictionaries \u2691 z = { ** x , ** y } Create user defined exceptions \u2691 Programs may name their own exceptions by creating a new exception class. Exceptions should typically be derived from the Exception class, either directly or indirectly. Exception classes are meant to be kept simple, only offering a number of attributes that allow information about the error to be extracted by handlers for the exception. When creating a module that can raise several distinct errors, a common practice is to create a base class for exceptions defined by that module, and subclass that to create specific exception classes for different error conditions: class Error ( Exception ): \"\"\"Base class for exceptions in this module.\"\"\" class ConceptNotFoundError ( Error ): \"\"\"Transactions with unmatched concept.\"\"\" def __init__ ( self , message : str , transactions : List [ Transaction ]) -> None : \"\"\"Initialize the exception.\"\"\" self . message = message self . transactions = transactions super () . __init__ ( self . message ) Most exceptions are defined with names that end in \u201cError\u201d, similar to the naming of the standard exceptions. Import a module or it's objects from within a python program \u2691 import importlib module = importlib . import_module ( \"os\" ) module_class = module . getcwd relative_module = importlib . import_module ( \".model\" , package = \"mypackage\" ) class_to_extract = \"MyModel\" extracted_class = geattr ( relative_module , class_to_extract ) The first argument specifies what module to import in absolute or relative terms (e.g. either pkg.mod or ..mod ). If the name is specified in relative terms, then the package argument must be set to the name of the package which is to act as the anchor for resolving the package name (e.g. import_module('..mod', 'pkg.subpkg') will import pkg.mod ). Get system's timezone and use it in datetime \u2691 To obtain timezone information in the form of a datetime.tzinfo object, use dateutil.tz.tzlocal() : from dateutil import tz myTimeZone = tz . tzlocal () This object can be used in the tz parameter of datetime.datetime.now() : from datetime import datetime from dateutil import tz localisedDatetime = datetime . now ( tz = tz . tzlocal ()) Capitalize a sentence \u2691 To change the caps of the first letter of the first word of a sentence use: >> sentence = \"add funny Emojis\" >> sentence [ 0 ] . upper () + sentence [ 1 :] Add funny Emojis The .capitalize method transforms the rest of words to lowercase. The .title transforms all sentence words to capitalize. Get the last monday datetime \u2691 import datetime today = datetime . date . today () last_monday = today - datetime . timedelta ( days = today . weekday ()) Issues \u2691 Pypi won't allow you to upload packages with direct dependencies : update the section above.", "title": "Python Snippets"}, {"location": "coding/python/python_snippets/#print-an-exception", "text": "", "title": "Print an exception"}, {"location": "coding/python/python_snippets/#using-the-logging-module", "text": "Logging an exception can be done with the module-level function logging.exception() like so: import logging try : 1 / 0 except BaseException : logging . exception ( \"An exception was thrown!\" ) ERROR:root:An exception was thrown! Traceback (most recent call last): File \".../Desktop/test.py\", line 4, in <module> 1/0 ZeroDivisionError: division by zero Notes The function logging.exception() should only be called from an exception handler. The logging module should not be used inside a logging handler to avoid a RecursionError . It's also possible to log the exception with another log level but still show the exception details by using the keyword argument exc_info=True , like so: logging . critical ( \"An exception was thrown!\" , exc_info = True ) logging . error ( \"An exception was thrown!\" , exc_info = True ) logging . warning ( \"An exception was thrown!\" , exc_info = True ) logging . info ( \"An exception was thrown!\" , exc_info = True ) logging . debug ( \"An exception was thrown!\" , exc_info = True ) # or the general form logging . log ( level , \"An exception was thrown!\" , exc_info = True )", "title": "Using the logging module"}, {"location": "coding/python/python_snippets/#with-the-traceback-module", "text": "The traceback module provides methods for formatting and printing exceptions and their tracebacks, e.g. this would print exception like the default handler does: import traceback try : 1 / 0 except Exception : traceback . print_exc () Traceback ( most recent call last ): File \"C:\\scripts\\divide_by_zero.py\" , line 4 , in < module > 1 / 0 ZeroDivisionError : division by zero", "title": "With the traceback module"}, {"location": "coding/python/python_snippets/#get-common-elements-of-two-lists", "text": ">>> a = [ 'a' , 'b' ] >>> b = [ 'c' , 'd' , 'b' ] >>> set ( a ) & set ( b ) { 'b' }", "title": "Get common elements of two lists"}, {"location": "coding/python/python_snippets/#recursively-find-files", "text": "", "title": "Recursively find files"}, {"location": "coding/python/python_snippets/#using-pathlibpathrglob", "text": "from pathlib import Path for path in Path ( \"src\" ) . rglob ( \"*.c\" ): print ( path . name ) If you don't want to use pathlib , use can use glob.glob('**/*.c') , but don't forget to pass in the recursive keyword parameter and it will use inordinate amount of time on large directories.", "title": "Using pathlib.Path.rglob"}, {"location": "coding/python/python_snippets/#oswalk", "text": "For older Python versions, use os.walk to recursively walk a directory and fnmatch.filter to match against a simple expression: import fnmatch import os matches = [] for root , dirnames , filenames in os . walk ( \"src\" ): for filename in fnmatch . filter ( filenames , \"*.c\" ): matches . append ( os . path . join ( root , filename ))", "title": "os.walk"}, {"location": "coding/python/python_snippets/#pad-a-string-with-spaces", "text": ">>> name = 'John' >>> name . ljust ( 15 ) 'John '", "title": "Pad a string with spaces"}, {"location": "coding/python/python_snippets/#get-hostname-of-the-machine", "text": "Any of the next three options: import os os . uname ()[ 1 ] import platform platform . node () import socket socket . gethostname ()", "title": "Get hostname of the machine"}, {"location": "coding/python/python_snippets/#pathlib-make-parent-directories-if-they-dont-exist", "text": "pathlib . Path ( \"/tmp/sub1/sub2\" ) . mkdir ( parents = True , exist_ok = True ) From the docs : If parents is true , any missing parents of this path are created as needed; they are created with the default permissions without taking mode into account (mimicking the POSIX mkdir -p command). If parents is false (the default), a missing parent raises FileNotFoundError . If exist_ok is false (the default), FileExistsError is raised if the target directory already exists. If exist_ok is true , FileExistsError exceptions will be ignored (same behavior as the POSIX mkdir -p command), but only if the last path component is not an existing non-directory file.", "title": "Pathlib make parent directories if they don't exist"}, {"location": "coding/python/python_snippets/#pathlib-touch-a-file", "text": "Create a file at this given path. pathlib . Path ( \"/tmp/file.txt\" ) . touch ( exist_ok = True ) If the file already exists, the function succeeds if exist_ok is true (and its modification time is updated to the current time), otherwise FileExistsError is raised. If the parent directory doesn't exist you need to create it first. global_conf_path = xdg_home / \"autoimport\" / \"config.toml\" global_conf_path . parent . mkdir ( parents = True ) global_conf_path . touch ( exist_ok = True )", "title": "Pathlib touch a file"}, {"location": "coding/python/python_snippets/#pad-integer-with-zeros", "text": ">>> length = 1 >>> print ( f 'length = { length : 03 } ' ) length = 001", "title": "Pad integer with zeros"}, {"location": "coding/python/python_snippets/#print-datetime-with-a-defined-format", "text": "now = datetime . now () today . strftime ( \"We are the %d , %b %Y\" ) Where the datetime format is a string built from these directives .", "title": "Print datetime with a defined format"}, {"location": "coding/python/python_snippets/#print-string-with-asciiart", "text": "pip install pyfiglet from pyfiglet import figlet_format print ( figlet_format ( \"09 : 30\" )) If you want to change the default width of 80 caracteres use: from pyfiglet import Figlet f = Figlet ( font = \"standard\" , width = 100 ) print ( f . renderText ( \"aaaaaaaaaaaaaaaaa\" ))", "title": "Print string with asciiart"}, {"location": "coding/python/python_snippets/#print-specific-time-format", "text": "datetime . datetime . now () . strftime ( \"%Y-%m- %d T%H:%M:%S\" ) Code Meaning Example %a Weekday as locale\u2019s abbreviated name. Mon %A Weekday as locale\u2019s full name. Monday %w Weekday as a decimal number, where 0 is Sunday and 6 is Saturday. 1 %d Day of the month as a zero-padded decimal number. 30 %-d Day of the month as a decimal number. (Platform specific) 30 %b Month as locale\u2019s abbreviated name. Sep %B Month as locale\u2019s full name. September %m Month as a zero-padded decimal number. 09 %-m Month as a decimal number. (Platform specific) 9 %y Year without century as a zero-padded decimal number. 13 %Y Year with century as a decimal number. 2013 %H Hour (24-hour clock) as a zero-padded decimal number. 07 %-H Hour (24-hour clock) as a decimal number. (Platform specific) 7 %I Hour (12-hour clock) as a zero-padded decimal number. 07 %-I Hour (12-hour clock) as a decimal number. (Platform specific) 7 %p Locale\u2019s equivalent of either AM or PM. AM %M Minute as a zero-padded decimal number. 06 %-M Minute as a decimal number. (Platform specific) 6 %S Second as a zero-padded decimal number. 05 %-S Second as a decimal number. (Platform specific) 5 %f Microsecond as a decimal number, zero-padded on the left. 000000 %z UTC offset in the form +HHMM or -HHMM (empty string if the the object is naive). %Z Time zone name (empty string if the object is naive). %j Day of the year as a zero-padded decimal number. 273 %-j Day of the year as a decimal number. (Platform specific) 273 %U Week number of the year (Sunday as the first day of the week) as a zero padded decimal number. All days in a new year preceding the first Sunday are considered to be in week 0. 39 %W Week number of the year (Monday as the first day of the week) as a decimal number. All days in a new year preceding the first Monday are considered to be in week 0. %c Locale\u2019s appropriate date and time representation. Mon Sep 30 07:06:05 2013 %x Locale\u2019s appropriate date representation. 09/30/13 %X Locale\u2019s appropriate time representation. 07:06:05 %% A literal '%' character. %", "title": "Print specific time format"}, {"location": "coding/python/python_snippets/#get-an-instance-of-an-enum-by-value", "text": "If you want to initialize a pydantic model with an Enum but all you have is the value of the Enum then you need to create a method to get the correct Enum. Otherwise mypy will complain that the type of the assignation is str and not Enum . So if the model is the next one: class ServiceStatus ( BaseModel ): \"\"\"Model the docker status of a service.\"\"\" name : str environment : Environment You can't do ServiceStatus(name='test', environment='production') . you need to add the get_by_value method to the Enum class: class Environment ( str , Enum ): \"\"\"Set the possible environments.\"\"\" STAGING = \"staging\" PRODUCTION = \"production\" @classmethod def get_by_value ( cls , value : str ) -> Enum : \"\"\"Return the Enum element that meets a value\"\"\" return [ member for member in cls if member . value == value ][ 0 ] Now you can do: ServiceStatus ( name = \"test\" , environment = Environment . get_by_value ( \"production\" ))", "title": "Get an instance of an Enum by value"}, {"location": "coding/python/python_snippets/#fix-r1728-consider-using-a-generator", "text": "Removing [] inside calls that can use containers or generators should be considered for performance reasons since a generator will have an upfront cost to pay. The performance will be better if you are working with long lists or sets. Problematic code: list ([ 0 for y in list ( range ( 10 ))]) # [consider-using-generator] tuple ([ 0 for y in list ( range ( 10 ))]) # [consider-using-generator] sum ([ y ** 2 for y in list ( range ( 10 ))]) # [consider-using-generator] max ([ y ** 2 for y in list ( range ( 10 ))]) # [consider-using-generator] min ([ y ** 2 for y in list ( range ( 10 ))]) # [consider-using-generator] Correct code: list ( 0 for y in list ( range ( 10 ))) tuple ( 0 for y in list ( range ( 10 ))) sum ( y ** 2 for y in list ( range ( 10 ))) max ( y ** 2 for y in list ( range ( 10 ))) min ( y ** 2 for y in list ( range ( 10 )))", "title": "Fix R1728: Consider using a generator"}, {"location": "coding/python/python_snippets/#fix-w1510-using-subprocessrun-without-explicitly-set-check-is-not-recommended", "text": "The run call in the example will succeed whether the command is successful or not. This is a problem because we silently ignore errors. import subprocess def example (): proc = subprocess . run ( \"ls\" ) return proc . stdout When we pass check=True , the behavior changes towards raising an exception when the return code of the command is non-zero.", "title": "Fix W1510: Using subprocess.run without explicitly set check is not recommended"}, {"location": "coding/python/python_snippets/#convert-bytes-to-string", "text": "byte_var . decode ( \"utf-8\" )", "title": "Convert bytes to string"}, {"location": "coding/python/python_snippets/#use-pipes-with-subprocess", "text": "To use pipes with subprocess you need to use the flag shell=True which is a bad idea . Instead you should use two processes and link them together in python: ps = subprocess . Popen (( \"ps\" , \"-A\" ), stdout = subprocess . PIPE ) output = subprocess . check_output (( \"grep\" , \"process_name\" ), stdin = ps . stdout ) ps . wait ()", "title": "Use pipes with subprocess"}, {"location": "coding/python/python_snippets/#pass-input-to-the-stdin-of-a-subprocess", "text": "import subprocess p = subprocess . run ([ \"myapp\" ], input = \"data_to_write\" , text = True )", "title": "Pass input to the stdin of a subprocess"}, {"location": "coding/python/python_snippets/#copy-and-paste-from-clipboard", "text": "You can use many libraries to do it, but if you don't want to add any other dependencies you can use subprocess run . To copy from the selection clipboard, assuming you've got xclip installed, you could do: subprocess . run ( [ \"xclip\" , \"-selection\" , \"clipboard\" , \"-i\" ], input = \"text to be copied\" , text = True , check = True , ) To paste it: subprocess . check_output ([ \"xclip\" , \"-o\" , \"-selection\" , \"clipboard\" ]) . decode ( \"utf-8\" )", "title": "Copy and paste from clipboard"}, {"location": "coding/python/python_snippets/#create-random-number", "text": "import random a = random . randint ( 1 , 10 )", "title": "Create random number"}, {"location": "coding/python/python_snippets/#check-if-local-port-is-available-or-in-use", "text": "Create a temporary socket and then try to bind to the port to see if it's available. Close the socket after validating that the port is available. def port_in_use ( port ): \"\"\"Test if a local port is used.\"\"\" sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) with suppress ( OSError ): sock . bind (( \"0.0.0.0\" , port )) return True sock . close () return False", "title": "Check if local port is available or in use"}, {"location": "coding/python/python_snippets/#initialize-a-dataclass-with-kwargs", "text": "If you care about accessing attributes by name, or if you can't distinguish between known and unknown arguments during initialisation, then your last resort without rewriting __init__ (which pretty much defeats the purpose of using dataclasses in the first place) is writing a @classmethod : from dataclasses import dataclass from inspect import signature @dataclass class Container : user_id : int body : str @classmethod def from_kwargs ( cls , ** kwargs ): # fetch the constructor's signature cls_fields = { field for field in signature ( cls ) . parameters } # split the kwargs into native ones and new ones native_args , new_args = {}, {} for key , value in kwargs . items (): if key in cls_fields : native_args [ key ] = value else : new_args [ key ] = value # use the native ones to create the class ... ret = cls ( ** native_args ) # ... and add the new ones by hand for new_key , new_value in new_args . items (): setattr ( ret , new_key , new_value ) return ret Usage: params = { \"user_id\" : 1 , \"body\" : \"foo\" , \"bar\" : \"baz\" , \"amount\" : 10 } Container ( ** params ) # still doesn't work, raises a TypeError c = Container . from_kwargs ( ** params ) print ( c . bar ) # prints: 'baz'", "title": "Initialize a dataclass with kwargs"}, {"location": "coding/python/python_snippets/#replace-a-substring-of-a-string", "text": "txt = \"I like bananas\" x = txt . replace ( \"bananas\" , \"apples\" )", "title": "Replace a substring of a string"}, {"location": "coding/python/python_snippets/#parse-an-rfc2822-date", "text": "Interesting to test the accepted format of RSS dates . >>> from email.utils import parsedate_to_datetime >>> datestr = 'Sun, 09 Mar 1997 13:45:00 -0500' >>> parsedate_to_datetime ( datestr ) datetime . datetime ( 1997 , 3 , 9 , 13 , 45 , tzinfo = datetime . timezone ( datetime . timedelta ( - 1 , 68400 )))", "title": "Parse an RFC2822 date"}, {"location": "coding/python/python_snippets/#convert-a-datetime-to-rfc2822", "text": "Interesting as it's the accepted format of RSS dates . >>> import datetime >>> from email import utils >>> nowdt = datetime . datetime . now () >>> utils . format_datetime ( nowdt ) 'Tue, 10 Feb 2020 10:06:53 -0000'", "title": "Convert a datetime to RFC2822"}, {"location": "coding/python/python_snippets/#encode-url", "text": "import urllib.parse from pydantic import AnyHttpUrl def _normalize_url ( url : str ) -> AnyHttpUrl : \"\"\"Encode url to make it compatible with AnyHttpUrl.\"\"\" return typing . cast ( AnyHttpUrl , urllib . parse . quote ( url , \":/\" ), ) The :/ is needed when you try to parse urls that have the protocol, otherwise https://www. gets transformed into https%3A//www. .", "title": "Encode url"}, {"location": "coding/python/python_snippets/#fix-sim113-use-enumerate", "text": "Use enumerate to get a running number over an iterable. # Bad idx = 0 for el in iterable : ... idx += 1 # Good for idx , el in enumerate ( iterable ): ...", "title": "Fix SIM113 Use enumerate"}, {"location": "coding/python/python_snippets/#define-a-property-of-a-class", "text": "If you're using Python 3.9 or above you can directly use the decorators: class G : @classmethod @property def __doc__ ( cls ): return f \"A doc for { cls . __name__ !r} \" If you're not, you can define the decorator classproperty : # N801: class name 'classproperty' should use CapWords convention, but it's a decorator. # C0103: Class name \"classproperty\" doesn't conform to PascalCase naming style but it's # a decorator. class classproperty : # noqa: N801, C0103 \"\"\"Define a class property. From Python 3.9 you can directly use the decorators directly. class G: @classmethod @property def __doc__(cls): return f'A doc for {cls.__name__!r}' \"\"\" def __init__ ( self , function : Callable [ ... , Any ]) -> None : \"\"\"Initialize the decorator.\"\"\" self . function = function # ANN401: Any not allowed in typings, but I don't know how to narrow the hints in # this case. def __get__ ( self , owner_self : Any , owner_cls : Any ) -> Any : # noqa: ANN401 \"\"\"Return the desired value.\"\"\" return self . function ( owner_self ) But you'll run into the W0143: Comparing against a callable, did you omit the parenthesis? (comparison-with-callable) mypy error when using it to compare the result of the property with anything, as it doesn't detect it's a property instead of a method.", "title": "Define a property of a class"}, {"location": "coding/python/python_snippets/#how-to-close-a-subprocess-process", "text": "subprocess . terminate ()", "title": "How to close a subprocess process"}, {"location": "coding/python/python_snippets/#how-to-extend-a-dictionary", "text": "a . update ( b )", "title": "How to extend a dictionary"}, {"location": "coding/python/python_snippets/#how-to-find-duplicates-in-a-list-in-python", "text": "numbers = [ 1 , 2 , 3 , 2 , 5 , 3 , 3 , 5 , 6 , 3 , 4 , 5 , 7 ] duplicates = [ number for number in numbers if numbers . count ( number ) > 1 ] unique_duplicates = list ( set ( duplicates )) # Returns: [2, 3, 5] If you want to count the number of occurrences of each duplicate, you can use: from collections import Counter numbers = [ 1 , 2 , 3 , 2 , 5 , 3 , 3 , 5 , 6 , 3 , 4 , 5 , 7 ] counts = dict ( Counter ( numbers )) duplicates = { key : value for key , value in counts . items () if value > 1 } # Returns: {2: 2, 3: 4, 5: 3} To remove the duplicates use a combination of list and set : unique = list ( set ( numbers )) # Returns: [1, 2, 3, 4, 5, 6, 7]", "title": "How to Find Duplicates in a List in Python"}, {"location": "coding/python/python_snippets/#how-to-decompress-a-gz-file", "text": "import gzip import shutil with gzip . open ( \"file.txt.gz\" , \"rb\" ) as f_in : with open ( \"file.txt\" , \"wb\" ) as f_out : shutil . copyfileobj ( f_in , f_out )", "title": "How to decompress a gz file"}, {"location": "coding/python/python_snippets/#how-to-compressdecompress-a-tar-file", "text": "def compress ( tar_file , members ): \"\"\" Adds files (`members`) to a tar_file and compress it \"\"\" tar = tarfile . open ( tar_file , mode = \"w:gz\" ) for member in members : tar . add ( member ) tar . close () def decompress ( tar_file , path , members = None ): \"\"\" Extracts `tar_file` and puts the `members` to `path`. If members is None, all members on `tar_file` will be extracted. \"\"\" tar = tarfile . open ( tar_file , mode = \"r:gz\" ) if members is None : members = tar . getmembers () for member in members : tar . extract ( member , path = path ) tar . close ()", "title": "How to compress/decompress a tar file"}, {"location": "coding/python/python_snippets/#parse-xml-file-with-beautifulsoup", "text": "You need both beautifulsoup4 and lxml : bs = BeautifulSoup ( requests . get ( url ), \"lxml\" )", "title": "Parse XML file with beautifulsoup"}, {"location": "coding/python/python_snippets/#get-a-traceback-from-an-exception", "text": "import traceback # `e` is an exception object that you get from somewhere traceback_str = \"\" . join ( traceback . format_tb ( e . __traceback__ ))", "title": "Get a traceback from an exception"}, {"location": "coding/python/python_snippets/#change-the-logging-level-of-a-library", "text": "For example to change the logging level of the library sh use: sh_logger = logging . getLogger ( \"sh\" ) sh_logger . setLevel ( logging . WARN )", "title": "Change the logging level of a library"}, {"location": "coding/python/python_snippets/#get-all-subdirectories-of-a-directory", "text": "[ x [ 0 ] for x in os . walk ( directory )]", "title": "Get all subdirectories of a directory"}, {"location": "coding/python/python_snippets/#move-a-file", "text": "import os os . rename ( \"path/to/current/file.foo\" , \"path/to/new/destination/for/file.foo\" )", "title": "Move a file"}, {"location": "coding/python/python_snippets/#ipv4-regular-expression", "text": "regex = re . compile ( r \"(?<![-\\.\\d])(?:0{0,2}?[0-9]\\.|1\\d?\\d?\\.|2[0-5]?[0-5]?\\.) {3} \" r '(?:0{0,2}?[0-9]|1\\d?\\d?|2[0-5]?[0-5]?)(?![\\.\\d])\"^[0-9]{1,3}*$' )", "title": "IPv4 regular expression"}, {"location": "coding/python/python_snippets/#remove-the-elements-of-a-list-from-another", "text": ">>> set ([ 1 , 2 , 6 , 8 ]) - set ([ 2 , 3 , 5 , 8 ]) set ([ 1 , 6 ]) Note, however, that sets do not preserve the order of elements, and cause any duplicated elements to be removed. The elements also need to be hashable. If these restrictions are tolerable, this may often be the simplest and highest performance option.", "title": "Remove the elements of a list from another"}, {"location": "coding/python/python_snippets/#copy-a-directory", "text": "import shutil shutil . copytree ( \"bar\" , \"foo\" )", "title": "Copy a directory"}, {"location": "coding/python/python_snippets/#copy-a-file", "text": "import shutil shutil . copyfile ( src_file , dest_file )", "title": "Copy a file"}, {"location": "coding/python/python_snippets/#capture-the-stdout-of-a-function", "text": "import io from contextlib import redirect_stdout f = io . StringIO () with redirect_stdout ( f ): do_something ( my_object ) out = f . getvalue ()", "title": "Capture the stdout of a function"}, {"location": "coding/python/python_snippets/#make-temporal-directory", "text": "import tempfile dirpath = tempfile . mkdtemp ()", "title": "Make temporal directory"}, {"location": "coding/python/python_snippets/#change-the-working-directory-of-a-test", "text": "The following function-level fixture will change to the test case directory, run the test ( yield ), then change back to the calling directory to avoid side-effects. @pytest . fixture ( name = \"change_test_dir\" ) def change_test_dir_ ( request : SubRequest ) -> Any : os . chdir ( request . fspath . dirname ) yield os . chdir ( request . config . invocation_dir ) request is a built-in pytest fixture fspath is the LocalPath to the test module being executed dirname is the directory of the test module request.config.invocationdir is the folder from which pytest was executed request.config.rootdir is the pytest root, doesn't change based on where you run pytest. Not used here, but could be useful. Any processes that are kicked off by the test will use the test case folder as their working directory and copy their logs, outputs, etc. there, regardless of where the test suite was executed.", "title": "Change the working directory of a test"}, {"location": "coding/python/python_snippets/#remove-a-substring-from-the-end-of-a-string", "text": "On Python 3.9 and newer you can use the removeprefix and removesuffix methods to remove an entire substring from either side of the string: url = \"abcdc.com\" url . removesuffix ( \".com\" ) # Returns 'abcdc' url . removeprefix ( \"abcdc.\" ) # Returns 'com' On Python 3.8 and older you can use endswith and slicing: url = \"abcdc.com\" if url . endswith ( \".com\" ): url = url [: - 4 ] Or a regular expression: import re url = \"abcdc.com\" url = re . sub ( \"\\.com$\" , \"\" , url )", "title": "Remove a substring from the end of a string"}, {"location": "coding/python/python_snippets/#make-a-flat-list-of-lists-with-a-list-comprehension", "text": "There is no nice way to do it :(. The best I've found is: t = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 ], [ 8 , 9 ]] flat_list = [ item for sublist in t for item in sublist ]", "title": "Make a flat list of lists with a list comprehension"}, {"location": "coding/python/python_snippets/#replace-all-characters-of-a-string-with-another-character", "text": "mystring = \"_\" * len ( mystring )", "title": "Replace all characters of a string with another character"}, {"location": "coding/python/python_snippets/#locate-element-in-list", "text": "a = [ \"a\" , \"b\" ] index = a . index ( \"b\" )", "title": "Locate element in list"}, {"location": "coding/python/python_snippets/#transpose-a-list-of-lists", "text": ">>> l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] >>> [ list ( i ) for i in zip ( * l )] ... [[ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ]]", "title": "Transpose a list of lists"}, {"location": "coding/python/python_snippets/#check-the-type-of-a-list-of-strings", "text": "def _is_list_of_lists ( data : Any ) -> bool : \"\"\"Check if data is a list of strings.\"\"\" if data and isinstance ( data , list ): return all ( isinstance ( elem , list ) for elem in data ) else : return False", "title": "Check the type of a list of strings"}, {"location": "coding/python/python_snippets/#install-default-directories-and-files-for-a-command-line-program", "text": "I've been trying for a long time to configure setup.py to run the required steps to configure the required directories and files when doing pip install without success. Finally, I decided that the program itself should create the data once the FileNotFoundError exception is found. That way, you don't penalize the load time because if the file or directory exists, that code is not run.", "title": "Install default directories and files for a command line program"}, {"location": "coding/python/python_snippets/#check-if-a-dictionary-is-a-subset-of-another", "text": "If you have two dictionaries big = {'a': 1, 'b': 2, 'c':3} and small = {'c': 3, 'a': 1} , and want to check whether small is a subset of big , use the next snippet: >>> small . items () <= big . items () True As the code is not very common or intuitive, I'd add a comment to explain what you're doing.", "title": "Check if a dictionary is a subset of another"}, {"location": "coding/python/python_snippets/#when-to-use-isinstance-and-when-to-use-type", "text": "isinstance takes into account inheritance, while type doesn't. So if we have the next code: class Shape : pass class Rectangle ( Shape ): def __init__ ( self , length , width ): self . length = length self . width = width self . area = length * width def get_area ( self ): return self . length * self . width class Square ( Rectangle ): def __init__ ( self , length ): Rectangle . __init__ ( self , length , length ) And we want to check if an object a = Square(5) is of type Rectangle , we could not use isinstance because it'll return True as it's a subclass of Rectangle : >>> isinstance ( a , Rectangle ) True Instead, use a comparison with type : >>> type ( a ) == Rectangle False", "title": "When to use isinstance and when to use type"}, {"location": "coding/python/python_snippets/#find-a-static-file-of-a-python-module", "text": "Useful when you want to initialize a configuration file of a cli program when it's not present. Imagine you have a setup.py with the next contents: setup ( name = \"pynbox\" , packages = find_packages ( \"src\" ), package_dir = { \"\" : \"src\" }, package_data = { \"pynbox\" : [ \"py.typed\" , \"assets/config.yaml\" ]}, Then you could import the data with: import pkg_resources file_path = ( pkg_resources . resource_filename ( \"pynbox\" , \"assets/config.yaml\" ),)", "title": "Find a static file of a python module"}, {"location": "coding/python/python_snippets/#delete-a-file", "text": "import os os . remove ( \"demofile.txt\" )", "title": "Delete a file"}, {"location": "coding/python/python_snippets/#measure-elapsed-time-between-lines-of-code", "text": "import time start = time . time () print ( \"hello\" ) end = time . time () print ( end - start )", "title": "Measure elapsed time between lines of code"}, {"location": "coding/python/python_snippets/#create-combination-of-elements-in-groups-of-two", "text": "Using the combinations function in Python's itertools module: >>> list ( itertools . combinations ( 'ABC' , 2 )) [( 'A' , 'B' ), ( 'A' , 'C' ), ( 'B' , 'C' )] If you want the permutations use itertools.permutations .", "title": "Create combination of elements in groups of two"}, {"location": "coding/python/python_snippets/#convert-html-to-readable-plaintext", "text": "pip install html2text import html2text html = open ( \"foobar.html\" ) . read () print ( html2text . html2text ( html ))", "title": "Convert html to readable plaintext"}, {"location": "coding/python/python_snippets/#parse-a-datetime-from-a-string", "text": "from dateutil import parser parser . parse ( \"Aug 28 1999 12:00AM\" ) # datetime.datetime(1999, 8, 28, 0, 0) If you don't want to use dateutil use datetime datetime . datetime . strptime ( \"2013-W26\" , \"%Y-W%W-%w\" ) Where the datetime format is a string built from the next directives: Directive Meaning Example %a Abbreviated weekday name. Sun, Mon, ... %A Full weekday name. Sunday, Monday, ... %w Weekday as a decimal number. 0, 1, ..., 6 %d Day of the month as a zero-padded decimal. 01, 02, ..., 31 %-d Day of the month as a decimal number. 1, 2, ..., 30 %b Abbreviated month name. Jan, Feb, ..., Dec %B Full month name. January, February, ... %m Month as a zero-padded decimal number. 01, 02, ..., 12 %-m Month as a decimal number. 1, 2, ..., 12 %y Year without century as a zero-padded decimal number. 00, 01, ..., 99 %-y Year without century as a decimal number. 0, 1, ..., 99 %Y Year with century as a decimal number. 2013, 2019 etc. %H Hour (24-hour clock) as a zero-padded decimal number. 00, 01, ..., 23 %-H Hour (24-hour clock) as a decimal number. 0, 1, ..., 23 %I Hour (12-hour clock) as a zero-padded decimal number. 01, 02, ..., 12 %-I Hour (12-hour clock) as a decimal number. 1, 2, ... 12 %p Locale\u2019s AM or PM. AM, PM %M Minute as a zero-padded decimal number. 00, 01, ..., 59 %-M Minute as a decimal number. 0, 1, ..., 59 %S Second as a zero-padded decimal number. 00, 01, ..., 59 %-S Second as a decimal number. 0, 1, ..., 59 %f Microsecond as a decimal number, zero-padded on the left. 000000 - 999999 %z UTC offset in the form +HHMM or -HHMM. %Z Time zone name. %j Day of the year as a zero-padded decimal number. 001, 002, ..., 366 %-j Day of the year as a decimal number. 1, 2, ..., 366 %U Week number of the year (Sunday as the first day of the week). 00, 01, ..., 53 %W Week number of the year (Monday as the first day of the week). 00, 01, ..., 53 %c Locale\u2019s appropriate date and time representation. Mon Sep 30 07:06:05 2013 %x Locale\u2019s appropriate date representation. 09/30/13 %X Locale\u2019s appropriate time representation. 07:06:05 %% A literal '%' character. %", "title": "Parse a datetime from a string"}, {"location": "coding/python/python_snippets/#install-a-python-dependency-from-a-git-repository", "text": "With pip you can : pip install git+git://github.com/path/to/repository@master If you want to hard code it in your setup.py , you need to: install_requires = [ \"some-pkg @ git+ssh://git@github.com/someorgname/pkg-repo-name@v1.1#egg=some-pkg\" , ] But Pypi won't allow you to upload the package , as it will give you an error: HTTPError: 400 Bad Request from https://test.pypi.org/legacy/ Invalid value for requires_dist. Error: Can't have direct dependency: 'deepdiff @ git+git://github.com/lyz-code/deepdiff@master' It looks like this is a conscious decision on the PyPI side. Basically, they don't want pip to reach out to URLs outside their site when installing from PyPI. An ugly patch is to install the dependencies in a PostInstall custom script in the setup.py of your program: from setuptools.command.install import install from subprocess import getoutput # ignore: cannot subclass install, has type Any. And what would you do? class PostInstall ( install ): # type: ignore \"\"\"Install direct dependency. Pypi doesn't allow uploading packages with direct dependencies, so we need to install them manually. \"\"\" def run ( self ) -> None : \"\"\"Install dependencies.\"\"\" install . run ( self ) print ( getoutput ( \"pip install git+git://github.com/lyz-code/deepdiff@master\" )) setup ( cmdclass = { \"install\" : PostInstall }) Warning: It may not work! Last time I used this solution, when I added the library on a setup.py the direct dependencies weren't installed :S", "title": "Install a python dependency from a git repository"}, {"location": "coding/python/python_snippets/#check-or-test-directories-and-files", "text": "def test_dir ( directory ): from os.path import exists from os import makedirs if not exists ( directory ): makedirs ( directory ) def test_file ( filepath , mode ): \"\"\"Check if a file exist and is accessible.\"\"\" def check_mode ( os_mode , mode ): if os . path . isfile ( filepath ) and os . access ( filepath , os_mode ): return else : raise IOError ( \"Can't access the file with mode \" + mode ) if mode is \"r\" : check_mode ( os . R_OK , mode ) elif mode is \"w\" : check_mode ( os . W_OK , mode ) elif mode is \"a\" : check_mode ( os . R_OK , mode ) check_mode ( os . W_OK , mode )", "title": "Check or test directories and files"}, {"location": "coding/python/python_snippets/#remove-the-extension-of-a-file", "text": "os . path . splitext ( \"/path/to/some/file.txt\" )[ 0 ]", "title": "Remove the extension of a file"}, {"location": "coding/python/python_snippets/#iterate-over-the-files-of-a-directory", "text": "import os directory = \"/path/to/directory\" for entry in os . scandir ( directory ): if ( entry . path . endswith ( \".jpg\" ) or entry . path . endswith ( \".png\" )) and entry . is_file (): print ( entry . path )", "title": "Iterate over the files of a directory"}, {"location": "coding/python/python_snippets/#create-directory", "text": "if not os . path . exists ( directory ): os . makedirs ( directory )", "title": "Create directory"}, {"location": "coding/python/python_snippets/#touch-a-file", "text": "from pathlib import Path Path ( \"path/to/file.txt\" ) . touch ()", "title": "Touch a file"}, {"location": "coding/python/python_snippets/#get-the-first-day-of-next-month", "text": "current = datetime . datetime ( mydate . year , mydate . month , 1 ) next_month = datetime . datetime ( mydate . year + int ( mydate . month / 12 ), (( mydate . month % 12 ) + 1 ), 1 )", "title": "Get the first day of next month"}, {"location": "coding/python/python_snippets/#get-the-week-number-of-a-datetime", "text": "datetime.datetime has a isocalendar() method, which returns a tuple containing the calendar week: >>> import datetime >>> datetime . datetime ( 2010 , 6 , 16 ) . isocalendar ()[ 1 ] 24 datetime.date.isocalendar() is an instance-method returning a tuple containing year, weeknumber and weekday in respective order for the given date instance.", "title": "Get the week number of a datetime"}, {"location": "coding/python/python_snippets/#get-the-monday-of-a-week-number", "text": "A week number is not enough to generate a date; you need a day of the week as well. Add a default: import datetime d = \"2013-W26\" r = datetime . datetime . strptime ( d + \"-1\" , \"%Y-W%W-%w\" ) The -1 and -%w pattern tells the parser to pick the Monday in that week.", "title": "Get the monday of a week number"}, {"location": "coding/python/python_snippets/#get-the-month-name-from-a-number", "text": "import calendar >> calendar . month_name [ 3 ] 'March'", "title": "Get the month name from a number"}, {"location": "coding/python/python_snippets/#get-ordinal-from-number", "text": "def int_to_ordinal ( number : int ) -> str : \"\"\"Convert an integer into its ordinal representation. make_ordinal(0) => '0th' make_ordinal(3) => '3rd' make_ordinal(122) => '122nd' make_ordinal(213) => '213th' Args: number: Number to convert Returns: ordinal representation of the number \"\"\" suffix = [ \"th\" , \"st\" , \"nd\" , \"rd\" , \"th\" ][ min ( number % 10 , 4 )] if 11 <= ( number % 100 ) <= 13 : suffix = \"th\" return f \" { number }{ suffix } \"", "title": "Get ordinal from number"}, {"location": "coding/python/python_snippets/#group-or-sort-a-list-of-dictionaries-or-objects-by-a-specific-key", "text": "Python lists have a built-in list.sort() method that modifies the list in-place. There is also a sorted() built-in function that builds a new sorted list from an iterable.", "title": "Group or sort a list of dictionaries or objects by a specific key"}, {"location": "coding/python/python_snippets/#sorting-basics", "text": "A simple ascending sort is very easy: just call the sorted() function. It returns a new sorted list: >>> sorted ([ 5 , 2 , 3 , 1 , 4 ]) [ 1 , 2 , 3 , 4 , 5 ]", "title": "Sorting basics"}, {"location": "coding/python/python_snippets/#key-functions", "text": "Both list.sort() and sorted() have a key parameter to specify a function (or other callable) to be called on each list element prior to making comparisons. For example, here\u2019s a case-insensitive string comparison: >>> sorted ( \"This is a test string from Andrew\" . split (), key = str . lower ) [ 'a' , 'Andrew' , 'from' , 'is' , 'string' , 'test' , 'This' ] The value of the key parameter should be a function (or other callable) that takes a single argument and returns a key to use for sorting purposes. This technique is fast because the key function is called exactly once for each input record. A common pattern is to sort complex objects using some of the object\u2019s indices as keys. For example: >>> from operator import itemgetter >>> student_tuples = [ ( 'john' , 'A' , 15 ), ( 'jane' , 'B' , 12 ), ( 'dave' , 'B' , 10 ), ] >>> sorted ( student_tuples , key = itemgetter ( 2 )) # sort by age [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] The same technique works for objects with named attributes. For example: >>> from operator import attrgetter >>> class Student : def __init__ ( self , name , grade , age ): self . name = name self . grade = grade self . age = age def __repr__ ( self ): return repr (( self . name , self . grade , self . age )) >>> student_objects = [ Student ( 'john' , 'A' , 15 ), Student ( 'jane' , 'B' , 12 ), Student ( 'dave' , 'B' , 10 ), ] >>> sorted ( student_objects , key = attrgetter ( 'age' )) # sort by age [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] The operator module functions allow multiple levels of sorting. For example, to sort by grade then by age: >>> sorted ( student_tuples , key = itemgetter ( 1 , 2 )) [( 'john' , 'A' , 15 ), ( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 )] >>> sorted ( student_objects , key = attrgetter ( 'grade' , 'age' )) [( 'john' , 'A' , 15 ), ( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 )]", "title": "Key functions"}, {"location": "coding/python/python_snippets/#sorts-stability-and-complex-sorts", "text": "Sorts are guaranteed to be stable. That means that when multiple records have the same key, their original order is preserved. >>> data = [( 'red' , 1 ), ( 'blue' , 1 ), ( 'red' , 2 ), ( 'blue' , 2 )] >>> sorted ( data , key = itemgetter ( 0 )) [( 'blue' , 1 ), ( 'blue' , 2 ), ( 'red' , 1 ), ( 'red' , 2 )] Notice how the two records for blue retain their original order so that ('blue', 1) is guaranteed to precede ('blue', 2) . This wonderful property lets you build complex sorts in a series of sorting steps. For example, to sort the student data by descending grade and then ascending age, do the age sort first and then sort again using grade: >>> s = sorted ( student_objects , key = attrgetter ( 'age' )) # sort on secondary key >>> sorted ( s , key = attrgetter ( 'grade' ), reverse = True ) # now sort on primary key, descending [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] This can be abstracted out into a wrapper function that can take a list and tuples of field and order to sort them on multiple passes. >>> def multisort ( xs , specs ): for key , reverse in reversed ( specs ): xs . sort ( key = attrgetter ( key ), reverse = reverse ) return xs >>> multisort ( list ( student_objects ), (( 'grade' , True ), ( 'age' , False ))) [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )]", "title": "Sorts stability and complex sorts"}, {"location": "coding/python/python_snippets/#get-the-attribute-of-an-attribute", "text": "To sort the list in place: ut . sort ( key = lambda x : x . count , reverse = True ) To return a new list, use the sorted() built-in function: newlist = sorted ( ut , key = lambda x : x . body . id_ , reverse = True )", "title": "Get the attribute of an attribute"}, {"location": "coding/python/python_snippets/#iterate-over-an-instance-objects-data-attributes-in-python", "text": "@dataclass ( frozen = True ) class Search : center : str distance : str se = Search ( \"a\" , \"b\" ) for key , value in se . __dict__ . items (): print ( key , value )", "title": "Iterate over an instance object's data attributes in Python"}, {"location": "coding/python/python_snippets/#generate-ssh-key", "text": "pip install cryptography from os import chmod from cryptography.hazmat.primitives import serialization from cryptography.hazmat.primitives.asymmetric import rsa from cryptography.hazmat.backends import default_backend as crypto_default_backend private_key = rsa . generate_private_key ( backend = crypto_default_backend (), public_exponent = 65537 , key_size = 4096 ) pem = private_key . private_bytes ( encoding = serialization . Encoding . PEM , format = serialization . PrivateFormat . TraditionalOpenSSL , encryption_algorithm = serialization . NoEncryption (), ) with open ( \"/tmp/private.key\" , \"wb\" ) as content_file : chmod ( \"/tmp/private.key\" , 0600 ) content_file . write ( pem ) public_key = ( private_key . public_key () . public_bytes ( encoding = serialization . Encoding . OpenSSH , format = serialization . PublicFormat . OpenSSH , ) + b \" user@email.org\" ) with open ( \"/tmp/public.key\" , \"wb\" ) as content_file : content_file . write ( public_key )", "title": "Generate ssh key"}, {"location": "coding/python/python_snippets/#make-multiline-code-look-clean", "text": "If you need variables that contain multiline strings inside functions or methods you need to remove the indentation def test (): # end first line with \\ to avoid the empty line! s = \"\"\" \\ hello world \"\"\" Which is inconvenient as it breaks some editor source code folding and it's ugly for the eye. The solution is to use textwrap.dedent() import textwrap def test (): # end first line with \\ to avoid the empty line! s = \"\"\" \\ hello world \"\"\" print ( repr ( s )) # prints ' hello\\n world\\n ' print ( repr ( textwrap . dedent ( s ))) # prints 'hello\\n world\\n' If you forget to add the trailing \\ character of s = '''\\ or use s = '''hello , you're going to have a bad time with black .", "title": "Make multiline code look clean"}, {"location": "coding/python/python_snippets/#play-a-sound", "text": "pip install playsound from playsound import playsound playsound ( \"path/to/file.wav\" )", "title": "Play a sound"}, {"location": "coding/python/python_snippets/#deep-copy-a-dictionary", "text": "import copy d = { ... } d2 = copy . deepcopy ( d )", "title": "Deep copy a dictionary"}, {"location": "coding/python/python_snippets/#find-the-root-directory-of-a-package", "text": "pyprojroot finds the root working directory for your project as a pathlib object. You can now use the here function to pass in a relative path from the project root directory (no matter what working directory you are in the project), and you will get a full path to the specified file.", "title": "Find the root directory of a package"}, {"location": "coding/python/python_snippets/#installation", "text": "pip install pyprojroot", "title": "Installation"}, {"location": "coding/python/python_snippets/#usage", "text": "from pyprojroot import here here ()", "title": "Usage"}, {"location": "coding/python/python_snippets/#check-if-an-object-has-an-attribute", "text": "if hasattr ( a , \"property\" ): a . property", "title": "Check if an object has an attribute"}, {"location": "coding/python/python_snippets/#check-if-a-loop-ends-completely", "text": "for loops can take an else block which is not run if the loop has ended with a break statement. for i in [ 1 , 2 , 3 ]: print ( i ) if i == 3 : break else : print ( \"for loop was not broken\" )", "title": "Check if a loop ends completely"}, {"location": "coding/python/python_snippets/#merge-two-lists", "text": "z = x + y", "title": "Merge two lists"}, {"location": "coding/python/python_snippets/#merge-two-dictionaries", "text": "z = { ** x , ** y }", "title": "Merge two dictionaries"}, {"location": "coding/python/python_snippets/#create-user-defined-exceptions", "text": "Programs may name their own exceptions by creating a new exception class. Exceptions should typically be derived from the Exception class, either directly or indirectly. Exception classes are meant to be kept simple, only offering a number of attributes that allow information about the error to be extracted by handlers for the exception. When creating a module that can raise several distinct errors, a common practice is to create a base class for exceptions defined by that module, and subclass that to create specific exception classes for different error conditions: class Error ( Exception ): \"\"\"Base class for exceptions in this module.\"\"\" class ConceptNotFoundError ( Error ): \"\"\"Transactions with unmatched concept.\"\"\" def __init__ ( self , message : str , transactions : List [ Transaction ]) -> None : \"\"\"Initialize the exception.\"\"\" self . message = message self . transactions = transactions super () . __init__ ( self . message ) Most exceptions are defined with names that end in \u201cError\u201d, similar to the naming of the standard exceptions.", "title": "Create user defined exceptions"}, {"location": "coding/python/python_snippets/#import-a-module-or-its-objects-from-within-a-python-program", "text": "import importlib module = importlib . import_module ( \"os\" ) module_class = module . getcwd relative_module = importlib . import_module ( \".model\" , package = \"mypackage\" ) class_to_extract = \"MyModel\" extracted_class = geattr ( relative_module , class_to_extract ) The first argument specifies what module to import in absolute or relative terms (e.g. either pkg.mod or ..mod ). If the name is specified in relative terms, then the package argument must be set to the name of the package which is to act as the anchor for resolving the package name (e.g. import_module('..mod', 'pkg.subpkg') will import pkg.mod ).", "title": "Import a module or it's objects from within a python program"}, {"location": "coding/python/python_snippets/#get-systems-timezone-and-use-it-in-datetime", "text": "To obtain timezone information in the form of a datetime.tzinfo object, use dateutil.tz.tzlocal() : from dateutil import tz myTimeZone = tz . tzlocal () This object can be used in the tz parameter of datetime.datetime.now() : from datetime import datetime from dateutil import tz localisedDatetime = datetime . now ( tz = tz . tzlocal ())", "title": "Get system's timezone and use it in datetime"}, {"location": "coding/python/python_snippets/#capitalize-a-sentence", "text": "To change the caps of the first letter of the first word of a sentence use: >> sentence = \"add funny Emojis\" >> sentence [ 0 ] . upper () + sentence [ 1 :] Add funny Emojis The .capitalize method transforms the rest of words to lowercase. The .title transforms all sentence words to capitalize.", "title": "Capitalize a sentence"}, {"location": "coding/python/python_snippets/#get-the-last-monday-datetime", "text": "import datetime today = datetime . date . today () last_monday = today - datetime . timedelta ( days = today . weekday ())", "title": "Get the last monday datetime"}, {"location": "coding/python/python_snippets/#issues", "text": "Pypi won't allow you to upload packages with direct dependencies : update the section above.", "title": "Issues"}, {"location": "coding/python/redis-py/", "text": "Redis-py is The Python interface to the Redis key-value store. The library encapsulates an actual TCP connection to a Redis server and sends raw commands, as bytes serialized using the REdis Serialization Protocol (RESP) , to the server. It then takes the raw reply and parses it back into a Python object such as bytes, int, or even datetime.datetime. Installation \u2691 pip install redis Usage \u2691 import redis r = redis . Redis ( host = 'localhost' , port = 6379 , db = 0 , password = None , socket_timeout = None , ) The arguments specified above are the default ones, so it's the same as calling r = redis.Redis() . The db parameter is the database number. You can manage multiple databases in Redis at once, and each is identified by an integer. The max number of databases is 16 by default. Common pitfalls \u2691 Redis returned objects are bytes type, so you may need to convert it to string with r.get(\"Bahamas\").decode(\"utf-8\") . References \u2691 Real Python introduction to Redis-py Git Docs : Very technical and small.", "title": "Redis-py"}, {"location": "coding/python/redis-py/#installation", "text": "pip install redis", "title": "Installation"}, {"location": "coding/python/redis-py/#usage", "text": "import redis r = redis . Redis ( host = 'localhost' , port = 6379 , db = 0 , password = None , socket_timeout = None , ) The arguments specified above are the default ones, so it's the same as calling r = redis.Redis() . The db parameter is the database number. You can manage multiple databases in Redis at once, and each is identified by an integer. The max number of databases is 16 by default.", "title": "Usage"}, {"location": "coding/python/redis-py/#common-pitfalls", "text": "Redis returned objects are bytes type, so you may need to convert it to string with r.get(\"Bahamas\").decode(\"utf-8\") .", "title": "Common pitfalls"}, {"location": "coding/python/redis-py/#references", "text": "Real Python introduction to Redis-py Git Docs : Very technical and small.", "title": "References"}, {"location": "coding/python/requests_mock/", "text": "The requests-mock library is a requests transport adapter that can be preloaded with responses that are returned if certain URIs are requested. This is particularly useful in unit tests where you want to return known responses from HTTP requests without making actual calls. Installation \u2691 pip install requests-mock Usage \u2691 Object initialization \u2691 Select one of the following ways to initialize the mock. As a pytest fixture \u2691 The ease of use with pytest it is awesome. requests-mock provides an external fixture registered with pytest such that it is usable simply by specifying it as a parameter. There is no need to import requests-mock it simply needs to be installed and specified as an argument in the test definition. import pytest import requests from requests_mock.mocker import Mocker def test_url ( requests_mock : Mocker ): requests_mock . get ( 'http://test.com' , text = 'data' ) assert 'data' == requests . get ( 'http://test.com' ) . text As a function decorator \u2691 >>> @requests_mock . Mocker () ... def test_function ( m ): ... m . get ( 'http://test.com' , text = 'resp' ) ... return requests . get ( 'http://test.com' ) . text ... >>> test_function () 'resp' As a context manager \u2691 >>> import requests >>> import requests_mock >>> with requests_mock . Mocker () as m : ... m . get ( 'http://test.com' , text = 'resp' ) ... requests . get ( 'http://test.com' ) . text ... 'resp' Mocking responses \u2691 Return a json \u2691 requests_mock . get ( ' {} /api/repos/owner/repository/builds' . format ( self . url ), json = { \"id\" : 882 , \"number\" : 209 , \"finished\" : 1591197904 , }, ) Add a header or a cookie to the response \u2691 requests_mock . post ( \"https://test.com\" , cookies = { \"Id\" : \"0\" }, headers = { \"id\" : \"0\" }, ) Multiple responses \u2691 Multiple responses can be provided to be returned in order by specifying the keyword parameters in a list. requests_mock . get ( 'https://test.com/4' , [ { 'text' : 'resp1' , 'status_code' : 300 }, { 'text' : 'resp2' , 'status_code' : 200 } ] ) Get requests history \u2691 Called \u2691 The easiest way to test if a request hit the adapter is to simply check the called property or the call_count property. >>> import requests >>> import requests_mock >>> with requests_mock . mock () as m : ... m . get ( 'http://test.com, text=' resp ') ... resp = requests . get ( 'http://test.com' ) ... >>> m . called True >>> m . call_count 1 Requests history \u2691 The history of objects that passed through the mocker/adapter can also be retrieved. >>> history = m . request_history >>> len ( history ) 1 >>> history [ 0 ] . method 'GET' >>> history [ 0 ] . url 'http://test.com/' References \u2691 Docs Git", "title": "Requests-mock"}, {"location": "coding/python/requests_mock/#installation", "text": "pip install requests-mock", "title": "Installation"}, {"location": "coding/python/requests_mock/#usage", "text": "", "title": "Usage"}, {"location": "coding/python/requests_mock/#object-initialization", "text": "Select one of the following ways to initialize the mock.", "title": "Object initialization"}, {"location": "coding/python/requests_mock/#as-a-pytest-fixture", "text": "The ease of use with pytest it is awesome. requests-mock provides an external fixture registered with pytest such that it is usable simply by specifying it as a parameter. There is no need to import requests-mock it simply needs to be installed and specified as an argument in the test definition. import pytest import requests from requests_mock.mocker import Mocker def test_url ( requests_mock : Mocker ): requests_mock . get ( 'http://test.com' , text = 'data' ) assert 'data' == requests . get ( 'http://test.com' ) . text", "title": "As a pytest fixture"}, {"location": "coding/python/requests_mock/#as-a-function-decorator", "text": ">>> @requests_mock . Mocker () ... def test_function ( m ): ... m . get ( 'http://test.com' , text = 'resp' ) ... return requests . get ( 'http://test.com' ) . text ... >>> test_function () 'resp'", "title": "As a function decorator"}, {"location": "coding/python/requests_mock/#as-a-context-manager", "text": ">>> import requests >>> import requests_mock >>> with requests_mock . Mocker () as m : ... m . get ( 'http://test.com' , text = 'resp' ) ... requests . get ( 'http://test.com' ) . text ... 'resp'", "title": "As a context manager"}, {"location": "coding/python/requests_mock/#mocking-responses", "text": "", "title": "Mocking responses"}, {"location": "coding/python/requests_mock/#return-a-json", "text": "requests_mock . get ( ' {} /api/repos/owner/repository/builds' . format ( self . url ), json = { \"id\" : 882 , \"number\" : 209 , \"finished\" : 1591197904 , }, )", "title": "Return a json"}, {"location": "coding/python/requests_mock/#add-a-header-or-a-cookie-to-the-response", "text": "requests_mock . post ( \"https://test.com\" , cookies = { \"Id\" : \"0\" }, headers = { \"id\" : \"0\" }, )", "title": "Add a header or a cookie to the response"}, {"location": "coding/python/requests_mock/#multiple-responses", "text": "Multiple responses can be provided to be returned in order by specifying the keyword parameters in a list. requests_mock . get ( 'https://test.com/4' , [ { 'text' : 'resp1' , 'status_code' : 300 }, { 'text' : 'resp2' , 'status_code' : 200 } ] )", "title": "Multiple responses"}, {"location": "coding/python/requests_mock/#get-requests-history", "text": "", "title": "Get requests history"}, {"location": "coding/python/requests_mock/#called", "text": "The easiest way to test if a request hit the adapter is to simply check the called property or the call_count property. >>> import requests >>> import requests_mock >>> with requests_mock . mock () as m : ... m . get ( 'http://test.com, text=' resp ') ... resp = requests . get ( 'http://test.com' ) ... >>> m . called True >>> m . call_count 1", "title": "Called"}, {"location": "coding/python/requests_mock/#requests-history", "text": "The history of objects that passed through the mocker/adapter can also be retrieved. >>> history = m . request_history >>> len ( history ) 1 >>> history [ 0 ] . method 'GET' >>> history [ 0 ] . url 'http://test.com/'", "title": "Requests history"}, {"location": "coding/python/requests_mock/#references", "text": "Docs Git", "title": "References"}, {"location": "coding/python/rq/", "text": "RQ (Redis Queue) is a simple Python library for queueing jobs and processing them in the background with workers. It is backed by Redis and it is designed to have a low barrier to entry. Check arq Next time you are going to use this, check if arq is better. Getting started \u2691 Assuming that a Redis server is running, define the function you want to run: import requests def count_words_at_url ( url ): resp = requests . get ( url ) return len ( resp . text . split ()) The, create a RQ queue: from redis import Redis from rq import Queue q = Queue ( connection = Redis ()) And enqueue the function call: from my_module import count_words_at_url result = q . enqueue ( count_words_at_url , 'http://nvie.com' ) To start executing enqueued function calls in the background, start a worker from your project\u2019s directory: $ rq worker *** Listening for work on default Got count_words_at_url ( 'http://nvie.com' ) from default Job result = 818 *** Listening for work on default Install \u2691 pip install rq Reference \u2691 Homepage Git Docs", "title": "Rq"}, {"location": "coding/python/rq/#getting-started", "text": "Assuming that a Redis server is running, define the function you want to run: import requests def count_words_at_url ( url ): resp = requests . get ( url ) return len ( resp . text . split ()) The, create a RQ queue: from redis import Redis from rq import Queue q = Queue ( connection = Redis ()) And enqueue the function call: from my_module import count_words_at_url result = q . enqueue ( count_words_at_url , 'http://nvie.com' ) To start executing enqueued function calls in the background, start a worker from your project\u2019s directory: $ rq worker *** Listening for work on default Got count_words_at_url ( 'http://nvie.com' ) from default Job result = 818 *** Listening for work on default", "title": "Getting started"}, {"location": "coding/python/rq/#install", "text": "pip install rq", "title": "Install"}, {"location": "coding/python/rq/#reference", "text": "Homepage Git Docs", "title": "Reference"}, {"location": "coding/python/ruamel_yaml/", "text": "ruamel.yaml is a YAML 1.2 loader/dumper package for Python. It is a derivative of Kirill Simonov\u2019s PyYAML 3.11. It has the following enhancements: Comments. Block style and key ordering are kept, so you can diff the round-tripped source. Flow style sequences ( \u2018a: b, c, d\u2019). Anchor names that are hand-crafted (i.e. not of the form idNNN ). Merges in dictionaries are preserved. Installation \u2691 I suggest to use the ruyaml fork, as it's maintained by the community and versioned with git. pip install ruyaml Usage \u2691 Very similar to PyYAML. If invoked with YAML(typ='safe') either the load or the write of the data, the comments of the yaml will be lost. Load from file \u2691 from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError try : with open ( os . path . expanduser ( file_path ), 'r' ) as f : try : data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( file_path ) ) sys . exit ( 1 ) Save to file \u2691 with open ( os . path . expanduser ( file_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( data , f ) Save to a string \u2691 For some unknown reason, they don't want to output the result to a string, you need to mess up with streams. # Configure YAML formatter yaml = YAML () yaml . indent ( mapping = 2 , sequence = 4 , offset = 2 ) yaml . allow_duplicate_keys = True yaml . explicit_start = False # Return the output to a string string_stream = StringIO () yaml . dump ({ 'products' : [ 'item 1' , 'item 2' ]}, string_stream ) source_code = string_stream . getvalue () string_stream . close () I've opened an issue in the ruyaml fork to solve it. References \u2691 Docs Code", "title": "Ruamel YAML"}, {"location": "coding/python/ruamel_yaml/#installation", "text": "I suggest to use the ruyaml fork, as it's maintained by the community and versioned with git. pip install ruyaml", "title": "Installation"}, {"location": "coding/python/ruamel_yaml/#usage", "text": "Very similar to PyYAML. If invoked with YAML(typ='safe') either the load or the write of the data, the comments of the yaml will be lost.", "title": "Usage"}, {"location": "coding/python/ruamel_yaml/#load-from-file", "text": "from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError try : with open ( os . path . expanduser ( file_path ), 'r' ) as f : try : data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( file_path ) ) sys . exit ( 1 )", "title": "Load from file"}, {"location": "coding/python/ruamel_yaml/#save-to-file", "text": "with open ( os . path . expanduser ( file_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( data , f )", "title": "Save to file"}, {"location": "coding/python/ruamel_yaml/#save-to-a-string", "text": "For some unknown reason, they don't want to output the result to a string, you need to mess up with streams. # Configure YAML formatter yaml = YAML () yaml . indent ( mapping = 2 , sequence = 4 , offset = 2 ) yaml . allow_duplicate_keys = True yaml . explicit_start = False # Return the output to a string string_stream = StringIO () yaml . dump ({ 'products' : [ 'item 1' , 'item 2' ]}, string_stream ) source_code = string_stream . getvalue () string_stream . close () I've opened an issue in the ruyaml fork to solve it.", "title": "Save to a string"}, {"location": "coding/python/ruamel_yaml/#references", "text": "Docs Code", "title": "References"}, {"location": "coding/python/sqlalchemy/", "text": "SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL. I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions . Creating an SQL Schema \u2691 First of all it's important to create a diagram with the database structure, it will help you in the design and it's a great improvement of the project's documentation. I usually use ondras wwwsqldesigner through his demo , as it's easy to use and it's possible to save the data in your repository in an xml file. I assume you've already set up your project to support sqlalchemy in your project. If not, do so before moving forward. Mapping styles \u2691 Modern SQLAlchemy features two distinct styles of mapper configuration. The \u201cClassical\u201d style is SQLAlchemy\u2019s original mapping API, whereas \u201cDeclarative\u201d is the richer and more succinct system that builds on top of \u201cClassical\u201d. The Classical, on the other hand, doesn't lock your models to the ORM, something that we avoid when using the repository pattern . If you aren't going to use the repository pattern, use the declarative way, otherwise use the classical one Creating Tables \u2691 If you simply want to create a table of association without any parameters, such as with a many to many relationship association table, use this type of object. Declarative type \u2691 class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) name = Column ( String , doc = 'User name' ) def __init__ ( self , id , name = None , ): self . id = id self . name = name There are different types of fields to add to a table: Boolean: is_true = Column(Boolean) . Datetime: created_date = Column(DateTime, doc='Date of creation') . Float: score = Column(Float) Integer: id = Column(Integer, primary_key=True, doc='Source ID') . String: title = Column(String) . Text: long_text = Column(Text) . To make sure that a field can't contain nulls set the nullable=False attribute in the definition of the Column . If you want the contents to be unique use unique=True . If you want to use the Mysql driver of SQLAlchemy make sure to specify the length of the colums, for example String(16) . For reference this are the common lengths: url: 2083 name: 64 (it occupies the same 2 and 255). email: 64 (it occupies the same 2 and 255). username: 64 (it occupies the same 2 and 255). Classical type \u2691 File: model.py class User (): def __init__ ( self , id , name = None ): self . id = id self . name = name File: orm.py from models import User from sqlalchemy import ( Column , MetaData , String , Table , Text , ) metadata = MetaData () user = Table ( \"user\" , metadata , Column ( \"id\" , String ( 64 ), primary_key = True ), Column ( \"name\" , String ( 64 )), ) def start_mappers (): mapper ( User , user ) Creating relationships \u2691 Joined table inheritance \u2691 In joined table inheritance, each class along a hierarchy of classes is represented by a distinct table. Querying for a particular subclass in the hierarchy will render as a SQL JOIN along all tables in its inheritance path. If the queried class is the base class, the default behavior is to include only the base table in a SELECT statement. In all cases, the ultimate class to instantiate for a given row is determined by a discriminator column or an expression that works against the base table. When a subclass is loaded only against a base table, resulting objects by default will have base attributes populated at first; attributes that are local to the subclass will lazy load when they are accessed. The base class in a joined inheritance hierarchy is configured with additional arguments that will refer to the polymorphic discriminator column as well as the identifier for the base class. Declarative \u2691 class Employee ( Base ): __tablename__ = 'employee' id = Column ( Integer , primary_key = True ) name = Column ( String ( 50 )) type = Column ( String ( 50 )) __mapper_args__ = { 'polymorphic_identity' : 'employee' , 'polymorphic_on' : type } class Engineer ( Employee ): __tablename__ = 'engineer' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) engineer_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'engineer' , } class Manager ( Employee ): __tablename__ = 'manager' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) manager_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'manager' , } Classical \u2691 File: model.py class Employee : def __init__ ( self , name ): self . name = name class Manager ( Employee ): def __init__ ( self , name , manager_data ): super () . __init__ ( name ) self . manager_data = manager_data class Engineer ( Employee ): def __init__ ( self , name , engineer_info ): super () . __init__ ( name ) self . engineer_info = engineer_info File: orm.py metadata = MetaData () employee = Table ( 'employee' , metadata , Column ( 'id' , Integer , primary_key = True ), Column ( 'name' , String ( 50 )), Column ( 'type' , String ( 20 )), Column ( 'manager_data' , String ( 50 )), Column ( 'engineer_info' , String ( 50 )) ) mapper ( Employee , employee , polymorphic_on = employee . c . type , polymorphic_identity = 'employee' , exclude_properties = { 'engineer_info' , 'manager_data' }) mapper ( Manager , inherits = Employee , polymorphic_identity = 'manager' , exclude_properties = { 'engineer_info' }) mapper ( Engineer , inherits = Employee , polymorphic_identity = 'engineer' , exclude_properties = { 'manager_data' }) One to many \u2691 from sqlalchemy.orm import relationship class User ( db . Model ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) posts = relationship ( 'Post' , back_populates = 'user' ) class Post ( db . Model ): id = Column ( Integer , primary_key = True ) body = Column ( String ( 140 )) user_id = Column ( Integer , ForeignKey ( 'user.id' )) user = relationship ( 'User' , back_populates = 'posts' ) In the tests of the Post class, only check that the user attribute is present. Factoryboy supports the creation of Dependent objects direct ForeignKey . Self referenced one to many \u2691 class Task ( Base ): __tablename__ = 'task' id = Column ( String , primary_key = True , doc = 'fulid of creation' ) parent_id = Column ( String , ForeignKey ( 'task.id' )) parent = relationship ( 'Task' , remote_side = [ id ], backref = 'children' ) Many to many \u2691 # Association tables source_has_category = Table ( 'source_has_category' , Base . metadata , Column ( 'source_id' , Integer , ForeignKey ( 'source.id' )), Column ( 'category_id' , Integer , ForeignKey ( 'category.id' )) ) # Tables class Category ( Base ): __tablename__ = 'category' id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'categories' , secondary = source_has_category , ) class Content ( Base ): __tablename__ = 'content' id = Column ( Integer , primary_key = True , doc = 'Content ID' ) categories = relationship ( 'Category' , back_populates = 'contents' , secondary = source_has_category , ) Self referenced many to many \u2691 Using the followers table as an association table. followers = db . Table ( 'followers' , Base . metadata , Column ( 'follower_id' , Integer , ForeignKey ( 'user.id' )), Column ( 'followed_id' , Integer , ForeignKey ( 'user.id' )), ) class User ( Base ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) followed = relationship ( 'User' , secondary = followers , primaryjoin = ( followers . c . follower_id == id ), secondaryjoin = ( followers . c . followed_id == id ), backref = db . backref ( 'followers' , lazy = 'dynamic' ), lazy = 'dynamic' , ) Links User instances to other User instances, so as a convention let's say that for a pair of users linked by this relationship, the left side user is following the right side user. The relationship definition is created as seen from the left side user with the name followed, because when this relationship is queried from the left side it will get the list of followed users (i.e those on the right side). User : Is the right side entity of the relationship. Since this is a self-referential relationship, The same class must be used on both sides. secondary : configures the association table that is used for this relationship. primaryjoin : Indicates the condition that links the left side entity (the follower user) with the association table. The join condition for the left side of the relationship is the user id matching the follower_id field of the association table. The followers.c.follower_id expression references the follower_id column of the association table. secondaryjoin : Indicates the condition that links the right side entity (the followed user) with the association table. This condition is similar to the one for primaryjoin . backref : Defines how this relationship will be accessed from the right side entity. From the left side, the relationship is named followed , so from the right side, the name followers represent all the left side users that are linked to the target user in the right side. The additional lazy argument indicates the execution mode for this query. A mode of dynamic sets up the query not to run until specifically requested. lazy : same as with backref , but this one applies to the left side query instead of the right side. Testing SQLAlchemy Code \u2691 The definition of the database can be tested, I usually use them to test that the attributes are loaded and that the factory objects work as expected. Several steps need to be set to make it work: Create the factory boy objects in tests/factories.py . Configure the tests to use a temporal sqlite database in the tests/conftest.py file with the following contents (changing {{ program_name }} ): from alembic.command import upgrade from alembic.config import Config from sqlalchemy.orm import sessionmaker import os import pytest import tempfile temp_ddbb = tempfile . mkstemp ()[ 1 ] os . environ [ '{{ program_name }} _DATABASE_URL' ] = 'sqlite:/// {} ' . format ( temp_ddbb ) # It needs to be after the environmental variable from {{ program_name }} . models import engine from tests import factories @pytest . fixture ( scope = 'module' ) def connection (): ''' Fixture to set up the connection to the temporal database, the path is stablished at conftest.py ''' # Create database connection connection = engine . connect () # Applies all alembic migrations. config = Config ( '{{ program_name }}/migrations/alembic.ini' ) upgrade ( config , 'head' ) # End of setUp yield connection # Start of tearDown connection . close () @pytest . fixture ( scope = 'function' ) def session ( connection ): ''' Fixture to set up the sqlalchemy session of the database. ''' # Begin a non-ORM transaction and bind session transaction = connection . begin () session = sessionmaker ()( bind = connection ) factories . UserFactory . _meta . sqlalchemy_session = session yield session # Close session and rollback transaction session . close () transaction . rollback () Define an abstract base test class BaseModelTest defined as following in the tests/unit/test_models.py file. from {{ program_name }} import models from tests import factories import pytest class BaseModelTest : \"\"\" Abstract base test class to refactor model tests. The Children classes must define the following attributes: self.model: The model object to test. self.dummy_instance: A factory object of the model to test. self.model_attributes: List of model attributes to test Public attributes: dummy_instance (Factory_boy object): Dummy instance of the model. \"\"\" @pytest . fixture ( autouse = True ) def base_setup ( self , session ): self . session = session def test_attributes_defined ( self ): for attribute in self . model_attributes : assert getattr ( self . model , attribute ) == \\ getattr ( self . dummy_instance , attribute ) @pytest . mark . usefixtures ( 'base_setup' ) class TestUser ( BaseModelTest ): @pytest . fixture ( autouse = True ) def setup ( self , session ): self . factory = factories . UserFactory self . dummy_instance = self . factory . create () self . model = models . User ( id = self . dummy_instance . id , name = self . dummy_instance . name , ) self . model_attributes = [ 'name' , 'id' , ] Then create the models table . Create an alembic revision Run pytest : python -m pytest . Exporting database to json \u2691 import json def dump_sqlalchemy ( output_connection_string , output_schema ): \"\"\" Returns the entire content of a database as lists of dicts\"\"\" engine = create_engine ( f ' { output_connection_string }{ output_schema } ' ) meta = MetaData () meta . reflect ( bind = engine ) # http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html result = {} for table in meta . sorted_tables : result [ table . name ] = [ dict ( row ) for row in engine . execute ( table . select ())] return json . dumps ( result ) Cloning an SQLAlchemy object \u2691 The following function: Copies all the non-primary-key columns from the input model to a new model instance. Allows definition of specific arguments. Leaves the original model object unmodified. def clone_model ( model , ** kwargs ): \"\"\"Clone an arbitrary sqlalchemy model object without its primary key values.\"\"\" table = model . __table__ non_primary_key_columns = [ column_name for column_name in table . __mapper__ . attrs .. keys () if column_name not in table . primary_key ] data = { column_name : getattr ( model , column_name ) for column_name in non_pk_columns } data . update ( kwargs ) return model . __class__ ( ** data ) References \u2691 Home Docs", "title": "SQLAlchemy"}, {"location": "coding/python/sqlalchemy/#creating-an-sql-schema", "text": "First of all it's important to create a diagram with the database structure, it will help you in the design and it's a great improvement of the project's documentation. I usually use ondras wwwsqldesigner through his demo , as it's easy to use and it's possible to save the data in your repository in an xml file. I assume you've already set up your project to support sqlalchemy in your project. If not, do so before moving forward.", "title": "Creating an SQL Schema"}, {"location": "coding/python/sqlalchemy/#mapping-styles", "text": "Modern SQLAlchemy features two distinct styles of mapper configuration. The \u201cClassical\u201d style is SQLAlchemy\u2019s original mapping API, whereas \u201cDeclarative\u201d is the richer and more succinct system that builds on top of \u201cClassical\u201d. The Classical, on the other hand, doesn't lock your models to the ORM, something that we avoid when using the repository pattern . If you aren't going to use the repository pattern, use the declarative way, otherwise use the classical one", "title": "Mapping styles"}, {"location": "coding/python/sqlalchemy/#creating-tables", "text": "If you simply want to create a table of association without any parameters, such as with a many to many relationship association table, use this type of object.", "title": "Creating Tables"}, {"location": "coding/python/sqlalchemy/#declarative-type", "text": "class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) name = Column ( String , doc = 'User name' ) def __init__ ( self , id , name = None , ): self . id = id self . name = name There are different types of fields to add to a table: Boolean: is_true = Column(Boolean) . Datetime: created_date = Column(DateTime, doc='Date of creation') . Float: score = Column(Float) Integer: id = Column(Integer, primary_key=True, doc='Source ID') . String: title = Column(String) . Text: long_text = Column(Text) . To make sure that a field can't contain nulls set the nullable=False attribute in the definition of the Column . If you want the contents to be unique use unique=True . If you want to use the Mysql driver of SQLAlchemy make sure to specify the length of the colums, for example String(16) . For reference this are the common lengths: url: 2083 name: 64 (it occupies the same 2 and 255). email: 64 (it occupies the same 2 and 255). username: 64 (it occupies the same 2 and 255).", "title": "Declarative type"}, {"location": "coding/python/sqlalchemy/#classical-type", "text": "File: model.py class User (): def __init__ ( self , id , name = None ): self . id = id self . name = name File: orm.py from models import User from sqlalchemy import ( Column , MetaData , String , Table , Text , ) metadata = MetaData () user = Table ( \"user\" , metadata , Column ( \"id\" , String ( 64 ), primary_key = True ), Column ( \"name\" , String ( 64 )), ) def start_mappers (): mapper ( User , user )", "title": "Classical type"}, {"location": "coding/python/sqlalchemy/#creating-relationships", "text": "", "title": "Creating relationships"}, {"location": "coding/python/sqlalchemy/#joined-table-inheritance", "text": "In joined table inheritance, each class along a hierarchy of classes is represented by a distinct table. Querying for a particular subclass in the hierarchy will render as a SQL JOIN along all tables in its inheritance path. If the queried class is the base class, the default behavior is to include only the base table in a SELECT statement. In all cases, the ultimate class to instantiate for a given row is determined by a discriminator column or an expression that works against the base table. When a subclass is loaded only against a base table, resulting objects by default will have base attributes populated at first; attributes that are local to the subclass will lazy load when they are accessed. The base class in a joined inheritance hierarchy is configured with additional arguments that will refer to the polymorphic discriminator column as well as the identifier for the base class.", "title": "Joined table inheritance"}, {"location": "coding/python/sqlalchemy/#declarative", "text": "class Employee ( Base ): __tablename__ = 'employee' id = Column ( Integer , primary_key = True ) name = Column ( String ( 50 )) type = Column ( String ( 50 )) __mapper_args__ = { 'polymorphic_identity' : 'employee' , 'polymorphic_on' : type } class Engineer ( Employee ): __tablename__ = 'engineer' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) engineer_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'engineer' , } class Manager ( Employee ): __tablename__ = 'manager' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) manager_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'manager' , }", "title": "Declarative"}, {"location": "coding/python/sqlalchemy/#classical", "text": "File: model.py class Employee : def __init__ ( self , name ): self . name = name class Manager ( Employee ): def __init__ ( self , name , manager_data ): super () . __init__ ( name ) self . manager_data = manager_data class Engineer ( Employee ): def __init__ ( self , name , engineer_info ): super () . __init__ ( name ) self . engineer_info = engineer_info File: orm.py metadata = MetaData () employee = Table ( 'employee' , metadata , Column ( 'id' , Integer , primary_key = True ), Column ( 'name' , String ( 50 )), Column ( 'type' , String ( 20 )), Column ( 'manager_data' , String ( 50 )), Column ( 'engineer_info' , String ( 50 )) ) mapper ( Employee , employee , polymorphic_on = employee . c . type , polymorphic_identity = 'employee' , exclude_properties = { 'engineer_info' , 'manager_data' }) mapper ( Manager , inherits = Employee , polymorphic_identity = 'manager' , exclude_properties = { 'engineer_info' }) mapper ( Engineer , inherits = Employee , polymorphic_identity = 'engineer' , exclude_properties = { 'manager_data' })", "title": "Classical"}, {"location": "coding/python/sqlalchemy/#one-to-many", "text": "from sqlalchemy.orm import relationship class User ( db . Model ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) posts = relationship ( 'Post' , back_populates = 'user' ) class Post ( db . Model ): id = Column ( Integer , primary_key = True ) body = Column ( String ( 140 )) user_id = Column ( Integer , ForeignKey ( 'user.id' )) user = relationship ( 'User' , back_populates = 'posts' ) In the tests of the Post class, only check that the user attribute is present. Factoryboy supports the creation of Dependent objects direct ForeignKey .", "title": "One to many"}, {"location": "coding/python/sqlalchemy/#self-referenced-one-to-many", "text": "class Task ( Base ): __tablename__ = 'task' id = Column ( String , primary_key = True , doc = 'fulid of creation' ) parent_id = Column ( String , ForeignKey ( 'task.id' )) parent = relationship ( 'Task' , remote_side = [ id ], backref = 'children' )", "title": "Self referenced one to many"}, {"location": "coding/python/sqlalchemy/#many-to-many", "text": "# Association tables source_has_category = Table ( 'source_has_category' , Base . metadata , Column ( 'source_id' , Integer , ForeignKey ( 'source.id' )), Column ( 'category_id' , Integer , ForeignKey ( 'category.id' )) ) # Tables class Category ( Base ): __tablename__ = 'category' id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'categories' , secondary = source_has_category , ) class Content ( Base ): __tablename__ = 'content' id = Column ( Integer , primary_key = True , doc = 'Content ID' ) categories = relationship ( 'Category' , back_populates = 'contents' , secondary = source_has_category , )", "title": "Many to many"}, {"location": "coding/python/sqlalchemy/#self-referenced-many-to-many", "text": "Using the followers table as an association table. followers = db . Table ( 'followers' , Base . metadata , Column ( 'follower_id' , Integer , ForeignKey ( 'user.id' )), Column ( 'followed_id' , Integer , ForeignKey ( 'user.id' )), ) class User ( Base ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) followed = relationship ( 'User' , secondary = followers , primaryjoin = ( followers . c . follower_id == id ), secondaryjoin = ( followers . c . followed_id == id ), backref = db . backref ( 'followers' , lazy = 'dynamic' ), lazy = 'dynamic' , ) Links User instances to other User instances, so as a convention let's say that for a pair of users linked by this relationship, the left side user is following the right side user. The relationship definition is created as seen from the left side user with the name followed, because when this relationship is queried from the left side it will get the list of followed users (i.e those on the right side). User : Is the right side entity of the relationship. Since this is a self-referential relationship, The same class must be used on both sides. secondary : configures the association table that is used for this relationship. primaryjoin : Indicates the condition that links the left side entity (the follower user) with the association table. The join condition for the left side of the relationship is the user id matching the follower_id field of the association table. The followers.c.follower_id expression references the follower_id column of the association table. secondaryjoin : Indicates the condition that links the right side entity (the followed user) with the association table. This condition is similar to the one for primaryjoin . backref : Defines how this relationship will be accessed from the right side entity. From the left side, the relationship is named followed , so from the right side, the name followers represent all the left side users that are linked to the target user in the right side. The additional lazy argument indicates the execution mode for this query. A mode of dynamic sets up the query not to run until specifically requested. lazy : same as with backref , but this one applies to the left side query instead of the right side.", "title": "Self referenced many to many"}, {"location": "coding/python/sqlalchemy/#testing-sqlalchemy-code", "text": "The definition of the database can be tested, I usually use them to test that the attributes are loaded and that the factory objects work as expected. Several steps need to be set to make it work: Create the factory boy objects in tests/factories.py . Configure the tests to use a temporal sqlite database in the tests/conftest.py file with the following contents (changing {{ program_name }} ): from alembic.command import upgrade from alembic.config import Config from sqlalchemy.orm import sessionmaker import os import pytest import tempfile temp_ddbb = tempfile . mkstemp ()[ 1 ] os . environ [ '{{ program_name }} _DATABASE_URL' ] = 'sqlite:/// {} ' . format ( temp_ddbb ) # It needs to be after the environmental variable from {{ program_name }} . models import engine from tests import factories @pytest . fixture ( scope = 'module' ) def connection (): ''' Fixture to set up the connection to the temporal database, the path is stablished at conftest.py ''' # Create database connection connection = engine . connect () # Applies all alembic migrations. config = Config ( '{{ program_name }}/migrations/alembic.ini' ) upgrade ( config , 'head' ) # End of setUp yield connection # Start of tearDown connection . close () @pytest . fixture ( scope = 'function' ) def session ( connection ): ''' Fixture to set up the sqlalchemy session of the database. ''' # Begin a non-ORM transaction and bind session transaction = connection . begin () session = sessionmaker ()( bind = connection ) factories . UserFactory . _meta . sqlalchemy_session = session yield session # Close session and rollback transaction session . close () transaction . rollback () Define an abstract base test class BaseModelTest defined as following in the tests/unit/test_models.py file. from {{ program_name }} import models from tests import factories import pytest class BaseModelTest : \"\"\" Abstract base test class to refactor model tests. The Children classes must define the following attributes: self.model: The model object to test. self.dummy_instance: A factory object of the model to test. self.model_attributes: List of model attributes to test Public attributes: dummy_instance (Factory_boy object): Dummy instance of the model. \"\"\" @pytest . fixture ( autouse = True ) def base_setup ( self , session ): self . session = session def test_attributes_defined ( self ): for attribute in self . model_attributes : assert getattr ( self . model , attribute ) == \\ getattr ( self . dummy_instance , attribute ) @pytest . mark . usefixtures ( 'base_setup' ) class TestUser ( BaseModelTest ): @pytest . fixture ( autouse = True ) def setup ( self , session ): self . factory = factories . UserFactory self . dummy_instance = self . factory . create () self . model = models . User ( id = self . dummy_instance . id , name = self . dummy_instance . name , ) self . model_attributes = [ 'name' , 'id' , ] Then create the models table . Create an alembic revision Run pytest : python -m pytest .", "title": "Testing SQLAlchemy Code"}, {"location": "coding/python/sqlalchemy/#exporting-database-to-json", "text": "import json def dump_sqlalchemy ( output_connection_string , output_schema ): \"\"\" Returns the entire content of a database as lists of dicts\"\"\" engine = create_engine ( f ' { output_connection_string }{ output_schema } ' ) meta = MetaData () meta . reflect ( bind = engine ) # http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html result = {} for table in meta . sorted_tables : result [ table . name ] = [ dict ( row ) for row in engine . execute ( table . select ())] return json . dumps ( result )", "title": "Exporting database to json"}, {"location": "coding/python/sqlalchemy/#cloning-an-sqlalchemy-object", "text": "The following function: Copies all the non-primary-key columns from the input model to a new model instance. Allows definition of specific arguments. Leaves the original model object unmodified. def clone_model ( model , ** kwargs ): \"\"\"Clone an arbitrary sqlalchemy model object without its primary key values.\"\"\" table = model . __table__ non_primary_key_columns = [ column_name for column_name in table . __mapper__ . attrs .. keys () if column_name not in table . primary_key ] data = { column_name : getattr ( model , column_name ) for column_name in non_pk_columns } data . update ( kwargs ) return model . __class__ ( ** data )", "title": "Cloning an SQLAlchemy object"}, {"location": "coding/python/sqlalchemy/#references", "text": "Home Docs", "title": "References"}, {"location": "coding/python/tinydb/", "text": "Tinydb is a document oriented database that stores data in a json file. It's the closest solution to a NoSQL SQLite solution that I've found. The advantages are that you can use a NoSQL database without installing a server. Tinydb is small, simple to use, well tested, optimized and extensible . On the other hand, if you are searching for advanced database features like more than one connection or high performance, you should consider using databases like SQLite or MongoDB. I think it's the perfect solution for initial versions of a program, when the database schema is variable and there is no need of high performance. Once the program is stabilized and the performance drops, you can change the storage provider to a production ready one. To make this change doable, I recommend implementing the repository pattern to decouple the storage layer from your application logic. Install \u2691 pip install tinydb Basic usage \u2691 TL;DR: Operation Cheatsheet Inserting data : db.insert(...) . Getting data : db.all() : Get all documents. iter(db) : Iterate over all the documents. db.search(query) : Get a list of documents matching the query. Updating : db.update(fields, query) : Update all documents matching the query to contain fields. Removing : db.remove(query) : Remove all documents matching the query. db.truncate() : Remove all documents. Querying : Query() : Create a new query object. Query().field == 2 : Match any document that has a key field with value == 2 (also possible: != , > , >= , < , <= ). First you need to setup the database: from tinydb import TinyDB , Query db = TinyDB ( 'db.json' ) TinyDB expects the data to be Python dictionaries: db . insert ({ 'type' : 'apple' , 'count' : 7 }) db . insert ({ 'type' : 'peach' , 'count' : 3 }) You can also iterate over stored documents: >>> for item in db : >>> print ( item ) { 'count' : 7 , 'type' : 'apple' } { 'count' : 3 , 'type' : 'peach' } You can search for specific documents: >>> Fruit = Query () >>> db . search ( Fruit . type == 'peach' ) [{ 'count' : 3 , 'type' : 'peach' }] >>> db . search ( Fruit . count > 5 ) [{ 'count' : 7 , 'type' : 'apple' }] You can update fields: >>> db . update ({ 'count' : 10 }, Fruit . type == 'apple' ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }, { 'count' : 3 , 'type' : 'peach' }] And remove documents: >>> db . remove ( Fruit . count < 5 ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }] Query construction \u2691 Match any document where a field called field exists: Query().field.exists() . Match any document with the whole field matching the regular expression: Query().field.matches(regex) . Match any document with a substring of the field matching the regular expression: Query().field.search(regex) . Match any document for which the function returns True : Query().field.test(func, *args) . If given a query, match all documents where all documents in the list field match the query. If given a list, matches all documents where all documents in the list field are a member of the given list: Query().field.all(query | list) . If given a query, match all documents where at least one document in the list field match the query. If given a list, matches all documents where at least one documents in the list field are a member of the given list: Query().field.any(query | list) . Match if the field is contained in the list: Query().field.one_of(list) . Logical operations on queries Match documents that don't match the query: ~ (query) . Match documents that match both queries: (query1) & (query2) . Match documents that match at least one of the queries: (query1) | (query2) . To retrieve the data from the database, you need to use Query objects in a similar way as you do with ORMs. from tinydb import Query User = Query () db . search ( User . name == 'John' ) db . search ( User . birthday . year == 1990 ) If the field is not a valid Python identifier use the following syntax: db . search ( User [ 'country-code' ] == 'foo' ) Advanced queries \u2691 TinyDB supports other ways to search in your data: Testing the existence of a field: db . search ( User . name . exists ()) Testing values against regular expressions: # Full item has to match the regex: db . search ( User . name . matches ( '[aZ]*' )) # Any part of the item has to match the regex: db . search ( User . name . search ( 'b+' )) Testing using custom tests: # Custom test: test_func = lambda s : s == 'John' db . search ( User . name . test ( test_func )) # Custom test with parameters: def test_func ( val , m , n ): return m <= val <= n db . search ( User . age . test ( test_func , 0 , 21 )) db . search ( User . age . test ( test_func , 21 , 99 )) Testing fields that contain lists with the any and all methods: Assuming we have a user object with a groups list like this: db . insert ({ 'name' : 'user1' , 'groups' : [ 'user' ]}) db . insert ({ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}) db . insert ({ 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}) You can use the following queries: # User's groups include at least one value from ['admin', 'sudo'] >>> db . search ( User . groups . any ([ 'admin' , 'sudo' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}, { 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}] # User's groups include all values from ['admin', 'user'] >>> db . search ( User . groups . all ([ 'admin' , 'user' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}] Testing nested queries: Assuming we have the following table: Group = Query () Permission = Query () groups = db . table ( 'groups' ) groups . insert ({ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}) groups . insert ({ 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}) groups . insert ({ 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}) You can search this table using nested any / all queries: # Group has a permission with type 'read' >>> groups . search ( Group . permissions . any ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}, { 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}, { 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}] # Group has ONLY permission 'read' >>> groups . search ( Group . permissions . all ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}] any tests if there is at least one document matching the query while all ensures all documents match the query. The opposite operation, checking if a list contains a single item, is also possible using one_of : >>> db . search ( User . name . one_of ([ 'jane' , 'john' ])) Query modifiers \u2691 TinyDB allows you to use logical operations to change and combine queries Negate a query: db.search(~ (User.name == 'John')) . Logical AND : db.search((User.name == 'John') & (User.age <= 30)) . Logical OR : db.search((User.name == 'John') | (User.name == 'Bob')) . Inserting more than one document \u2691 In case you want to insert more than one document, you can use db.insert_multiple(...) : >>> db . insert_multiple ([ { 'name' : 'John' , 'age' : 22 }, { 'name' : 'John' , 'age' : 37 }]) >>> db . insert_multiple ({ 'int' : 1 , 'value' : i } for i in range ( 2 )) Updating data \u2691 To update all the documents of the database, leave out the query argument: db . update ({ 'foo' : 'bar' }) When you pass a dictionary to db.update(fields, query) , you update a document by adding or overwriting its values. TinyDB also supports some common operations you can do on your data: delete(key) : Delete a key from the document. increment(key) : Increment the value of a key. decrement(key) : Decrement the value of a key. add(key, value) : Add value to the value of a key (also works for strings). subtract(key, value) : Subtract value from the value of a key. set(key, value) : Set key to value. >>> from tinydb.operations import delete >>> db . update ( delete ( 'key1' ), User . name == 'John' ) This will remove the key key1 from all matching documents. You also can write your own operations: >>> def your_operation ( your_arguments ): ... def transform ( doc ): ... # do something with the document ... # ... ... return transform ... >>> db . update ( your_operation ( arguments ), query ) Retrieving data \u2691 If you want to get one element use db.get(...) . Be warned, if more than one document match the query, a random will be returned. >>> db . get ( User . name == 'John' ) { 'name' : 'John' , 'age' : 22 } If you want to know if the database stores a document, use db.contains(...) . >>> db . contains ( User . name == 'John' ) True If you want to know the number of documents that match a query use db.count(...) . >>> db . count ( User . name == 'John' ) 2 Serializing custom data \u2691 TinyDB has a limited support to serialize common objects, they added support for custom serializers but it's not yet documented . Check the tinydb-serialization package to see how to implement your own. Serializing datetime objects \u2691 The tinydb-serialization package gives serialization objects for datetime objects. from tinydb import TinyDB from tinydb.storages import JSONStorage from tinydb_serialization import SerializationMiddleware from tinydb_serialization.serializers import DateTimeSerializer serialization = SerializationMiddleware ( JSONStorage ) serialization . register_serializer ( DateTimeSerializer (), 'TinyDate' ) db = TinyDB ( 'db.json' , storage = serialization ) Tables \u2691 TinyDB supports working with more than one table. To create and use a table, use db.table(name) . They behave as the TinyDB class. >>> table = db . table ( 'table_name' ) >>> table . insert ({ 'value' : True }) >>> table . all () [{ 'value' : True }] >>> for row in table : >>> print ( row ) { 'value' : True } To remove a table from a database, use: db . drop_table ( 'table_name' ) To remove all tables, use: db . drop_tables () To get a list with the names of all tables in your database: >>> db . tables () { '_default' , 'table_name' } Query caching \u2691 TinyDB caches query result for performance. That way re-running a query won't have to read the data from the storage as long as the database hasn't been modified. You can optimize the query cache size by passing the cache_size to the table(...) function: table = db . table ( 'table_name' , cache_size = 30 ) You can set cache_size to None to make the cache unlimited in size. Also, you can set cache_size to 0 to disable it. Storage types \u2691 TinyDB comes with two storage types: JSON and in-memory. By default TinyDB stores its data in JSON files so you have to specify the path where to store it: from tinydb import TinyDB , where db = TinyDB ( 'path/to/db.json' ) To use the in-memory storage, use: from tinydb.storages import MemoryStorage db = TinyDB ( storage = MemoryStorage ) All arguments except for the storage argument are forwarded to the underlying storage. For the JSON storage you can use this to pass additional keyword arguments to Python\u2019s json.dump(\u2026) method. For example, you can set it to create prettified JSON files like this: >>> db = TinyDB ( 'db.json' , sort_keys = True , indent = 4 , separators = ( ',' , ': ' )) References \u2691 Docs Git Issues Reference", "title": "TinyDB"}, {"location": "coding/python/tinydb/#install", "text": "pip install tinydb", "title": "Install"}, {"location": "coding/python/tinydb/#basic-usage", "text": "TL;DR: Operation Cheatsheet Inserting data : db.insert(...) . Getting data : db.all() : Get all documents. iter(db) : Iterate over all the documents. db.search(query) : Get a list of documents matching the query. Updating : db.update(fields, query) : Update all documents matching the query to contain fields. Removing : db.remove(query) : Remove all documents matching the query. db.truncate() : Remove all documents. Querying : Query() : Create a new query object. Query().field == 2 : Match any document that has a key field with value == 2 (also possible: != , > , >= , < , <= ). First you need to setup the database: from tinydb import TinyDB , Query db = TinyDB ( 'db.json' ) TinyDB expects the data to be Python dictionaries: db . insert ({ 'type' : 'apple' , 'count' : 7 }) db . insert ({ 'type' : 'peach' , 'count' : 3 }) You can also iterate over stored documents: >>> for item in db : >>> print ( item ) { 'count' : 7 , 'type' : 'apple' } { 'count' : 3 , 'type' : 'peach' } You can search for specific documents: >>> Fruit = Query () >>> db . search ( Fruit . type == 'peach' ) [{ 'count' : 3 , 'type' : 'peach' }] >>> db . search ( Fruit . count > 5 ) [{ 'count' : 7 , 'type' : 'apple' }] You can update fields: >>> db . update ({ 'count' : 10 }, Fruit . type == 'apple' ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }, { 'count' : 3 , 'type' : 'peach' }] And remove documents: >>> db . remove ( Fruit . count < 5 ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }]", "title": "Basic usage"}, {"location": "coding/python/tinydb/#query-construction", "text": "Match any document where a field called field exists: Query().field.exists() . Match any document with the whole field matching the regular expression: Query().field.matches(regex) . Match any document with a substring of the field matching the regular expression: Query().field.search(regex) . Match any document for which the function returns True : Query().field.test(func, *args) . If given a query, match all documents where all documents in the list field match the query. If given a list, matches all documents where all documents in the list field are a member of the given list: Query().field.all(query | list) . If given a query, match all documents where at least one document in the list field match the query. If given a list, matches all documents where at least one documents in the list field are a member of the given list: Query().field.any(query | list) . Match if the field is contained in the list: Query().field.one_of(list) . Logical operations on queries Match documents that don't match the query: ~ (query) . Match documents that match both queries: (query1) & (query2) . Match documents that match at least one of the queries: (query1) | (query2) . To retrieve the data from the database, you need to use Query objects in a similar way as you do with ORMs. from tinydb import Query User = Query () db . search ( User . name == 'John' ) db . search ( User . birthday . year == 1990 ) If the field is not a valid Python identifier use the following syntax: db . search ( User [ 'country-code' ] == 'foo' )", "title": "Query construction"}, {"location": "coding/python/tinydb/#advanced-queries", "text": "TinyDB supports other ways to search in your data: Testing the existence of a field: db . search ( User . name . exists ()) Testing values against regular expressions: # Full item has to match the regex: db . search ( User . name . matches ( '[aZ]*' )) # Any part of the item has to match the regex: db . search ( User . name . search ( 'b+' )) Testing using custom tests: # Custom test: test_func = lambda s : s == 'John' db . search ( User . name . test ( test_func )) # Custom test with parameters: def test_func ( val , m , n ): return m <= val <= n db . search ( User . age . test ( test_func , 0 , 21 )) db . search ( User . age . test ( test_func , 21 , 99 )) Testing fields that contain lists with the any and all methods: Assuming we have a user object with a groups list like this: db . insert ({ 'name' : 'user1' , 'groups' : [ 'user' ]}) db . insert ({ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}) db . insert ({ 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}) You can use the following queries: # User's groups include at least one value from ['admin', 'sudo'] >>> db . search ( User . groups . any ([ 'admin' , 'sudo' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}, { 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}] # User's groups include all values from ['admin', 'user'] >>> db . search ( User . groups . all ([ 'admin' , 'user' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}] Testing nested queries: Assuming we have the following table: Group = Query () Permission = Query () groups = db . table ( 'groups' ) groups . insert ({ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}) groups . insert ({ 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}) groups . insert ({ 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}) You can search this table using nested any / all queries: # Group has a permission with type 'read' >>> groups . search ( Group . permissions . any ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}, { 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}, { 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}] # Group has ONLY permission 'read' >>> groups . search ( Group . permissions . all ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}] any tests if there is at least one document matching the query while all ensures all documents match the query. The opposite operation, checking if a list contains a single item, is also possible using one_of : >>> db . search ( User . name . one_of ([ 'jane' , 'john' ]))", "title": "Advanced queries"}, {"location": "coding/python/tinydb/#query-modifiers", "text": "TinyDB allows you to use logical operations to change and combine queries Negate a query: db.search(~ (User.name == 'John')) . Logical AND : db.search((User.name == 'John') & (User.age <= 30)) . Logical OR : db.search((User.name == 'John') | (User.name == 'Bob')) .", "title": "Query modifiers"}, {"location": "coding/python/tinydb/#inserting-more-than-one-document", "text": "In case you want to insert more than one document, you can use db.insert_multiple(...) : >>> db . insert_multiple ([ { 'name' : 'John' , 'age' : 22 }, { 'name' : 'John' , 'age' : 37 }]) >>> db . insert_multiple ({ 'int' : 1 , 'value' : i } for i in range ( 2 ))", "title": "Inserting more than one document"}, {"location": "coding/python/tinydb/#updating-data", "text": "To update all the documents of the database, leave out the query argument: db . update ({ 'foo' : 'bar' }) When you pass a dictionary to db.update(fields, query) , you update a document by adding or overwriting its values. TinyDB also supports some common operations you can do on your data: delete(key) : Delete a key from the document. increment(key) : Increment the value of a key. decrement(key) : Decrement the value of a key. add(key, value) : Add value to the value of a key (also works for strings). subtract(key, value) : Subtract value from the value of a key. set(key, value) : Set key to value. >>> from tinydb.operations import delete >>> db . update ( delete ( 'key1' ), User . name == 'John' ) This will remove the key key1 from all matching documents. You also can write your own operations: >>> def your_operation ( your_arguments ): ... def transform ( doc ): ... # do something with the document ... # ... ... return transform ... >>> db . update ( your_operation ( arguments ), query )", "title": "Updating data"}, {"location": "coding/python/tinydb/#retrieving-data", "text": "If you want to get one element use db.get(...) . Be warned, if more than one document match the query, a random will be returned. >>> db . get ( User . name == 'John' ) { 'name' : 'John' , 'age' : 22 } If you want to know if the database stores a document, use db.contains(...) . >>> db . contains ( User . name == 'John' ) True If you want to know the number of documents that match a query use db.count(...) . >>> db . count ( User . name == 'John' ) 2", "title": "Retrieving data"}, {"location": "coding/python/tinydb/#serializing-custom-data", "text": "TinyDB has a limited support to serialize common objects, they added support for custom serializers but it's not yet documented . Check the tinydb-serialization package to see how to implement your own.", "title": "Serializing custom data"}, {"location": "coding/python/tinydb/#serializing-datetime-objects", "text": "The tinydb-serialization package gives serialization objects for datetime objects. from tinydb import TinyDB from tinydb.storages import JSONStorage from tinydb_serialization import SerializationMiddleware from tinydb_serialization.serializers import DateTimeSerializer serialization = SerializationMiddleware ( JSONStorage ) serialization . register_serializer ( DateTimeSerializer (), 'TinyDate' ) db = TinyDB ( 'db.json' , storage = serialization )", "title": "Serializing datetime objects"}, {"location": "coding/python/tinydb/#tables", "text": "TinyDB supports working with more than one table. To create and use a table, use db.table(name) . They behave as the TinyDB class. >>> table = db . table ( 'table_name' ) >>> table . insert ({ 'value' : True }) >>> table . all () [{ 'value' : True }] >>> for row in table : >>> print ( row ) { 'value' : True } To remove a table from a database, use: db . drop_table ( 'table_name' ) To remove all tables, use: db . drop_tables () To get a list with the names of all tables in your database: >>> db . tables () { '_default' , 'table_name' }", "title": "Tables"}, {"location": "coding/python/tinydb/#query-caching", "text": "TinyDB caches query result for performance. That way re-running a query won't have to read the data from the storage as long as the database hasn't been modified. You can optimize the query cache size by passing the cache_size to the table(...) function: table = db . table ( 'table_name' , cache_size = 30 ) You can set cache_size to None to make the cache unlimited in size. Also, you can set cache_size to 0 to disable it.", "title": "Query caching"}, {"location": "coding/python/tinydb/#storage-types", "text": "TinyDB comes with two storage types: JSON and in-memory. By default TinyDB stores its data in JSON files so you have to specify the path where to store it: from tinydb import TinyDB , where db = TinyDB ( 'path/to/db.json' ) To use the in-memory storage, use: from tinydb.storages import MemoryStorage db = TinyDB ( storage = MemoryStorage ) All arguments except for the storage argument are forwarded to the underlying storage. For the JSON storage you can use this to pass additional keyword arguments to Python\u2019s json.dump(\u2026) method. For example, you can set it to create prettified JSON files like this: >>> db = TinyDB ( 'db.json' , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ))", "title": "Storage types"}, {"location": "coding/python/tinydb/#references", "text": "Docs Git Issues Reference", "title": "References"}, {"location": "coding/python/type_hints/", "text": "Type hints are the Python native way to define the type of the objects in a program. Traditionally, the Python interpreter handles types in a flexible but implicit way. Recent versions of Python allow you to specify explicit type hints that different tools can use to help you develop your code more efficiently. TL;DR Use Type hints whenever unit tests are worth writing def headline ( text : str , align : bool = True ) -> str : if align : return f \" { text . title () } \\n { '-' * len ( text ) } \" else : return f \" { text . title () } \" . center ( 50 , \"o\" ) Type hints are not enforced on their own by python. So you won't catch an error if you try to run headline(\"use mypy\", align=\"center\") unless you use a static type checker like Mypy . Advantages and disadvantages \u2691 Advantages: Help catch certain errors if used with a static type checker. Help check your code . It's not trivial to use docstrings to do automatic checks. Help to reason about code: Knowing the parameters type makes it a lot easier to understand and maintain a code base. It can speed up the time required to catch up with a code snippet. Always remember that you read code a lot more often than you write it, so you should optimize for ease of reading. Help you build and maintain a cleaner architecture . The act of writing type hints force you to think about the types in your program. Helps refactoring: Type hints make it trivial to find where a given class is used when you're trying to refactor your code base. Improve IDEs and linters. Cons: Type hints take developer time and effort to add . Even though it probably pays off in spending less time debugging, you will spend more time entering code. Introduce a slight penalty in start-up time . If you need to use the typing module, the import time may be significant, even more in short scripts. Work best in modern Pythons. Follow these guidelines when deciding if you want to add types to your project: In libraries that will be used by others, they add a lot of value. In complex projects, type hints help you understand how types flow through your code and are highly recommended. If you are beginning to learn Python, don't use them yet. If you are writing throw-away scripts, don't use them. So, Use Type hints whenever unit tests are worth writing . Usage \u2691 Function annotations \u2691 def func ( arg : arg_type , optarg : arg_type = default ) -> return_type : ... For arguments the syntax is argument: annotation , while the return type is annotated using -> annotation . Note that the annotation must be a valid Python expression. When running the code, the special .__annotations__ attribute on the function stores the typing information. Variable annotations \u2691 Sometimes the type checker needs help in figuring out the types of variables as well. The syntax is similar: pi : float = 3.142 def circumference ( radius : float ) -> float : return 2 * pi * radius Composite types \u2691 If you need to hint other types than str , float and bool , you'll need to import the typing module. For example to define the hint types of list, dictionaries and tuples: >>> from typing import Dict , List , Tuple >>> names : List [ str ] = [ \"Guido\" , \"Jukka\" , \"Ivan\" ] >>> version : Tuple [ int , int , int ] = ( 3 , 7 , 1 ) >>> options : Dict [ str , bool ] = { \"centered\" : False , \"capitalize\" : True } If your function expects some kind of sequence but don't care whether it's a list or a tuple, use the typing.Sequence object. In fact, try to use Sequence if you can because using List could lead to some unexpected errors when combined with type inference. For example: class A : ... class B ( A ): ... lst = [ A (), A ()] # Inferred type is List[A] new_lst = [ B (), B ()] # inferred type is List[B] lst = new_lst # mypy will complain about this, because List is invariant Possible strategies in such situations are: Use an explicit type annotation: new_lst : List [ A ] = [ B (), B ()] lst = new_lst # OK Make a copy of the right hand side: lst = list ( new_lst ) # Also OK Use immutable collections as annotations whenever possible: def f_bad ( x : List [ A ]) -> A : return x [ 0 ] f_bad ( new_lst ) # Fails def f_good ( x : Sequence [ A ]) -> A : return x [ 0 ] f_good ( new_lst ) # OK Dictionaries with different value types per key . \u2691 TypedDict declares a dictionary type that expects all of its instances to have a certain set of keys, where each key is associated with a value of a consistent type. This expectation is not checked at runtime but is only enforced by type checkers. TypedDict started life as an experimental Mypy feature to wrangle typing onto the heterogeneous, structure-oriented use of dictionaries. As of Python 3.8, it was adopted into the standard library. try : from typing import TypedDict # >=3.8 except ImportError : from mypy_extensions import TypedDict # <=3.7 Movie = TypedDict ( 'Movie' , { 'name' : str , 'year' : int }) A class-based type constructor is also available: class Movie ( TypedDict ): name : str year : int By default, all keys must be present in a TypedDict . It is possible to override this by specifying totality. Usage: class point2D ( TypedDict , total = False ): x : int y : int This means that a point2D TypedDict can have any of the keys omitted. A type checker is only expected to support a literal False or True as the value of the total argument. True is the default, and makes all items defined in the class body be required. Functions without return values \u2691 Some functions aren't meant to return anything. Use the -> None hint in these cases. def play ( player_name : str ) -> None : print ( f \" { player_name } plays\" ) ret_val = play ( \"Filip\" ) The annotation help catch the kinds of subtle bugs where you are trying to use a meaningless return value. If your function doesn't return any object, use the NoReturn type. from typing import NoReturn def black_hole () -> NoReturn : raise Exception ( \"There is no going back ...\" ) Note This is the first iteration of the synoptical reading of the full Real python article on type checking . Optional arguments \u2691 A common pattern is to use None as a default value for an argument. This is done either to avoid problems with mutable default values or to have a sentinel value flagging special behavior. This creates a challenge for type hinting as the argument may be of type string (for example) but it can also be None . We use the Optional type to address this case. from typing import Optional def player ( name : str , start : Optional [ str ] = None ) -> str : ... A similar way would be to use Union[None, str] . Type aliases \u2691 Type hints might become oblique when working with nested types. If it's the case, save them into a new variable, and use that instead. from typing import List , Tuple Card = Tuple [ str , str ] Deck = List [ Card ] def deal_hands ( deck : Deck ) -> Tuple [ Deck , Deck , Deck , Deck ]: \"\"\"Deal the cards in the deck into four hands\"\"\" return ( deck [ 0 :: 4 ], deck [ 1 :: 4 ], deck [ 2 :: 4 ], deck [ 3 :: 4 ]) Allow any subclass \u2691 Every class is also a valid type. Any instance of a subclass is also compatible with all superclasses \u2013 it follows that every value is compatible with the object type (and incidentally also the Any type, discussed below). Mypy analyzes the bodies of classes to determine which methods and attributes are available in instances. For example class A : def f ( self ) -> int : # Type of self inferred (A) return 2 class B ( A ): def f ( self ) -> int : return 3 def g ( self ) -> int : return 4 def foo ( a : A ) -> None : print ( a . f ()) # 3 a . g () # Error: \"A\" has no attribute \"g\" foo ( B ()) # OK (B is a subclass of A) Deduce returned value type from the arguments \u2691 The previous approach works if you don't need to use class objects that inherit from a given class. For example: class User : # Defines fields like name, email class BasicUser ( User ): def upgrade ( self ): \"\"\"Upgrade to Pro\"\"\" class ProUser ( User ): def pay ( self ): \"\"\"Pay bill\"\"\" def new_user ( user_class ) -> User : user = user_class () # (Here we could write the user object to a database) return user Where: ProUser doesn't inherit from BasicUser . new_user creates an instance of one of these classes if you pass it the right class object. The problem is that right now mypy doesn't know which subclass of User you're giving it, and will only accept the methods and attributes defined in the parent class User . buyer = new_user ( ProUser ) buyer . pay () # Rejected, not a method on User This can be solved using Type variables with upper bounds . UserT = TypeVar ( 'UserT' , bound = User ) def new_user ( user_class : Type [ UserT ]) -> UserT : # Same implementation as before We're creating a new type UserT that is linked to the class or subclasses of User . That way, mypy knows that the return value is an object created from the class given in the argument user_class . beginner = new_user ( BasicUser ) # Inferred type is BasicUser beginner . upgrade () # OK Note \"Using UserType is not supported by pylint , use UserT instead.\" Keep in mind that the TypeVar is a Generic type , as such, they take one or more type parameters, similar to built-in types such as List[X] . That means that when you create type aliases, you'll need to give the type parameter. So: UserT = TypeVar ( \"UserT\" , bound = User ) UserTs = List [ Type [ UserT ]] def new_users ( user_class : UserTs ) -> UserT : # Type error! pass Will give a Missing type parameters for generic type \"UserTs\" error. To solve it use: def new_users ( user_class : UserTs [ UserT ]) -> UserT : # OK! pass Define a TypeVar with restrictions \u2691 By default, a type variable can be replaced with any type. However, sometimes it\u2019s useful to have a type variable that can only have some specific types as its value. A typical example is a type variable that can only have values str and bytes : from typing import TypeVar AnyStr = TypeVar ( 'AnyStr' , str , bytes ) This is actually such a common type variable that AnyStr is defined in typing and we don\u2019t need to define it ourselves. We can use AnyStr to define a function that can concatenate two strings or bytes objects, but it can\u2019t be called with other argument types: from typing import AnyStr def concat ( x : AnyStr , y : AnyStr ) -> AnyStr : return x + y concat ( 'a' , 'b' ) # Okay concat ( b 'a' , b 'b' ) # Okay concat ( 1 , 2 ) # Error! Note that this is different from a union type, since combinations of str and bytes are not accepted: concat ( 'string' , b 'bytes' ) # Error! In this case, this is exactly what we want, since it\u2019s not possible to concatenate a string and a bytes object! The type checker will reject this function: def union_concat ( x : Union [ str , bytes ], y : Union [ str , bytes ]) -> Union [ str , bytes ]: return x + y # Error: can't concatenate str and bytes Overloading the methods \u2691 Sometimes the types of several variables are related, such as \u201cif x is type A, y is type B, else y is type C\u201d. Basic type hints cannot describe such relationships, making type checking cumbersome or inaccurate. We can instead use @typing.overload to represent type relationships properly. from __future__ import annotations from collections.abc import Sequence from typing import overload @overload def double ( input_ : int ) -> int : ... @overload def double ( input_ : Sequence [ int ]) -> list [ int ]: ... def double ( input_ : int | Sequence [ int ]) -> int | list [ int ]: if isinstance ( input_ , Sequence ): return [ i * 2 for i in input_ ] return input_ * 2 This looks a bit weird at first glance\u2014we are defining double three times! Let\u2019s take it apart. The first two @overload definitions exist only for their type hints. Each definition represents an allowed combination of types. These definitions never run, so their bodies could contain anything, but it\u2019s idiomatic to use Python\u2019s ... (ellipsis) literal. The third definition is the actual implementation. In this case, we need to provide type hints that union all the possible types for each variable. Without such hints, Mypy will skip type checking the function body. When Mypy checks the file, it collects the @overload definitions as type hints. It then uses the first non- @overload definition as the implementation. All @overload definitions must come before the implementation, and multiple implementations are not allowed. When Python imports the file, the @overload definitions create temporary double functions, but each is overridden by the next definition. After importing, only the implementation exists. As a protection against accidentally missing implementations, attempting to call an @overload definition will raise a NotImplementedError . @overload can represent arbitrarily complex scenarios. For a couple more examples, see the function overloading section of the Mypy docs . Use a constrained TypeVar in the definition of a class attributes. \u2691 If you try to use a TypeVar in the definition of a class attribute: class File : \"\"\"Model a computer file.\"\"\" path : str content : Optional [ AnyStr ] = None # mypy error! mypy will complain with Type variable AnyStr is unbound [valid-type] , to solve it, you need to make the class inherit from the Generic[AnyStr] . class File ( Generic [ AnyStr ]): \"\"\"Model a computer file.\"\"\" path : str content : Optional [ AnyStr ] = None Why you ask? I have absolutely no clue. I've asked that question in the gitter python typing channel but the kind answer that @ktbarrett gave me sounded like Chinese. You can't just use a type variable for attributes or variables, you have to create some generic context, whether that be a function or a class, so that you can instantiate the generic context (or the analyzer can infer it) (i.e. context[var]). That's not possible if you don't specify that the class is a generic context. It also ensure than all uses of that variable in the context resolve to the same type. If you don't mind helping me understand it, please contact me . Specify the type of the class in it's method and attributes \u2691 If you are using Python 3.10 or later, it just works. Python 3.7 introduces PEP 563: postponed evaluation of annotations. A module that uses the future statement from __future__ import annotations to store annotations as strings automatically: from __future__ import annotations class Position : def __add__ ( self , other : Position ) -> Position : ... But pyflakes will still complain, so I've used strings. from __future__ import annotations class Position : def __add__ ( self , other : 'Position' ) -> 'Position' : ... Type hints of Generators \u2691 from typing import Generator def generate () -> Generator [ int , None , None ]: Where the first argument of Generator is the type of the yielded value. Usage of ellipsis on Tuple type hints \u2691 The ellipsis is used to specify an arbitrary-length homogeneous tuples, for example Tuple[int, ...] . Using typing.cast \u2691 Sometimes the type hints of your program don't work as you expect, if you've given up on fixing the issue you can # type: ignore it, but if you know what type you want to enforce, you can use typing.cast() explicitly or implicitly from Any with type hints. With casting we can force the type checker to treat a variable as a given type. This is an ugly patch, always try to fix your types The simplest cast() \u2691 When we call cast() , we pass it two arguments: a type, and a value. cast() returns value unchanged, but type checkers will treat the return value as the given type instead of the input type. For example, we can make Mypy treat an integer as a string: from typing import cast x = 1 reveal_type ( x ) y = cast ( str , x ) reveal_type ( y ) y . upper () Checking this program with Mypy, it doesn't report any errors, but it does debug the types of x and y for us: $ mypy example.py example.py:6: note: Revealed type is \"builtins.int\" example.py:8: note: Revealed type is \"builtins.str\" But, if we remove the reveal_type() calls and run the code, it crashes: $ python example.py Traceback ( most recent call last ) : File \"/.../example.py\" , line 7 , in <module> y.upper () AttributeError: 'int' object has no attribute 'upper' Usually Mypy would detect this bug, as it knows int objects do not have an upper() method. But our cast() forced Mypy to treat y as a str , so it assumed the call would succeed. Use cases \u2691 The main case to reach for cast() are when the type hints for a module are either missing, incomplete, or incorrect. This may be the case for third party packages, or occasionally for things in the standard library. Take this example: import datetime as dt from typing import cast from third_party import get_data data = get_data () last_import_time = cast ( dt . datetime , data [ \"last_import_time\" ]) Imagine get_data() has a return type of dict[str, Any] , rather than using stricter per-key types with a TypedDict . From reading the documentation or source we might find that the last_import_time key always contains a datetime object. Therefore, when we access it, we can wrap it in a cast() , to tell our type checker the real type rather than continuing with Any . When we encounter missing, incomplete, or incorrect type hints, we can contribute back a fix. This may be in the package itself, its related stubs package, or separate stubs in Python\u2019s typeshed. But until such a fix is released, we will need to use cast() to make our code pass type checking. Implicit Casting From Any \u2691 It\u2019s worth noting that Any has special treatment: when we store a variable with type Any in a variable with a specific type, type checkers treat this as an implicit cast. We can thus write our previous example without cast() : import datetime as dt from third_party import get_data data = get_data () last_import_time : dt . datetime = data [ \"last_import_time\" ] This kind of implicit casting is the first tool we should reach for when interacting with libraries that return Any . It also applies when we pass a variable typed Any as a specifically typed function argument or return value. Calling cast() directly is often more useful when dealing with incorrect types other than Any . Mypy\u2019s warn_redundant_casts option \u2691 When we use cast() to override a third party function\u2019s type, that type be corrected in a later version (perhaps from our own PR!). After such an update, the cast() is unnecessary clutter that may confuse readers. We can detect such unnecessary casts by activating Mypy\u2019s warn_redundant_casts option. With this flag turned on, Mypy will log an error for each use of cast() that casts a variable to the type it already has. Using mypy with an existing codebase \u2691 These steps will get you started with mypy on an existing codebase: Start small : Pick a subset of your codebase to run mypy on, without any annotations. You\u2019ll probably need to fix some mypy errors, either by inserting annotations requested by mypy or by adding # type: ignore comments to silence errors you don\u2019t want to fix now. Get a clean mypy build for some files, with some annotations. * Write a mypy runner script to ensure consistent results. Here are some steps you may want to do in the script: * Ensure that you install the correct version of mypy. * Specify mypy config file or command-line options. * Provide set of files to type check. You may want to configure the inclusion and exclusion filters for full control of the file list. * Run mypy in Continuous Integration to prevent type errors : Once you have a clean mypy run and a runner script for a part of your codebase, set up your Continuous Integration (CI) system to run mypy to ensure that developers won\u2019t introduce bad annotations. A small CI script could look something like this: python3 - m pip install mypy == 0.600 # Pinned version avoids surprises scripts / mypy # Runs with the correct options * Gradually annotate commonly imported modules: Most projects have some widely imported modules, such as utilities or model classes. It\u2019s a good idea to annotate these soon, since this allows code using these modules to be type checked more effectively. Since mypy supports gradual typing, it\u2019s okay to leave some of these modules unannotated. The more you annotate, the more useful mypy will be, but even a little annotation coverage is useful. * Write annotations as you change existing code and write new code: Now you are ready to include type annotations in your development workflows. Consider adding something like these in your code style conventions: Developers should add annotations for any new code. It\u2019s also encouraged to write annotations when you change existing code. If you need to ignore a linter error and a type error use first the type and then the linter. For example, # type: ignore # noqa: W0212 . Reveal the type of an expression \u2691 You can use reveal_type(expr) to ask mypy to display the inferred static type of an expression. This can be useful when you don't quite understand how mypy handles a particular piece of code. Example: reveal_type (( 1 , 'hello' )) # Revealed type is 'Tuple[builtins.int, builtins.str]' You can also use reveal_locals() at any line in a file to see the types of all local variables at once. Example: a = 1 b = 'one' reveal_locals () # Revealed local types are: # a: builtins.int # b: builtins.str reveal_type and reveal_locals are only understood by mypy and don't exist in Python. If you try to run your program, you\u2019ll have to remove any reveal_type and reveal_locals calls before you can run your code. Both are always available and you don't need to import them. Solve cyclic imports due to typing \u2691 You can use a conditional import that is only active in \"type hinting mode\", but doesn't interfere at run time. The typing.TYPE_CHECKING constant makes this easily possible. For example: # thing.py from typing import TYPE_CHECKING if TYPE_CHECKING : from connection import ApiConnection class Thing : def __init__ ( self , connection : 'ApiConnection' ): self . _conn = connection The code will now execute properly as there is no circular import issue anymore. Type hinting tools on the other hand should still be able to resolve the ApiConnection type hint in Thing.__init__ . Make your library compatible with mypy \u2691 PEP 561 notes three main ways to distribute type information. The first is a package that has only inline type annotations in the code itself. The second is a package that ships stub files with type information alongside the runtime code. The third method, also known as a \u201cstub only package\u201d is a package that ships type information for a package separately as stub files. If you would like to publish a library package to a package repository (e.g. PyPI) for either internal or external use in type checking, packages that supply type information via type comments or annotations in the code should put a py.typed file in their package directory. For example, with a directory structure as follows setup.py package_a/ __init__.py lib.py py.typed the setup.py might look like: from distutils.core import setup setup ( name = \"SuperPackageA\" , author = \"Me\" , version = \"0.1\" , package_data = { \"package_a\" : [ \"py.typed\" ]}, packages = [ \"package_a\" ] ) If you use setuptools, you must pass the option zip_safe=False to setup() , or mypy will not be able to find the installed package. Reference \u2691 Bernat gabor article on the state of type hints in python Real python article on type checking", "title": "Type Hints"}, {"location": "coding/python/type_hints/#advantages-and-disadvantages", "text": "Advantages: Help catch certain errors if used with a static type checker. Help check your code . It's not trivial to use docstrings to do automatic checks. Help to reason about code: Knowing the parameters type makes it a lot easier to understand and maintain a code base. It can speed up the time required to catch up with a code snippet. Always remember that you read code a lot more often than you write it, so you should optimize for ease of reading. Help you build and maintain a cleaner architecture . The act of writing type hints force you to think about the types in your program. Helps refactoring: Type hints make it trivial to find where a given class is used when you're trying to refactor your code base. Improve IDEs and linters. Cons: Type hints take developer time and effort to add . Even though it probably pays off in spending less time debugging, you will spend more time entering code. Introduce a slight penalty in start-up time . If you need to use the typing module, the import time may be significant, even more in short scripts. Work best in modern Pythons. Follow these guidelines when deciding if you want to add types to your project: In libraries that will be used by others, they add a lot of value. In complex projects, type hints help you understand how types flow through your code and are highly recommended. If you are beginning to learn Python, don't use them yet. If you are writing throw-away scripts, don't use them. So, Use Type hints whenever unit tests are worth writing .", "title": "Advantages and disadvantages"}, {"location": "coding/python/type_hints/#usage", "text": "", "title": "Usage"}, {"location": "coding/python/type_hints/#function-annotations", "text": "def func ( arg : arg_type , optarg : arg_type = default ) -> return_type : ... For arguments the syntax is argument: annotation , while the return type is annotated using -> annotation . Note that the annotation must be a valid Python expression. When running the code, the special .__annotations__ attribute on the function stores the typing information.", "title": "Function annotations"}, {"location": "coding/python/type_hints/#variable-annotations", "text": "Sometimes the type checker needs help in figuring out the types of variables as well. The syntax is similar: pi : float = 3.142 def circumference ( radius : float ) -> float : return 2 * pi * radius", "title": "Variable annotations"}, {"location": "coding/python/type_hints/#composite-types", "text": "If you need to hint other types than str , float and bool , you'll need to import the typing module. For example to define the hint types of list, dictionaries and tuples: >>> from typing import Dict , List , Tuple >>> names : List [ str ] = [ \"Guido\" , \"Jukka\" , \"Ivan\" ] >>> version : Tuple [ int , int , int ] = ( 3 , 7 , 1 ) >>> options : Dict [ str , bool ] = { \"centered\" : False , \"capitalize\" : True } If your function expects some kind of sequence but don't care whether it's a list or a tuple, use the typing.Sequence object. In fact, try to use Sequence if you can because using List could lead to some unexpected errors when combined with type inference. For example: class A : ... class B ( A ): ... lst = [ A (), A ()] # Inferred type is List[A] new_lst = [ B (), B ()] # inferred type is List[B] lst = new_lst # mypy will complain about this, because List is invariant Possible strategies in such situations are: Use an explicit type annotation: new_lst : List [ A ] = [ B (), B ()] lst = new_lst # OK Make a copy of the right hand side: lst = list ( new_lst ) # Also OK Use immutable collections as annotations whenever possible: def f_bad ( x : List [ A ]) -> A : return x [ 0 ] f_bad ( new_lst ) # Fails def f_good ( x : Sequence [ A ]) -> A : return x [ 0 ] f_good ( new_lst ) # OK", "title": "Composite types"}, {"location": "coding/python/type_hints/#dictionaries-with-different-value-types-per-key", "text": "TypedDict declares a dictionary type that expects all of its instances to have a certain set of keys, where each key is associated with a value of a consistent type. This expectation is not checked at runtime but is only enforced by type checkers. TypedDict started life as an experimental Mypy feature to wrangle typing onto the heterogeneous, structure-oriented use of dictionaries. As of Python 3.8, it was adopted into the standard library. try : from typing import TypedDict # >=3.8 except ImportError : from mypy_extensions import TypedDict # <=3.7 Movie = TypedDict ( 'Movie' , { 'name' : str , 'year' : int }) A class-based type constructor is also available: class Movie ( TypedDict ): name : str year : int By default, all keys must be present in a TypedDict . It is possible to override this by specifying totality. Usage: class point2D ( TypedDict , total = False ): x : int y : int This means that a point2D TypedDict can have any of the keys omitted. A type checker is only expected to support a literal False or True as the value of the total argument. True is the default, and makes all items defined in the class body be required.", "title": "Dictionaries with different value types per key."}, {"location": "coding/python/type_hints/#functions-without-return-values", "text": "Some functions aren't meant to return anything. Use the -> None hint in these cases. def play ( player_name : str ) -> None : print ( f \" { player_name } plays\" ) ret_val = play ( \"Filip\" ) The annotation help catch the kinds of subtle bugs where you are trying to use a meaningless return value. If your function doesn't return any object, use the NoReturn type. from typing import NoReturn def black_hole () -> NoReturn : raise Exception ( \"There is no going back ...\" ) Note This is the first iteration of the synoptical reading of the full Real python article on type checking .", "title": "Functions without return values"}, {"location": "coding/python/type_hints/#optional-arguments", "text": "A common pattern is to use None as a default value for an argument. This is done either to avoid problems with mutable default values or to have a sentinel value flagging special behavior. This creates a challenge for type hinting as the argument may be of type string (for example) but it can also be None . We use the Optional type to address this case. from typing import Optional def player ( name : str , start : Optional [ str ] = None ) -> str : ... A similar way would be to use Union[None, str] .", "title": "Optional arguments"}, {"location": "coding/python/type_hints/#type-aliases", "text": "Type hints might become oblique when working with nested types. If it's the case, save them into a new variable, and use that instead. from typing import List , Tuple Card = Tuple [ str , str ] Deck = List [ Card ] def deal_hands ( deck : Deck ) -> Tuple [ Deck , Deck , Deck , Deck ]: \"\"\"Deal the cards in the deck into four hands\"\"\" return ( deck [ 0 :: 4 ], deck [ 1 :: 4 ], deck [ 2 :: 4 ], deck [ 3 :: 4 ])", "title": "Type aliases"}, {"location": "coding/python/type_hints/#allow-any-subclass", "text": "Every class is also a valid type. Any instance of a subclass is also compatible with all superclasses \u2013 it follows that every value is compatible with the object type (and incidentally also the Any type, discussed below). Mypy analyzes the bodies of classes to determine which methods and attributes are available in instances. For example class A : def f ( self ) -> int : # Type of self inferred (A) return 2 class B ( A ): def f ( self ) -> int : return 3 def g ( self ) -> int : return 4 def foo ( a : A ) -> None : print ( a . f ()) # 3 a . g () # Error: \"A\" has no attribute \"g\" foo ( B ()) # OK (B is a subclass of A)", "title": "Allow any subclass"}, {"location": "coding/python/type_hints/#deduce-returned-value-type-from-the-arguments", "text": "The previous approach works if you don't need to use class objects that inherit from a given class. For example: class User : # Defines fields like name, email class BasicUser ( User ): def upgrade ( self ): \"\"\"Upgrade to Pro\"\"\" class ProUser ( User ): def pay ( self ): \"\"\"Pay bill\"\"\" def new_user ( user_class ) -> User : user = user_class () # (Here we could write the user object to a database) return user Where: ProUser doesn't inherit from BasicUser . new_user creates an instance of one of these classes if you pass it the right class object. The problem is that right now mypy doesn't know which subclass of User you're giving it, and will only accept the methods and attributes defined in the parent class User . buyer = new_user ( ProUser ) buyer . pay () # Rejected, not a method on User This can be solved using Type variables with upper bounds . UserT = TypeVar ( 'UserT' , bound = User ) def new_user ( user_class : Type [ UserT ]) -> UserT : # Same implementation as before We're creating a new type UserT that is linked to the class or subclasses of User . That way, mypy knows that the return value is an object created from the class given in the argument user_class . beginner = new_user ( BasicUser ) # Inferred type is BasicUser beginner . upgrade () # OK Note \"Using UserType is not supported by pylint , use UserT instead.\" Keep in mind that the TypeVar is a Generic type , as such, they take one or more type parameters, similar to built-in types such as List[X] . That means that when you create type aliases, you'll need to give the type parameter. So: UserT = TypeVar ( \"UserT\" , bound = User ) UserTs = List [ Type [ UserT ]] def new_users ( user_class : UserTs ) -> UserT : # Type error! pass Will give a Missing type parameters for generic type \"UserTs\" error. To solve it use: def new_users ( user_class : UserTs [ UserT ]) -> UserT : # OK! pass", "title": "Deduce returned value type from the arguments"}, {"location": "coding/python/type_hints/#define-a-typevar-with-restrictions", "text": "By default, a type variable can be replaced with any type. However, sometimes it\u2019s useful to have a type variable that can only have some specific types as its value. A typical example is a type variable that can only have values str and bytes : from typing import TypeVar AnyStr = TypeVar ( 'AnyStr' , str , bytes ) This is actually such a common type variable that AnyStr is defined in typing and we don\u2019t need to define it ourselves. We can use AnyStr to define a function that can concatenate two strings or bytes objects, but it can\u2019t be called with other argument types: from typing import AnyStr def concat ( x : AnyStr , y : AnyStr ) -> AnyStr : return x + y concat ( 'a' , 'b' ) # Okay concat ( b 'a' , b 'b' ) # Okay concat ( 1 , 2 ) # Error! Note that this is different from a union type, since combinations of str and bytes are not accepted: concat ( 'string' , b 'bytes' ) # Error! In this case, this is exactly what we want, since it\u2019s not possible to concatenate a string and a bytes object! The type checker will reject this function: def union_concat ( x : Union [ str , bytes ], y : Union [ str , bytes ]) -> Union [ str , bytes ]: return x + y # Error: can't concatenate str and bytes", "title": "Define a TypeVar with restrictions"}, {"location": "coding/python/type_hints/#overloading-the-methods", "text": "Sometimes the types of several variables are related, such as \u201cif x is type A, y is type B, else y is type C\u201d. Basic type hints cannot describe such relationships, making type checking cumbersome or inaccurate. We can instead use @typing.overload to represent type relationships properly. from __future__ import annotations from collections.abc import Sequence from typing import overload @overload def double ( input_ : int ) -> int : ... @overload def double ( input_ : Sequence [ int ]) -> list [ int ]: ... def double ( input_ : int | Sequence [ int ]) -> int | list [ int ]: if isinstance ( input_ , Sequence ): return [ i * 2 for i in input_ ] return input_ * 2 This looks a bit weird at first glance\u2014we are defining double three times! Let\u2019s take it apart. The first two @overload definitions exist only for their type hints. Each definition represents an allowed combination of types. These definitions never run, so their bodies could contain anything, but it\u2019s idiomatic to use Python\u2019s ... (ellipsis) literal. The third definition is the actual implementation. In this case, we need to provide type hints that union all the possible types for each variable. Without such hints, Mypy will skip type checking the function body. When Mypy checks the file, it collects the @overload definitions as type hints. It then uses the first non- @overload definition as the implementation. All @overload definitions must come before the implementation, and multiple implementations are not allowed. When Python imports the file, the @overload definitions create temporary double functions, but each is overridden by the next definition. After importing, only the implementation exists. As a protection against accidentally missing implementations, attempting to call an @overload definition will raise a NotImplementedError . @overload can represent arbitrarily complex scenarios. For a couple more examples, see the function overloading section of the Mypy docs .", "title": "Overloading the methods"}, {"location": "coding/python/type_hints/#use-a-constrained-typevar-in-the-definition-of-a-class-attributes", "text": "If you try to use a TypeVar in the definition of a class attribute: class File : \"\"\"Model a computer file.\"\"\" path : str content : Optional [ AnyStr ] = None # mypy error! mypy will complain with Type variable AnyStr is unbound [valid-type] , to solve it, you need to make the class inherit from the Generic[AnyStr] . class File ( Generic [ AnyStr ]): \"\"\"Model a computer file.\"\"\" path : str content : Optional [ AnyStr ] = None Why you ask? I have absolutely no clue. I've asked that question in the gitter python typing channel but the kind answer that @ktbarrett gave me sounded like Chinese. You can't just use a type variable for attributes or variables, you have to create some generic context, whether that be a function or a class, so that you can instantiate the generic context (or the analyzer can infer it) (i.e. context[var]). That's not possible if you don't specify that the class is a generic context. It also ensure than all uses of that variable in the context resolve to the same type. If you don't mind helping me understand it, please contact me .", "title": "Use a constrained TypeVar in the definition of a class attributes."}, {"location": "coding/python/type_hints/#specify-the-type-of-the-class-in-its-method-and-attributes", "text": "If you are using Python 3.10 or later, it just works. Python 3.7 introduces PEP 563: postponed evaluation of annotations. A module that uses the future statement from __future__ import annotations to store annotations as strings automatically: from __future__ import annotations class Position : def __add__ ( self , other : Position ) -> Position : ... But pyflakes will still complain, so I've used strings. from __future__ import annotations class Position : def __add__ ( self , other : 'Position' ) -> 'Position' : ...", "title": "Specify the type of the class in it's method and attributes"}, {"location": "coding/python/type_hints/#type-hints-of-generators", "text": "from typing import Generator def generate () -> Generator [ int , None , None ]: Where the first argument of Generator is the type of the yielded value.", "title": "Type hints of Generators"}, {"location": "coding/python/type_hints/#usage-of-ellipsis-on-tuple-type-hints", "text": "The ellipsis is used to specify an arbitrary-length homogeneous tuples, for example Tuple[int, ...] .", "title": "Usage of ellipsis on Tuple type hints"}, {"location": "coding/python/type_hints/#using-typingcast", "text": "Sometimes the type hints of your program don't work as you expect, if you've given up on fixing the issue you can # type: ignore it, but if you know what type you want to enforce, you can use typing.cast() explicitly or implicitly from Any with type hints. With casting we can force the type checker to treat a variable as a given type. This is an ugly patch, always try to fix your types", "title": "Using typing.cast"}, {"location": "coding/python/type_hints/#the-simplest-cast", "text": "When we call cast() , we pass it two arguments: a type, and a value. cast() returns value unchanged, but type checkers will treat the return value as the given type instead of the input type. For example, we can make Mypy treat an integer as a string: from typing import cast x = 1 reveal_type ( x ) y = cast ( str , x ) reveal_type ( y ) y . upper () Checking this program with Mypy, it doesn't report any errors, but it does debug the types of x and y for us: $ mypy example.py example.py:6: note: Revealed type is \"builtins.int\" example.py:8: note: Revealed type is \"builtins.str\" But, if we remove the reveal_type() calls and run the code, it crashes: $ python example.py Traceback ( most recent call last ) : File \"/.../example.py\" , line 7 , in <module> y.upper () AttributeError: 'int' object has no attribute 'upper' Usually Mypy would detect this bug, as it knows int objects do not have an upper() method. But our cast() forced Mypy to treat y as a str , so it assumed the call would succeed.", "title": "The simplest cast()"}, {"location": "coding/python/type_hints/#use-cases", "text": "The main case to reach for cast() are when the type hints for a module are either missing, incomplete, or incorrect. This may be the case for third party packages, or occasionally for things in the standard library. Take this example: import datetime as dt from typing import cast from third_party import get_data data = get_data () last_import_time = cast ( dt . datetime , data [ \"last_import_time\" ]) Imagine get_data() has a return type of dict[str, Any] , rather than using stricter per-key types with a TypedDict . From reading the documentation or source we might find that the last_import_time key always contains a datetime object. Therefore, when we access it, we can wrap it in a cast() , to tell our type checker the real type rather than continuing with Any . When we encounter missing, incomplete, or incorrect type hints, we can contribute back a fix. This may be in the package itself, its related stubs package, or separate stubs in Python\u2019s typeshed. But until such a fix is released, we will need to use cast() to make our code pass type checking.", "title": "Use cases"}, {"location": "coding/python/type_hints/#implicit-casting-from-any", "text": "It\u2019s worth noting that Any has special treatment: when we store a variable with type Any in a variable with a specific type, type checkers treat this as an implicit cast. We can thus write our previous example without cast() : import datetime as dt from third_party import get_data data = get_data () last_import_time : dt . datetime = data [ \"last_import_time\" ] This kind of implicit casting is the first tool we should reach for when interacting with libraries that return Any . It also applies when we pass a variable typed Any as a specifically typed function argument or return value. Calling cast() directly is often more useful when dealing with incorrect types other than Any .", "title": "Implicit Casting From Any"}, {"location": "coding/python/type_hints/#mypys-warn_redundant_casts-option", "text": "When we use cast() to override a third party function\u2019s type, that type be corrected in a later version (perhaps from our own PR!). After such an update, the cast() is unnecessary clutter that may confuse readers. We can detect such unnecessary casts by activating Mypy\u2019s warn_redundant_casts option. With this flag turned on, Mypy will log an error for each use of cast() that casts a variable to the type it already has.", "title": "Mypy\u2019s warn_redundant_casts option"}, {"location": "coding/python/type_hints/#using-mypy-with-an-existing-codebase", "text": "These steps will get you started with mypy on an existing codebase: Start small : Pick a subset of your codebase to run mypy on, without any annotations. You\u2019ll probably need to fix some mypy errors, either by inserting annotations requested by mypy or by adding # type: ignore comments to silence errors you don\u2019t want to fix now. Get a clean mypy build for some files, with some annotations. * Write a mypy runner script to ensure consistent results. Here are some steps you may want to do in the script: * Ensure that you install the correct version of mypy. * Specify mypy config file or command-line options. * Provide set of files to type check. You may want to configure the inclusion and exclusion filters for full control of the file list. * Run mypy in Continuous Integration to prevent type errors : Once you have a clean mypy run and a runner script for a part of your codebase, set up your Continuous Integration (CI) system to run mypy to ensure that developers won\u2019t introduce bad annotations. A small CI script could look something like this: python3 - m pip install mypy == 0.600 # Pinned version avoids surprises scripts / mypy # Runs with the correct options * Gradually annotate commonly imported modules: Most projects have some widely imported modules, such as utilities or model classes. It\u2019s a good idea to annotate these soon, since this allows code using these modules to be type checked more effectively. Since mypy supports gradual typing, it\u2019s okay to leave some of these modules unannotated. The more you annotate, the more useful mypy will be, but even a little annotation coverage is useful. * Write annotations as you change existing code and write new code: Now you are ready to include type annotations in your development workflows. Consider adding something like these in your code style conventions: Developers should add annotations for any new code. It\u2019s also encouraged to write annotations when you change existing code. If you need to ignore a linter error and a type error use first the type and then the linter. For example, # type: ignore # noqa: W0212 .", "title": "Using mypy with an existing codebase"}, {"location": "coding/python/type_hints/#reveal-the-type-of-an-expression", "text": "You can use reveal_type(expr) to ask mypy to display the inferred static type of an expression. This can be useful when you don't quite understand how mypy handles a particular piece of code. Example: reveal_type (( 1 , 'hello' )) # Revealed type is 'Tuple[builtins.int, builtins.str]' You can also use reveal_locals() at any line in a file to see the types of all local variables at once. Example: a = 1 b = 'one' reveal_locals () # Revealed local types are: # a: builtins.int # b: builtins.str reveal_type and reveal_locals are only understood by mypy and don't exist in Python. If you try to run your program, you\u2019ll have to remove any reveal_type and reveal_locals calls before you can run your code. Both are always available and you don't need to import them.", "title": "Reveal the type of an expression"}, {"location": "coding/python/type_hints/#solve-cyclic-imports-due-to-typing", "text": "You can use a conditional import that is only active in \"type hinting mode\", but doesn't interfere at run time. The typing.TYPE_CHECKING constant makes this easily possible. For example: # thing.py from typing import TYPE_CHECKING if TYPE_CHECKING : from connection import ApiConnection class Thing : def __init__ ( self , connection : 'ApiConnection' ): self . _conn = connection The code will now execute properly as there is no circular import issue anymore. Type hinting tools on the other hand should still be able to resolve the ApiConnection type hint in Thing.__init__ .", "title": "Solve cyclic imports due to typing"}, {"location": "coding/python/type_hints/#make-your-library-compatible-with-mypy", "text": "PEP 561 notes three main ways to distribute type information. The first is a package that has only inline type annotations in the code itself. The second is a package that ships stub files with type information alongside the runtime code. The third method, also known as a \u201cstub only package\u201d is a package that ships type information for a package separately as stub files. If you would like to publish a library package to a package repository (e.g. PyPI) for either internal or external use in type checking, packages that supply type information via type comments or annotations in the code should put a py.typed file in their package directory. For example, with a directory structure as follows setup.py package_a/ __init__.py lib.py py.typed the setup.py might look like: from distutils.core import setup setup ( name = \"SuperPackageA\" , author = \"Me\" , version = \"0.1\" , package_data = { \"package_a\" : [ \"py.typed\" ]}, packages = [ \"package_a\" ] ) If you use setuptools, you must pass the option zip_safe=False to setup() , or mypy will not be able to find the installed package.", "title": "Make your library compatible with mypy"}, {"location": "coding/python/type_hints/#reference", "text": "Bernat gabor article on the state of type hints in python Real python article on type checking", "title": "Reference"}, {"location": "coding/python/yoyo/", "text": "Yoyo is a database schema migration tool. Migrations are written as SQL files or Python scripts that define a list of migration steps. Installation \u2691 pip install yoyo-migrations Usage \u2691 Command line \u2691 Start a new migration: yoyo new ./migrations -m \"Add column to foo\" Apply migrations from directory migrations to a PostgreSQL database: yoyo apply --database postgresql://scott:tiger@localhost/db ./migrations Rollback migrations previously applied to a MySQL database: yoyo rollback --database mysql://scott:tiger@localhost/database ./migrations Reapply (ie rollback then apply again) migrations to a SQLite database at location /home/sheila/important.db: yoyo reapply --database sqlite:////home/sheila/important.db ./migrations List available migrations: yoyo list --database sqlite:////home/sheila/important.db ./migrations By default, yoyo-migrations starts in an interactive mode, prompting you for each migration file before applying it, making it easy to preview which migrations to apply and rollback. Connecting to the database \u2691 Database connections are specified using a URL. Examples: # SQLite: use 4 slashes for an absolute database path on unix like platforms database = sqlite : //// home / user / mydb . sqlite # SQLite: use 3 slashes for a relative path database = sqlite : /// mydb . sqlite # SQLite: absolute path on Windows. database = sqlite : /// c : \\ home \\ user \\ mydb . sqlite # MySQL: Network database connection database = mysql : // scott : tiger @localhost / mydatabase # MySQL: unix socket connection database = mysql : // scott : tiger @/ mydatabase ? unix_socket =/ tmp / mysql . sock # MySQL with the MySQLdb driver (instead of pymysql) database = mysql + mysqldb : // scott : tiger @localhost / mydatabase # MySQL with SSL/TLS enabled database = mysql + mysqldb : // scott : tiger @localhost / mydatabase ? ssl = yes & sslca =/ path / to / cert # PostgreSQL: database connection database = postgresql : // scott : tiger @localhost / mydatabase # PostgreSQL: unix socket connection database = postgresql : // scott : tiger @/ mydatabase # PostgreSQL: changing the schema (via set search_path) database = postgresql : // scott : tiger @/ mydatabase ? schema = some_schema You can specify your database username and password either as part of the database connection string on the command line (exposing your database password in the process list) or in a configuration file where other users may be able to read it. The -p or --prompt-password flag causes yoyo to prompt for a password, helping prevent your credentials from being leaked. Migration files \u2691 The migrations directory contains a series of migration scripts. Each migration script is a Python (.py) or SQL file (.sql). The name of each file without the extension is used as the migration\u2019s unique identifier. Migrations scripts are run in dependency then filename order. Each migration file is run in a single transaction where this is supported by the database. Yoyo creates tables in your target database to track which migrations have been applied. Migrations as Python scripts \u2691 A migration script written in Python has the following structure: # # file: migrations/0001_create_foo.py # from yoyo import step __depends__ = { \"0000.initial-schema\" } steps = [ step ( \"CREATE TABLE foo (id INT, bar VARCHAR(20), PRIMARY KEY (id))\" , \"DROP TABLE foo\" , ), step ( \"ALTER TABLE foo ADD COLUMN baz INT NOT NULL\" ) ] The step function may take up to 3 arguments: apply : an SQL query (or Python function, see below) to apply the migration step. rollback : (optional) an SQL query (or Python function) to rollback the migration step. ignore_errors : (optional, one of apply , rollback or all ) causes yoyo to ignore database errors in either the apply stage, rollback stage or both. Migrations may declare dependencies on other migrations via the __depends__ attribute: If you use the yoyo new command the __depends__ attribute will be auto populated for you. Migration steps as Python functions \u2691 If SQL is not flexible enough, you may supply a Python function as either or both of the apply or rollback arguments of step. Each function should take a database connection as its only argument: # # file: migrations/0001_create_foo.py # from yoyo import step def apply_step ( conn ): cursor = conn . cursor () cursor . execute ( # query to perform the migration ) def rollback_step ( conn ): cursor = conn . cursor () cursor . execute ( # query to undo the above ) steps = [ step ( apply_step , rollback_step ) ] Post-apply hook \u2691 It can be useful to have a script that is run after every successful migration. For example you could use this to update database permissions or re-create views. To do this, create a special migration file called post-apply.py . Configuration file \u2691 Yoyo looks for a configuration file named yoyo.ini in the current working directory or any ancestor directory. The configuration file may contain the following options: [DEFAULT] # List of migration source directories. \"%(here)s\" is expanded to the # full path of the directory containing this ini file. sources = %(here)s/migrations %(here)s/lib/module/migrations # Target database database = postgresql://scott:tiger@localhost/mydb # Verbosity level. Goes from 0 (least verbose) to 3 (most verbose) verbosity = 3 # Disable interactive features batch_mode = on # Editor to use when starting new migrations # \"{}\" is expanded to the filename of the new migration editor = /usr/local/bin/vim -f {} # An arbitrary command to run after a migration has been created # \"{}\" is expanded to the filename of the new migration post_create_command = hg add {} # A prefix to use for generated migration filenames prefix = myproject_ Calling Yoyo from Python code \u2691 The following example shows how to apply migrations from inside python code: from yoyo import read_migrations from yoyo import get_backend backend = get_backend ( 'postgres://myuser@localhost/mydatabase' ) migrations = read_migrations ( 'path/to/migrations' ) with backend . lock (): # Apply any outstanding migrations backend . apply_migrations ( backend . to_apply ( migrations )) # Rollback all migrations backend . rollback_migrations ( backend . to_rollback ( migrations )) References \u2691 Docs Source Issue Tracker Mailing list", "title": "Yoyo"}, {"location": "coding/python/yoyo/#installation", "text": "pip install yoyo-migrations", "title": "Installation"}, {"location": "coding/python/yoyo/#usage", "text": "", "title": "Usage"}, {"location": "coding/python/yoyo/#command-line", "text": "Start a new migration: yoyo new ./migrations -m \"Add column to foo\" Apply migrations from directory migrations to a PostgreSQL database: yoyo apply --database postgresql://scott:tiger@localhost/db ./migrations Rollback migrations previously applied to a MySQL database: yoyo rollback --database mysql://scott:tiger@localhost/database ./migrations Reapply (ie rollback then apply again) migrations to a SQLite database at location /home/sheila/important.db: yoyo reapply --database sqlite:////home/sheila/important.db ./migrations List available migrations: yoyo list --database sqlite:////home/sheila/important.db ./migrations By default, yoyo-migrations starts in an interactive mode, prompting you for each migration file before applying it, making it easy to preview which migrations to apply and rollback.", "title": "Command line"}, {"location": "coding/python/yoyo/#connecting-to-the-database", "text": "Database connections are specified using a URL. Examples: # SQLite: use 4 slashes for an absolute database path on unix like platforms database = sqlite : //// home / user / mydb . sqlite # SQLite: use 3 slashes for a relative path database = sqlite : /// mydb . sqlite # SQLite: absolute path on Windows. database = sqlite : /// c : \\ home \\ user \\ mydb . sqlite # MySQL: Network database connection database = mysql : // scott : tiger @localhost / mydatabase # MySQL: unix socket connection database = mysql : // scott : tiger @/ mydatabase ? unix_socket =/ tmp / mysql . sock # MySQL with the MySQLdb driver (instead of pymysql) database = mysql + mysqldb : // scott : tiger @localhost / mydatabase # MySQL with SSL/TLS enabled database = mysql + mysqldb : // scott : tiger @localhost / mydatabase ? ssl = yes & sslca =/ path / to / cert # PostgreSQL: database connection database = postgresql : // scott : tiger @localhost / mydatabase # PostgreSQL: unix socket connection database = postgresql : // scott : tiger @/ mydatabase # PostgreSQL: changing the schema (via set search_path) database = postgresql : // scott : tiger @/ mydatabase ? schema = some_schema You can specify your database username and password either as part of the database connection string on the command line (exposing your database password in the process list) or in a configuration file where other users may be able to read it. The -p or --prompt-password flag causes yoyo to prompt for a password, helping prevent your credentials from being leaked.", "title": "Connecting to the database"}, {"location": "coding/python/yoyo/#migration-files", "text": "The migrations directory contains a series of migration scripts. Each migration script is a Python (.py) or SQL file (.sql). The name of each file without the extension is used as the migration\u2019s unique identifier. Migrations scripts are run in dependency then filename order. Each migration file is run in a single transaction where this is supported by the database. Yoyo creates tables in your target database to track which migrations have been applied.", "title": "Migration files"}, {"location": "coding/python/yoyo/#migrations-as-python-scripts", "text": "A migration script written in Python has the following structure: # # file: migrations/0001_create_foo.py # from yoyo import step __depends__ = { \"0000.initial-schema\" } steps = [ step ( \"CREATE TABLE foo (id INT, bar VARCHAR(20), PRIMARY KEY (id))\" , \"DROP TABLE foo\" , ), step ( \"ALTER TABLE foo ADD COLUMN baz INT NOT NULL\" ) ] The step function may take up to 3 arguments: apply : an SQL query (or Python function, see below) to apply the migration step. rollback : (optional) an SQL query (or Python function) to rollback the migration step. ignore_errors : (optional, one of apply , rollback or all ) causes yoyo to ignore database errors in either the apply stage, rollback stage or both. Migrations may declare dependencies on other migrations via the __depends__ attribute: If you use the yoyo new command the __depends__ attribute will be auto populated for you.", "title": "Migrations as Python scripts"}, {"location": "coding/python/yoyo/#migration-steps-as-python-functions", "text": "If SQL is not flexible enough, you may supply a Python function as either or both of the apply or rollback arguments of step. Each function should take a database connection as its only argument: # # file: migrations/0001_create_foo.py # from yoyo import step def apply_step ( conn ): cursor = conn . cursor () cursor . execute ( # query to perform the migration ) def rollback_step ( conn ): cursor = conn . cursor () cursor . execute ( # query to undo the above ) steps = [ step ( apply_step , rollback_step ) ]", "title": "Migration steps as Python functions"}, {"location": "coding/python/yoyo/#post-apply-hook", "text": "It can be useful to have a script that is run after every successful migration. For example you could use this to update database permissions or re-create views. To do this, create a special migration file called post-apply.py .", "title": "Post-apply hook"}, {"location": "coding/python/yoyo/#configuration-file", "text": "Yoyo looks for a configuration file named yoyo.ini in the current working directory or any ancestor directory. The configuration file may contain the following options: [DEFAULT] # List of migration source directories. \"%(here)s\" is expanded to the # full path of the directory containing this ini file. sources = %(here)s/migrations %(here)s/lib/module/migrations # Target database database = postgresql://scott:tiger@localhost/mydb # Verbosity level. Goes from 0 (least verbose) to 3 (most verbose) verbosity = 3 # Disable interactive features batch_mode = on # Editor to use when starting new migrations # \"{}\" is expanded to the filename of the new migration editor = /usr/local/bin/vim -f {} # An arbitrary command to run after a migration has been created # \"{}\" is expanded to the filename of the new migration post_create_command = hg add {} # A prefix to use for generated migration filenames prefix = myproject_", "title": "Configuration file"}, {"location": "coding/python/yoyo/#calling-yoyo-from-python-code", "text": "The following example shows how to apply migrations from inside python code: from yoyo import read_migrations from yoyo import get_backend backend = get_backend ( 'postgres://myuser@localhost/mydatabase' ) migrations = read_migrations ( 'path/to/migrations' ) with backend . lock (): # Apply any outstanding migrations backend . apply_migrations ( backend . to_apply ( migrations )) # Rollback all migrations backend . rollback_migrations ( backend . to_rollback ( migrations ))", "title": "Calling Yoyo from Python code"}, {"location": "coding/python/yoyo/#references", "text": "Docs Source Issue Tracker Mailing list", "title": "References"}, {"location": "coding/python/python_project_template/python_cli_template/", "text": "Create the tests directories mkdir -p tests/unit touch tests/__init__.py touch tests/unit/__init__.py Create the program module structure mkdir {{ program_name }} Create the program setup.py file from setuptools import find_packages , setup # Get the version from drode/version.py without importing the package exec ( compile ( open ( '{{ program_name }}/version.py' ) . read (), '{{ program_name }}/version.py' , 'exec' )) setup ( name = '{{ program_name }}' , version = __version__ , # noqa: F821 description = '{{ program_description }}' , author = '{{ author }}' , author_email = '{{ author_email }}' , license = 'GPLv3' , long_description = open ( 'README.md' ) . read (), packages = find_packages ( exclude = ( 'tests' ,)), package_data = { '{{ program_name }}' : [ 'migrations/*' , 'migrations/versions/*' , ]}, entry_points = { 'console_scripts' : [ '{{ program_name }} = {{ program_name }}:main' ]}, install_requires = [ ] ) Remember to fill up the install_requirements with the dependencies that need to be installed at installation time. Create the {{ program_name }}/version.py file with the following contents: __version__ = '1.0.0' This ugly way of loading the __version__ was stolen from youtube-dl, it loads and executes the version.py without loading the whole module. Solutions like from {{ program_name }}.version import __version__ will fail as it tries to load the whole module. Defining it in the setup.py file doesn't work either if you need to load the version in your program code. from setup.py import __version__ will also fail. The only problem with this approach is that as the __version__ is not defined in the code it will raise a Flake8 error, therefore the # noqa: F821 in the setup.py code. Create the requirements.txt file. It should contain the install_requirements in addition to the testing requirements such as: pytest pytest-cov Configure SQLAlchemy for projects without flask", "title": "Command-line Project Template"}, {"location": "coding/python/python_project_template/python_docker/", "text": "Docker is a popular way to distribute applications. Assuming that you've set all required dependencies in the setup.py , we're going to create an image with these properties: Run by an unprivileged user : Create an unprivileged user with permissions to run our program. Robust to vulnerabilities: Don't use Alpine as it's known to react slow to new vulnerabilities. Use a base of Debian instead. Smallest possible: Use Docker multi build step. Create a builder Docker that will run pip install and copies the required executables to the final image. FROM python:3.8-slim-buster as base FROM base as builder RUN python -m venv /opt/venv # Make sure we use the virtualenv: ENV PATH = \"/opt/venv/bin: $PATH \" COPY . /app WORKDIR /app RUN pip install . FROM base COPY --from = builder /opt/venv /opt/venv RUN useradd -m myapp WORKDIR /home/myapp # Copy the required directories for your program to work. COPY --from = builder /root/.local/share/myapp /home/myapp/.local/share/myapp COPY --from = builder /app/myapp /home/myapp/myapp RUN chown -R myapp:myapp /home/myapp/.local USER myapp ENV PATH = \"/opt/venv/bin: $PATH \" ENTRYPOINT [ \"/opt/venv/bin/myapp\" ] If we need to use it with MariaDB or with Redis, the easiest way is to use docker-compose . version : '3.8' services : myapp : image : myapp:latest restart : always links : - db depends_on : - db environment : - AIRSS_DATABASE_URL=mysql+pymysql://myapp:supersecurepassword@db/myapp db : image : mariadb:latest restart : always environment : - MYSQL_USER=myapp - MYSQL_PASSWORD=supersecurepassword - MYSQL_DATABASE=myapp - MYSQL_ALLOW_EMPTY_PASSWORD=yes ports : - 3306:3306 command : - '--character-set-server=utf8mb4' - '--collation-server=utf8mb4_unicode_ci' volumes : - /data/myapp/mariadb:/var/lib/mysql The depends_on flag is not enough to ensure that the database is up when our application tries to connect. So we need to use external programs like wait-for-it . To use it, change the earlier Dockerfile to match these lines: ... FROM base RUN apt-get update && apt-get install -y \\ wait-for-it \\ && rm -rf /var/lib/apt/lists/* ... ENTRYPOINT [ \"/home/myapp/entrypoint.sh\" ] Where entrypoint.sh is something like: #!/bin/bash # Wait for the database to be up if [[ -n $DATABASE_URL ]] ; then wait-for-it db:3306 fi # Execute database migrations /opt/venv/bin/myapp install # Enter in daemon mode /opt/venv/bin/myapp daemon Remember to add the permissions to run the script: chmod +x entrypoint.sh", "title": "Configure Docker to host the application"}, {"location": "coding/python/python_project_template/python_docs/", "text": "It's important to create the documentation at the same time as you code your project, otherwise you won't ever do it or you'll die trying. Right now I use mkdocs with Github Pages for the documentation. Follow the steps under Installation to configure it. I've automated the creation of the mkdocs site in this cookiecutter template .", "title": "Create the documentation repository"}, {"location": "coding/python/python_project_template/python_flask_template/", "text": "Flask is very flexible when it comes to define the project layout, as a result, there are several different approaches, which can be confusing if you're building your first application. Follow this template if you want an application that meets these requirements: Use SQLAlchemy as ORM. Use pytest as testing framework (instead of unittest). Sets a robust foundation for application growth. Set a clear defined project structure that can be used for frontend applications as well as backend APIs. Microservice friendly. I've crafted this template after studying the following projects: Miguel's Flask mega tutorial ( code ). Greb Obinna Flask-RESTPlus tutorial ( code ). Abhinav Suri Flask tutorial ( code ). Patrick's software blog project layout and pytest definition ( code ). Jaime Buelta Hands On Docker for Microservices with Python ( code ). Each has it's strengths and weaknesses: Project Alembic Pytest Complex app Friendly layout Strong points Miguel True False True False Has a book explaining the code Greb False False False False flask-restplus Abhinav True False True True flask-base Patrick False True False True pytest Jaime False True True False Microservices, CI, Kubernetes, logging, metrics I'm going to start with Abhinav base layout as it's the most clear and complete. Furthermore, it's based in flask-base , a simple Flask boilerplate app with SQLAlchemy, Redis, User Authentication, and more. Which can be used directly to start a frontend flask project. I won't use it for a backend API though. With that base layout, I'm going to take Patrick's pytest layout to configure the tests using pytest-flask , Greb flask-restplus code to create the API and Miguel's book to glue everything together. Finally, I'll follow Jaime's book to merge the different microservices into an integrated project. As well as defining the deployment process, CI definition, logging, metrics and integration with Kubernetes.", "title": "Flask Project Template"}, {"location": "coding/python/python_project_template/python_microservices_template/", "text": "Follow this template if you want to build a project that meets these requirements: Based on Python Flask microservices. Easily expandable. Tested by unit, functional and integration tests through continuous integration pipelines. Deployed through uWSGI and Nginx dockers. Orchestrated through docker-compose or Kubernetes. Defining the project layout of a flask application is not easy , even less one with several", "title": "Microservices Project Template"}, {"location": "coding/python/python_project_template/python_sqlalchemy_mariadb/", "text": "I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions . To use Mysql you'll need to first install (or add to your requirements) pymysql : pip install pymysql The url to connect to the database will be: 'mysql+pymysql:// {} : {} @ {} : {} / {} ' . format ( DB_USER , DB_PASS , DB_HOST , DB_PORT , DATABASE ) It's probable that you'll need to use UTF8 with multi byte , otherwise the addition of some strings into the database will fail. I've tried adding it to the database url without success. So I've modified the MariaDB Docker-compose section to use that character and collation set: services : db : image : mariadb:latest restart : always environment : - MYSQL_USER=xxxx - MYSQL_PASSWORD=xxxx - MYSQL_DATABASE=xxxx - MYSQL_ALLOW_EMPTY_PASSWORD=yes ports : - 3306:3306 command : - '--character-set-server=utf8mb4' - '--collation-server=utf8mb4_unicode_ci'", "title": "Configure SQLAlchemy to use the MariaDB/Mysql backend"}, {"location": "coding/python/python_project_template/python_sqlalchemy_without_flask/", "text": "I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions . Install Alembic : pip install alembic It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations Create the basic models.py file under the project code. \"\"\" Module to store the models. Classes: Class_name: Class description. ... \"\"\" import os from sqlalchemy import \\ create_engine , \\ Column , \\ Integer from sqlalchemy.ext.declarative import declarative_base db_path = os . path . expanduser ( '{{ path_to_sqlite_file }}' ) engine = create_engine ( os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or 'sqlite:///' + db_path ) Base = declarative_base ( bind = engine ) class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) Create the migrations/env.py file as specified in the alembic article . Create the first alembic revision. alembic \\ -c {{ program_name }} /migrations/alembic.ini \\ revision \\ --autogenerate \\ -m \"Initial schema\" Set up the testing environment for SQLAlchemy", "title": "Configure SQLAlchemy for projects without flask"}, {"location": "coding/react/react/", "text": "React is a declarative, efficient, and flexible JavaScript library for building user interfaces. It lets you compose complex UIs from small and isolated pieces of code called \u201ccomponents\u201d. Set up a new project \u2691 Install Node.js Create the project baseline with Create React App . Using this tool avoids: Learning and configuring many build tools. Optimize your bundles. Worry about the incompatibility of versions between the underlying pieces. So you can focus on the development of your code. npx create-react-app my-app Delete all files in the src/ folder of the new project. cd my-app rm src/* Create the basic files index.css , index.js in the src directory. Run the server: npm start . Start a React + Flask project \u2691 Create the api directory. mkdir api Make the virtualenv. mkvirtualenv \\ --python = python3 \\ -a ~/projects/my-app \\ my-app Install flask. pip install flask python-dotenv Add a basic file to api/api.py . import time from flask import Flask app = Flask ( __name__ ) @app . route ( '/api/time' ) def get_current_time (): return { 'time' : time . time ()} Create the .flaskenv file. FLASK_APP = api/api.py FLASK_ENV = development Make sure everything is alright. flask run The basics \u2691 Components tell React what to show on the screen. When the data changes, React will efficiently update and re-render the components. class ShoppingList extends React . Component { render () { return ( < div className = \"shopping-list\" > < h1 > Shopping List for { this . props . name } < /h1> < ul > < li > Instagram < /li> < li > WhatsApp < /li> < li > Oculus < /li> < /ul> < /div> ); } } // Example usage: <ShoppingList name=\"Mark\" /> ShoppingList is a React component class , or React component type . A component takes in parameters, called props (short for \u201cproperties\u201d), and returns a hierarchy of views to display via the render method. The render method returns a description of what you want to see on the screen. React takes the description and displays the result. In particular, render returns a React element , which is a lightweight description of what to render. Most React developers use a special syntax called \u201cJSX\u201d which makes these structures easier to write. The syntax is transformed at build time to React.createElement('div'). The example above is equivalent to: return React . createElement ( 'div' , { className : 'shopping-list' }, React . createElement ( 'h1' , /* ... h1 children ... */ ), React . createElement ( 'ul' , /* ... ul children ... */ ) ); The ShoppingList component above only renders built-in DOM components like <div /> and <li /> . But it can compose and render custom React components too. For example, Use <ShoppingList /> to refer to the whole shopping list. Each React component is encapsulated and can operate independently; this allows the building of complex UIs from simple components. Pass data between components \u2691 Data is passed between components through the props method. class Square extends React . Component { render () { return ( < button className = \"square\" > { this . props . value } < /button> ); } } class Board extends React . Component { renderSquare ( i ) { return < Square value = { i } /> ; } ... } Use of the state \u2691 React components can have state by setting this.state in their constructors. this.state should be considered as private to a React component that it\u2019s defined in. Components need a constructor to initialize the state. In JavaScript classes, it's required to call super when defining the constructor of a subclass. All React component classes that have a constructor should start with a super(props) call. class Square extends React . Component { constructor ( props ){ super ( props ); this . state = { value : null , } } render () { ... } } Then use the this.setState method to set the value ... render () { return ( < button className = \"square\" onClick = {() => this . setState ({ value : 'X' })} > { this . state . value } < /button> ); } Share the state between parent and children \u2691 To collect data from multiple children, or to have two child components communicate with each other, you need to declare the shared state in their parent component instead. The parent component can pass the state back down to the children by using props; this keeps the child components in sync with each other and with the parent component. First define the parent state class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } class Board extends React . Component { constructor ( props ) { super ( props ); this . state = { squares : Array ( 9 ). fill ( null ), }; } handleClick ( i ) { const squares = this . state . squares . slice (); squares [ i ] = 'X' ; this . setState ({ squares : squares }); } renderSquare ( i ) { return < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ; } ... } Since state is considered to be private to a component that defines it, it's not possible to update the parent\u2019s state directly from the children. Instead, A function needs to be passed down from the parent to the children, so the children can call that function when required. For example the onClick={() => this.handleClick(i)} in the example above. When a Square is clicked, the onClick function provided by the Board is called. Here\u2019s a review of how this is achieved: The onClick prop on the built-in DOM <button> component tells React to set up a click event listener. When the button is clicked, React will call the onClick event handler that is defined in Square \u2019s render() method. This event handler calls this.props.onClick() . The Square \u2019s onClick prop was specified by the Board . Since the Board passed onClick={() => this.handleClick(i)} to Square , the Square calls this.handleClick(i) when clicked. So now the state is stored in Board instead of the individual Square components. When the Board \u2019s state changes, the Square components re-render automatically. In React terms, the Square components are now controlled components. Handling data change \u2691 There are generally two approaches to changing data. The first one is to mutate the data by directly changing the it\u2019s values. The second is to replace the data with a new copy which has the desired changes. Data Change with Mutation. var player = { score : 1 , name : 'Jeff' }; player . score = 2 ; // Now player is {score: 2, name: 'Jeff'} Data Change without Mutation. var player = { score : 1 , name : 'Jeff' }; var newPlayer = Object . assign ({}, player , { score : 2 }); // Now player is unchanged, but newPlayer is {score: 2, name: 'Jeff'} // Or if you are using object spread syntax proposal, you can write: // var newPlayer = {...player, score: 2}; By not mutating directly, several benefits are gained: Complex features become simple: Immutability makes complex features much easier to implement. Detecting Changes: Detecting changes in mutable objects is difficult because they are modified directly. This detection requires the mutable object to be compared to previous copies of itself and the entire object tree to be traversed. Detecting changes in immutable objects is considerably easier. If the immutable object that is being referenced is different than the previous one, then the object has changed. Determining When to Re-Render in React: The main benefit of immutability is that it helps you build pure components in React. Immutable data can easily determine if changes have been made which helps to determine when a component requires re-rendering. Function components \u2691 Function components are a simpler way to write components that only contain a render method and don\u2019t have their own state. Instead of defining a class which extends React.Component , we can write a function that takes props as input and returns what should be rendered. Function components are less tedious to write than classes, and many components can be expressed this way. Instead of class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } Use function Square ( props ) { return ( < button ClassName = \"square\" onClick = { props . onClick } > { props . value } < /button> ); } Miscellaneous \u2691 List rendering \u2691 When React renders a list, it stores some information about each rendered list item. Once a list is updated, React needs to determine what has changed. A key property needs to be specified for each list item to differentiate each list item from its siblings. So for: < li > Ben : 9 tasks left < /li> < li > Claudia : 8 tasks left < /li> < li > Alexa : 5 tasks left < /li> < li key = { user . id } > { user . name } : { user . taskCount } tasks left < /li> Could be used. When a list is re-rendered, React takes each list item\u2019s key and searches the previous list\u2019s items for a matching key. If the current list has a key that didn\u2019t exist before, React creates a component. If the current list is missing a key that existed in the previous list, React destroys the previous component. If two keys match, the corresponding component is moved. Keys tell React about the identity of each compone. Keys tell React about the identity of each component which allows React to maintain state between re-renders. If a component\u2019s key changes, the component will be destroyed and re-created with a new state. key is a special and reserved property in React. When an element is created, React extracts the key property and stores the key directly on the returned element. Even though key may look like it belongs in props , key cannot be referenced using this.props.key . React automatically uses key to decide which components to update. A component cannot inquire about its key . Links \u2691 React tutorial Awesome React Awesome React components Responsive React \u2691 Responsive react Responsive websites without css react-responsive library With Flask \u2691 How to create a react + flask project How to deploy a react + flask project", "title": "React"}, {"location": "coding/react/react/#set-up-a-new-project", "text": "Install Node.js Create the project baseline with Create React App . Using this tool avoids: Learning and configuring many build tools. Optimize your bundles. Worry about the incompatibility of versions between the underlying pieces. So you can focus on the development of your code. npx create-react-app my-app Delete all files in the src/ folder of the new project. cd my-app rm src/* Create the basic files index.css , index.js in the src directory. Run the server: npm start .", "title": "Set up a new project"}, {"location": "coding/react/react/#start-a-react-flask-project", "text": "Create the api directory. mkdir api Make the virtualenv. mkvirtualenv \\ --python = python3 \\ -a ~/projects/my-app \\ my-app Install flask. pip install flask python-dotenv Add a basic file to api/api.py . import time from flask import Flask app = Flask ( __name__ ) @app . route ( '/api/time' ) def get_current_time (): return { 'time' : time . time ()} Create the .flaskenv file. FLASK_APP = api/api.py FLASK_ENV = development Make sure everything is alright. flask run", "title": "Start a React + Flask project"}, {"location": "coding/react/react/#the-basics", "text": "Components tell React what to show on the screen. When the data changes, React will efficiently update and re-render the components. class ShoppingList extends React . Component { render () { return ( < div className = \"shopping-list\" > < h1 > Shopping List for { this . props . name } < /h1> < ul > < li > Instagram < /li> < li > WhatsApp < /li> < li > Oculus < /li> < /ul> < /div> ); } } // Example usage: <ShoppingList name=\"Mark\" /> ShoppingList is a React component class , or React component type . A component takes in parameters, called props (short for \u201cproperties\u201d), and returns a hierarchy of views to display via the render method. The render method returns a description of what you want to see on the screen. React takes the description and displays the result. In particular, render returns a React element , which is a lightweight description of what to render. Most React developers use a special syntax called \u201cJSX\u201d which makes these structures easier to write. The syntax is transformed at build time to React.createElement('div'). The example above is equivalent to: return React . createElement ( 'div' , { className : 'shopping-list' }, React . createElement ( 'h1' , /* ... h1 children ... */ ), React . createElement ( 'ul' , /* ... ul children ... */ ) ); The ShoppingList component above only renders built-in DOM components like <div /> and <li /> . But it can compose and render custom React components too. For example, Use <ShoppingList /> to refer to the whole shopping list. Each React component is encapsulated and can operate independently; this allows the building of complex UIs from simple components.", "title": "The basics"}, {"location": "coding/react/react/#pass-data-between-components", "text": "Data is passed between components through the props method. class Square extends React . Component { render () { return ( < button className = \"square\" > { this . props . value } < /button> ); } } class Board extends React . Component { renderSquare ( i ) { return < Square value = { i } /> ; } ... }", "title": "Pass data between components"}, {"location": "coding/react/react/#use-of-the-state", "text": "React components can have state by setting this.state in their constructors. this.state should be considered as private to a React component that it\u2019s defined in. Components need a constructor to initialize the state. In JavaScript classes, it's required to call super when defining the constructor of a subclass. All React component classes that have a constructor should start with a super(props) call. class Square extends React . Component { constructor ( props ){ super ( props ); this . state = { value : null , } } render () { ... } } Then use the this.setState method to set the value ... render () { return ( < button className = \"square\" onClick = {() => this . setState ({ value : 'X' })} > { this . state . value } < /button> ); }", "title": "Use of the state"}, {"location": "coding/react/react/#share-the-state-between-parent-and-children", "text": "To collect data from multiple children, or to have two child components communicate with each other, you need to declare the shared state in their parent component instead. The parent component can pass the state back down to the children by using props; this keeps the child components in sync with each other and with the parent component. First define the parent state class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } class Board extends React . Component { constructor ( props ) { super ( props ); this . state = { squares : Array ( 9 ). fill ( null ), }; } handleClick ( i ) { const squares = this . state . squares . slice (); squares [ i ] = 'X' ; this . setState ({ squares : squares }); } renderSquare ( i ) { return < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ; } ... } Since state is considered to be private to a component that defines it, it's not possible to update the parent\u2019s state directly from the children. Instead, A function needs to be passed down from the parent to the children, so the children can call that function when required. For example the onClick={() => this.handleClick(i)} in the example above. When a Square is clicked, the onClick function provided by the Board is called. Here\u2019s a review of how this is achieved: The onClick prop on the built-in DOM <button> component tells React to set up a click event listener. When the button is clicked, React will call the onClick event handler that is defined in Square \u2019s render() method. This event handler calls this.props.onClick() . The Square \u2019s onClick prop was specified by the Board . Since the Board passed onClick={() => this.handleClick(i)} to Square , the Square calls this.handleClick(i) when clicked. So now the state is stored in Board instead of the individual Square components. When the Board \u2019s state changes, the Square components re-render automatically. In React terms, the Square components are now controlled components.", "title": "Share the state between parent and children"}, {"location": "coding/react/react/#handling-data-change", "text": "There are generally two approaches to changing data. The first one is to mutate the data by directly changing the it\u2019s values. The second is to replace the data with a new copy which has the desired changes. Data Change with Mutation. var player = { score : 1 , name : 'Jeff' }; player . score = 2 ; // Now player is {score: 2, name: 'Jeff'} Data Change without Mutation. var player = { score : 1 , name : 'Jeff' }; var newPlayer = Object . assign ({}, player , { score : 2 }); // Now player is unchanged, but newPlayer is {score: 2, name: 'Jeff'} // Or if you are using object spread syntax proposal, you can write: // var newPlayer = {...player, score: 2}; By not mutating directly, several benefits are gained: Complex features become simple: Immutability makes complex features much easier to implement. Detecting Changes: Detecting changes in mutable objects is difficult because they are modified directly. This detection requires the mutable object to be compared to previous copies of itself and the entire object tree to be traversed. Detecting changes in immutable objects is considerably easier. If the immutable object that is being referenced is different than the previous one, then the object has changed. Determining When to Re-Render in React: The main benefit of immutability is that it helps you build pure components in React. Immutable data can easily determine if changes have been made which helps to determine when a component requires re-rendering.", "title": "Handling data change"}, {"location": "coding/react/react/#function-components", "text": "Function components are a simpler way to write components that only contain a render method and don\u2019t have their own state. Instead of defining a class which extends React.Component , we can write a function that takes props as input and returns what should be rendered. Function components are less tedious to write than classes, and many components can be expressed this way. Instead of class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } Use function Square ( props ) { return ( < button ClassName = \"square\" onClick = { props . onClick } > { props . value } < /button> ); }", "title": "Function components"}, {"location": "coding/react/react/#miscellaneous", "text": "", "title": "Miscellaneous"}, {"location": "coding/react/react/#list-rendering", "text": "When React renders a list, it stores some information about each rendered list item. Once a list is updated, React needs to determine what has changed. A key property needs to be specified for each list item to differentiate each list item from its siblings. So for: < li > Ben : 9 tasks left < /li> < li > Claudia : 8 tasks left < /li> < li > Alexa : 5 tasks left < /li> < li key = { user . id } > { user . name } : { user . taskCount } tasks left < /li> Could be used. When a list is re-rendered, React takes each list item\u2019s key and searches the previous list\u2019s items for a matching key. If the current list has a key that didn\u2019t exist before, React creates a component. If the current list is missing a key that existed in the previous list, React destroys the previous component. If two keys match, the corresponding component is moved. Keys tell React about the identity of each compone. Keys tell React about the identity of each component which allows React to maintain state between re-renders. If a component\u2019s key changes, the component will be destroyed and re-created with a new state. key is a special and reserved property in React. When an element is created, React extracts the key property and stores the key directly on the returned element. Even though key may look like it belongs in props , key cannot be referenced using this.props.key . React automatically uses key to decide which components to update. A component cannot inquire about its key .", "title": "List rendering"}, {"location": "coding/react/react/#links", "text": "React tutorial Awesome React Awesome React components", "title": "Links"}, {"location": "coding/react/react/#responsive-react", "text": "Responsive react Responsive websites without css react-responsive library", "title": "Responsive React"}, {"location": "coding/react/react/#with-flask", "text": "How to create a react + flask project How to deploy a react + flask project", "title": "With Flask"}, {"location": "coding/sql/sql/", "text": "SQL Data Types \u2691 String data types: VARCHAR(size) : A variable length string (can contain letters, numbers, and special characters). The size parameter specifies the maximum column length in characters - can be from 0 to 65535. TEXT(size) : Holds a string with a maximum length of 65,535 bytes. MEDIUMTEXT : Holds a string with a maximum length of 16,777,215 characters. LONGTEXT : Holds a string with a maximum length of 4,294,967,295 characters. ENUM(val1, val2, val3, ...) : A string object that can have only one value, chosen from a list of possible values. You can list up to 65535 values in an ENUM list. If a value is inserted that is not in the list, a blank value will be inserted. The values are sorted in the order you enter them. SET(val1, val2, val3, ...) : A string object that can have 0 or more values, chosen from a list of possible values. You can list up to 64 values in a SET list. Numeric data types: BOOL or BOOLEAN : Zero is considered as false, nonzero values are considered as true. TINYINT(size) : A very small integer. Signed range is from -128 to 127. Unsigned range is from 0 to 255. The size parameter specifies the maximum display width (which is 255). SMALLINT(size) : A small integer. Signed range is from -32768 to 32767. Unsigned range is from 0 to 65535. The size parameter specifies the maximum display width (which is 255). INT(size) : A medium integer. Signed range is from -2147483648 to 2147483647. Unsigned range is from 0 to 4294967295. The size parameter specifies the maximum display width (which is 255). FLOAT(p) : A floating point number. MySQL uses the p value to determine whether to use FLOAT or DOUBLE for the resulting data type. If p is from 0 to 24, the data type becomes FLOAT() . If p is from 25 to 53, the data type becomes DOUBLE() . Date and time data types: DATE : A date. Format: YYYY-MM-DD. The supported range is from '1000-01-01' to '9999-12-31'. DATETIME(fsp) : A date and time combination. Format: YYYY-MM-DD hh:mm:ss . The supported range is from 1000-01-01 00:00:00 to 9999-12-31 23:59:59 . Adding DEFAULT and ON UPDATE in the column definition to get automatic initialization and updating to the current date and time. List all tables \u2691 Mysql: show tables ; Postgresql: \\ dt Sqlite: . tables Table relationships \u2691 One to One \u2691 A one-to-one relationship between two entities exists when a particular entity instance exists in one table, and it can have only one associated entity instance in another table. For example, a user can have only one address, and an address belongs to only one user. This sort of relationship is implemented by setting the PRIMARY KEY of the users table ( id ) as both the FOREIGN KEY and PRIMARY KEY of the addresses table. CREATE TABLE addresses ( user_id int , -- Both a primary and foreign key street varchar ( 30 ) NOT NULL , city varchar ( 30 ) NOT NULL , state varchar ( 30 ) NOT NULL , PRIMARY KEY ( user_id ), FOREIGN KEY ( user_id ) REFERENCES users ( id ) ON DELETE CASCADE ); The ON DELETE CASCADE clause of the FOREIGN_KEY definition instructs the database to delete the referencing row if the referenced row is deleted. There are alternatives to CASCADE such as SET NULL or SET DEFAULT which instead of deleting the referencing row will set a new value in the appropriate column for that row. One to many \u2691 A one-to-many relationship exists between two entities if an entity instance in one of the tables can be associated with multiple records (entity instances) in the other table. The opposite relationship does not exist; that is, each entity instance in the second table can only be associated with one entity instance in the first table. For example, a review belongs to only one book while a book has many reviews. CREATE TABLE books ( id serial , title varchar ( 100 ) NOT NULL , author varchar ( 100 ) NOT NULL , published_date timestamp NOT NULL , isbn char ( 12 ), PRIMARY KEY ( id ), UNIQUE ( isbn ) ); /* one to many: Book has many reviews */ CREATE TABLE reviews ( id serial , book_id integer NOT NULL , reviewer_name varchar ( 255 ), content varchar ( 255 ), rating integer , published_date timestamp DEFAULT CURRENT_TIMESTAMP , PRIMARY KEY ( id ), FOREIGN KEY ( book_id ) REFERENCES books ( id ) ON DELETE CASCADE ); Unlike our addresses table, the PRIMARY KEY and FOREIGN KEY reference different columns, id and book_id respectively. This means that the FOREIGN KEY column, book_id is not bound by the UNIQUE constraint of our PRIMARY KEY and so the same value from the id column of the books table can appear in this column more than once. In other words a book can have many reviews. Many to many \u2691 A many-to-many relationship exists between two entities if for one entity instance there may be multiple records in the other table, and vice versa. For example, a user can check out many books, and a book can be checked out by many users. In order to implement this sort of relationship we need to introduce a third, cross-reference, table. This table holds the relationship between the two entities, by having two FOREIGN KEY s, each of which references the PRIMARY KEY of one of the tables for which we want to create this relationship. We already have our books and users tables, so we just need to create the cross-reference table: checkouts . CREATE TABLE checkouts ( id serial , user_id int NOT NULL , book_id int NOT NULL , checkout_date timestamp , return_date timestamp , PRIMARY KEY ( id ), FOREIGN KEY ( user_id ) REFERENCES users ( id ) ON DELETE CASCADE , FOREIGN KEY ( book_id ) REFERENCES books ( id ) ON DELETE CASCADE ); Joins \u2691 A JOIN clause is used to combine rows from two or more tables, based on a related column between them. One to one join \u2691 SELECT users . id , addresses . street FROM users LEFT JOIN addresses ON users . id = addresses . user_id It will return one line. One to many join \u2691 SELECT books . id , reviews . rating FROM books LEFT JOIN reviews ON books . id = reviews . book_id It will return many lines. Many to many join \u2691 SELECT users . id , books . id FROM users LEFT OUTER JOIN checkouts ON users . id == checkouts . user_id Left OUTER JOIN books ON checkouts . book_id == books . id", "title": "SQL"}, {"location": "coding/sql/sql/#sql-data-types", "text": "String data types: VARCHAR(size) : A variable length string (can contain letters, numbers, and special characters). The size parameter specifies the maximum column length in characters - can be from 0 to 65535. TEXT(size) : Holds a string with a maximum length of 65,535 bytes. MEDIUMTEXT : Holds a string with a maximum length of 16,777,215 characters. LONGTEXT : Holds a string with a maximum length of 4,294,967,295 characters. ENUM(val1, val2, val3, ...) : A string object that can have only one value, chosen from a list of possible values. You can list up to 65535 values in an ENUM list. If a value is inserted that is not in the list, a blank value will be inserted. The values are sorted in the order you enter them. SET(val1, val2, val3, ...) : A string object that can have 0 or more values, chosen from a list of possible values. You can list up to 64 values in a SET list. Numeric data types: BOOL or BOOLEAN : Zero is considered as false, nonzero values are considered as true. TINYINT(size) : A very small integer. Signed range is from -128 to 127. Unsigned range is from 0 to 255. The size parameter specifies the maximum display width (which is 255). SMALLINT(size) : A small integer. Signed range is from -32768 to 32767. Unsigned range is from 0 to 65535. The size parameter specifies the maximum display width (which is 255). INT(size) : A medium integer. Signed range is from -2147483648 to 2147483647. Unsigned range is from 0 to 4294967295. The size parameter specifies the maximum display width (which is 255). FLOAT(p) : A floating point number. MySQL uses the p value to determine whether to use FLOAT or DOUBLE for the resulting data type. If p is from 0 to 24, the data type becomes FLOAT() . If p is from 25 to 53, the data type becomes DOUBLE() . Date and time data types: DATE : A date. Format: YYYY-MM-DD. The supported range is from '1000-01-01' to '9999-12-31'. DATETIME(fsp) : A date and time combination. Format: YYYY-MM-DD hh:mm:ss . The supported range is from 1000-01-01 00:00:00 to 9999-12-31 23:59:59 . Adding DEFAULT and ON UPDATE in the column definition to get automatic initialization and updating to the current date and time.", "title": "SQL Data Types"}, {"location": "coding/sql/sql/#list-all-tables", "text": "Mysql: show tables ; Postgresql: \\ dt Sqlite: . tables", "title": "List all tables"}, {"location": "coding/sql/sql/#table-relationships", "text": "", "title": "Table relationships"}, {"location": "coding/sql/sql/#one-to-one", "text": "A one-to-one relationship between two entities exists when a particular entity instance exists in one table, and it can have only one associated entity instance in another table. For example, a user can have only one address, and an address belongs to only one user. This sort of relationship is implemented by setting the PRIMARY KEY of the users table ( id ) as both the FOREIGN KEY and PRIMARY KEY of the addresses table. CREATE TABLE addresses ( user_id int , -- Both a primary and foreign key street varchar ( 30 ) NOT NULL , city varchar ( 30 ) NOT NULL , state varchar ( 30 ) NOT NULL , PRIMARY KEY ( user_id ), FOREIGN KEY ( user_id ) REFERENCES users ( id ) ON DELETE CASCADE ); The ON DELETE CASCADE clause of the FOREIGN_KEY definition instructs the database to delete the referencing row if the referenced row is deleted. There are alternatives to CASCADE such as SET NULL or SET DEFAULT which instead of deleting the referencing row will set a new value in the appropriate column for that row.", "title": "One to One"}, {"location": "coding/sql/sql/#one-to-many", "text": "A one-to-many relationship exists between two entities if an entity instance in one of the tables can be associated with multiple records (entity instances) in the other table. The opposite relationship does not exist; that is, each entity instance in the second table can only be associated with one entity instance in the first table. For example, a review belongs to only one book while a book has many reviews. CREATE TABLE books ( id serial , title varchar ( 100 ) NOT NULL , author varchar ( 100 ) NOT NULL , published_date timestamp NOT NULL , isbn char ( 12 ), PRIMARY KEY ( id ), UNIQUE ( isbn ) ); /* one to many: Book has many reviews */ CREATE TABLE reviews ( id serial , book_id integer NOT NULL , reviewer_name varchar ( 255 ), content varchar ( 255 ), rating integer , published_date timestamp DEFAULT CURRENT_TIMESTAMP , PRIMARY KEY ( id ), FOREIGN KEY ( book_id ) REFERENCES books ( id ) ON DELETE CASCADE ); Unlike our addresses table, the PRIMARY KEY and FOREIGN KEY reference different columns, id and book_id respectively. This means that the FOREIGN KEY column, book_id is not bound by the UNIQUE constraint of our PRIMARY KEY and so the same value from the id column of the books table can appear in this column more than once. In other words a book can have many reviews.", "title": "One to many"}, {"location": "coding/sql/sql/#many-to-many", "text": "A many-to-many relationship exists between two entities if for one entity instance there may be multiple records in the other table, and vice versa. For example, a user can check out many books, and a book can be checked out by many users. In order to implement this sort of relationship we need to introduce a third, cross-reference, table. This table holds the relationship between the two entities, by having two FOREIGN KEY s, each of which references the PRIMARY KEY of one of the tables for which we want to create this relationship. We already have our books and users tables, so we just need to create the cross-reference table: checkouts . CREATE TABLE checkouts ( id serial , user_id int NOT NULL , book_id int NOT NULL , checkout_date timestamp , return_date timestamp , PRIMARY KEY ( id ), FOREIGN KEY ( user_id ) REFERENCES users ( id ) ON DELETE CASCADE , FOREIGN KEY ( book_id ) REFERENCES books ( id ) ON DELETE CASCADE );", "title": "Many to many"}, {"location": "coding/sql/sql/#joins", "text": "A JOIN clause is used to combine rows from two or more tables, based on a related column between them.", "title": "Joins"}, {"location": "coding/sql/sql/#one-to-one-join", "text": "SELECT users . id , addresses . street FROM users LEFT JOIN addresses ON users . id = addresses . user_id It will return one line.", "title": "One to one join"}, {"location": "coding/sql/sql/#one-to-many-join", "text": "SELECT books . id , reviews . rating FROM books LEFT JOIN reviews ON books . id = reviews . book_id It will return many lines.", "title": "One to many join"}, {"location": "coding/sql/sql/#many-to-many-join", "text": "SELECT users . id , books . id FROM users LEFT OUTER JOIN checkouts ON users . id == checkouts . user_id Left OUTER JOIN books ON checkouts . book_id == books . id", "title": "Many to many join"}, {"location": "coding/yaml/yaml/", "text": "YAML (a recursive acronym for \"YAML Ain't Markup Language\") is a human-readable data-serialization language. It is commonly used for configuration files and in applications where data is being stored or transmitted. YAML targets many of the same communications applications as Extensible Markup Language (XML) but has a minimal syntax which intentionally differs from SGML. It uses both Python-style indentation to indicate nesting, and a more compact format that uses [...] for lists and {...} for maps making YAML 1.2 a superset of JSON. Break long lines \u2691 Use > most of the time: interior line breaks are stripped out, although you get one at the end: key : > Your long string here. Use | if you want those line breaks to be preserved as \\n (for instance, embedded markdown with paragraphs): key : | ### Heading * Bullet * Points Use >- or |- instead if you don't want a line break appended at the end. Use \"...\" if you need to split lines in the middle of words or want to literally type line breaks as \\n : key : \"Antidisestab\\ lishmentarianism.\\n\\nGet on it.\" YAML is crazy.", "title": "YAML"}, {"location": "coding/yaml/yaml/#break-long-lines", "text": "Use > most of the time: interior line breaks are stripped out, although you get one at the end: key : > Your long string here. Use | if you want those line breaks to be preserved as \\n (for instance, embedded markdown with paragraphs): key : | ### Heading * Bullet * Points Use >- or |- instead if you don't want a line break appended at the end. Use \"...\" if you need to split lines in the middle of words or want to literally type line breaks as \\n : key : \"Antidisestab\\ lishmentarianism.\\n\\nGet on it.\" YAML is crazy.", "title": "Break long lines"}, {"location": "data_analysis/recommender_systems/recommender_systems/", "text": "A recommender system, or a recommendation system , is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. The entity to which the recommendation is provided is referred to as the user, and the product being recommended is also referred to as an item. Therefore, recommendation analysis is often based on the previous interaction between users and items, because past interests and proclivities are often good indicators of future choices. These relations can be learned in a data-driven manner from the ratings matrix, and the resulting model is used to make predictions for target users.The larger the number of rated items that are available for a user, the easier it is to make robust predictions about the future behavior of the user. The problem may be formulated in two ways: Prediction version of problem: This approach aims to predict the rating value for a user-item combination. It is assumed that training data is available, indicating user preferences for items. For m users and n items, this corresponds to an incomplete m x n matrix, hwere the specified (or observed) values are used for training. The missing (or unobserved) values are predicted using this training model. This problem is also referred to as the matrix completion problem . Ranking version of problem: This approach aims to recommend the top- k items for a particular user, or determine the top- k users to target for a particular item. Being the first one more common. The problem is also referred to as the top-k recommendation problem . Goals of recommender systems \u2691 The common operational and technical goals of recommender systems are: Relevance : Recommend items that are relevant to the user at hand. Although it's the primary operational goal, it is not sufficient in isolation. Novelty : Recommend items that the user has not seen in the past. Serendipity : Recommend items that are outside the user's bubble, rather than simply something they did not know about before. For example, if a new Indian restaurant opens in a neighborhood, then the recommendation of that restaurant to a user who normally eats Indian food is novel but not necessarily serendipitous. On the other hand, when the same user is recommended Ethiopian food, and it was unknown to the user that such food might appeal to her, then the recommendation is serendipitous. Serendipity has the beneficial side effect of beginning new areas of interest. Increase recommendation diversity: Recommend items that aren't similar to increase the chances that the user likes at least one of these items. Basic Models of recommender systems \u2691 There are four types of basic models: Collaborative filtering : Use collaborative power of the ratings provided by multiple users to make recommendations. Content-based : Use the descriptive attributes of the user rated items to create a user-specific model that predicts the rating of unobserved items. Knowledge-based : Use the similarities between customer requirements and item descriptions. Hybrid systems : Combine the above to benefit from the mix of their strengths to perform more robustly. Collaborative Filtering Models \u2691 These models use the collaborative power of the ratings provided by multiple users to make recommendations. The main challenge is that the underlying ratings matrices are sparse. To solve it, unspecified ratings are guessed by analyzing the relations of high correlation across various users and items. There are two common types of methods. Memory-based methods : Also referred to as neighborhood-based collaborative filtering algorithms , predict ratings on the basis of their neighborhoods. These can be defined through two ways: User-based collaborative filtering : Ratings provided by like-minded users of a target user A are used in order to make the recommendations for A. Thus, the goal is to determine users who are similar to the target user A, and recommend ratings for the unobserved ratings of A by computing the averages of the ratings of this peer group. Item-based collaborative filtering : In order to make the rating predictions for target item B by user A, the first step is to determine a set S of items that are most similar to target item B. The ratings in the item set S, which are defined by A, are used to predict whether the user A will like item B. These systems are simple to implement and the resulting recommendations are often easy to explain. On the other hand, they do not work very well with sparse rating matrices. Model-based methods : Machine learning and data mining methods are used in the context of predictive models. In cases where the model is parameterized, the parameters of this model are learned within the context of an optimization framework. Some examples of such methods include decision trees, rule-based models, Bayesian methods and latent factor models. Many of these methods have a high level of coverage even for sparse ratings matrices. Types of ratings \u2691 The design of recommendation algorithms is influenced by the system used for tracking ratings. There are different types of ratings: interval-based : A discrete set of ordered numbers are used to quantify like or dislike. ordinal : A discrete set of ordered categorical values, such as agree or strongly agree, are used to achieve the same goal. binary : Only the like or dislike for the item can be specified. unary : Only liking of an item can be specified. Another categorization of rating systems is based in the way the feedback is retrieved: explicit ratings : Users actively give information on their preferences. implicit ratings : Users preferences are derived from their behavior. Such as visiting a link. Therefore, implicit ratings are usually unary. Content-Based Recommender Systems \u2691 In content-based recommender systems, descriptive attributes of the user rated items are used to create a user-specific model that predicts the rating of unobserved items. These systems have the following advantages: Works well for new items, when sufficient data is not available. If the user has rated items with similar attributes. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items. It requires large number of rating user data to produce robust predictions without overfitting. Knowledge-based Recommender Systems \u2691 The recommendation process is performed based on the similarities between user requirements and item descriptions or the user requirements constrains. The process is facilitated with the use of knowledge bases, which contain data about rules and similarity functions to use during the retrieval process. Knowledge-based systems can be classified by the type of user interface: Constraint-based recommender systems : Users specify requirements on the item attributes or give information about their attributes. Then domain specific rules are used to select the items to recommend. Case-based recommender systems : Specific cases are selected by the user as targets or anchor points. Similarity metrics are defined in a domain specific way on the item attributes to retrieve similar items to these cases. These user interfaces can interact with the users through several ways: Conversational systems : User preferences are determined iteratively in the context of a feedback loop. It's useful if the item domain is complex. Search-based systems : User preferences are elicited by using a preset of questions. Navigation-based recommendation : Users specify a number of attribute changes to the item being recommended. Also known as critiquing recommender systems . The main difference between content-based systems and knowledge-based systems is that while the former learns from past user behavior, the latter does it from active user specification of their needs and interests. These systems have the following advantages: Works well for items with varied properties and/or few ratings. Such as in cold start scenarios, if it's difficult to capture the user interests with historical data or if the item is not often consumed. Allows the users to explicitly specify what they want, thus giving them a greater control over the recommendation process. Allows the user to iteratively change their specified requirements to reach the desired items. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items. Domain-Specific recommender systems \u2691 Demographic recommender systems \u2691 In these systems the demographic information about the user is leveraged to learn classifiers that can map specific demographics to ratings. Although they do not usually provide the best results on a standalone basis, they enhance and increase robustness if used as a component of hybrid systems. Pitfalls to avoid \u2691 Pre-substitution of missing ratings is not recommended in explicit rating matrices as it leads to a significant amount of bias in the analysis. In unary ratings it's common to substitute the missing data by 0 as even though it adds some bias, it's not as great because it's assumed that the default behavior. Interesting resources \u2691 Bookworm looks to be a promising source to build book recommender systems. Content indexers \u2691 Open Library : Open, editable library catalog, building towards a web page for every book ever published. Data can be retrieved through their API or bulk downloaded . Rating Datasets \u2691 Books \u2691 Book-Crossing : 278,858 users providing 1,149,780 ratings (explicit / implicit) about 271,379 books. Movies \u2691 MovieLens : 27,000,000 ratings and 1,100,000 tag applications applied to 58,000 movies by 280,000 users. HetRec 2011 Movielens + IMDB/rotten Tomatoes : 86,000 ratings from 2113 users. Netflix prize dataset : 480,000 users doing 100 million ratings on 17,000 movies. Music \u2691 HetRec 2011 Last.FM : 92,800 artist listening records from 1892 users. Web \u2691 HetRec 2011 Delicious : 105,000 bookmarks from 1867 users. Miscelaneous \u2691 Wikilens : generalized collaborative recommender system that allowed its community to define item types (e.g. beer) and categories (e.g. microbrews, pale ales, stouts), and then rate and get recommendations for items. Past Projects \u2691 GroupLens: Pioneer recommender system to recommend Usenet news. BookLens : Books implementation of Grouplens. MovieLens : Movies implementation of Grouplens. References \u2691 Books \u2691 Recommender Systems by Chary C.Aggarwal . Recommender systems, an introduction by Dietmar Jannach, Markus Zanker, Alexander Felfernig and Gerhard Friedrich. Practical Recommender Systems by Kim Falk. Hands On recommendation systems in Python by Rounak Banik. Awesome recommender systems \u2691 Grahamjenson", "title": "Recommender Systems"}, {"location": "data_analysis/recommender_systems/recommender_systems/#goals-of-recommender-systems", "text": "The common operational and technical goals of recommender systems are: Relevance : Recommend items that are relevant to the user at hand. Although it's the primary operational goal, it is not sufficient in isolation. Novelty : Recommend items that the user has not seen in the past. Serendipity : Recommend items that are outside the user's bubble, rather than simply something they did not know about before. For example, if a new Indian restaurant opens in a neighborhood, then the recommendation of that restaurant to a user who normally eats Indian food is novel but not necessarily serendipitous. On the other hand, when the same user is recommended Ethiopian food, and it was unknown to the user that such food might appeal to her, then the recommendation is serendipitous. Serendipity has the beneficial side effect of beginning new areas of interest. Increase recommendation diversity: Recommend items that aren't similar to increase the chances that the user likes at least one of these items.", "title": "Goals of recommender systems"}, {"location": "data_analysis/recommender_systems/recommender_systems/#basic-models-of-recommender-systems", "text": "There are four types of basic models: Collaborative filtering : Use collaborative power of the ratings provided by multiple users to make recommendations. Content-based : Use the descriptive attributes of the user rated items to create a user-specific model that predicts the rating of unobserved items. Knowledge-based : Use the similarities between customer requirements and item descriptions. Hybrid systems : Combine the above to benefit from the mix of their strengths to perform more robustly.", "title": "Basic Models of recommender systems"}, {"location": "data_analysis/recommender_systems/recommender_systems/#collaborative-filtering-models", "text": "These models use the collaborative power of the ratings provided by multiple users to make recommendations. The main challenge is that the underlying ratings matrices are sparse. To solve it, unspecified ratings are guessed by analyzing the relations of high correlation across various users and items. There are two common types of methods. Memory-based methods : Also referred to as neighborhood-based collaborative filtering algorithms , predict ratings on the basis of their neighborhoods. These can be defined through two ways: User-based collaborative filtering : Ratings provided by like-minded users of a target user A are used in order to make the recommendations for A. Thus, the goal is to determine users who are similar to the target user A, and recommend ratings for the unobserved ratings of A by computing the averages of the ratings of this peer group. Item-based collaborative filtering : In order to make the rating predictions for target item B by user A, the first step is to determine a set S of items that are most similar to target item B. The ratings in the item set S, which are defined by A, are used to predict whether the user A will like item B. These systems are simple to implement and the resulting recommendations are often easy to explain. On the other hand, they do not work very well with sparse rating matrices. Model-based methods : Machine learning and data mining methods are used in the context of predictive models. In cases where the model is parameterized, the parameters of this model are learned within the context of an optimization framework. Some examples of such methods include decision trees, rule-based models, Bayesian methods and latent factor models. Many of these methods have a high level of coverage even for sparse ratings matrices.", "title": "Collaborative Filtering Models"}, {"location": "data_analysis/recommender_systems/recommender_systems/#types-of-ratings", "text": "The design of recommendation algorithms is influenced by the system used for tracking ratings. There are different types of ratings: interval-based : A discrete set of ordered numbers are used to quantify like or dislike. ordinal : A discrete set of ordered categorical values, such as agree or strongly agree, are used to achieve the same goal. binary : Only the like or dislike for the item can be specified. unary : Only liking of an item can be specified. Another categorization of rating systems is based in the way the feedback is retrieved: explicit ratings : Users actively give information on their preferences. implicit ratings : Users preferences are derived from their behavior. Such as visiting a link. Therefore, implicit ratings are usually unary.", "title": "Types of ratings"}, {"location": "data_analysis/recommender_systems/recommender_systems/#content-based-recommender-systems", "text": "In content-based recommender systems, descriptive attributes of the user rated items are used to create a user-specific model that predicts the rating of unobserved items. These systems have the following advantages: Works well for new items, when sufficient data is not available. If the user has rated items with similar attributes. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items. It requires large number of rating user data to produce robust predictions without overfitting.", "title": "Content-Based Recommender Systems"}, {"location": "data_analysis/recommender_systems/recommender_systems/#knowledge-based-recommender-systems", "text": "The recommendation process is performed based on the similarities between user requirements and item descriptions or the user requirements constrains. The process is facilitated with the use of knowledge bases, which contain data about rules and similarity functions to use during the retrieval process. Knowledge-based systems can be classified by the type of user interface: Constraint-based recommender systems : Users specify requirements on the item attributes or give information about their attributes. Then domain specific rules are used to select the items to recommend. Case-based recommender systems : Specific cases are selected by the user as targets or anchor points. Similarity metrics are defined in a domain specific way on the item attributes to retrieve similar items to these cases. These user interfaces can interact with the users through several ways: Conversational systems : User preferences are determined iteratively in the context of a feedback loop. It's useful if the item domain is complex. Search-based systems : User preferences are elicited by using a preset of questions. Navigation-based recommendation : Users specify a number of attribute changes to the item being recommended. Also known as critiquing recommender systems . The main difference between content-based systems and knowledge-based systems is that while the former learns from past user behavior, the latter does it from active user specification of their needs and interests. These systems have the following advantages: Works well for items with varied properties and/or few ratings. Such as in cold start scenarios, if it's difficult to capture the user interests with historical data or if the item is not often consumed. Allows the users to explicitly specify what they want, thus giving them a greater control over the recommendation process. Allows the user to iteratively change their specified requirements to reach the desired items. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items.", "title": "Knowledge-based Recommender Systems"}, {"location": "data_analysis/recommender_systems/recommender_systems/#domain-specific-recommender-systems", "text": "", "title": "Domain-Specific recommender systems"}, {"location": "data_analysis/recommender_systems/recommender_systems/#demographic-recommender-systems", "text": "In these systems the demographic information about the user is leveraged to learn classifiers that can map specific demographics to ratings. Although they do not usually provide the best results on a standalone basis, they enhance and increase robustness if used as a component of hybrid systems.", "title": "Demographic recommender systems"}, {"location": "data_analysis/recommender_systems/recommender_systems/#pitfalls-to-avoid", "text": "Pre-substitution of missing ratings is not recommended in explicit rating matrices as it leads to a significant amount of bias in the analysis. In unary ratings it's common to substitute the missing data by 0 as even though it adds some bias, it's not as great because it's assumed that the default behavior.", "title": "Pitfalls to avoid"}, {"location": "data_analysis/recommender_systems/recommender_systems/#interesting-resources", "text": "Bookworm looks to be a promising source to build book recommender systems.", "title": "Interesting resources"}, {"location": "data_analysis/recommender_systems/recommender_systems/#content-indexers", "text": "Open Library : Open, editable library catalog, building towards a web page for every book ever published. Data can be retrieved through their API or bulk downloaded .", "title": "Content indexers"}, {"location": "data_analysis/recommender_systems/recommender_systems/#rating-datasets", "text": "", "title": "Rating Datasets"}, {"location": "data_analysis/recommender_systems/recommender_systems/#books", "text": "Book-Crossing : 278,858 users providing 1,149,780 ratings (explicit / implicit) about 271,379 books.", "title": "Books"}, {"location": "data_analysis/recommender_systems/recommender_systems/#movies", "text": "MovieLens : 27,000,000 ratings and 1,100,000 tag applications applied to 58,000 movies by 280,000 users. HetRec 2011 Movielens + IMDB/rotten Tomatoes : 86,000 ratings from 2113 users. Netflix prize dataset : 480,000 users doing 100 million ratings on 17,000 movies.", "title": "Movies"}, {"location": "data_analysis/recommender_systems/recommender_systems/#music", "text": "HetRec 2011 Last.FM : 92,800 artist listening records from 1892 users.", "title": "Music"}, {"location": "data_analysis/recommender_systems/recommender_systems/#web", "text": "HetRec 2011 Delicious : 105,000 bookmarks from 1867 users.", "title": "Web"}, {"location": "data_analysis/recommender_systems/recommender_systems/#miscelaneous", "text": "Wikilens : generalized collaborative recommender system that allowed its community to define item types (e.g. beer) and categories (e.g. microbrews, pale ales, stouts), and then rate and get recommendations for items.", "title": "Miscelaneous"}, {"location": "data_analysis/recommender_systems/recommender_systems/#past-projects", "text": "GroupLens: Pioneer recommender system to recommend Usenet news. BookLens : Books implementation of Grouplens. MovieLens : Movies implementation of Grouplens.", "title": "Past Projects"}, {"location": "data_analysis/recommender_systems/recommender_systems/#references", "text": "", "title": "References"}, {"location": "data_analysis/recommender_systems/recommender_systems/#books_1", "text": "Recommender Systems by Chary C.Aggarwal . Recommender systems, an introduction by Dietmar Jannach, Markus Zanker, Alexander Felfernig and Gerhard Friedrich. Practical Recommender Systems by Kim Falk. Hands On recommendation systems in Python by Rounak Banik.", "title": "Books"}, {"location": "data_analysis/recommender_systems/recommender_systems/#awesome-recommender-systems", "text": "Grahamjenson", "title": "Awesome recommender systems"}, {"location": "devops/alex/", "text": "Alex helps you find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. For example, when We\u2019ve confirmed his identity is given , alex will warn you and suggest using their instead of his . Give alex a spin on the Online demo . Installation \u2691 npm install alex --global You can use it with Vim through the ALE plugin . As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter. References \u2691 Git", "title": "Alex"}, {"location": "devops/alex/#installation", "text": "npm install alex --global You can use it with Vim through the ALE plugin . As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter.", "title": "Installation"}, {"location": "devops/alex/#references", "text": "Git", "title": "References"}, {"location": "devops/api_management/", "text": "API management is the process of creating and publishing web application programming interfaces (APIs) under a service that: Enforces the usage of policies. Controls access. Collects and analyzes usage statistics. Reports on performance. Components \u2691 While solutions vary, components that provide the following functionality are typically found in API management products: Gateway : a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance. Publishing tools : a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle. Developer portal/API store : community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community. Reporting and analytics : functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs. Monetization : functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.", "title": "API Management"}, {"location": "devops/api_management/#components", "text": "While solutions vary, components that provide the following functionality are typically found in API management products: Gateway : a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance. Publishing tools : a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle. Developer portal/API store : community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community. Reporting and analytics : functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs. Monetization : functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.", "title": "Components"}, {"location": "devops/bandit/", "text": "Bandit finds common security issues in Python code. To do this, Bandit processes each file, builds an AST from it, and runs appropriate plugins against the AST nodes. Once Bandit has finished scanning all the files, it generates a report. You can use this cookiecutter template to create a python project with bandit already configured. Installation \u2691 pip install bandit Usage \u2691 Ignore an error. \u2691 Add the # nosec comment in the line. Configuration \u2691 You can run bandit through: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-bandit rev : v1.0.4 hooks : - id : python-bandit-vulnerability-check bandit takes a lot of time to run, so it slows down too much the commiting, therefore it should be run only in the CI. Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : bandit : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install bandit - name : Execute bandit run : bandit -r project References \u2691 Docs", "title": "Bandit"}, {"location": "devops/bandit/#installation", "text": "pip install bandit", "title": "Installation"}, {"location": "devops/bandit/#usage", "text": "", "title": "Usage"}, {"location": "devops/bandit/#ignore-an-error", "text": "Add the # nosec comment in the line.", "title": "Ignore an error."}, {"location": "devops/bandit/#configuration", "text": "You can run bandit through: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-bandit rev : v1.0.4 hooks : - id : python-bandit-vulnerability-check bandit takes a lot of time to run, so it slows down too much the commiting, therefore it should be run only in the CI. Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : bandit : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install bandit - name : Execute bandit run : bandit -r project", "title": "Configuration"}, {"location": "devops/bandit/#references", "text": "Docs", "title": "References"}, {"location": "devops/black/", "text": "Black is a style guide enforcement tool. You can use this cookiecutter template to create a python project with black already configured. Installation \u2691 pip install black Configuration \u2691 Its configuration is stored in pyproject.toml . File: pyproject.toml # Example configuration for Black. # NOTE: you have to use single-quoted strings in TOML for regular expressions. # It's the equivalent of r-strings in Python. Multiline strings are treated as # verbose regular expressions by Black. Use [ ] to denote a significant space # character. [tool.black] line-length = 88 target-version = ['py36', 'py37', 'py38'] include = '\\.pyi?$' exclude = ''' /( \\.eggs | \\.git | \\.hg | \\.mypy_cache | \\.tox | \\.venv | _build | buck-out | build | dist # The following are specific to Black, you probably don't want those. | blib2to3 | tests/data | profiling )/ ''' You can use it both with: The Vim plugin Pre-commit : File: .pre-commit-config.yaml repos : - repo : https://github.com/ambv/black rev : stable hooks : - id : black language_version : python3.7 Github Actions: File: .github/workflows/lint.yml --- name : Lint on : [ push , pull_request ] jobs : Black : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 - name : Black uses : psf/black@stable Split long lines \u2691 If you want to split long lines, you need to use the --experimental-string-processing flag. I haven't found how to set that option in the config file. Disable the formatting of some lines \u2691 You can use the comments # fmt: off and # fmt: on References \u2691 Docs Git", "title": "Black"}, {"location": "devops/black/#installation", "text": "pip install black", "title": "Installation"}, {"location": "devops/black/#configuration", "text": "Its configuration is stored in pyproject.toml . File: pyproject.toml # Example configuration for Black. # NOTE: you have to use single-quoted strings in TOML for regular expressions. # It's the equivalent of r-strings in Python. Multiline strings are treated as # verbose regular expressions by Black. Use [ ] to denote a significant space # character. [tool.black] line-length = 88 target-version = ['py36', 'py37', 'py38'] include = '\\.pyi?$' exclude = ''' /( \\.eggs | \\.git | \\.hg | \\.mypy_cache | \\.tox | \\.venv | _build | buck-out | build | dist # The following are specific to Black, you probably don't want those. | blib2to3 | tests/data | profiling )/ ''' You can use it both with: The Vim plugin Pre-commit : File: .pre-commit-config.yaml repos : - repo : https://github.com/ambv/black rev : stable hooks : - id : black language_version : python3.7 Github Actions: File: .github/workflows/lint.yml --- name : Lint on : [ push , pull_request ] jobs : Black : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 - name : Black uses : psf/black@stable", "title": "Configuration"}, {"location": "devops/black/#split-long-lines", "text": "If you want to split long lines, you need to use the --experimental-string-processing flag. I haven't found how to set that option in the config file.", "title": "Split long lines"}, {"location": "devops/black/#disable-the-formatting-of-some-lines", "text": "You can use the comments # fmt: off and # fmt: on", "title": "Disable the formatting of some lines"}, {"location": "devops/black/#references", "text": "Docs Git", "title": "References"}, {"location": "devops/ci/", "text": "Continuous Integration (CI) allows to automatically run processes on the code each time a commit is pushed. For example it can be used to run the tests, build the documentation, build a package or maintain dependencies updated. I've automated the configuration of CI/CD pipelines for python projects in this cookiecutter template . There are three non exclusive ways to run the tests: Integrate them in your editor, so it's executed each time you save the file. Through a pre-commit hook to make it easy for the collaborator to submit correctly formatted code. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. Through a CI server (like Drone or Github Actions) to ensure that the commited code meets the quality standards. Developers can bypass the pre-commit filter, so we need to set up the quality gate in an agnostic environment. Depending on the time the test takes to run and their different implementations, we'll choose from one to three of the choices above. Configuring pre-commit \u2691 To adopt pre-commit to our system we have to: Install pre-commit: pip3 install pre-commit and add it to the development requirements.txt . Define .pre-commit-config.yaml with the hooks you want to include (they don't plan to support pyproject.toml ). Execute pre-commit install to install git hooks in your .git/ directory. Execute pre-commit run --all-files to tests all the files. Usually pre-commit will only run on the changed files during git hooks. Static analysis checkers \u2691 Static analysis is the analysis of computer software that is performed without actually executing programs. Formatters \u2691 Formatters are tools that change your files to meet a linter requirements. Black : A python style guide formatter tool. Linters \u2691 Lint, or a linter , is a static code analysis tool used to flag programming errors, bugs, stylistic errors, and suspicious constructs. The term originates from a Unix utility that examined C language source code. alex to find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. Flake8 : A python style guide checker tool. markdownlint : A linter for Markdown files. proselint : Is another linter for prose. Yamllint : A linter for YAML files. write-good is a naive linter for English prose. Type checkers \u2691 Type checkers are programs that the code is compliant with a defined type system which is a logical system comprising a set of rules that assigns a property called a type to the various constructs of a computer program, such as variables, expressions, functions or modules. The main purpose of a type system is to reduce possibilities for bugs by defining interfaces between different parts of the program, and then checking that the parts have been connected in a consistent way. Mypy : A static type checker for Python. Security vulnerability checkers \u2691 Tools that check potential vulnerabilities in the code. Bandit : Finds common security issues in Python code. Safety : Checks your installed dependencies for known security vulnerabilities. Other pre-commit tests \u2691 Pre-commit comes with several tests by default. These are the ones I've chosen. File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v3.1.0 hooks : - id : trailing-whitespace - id : check-added-large-files - id : check-docstring-first - id : check-merge-conflict - id : end-of-file-fixer - id : detect-private-key Update package dependencies \u2691 Tools to automatically keep your dependencies updated. pip-tools Coverage reports \u2691 Coveralls is a service that monitors and writes statistics on the coverage of your repositories. To use them, you'll need to log in with your Github account and enable the repos you want to test. Save the secret in the repository configuration and add this step to your tests job. - name : Coveralls uses : coverallsapp/github-action@master with : github-token : ${{ secrets.COVERALLS_TOKEN }} Add the following badge to your README.md. Variables to substitute: repository_path : Github repository path, like lyz-code/pydo . [ ![Coverage Status ]( https://coveralls.io/repos/github/{{ repository_path }}/badge.svg?branch=master )](https://coveralls.io/github/{{ repository_path }}?branch=master) Troubleshooting \u2691 error: pathspec 'master' did not match any file(s) known to git \u2691 If you have this error while making a commit through a pipeline step, it may be the pre-commits stepping in. To fix it, remove all git hooks with rm -r .git/hooks .", "title": "Continuous Integration"}, {"location": "devops/ci/#configuring-pre-commit", "text": "To adopt pre-commit to our system we have to: Install pre-commit: pip3 install pre-commit and add it to the development requirements.txt . Define .pre-commit-config.yaml with the hooks you want to include (they don't plan to support pyproject.toml ). Execute pre-commit install to install git hooks in your .git/ directory. Execute pre-commit run --all-files to tests all the files. Usually pre-commit will only run on the changed files during git hooks.", "title": "Configuring pre-commit"}, {"location": "devops/ci/#static-analysis-checkers", "text": "Static analysis is the analysis of computer software that is performed without actually executing programs.", "title": "Static analysis checkers"}, {"location": "devops/ci/#formatters", "text": "Formatters are tools that change your files to meet a linter requirements. Black : A python style guide formatter tool.", "title": "Formatters"}, {"location": "devops/ci/#linters", "text": "Lint, or a linter , is a static code analysis tool used to flag programming errors, bugs, stylistic errors, and suspicious constructs. The term originates from a Unix utility that examined C language source code. alex to find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. Flake8 : A python style guide checker tool. markdownlint : A linter for Markdown files. proselint : Is another linter for prose. Yamllint : A linter for YAML files. write-good is a naive linter for English prose.", "title": "Linters"}, {"location": "devops/ci/#type-checkers", "text": "Type checkers are programs that the code is compliant with a defined type system which is a logical system comprising a set of rules that assigns a property called a type to the various constructs of a computer program, such as variables, expressions, functions or modules. The main purpose of a type system is to reduce possibilities for bugs by defining interfaces between different parts of the program, and then checking that the parts have been connected in a consistent way. Mypy : A static type checker for Python.", "title": "Type checkers"}, {"location": "devops/ci/#security-vulnerability-checkers", "text": "Tools that check potential vulnerabilities in the code. Bandit : Finds common security issues in Python code. Safety : Checks your installed dependencies for known security vulnerabilities.", "title": "Security vulnerability checkers"}, {"location": "devops/ci/#other-pre-commit-tests", "text": "Pre-commit comes with several tests by default. These are the ones I've chosen. File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v3.1.0 hooks : - id : trailing-whitespace - id : check-added-large-files - id : check-docstring-first - id : check-merge-conflict - id : end-of-file-fixer - id : detect-private-key", "title": "Other pre-commit tests"}, {"location": "devops/ci/#update-package-dependencies", "text": "Tools to automatically keep your dependencies updated. pip-tools", "title": "Update package dependencies"}, {"location": "devops/ci/#coverage-reports", "text": "Coveralls is a service that monitors and writes statistics on the coverage of your repositories. To use them, you'll need to log in with your Github account and enable the repos you want to test. Save the secret in the repository configuration and add this step to your tests job. - name : Coveralls uses : coverallsapp/github-action@master with : github-token : ${{ secrets.COVERALLS_TOKEN }} Add the following badge to your README.md. Variables to substitute: repository_path : Github repository path, like lyz-code/pydo . [ ![Coverage Status ]( https://coveralls.io/repos/github/{{ repository_path }}/badge.svg?branch=master )](https://coveralls.io/github/{{ repository_path }}?branch=master)", "title": "Coverage reports"}, {"location": "devops/ci/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "devops/ci/#error-pathspec-master-did-not-match-any-files-known-to-git", "text": "If you have this error while making a commit through a pipeline step, it may be the pre-commits stepping in. To fix it, remove all git hooks with rm -r .git/hooks .", "title": "error: pathspec 'master' did not match any file(s) known to git"}, {"location": "devops/devops/", "text": "DevOps is a set of practices that combines software development (Dev) and information-technology operations (Ops) which aims to shorten the systems development life cycle and provide continuous delivery with high software quality. One of the most important goals of the DevOps initiative is to break the silos between the developers and the sysadmins, that lead to ill feelings and unproductivity. It's a relatively new concept , the main ideas emerged in the 1990s and the first conference was in 2009. That means that as of 2021 there is still a lot of debate of what people understand as DevOps. DevOps pitfalls \u2691 I've found that the DevOps word leads to some pitfalls that we should try to avoid. Getting lost in the label \u2691 Labels are a language tool used to speed up communication by describing someone or something in a word or short phrase. However, there are times when labels achieve the complete oposite, as it's the case with DevOps, where there are different views on what the label represents and usually one of the communication parties strongly feels they belong to the label while the other doesn't agree. These discussions can fall into an unproductive, agitated semantic debate where each part tries to convince each other. So instead of starting a twitter thread telling people why they aren't a DevOps team, we could invest those energies in creating resources that close the gap between both parties. Similarly, instead of starting an internal discussion of what do we understand as DevOps? , we could discuss how to improve our existent processes so that the team members feel more comfortable contributing to the application or infrastructure code. You need to do it all to be awarded the DevOps pin \u2691 I find specially harmful the idea that to be qualified as DevOps you need to develop and maintain the application at the same time as the infrastructure that holds it. To be able to do that in a typical company product you'll need to know (between another thousand more things): How to operate the cloud infrastructure where the project lives, which can be AWS , Google Cloud, Azure or/and baremetal servers. Deploy new resources in that infrastructure, which probably would mean knowing Terraform, Ansible, Docker or/and Kubernetes . How to integrate the new resources with the operations processes, for example: The monitoring system, so you'll need to know how to use Prometheus , Nagios, Zabbix or the existent solution. The continuous integration or delivery system, that you'll need to know how to maintain, so you have to know how it works and how is it built. The backup system. The log centralizer system. Infrastructure architecture to know what you need to deploy, how and where. To code efficiently in the language that the application is developed in, for example Java, Python, Rust, Go, PHP or Javascript, in a way that meets the quality requirements (code style, linters, coverage and documentation). Knowing how to test your code in that language. Software architecture to structure complex code projects in a maintainable way. The product you're developing to be able to suggest features and fixtures when the product owner or the stakeholders show their needs. How to make the application user friendly so anyone wants to use it. And don't forget that you also need to do that in a secure way, so you should also have to know about pentesting, static and dynamic security tools, common security vulnerabilities... And I could keep on going forever. Even if you could get there (and you won't), it wouldn't matter, because when you did, the technologies will have changed so much that you will already be outdated and would need to start over. It's the sickness of the fullstack developer. If you make job openings with this mindset you're going to end up with a team of cis white males in their thirties or forties that are used to earn 3 or 4 times the minimum salary. No other kind of people can reach that point and hold it in time. But bare with me a little longer, even if you make there. What happens when the project changes so that you need to: Change the programming language of your application. Change the cloud provider. Change the deployment system. Change the program architecture. Change the language framework. Or any other thousand of possible changes? You would need to be able to keep up with them. Noooo way. Luckily you are not alone. You are a cog in a team, that as a whole is able to overcome these changes. That is why we have developers, sysadmins, security, user experience, quality, scrum master and product owners. So each profile can design, create, investigate and learn it's best in their area of expertise. In the merge of all that personal knowledge is where the team thrives. DevOps then, as I understand it, is the philosophy where the developers and sysadmins try their best to break the barriers that separate them. That idea can be brought to earth by for example: Open discussions on how to: Improve the development workflow. Make developers or sysadmins life easier. Make it easier to use the sysadmin tools. Make it easier to understand the developers code. Formations on the technologies or architecture used by either side. A clear documentation that allows either side to catch up with new changes. Periodic meetings to update each other with the changes. Periodic meetings to release the tension that have appeared between them. Joined design sessions to decide how to solve problems. Learn path \u2691 DevOps has become a juicy work, if you want to become one, I think you first need to get the basic knowledge of each of them (developing and operating) before being able to unlock the benefits of the combination of both. You can try to learn both at the same time, but I think it can be a daunting task. To get the basic knowledge of the Ops side I would: Learn basic Linux administration, otherwise you'll be lost. Learn how to be comfortable searching for anything you don't know, most of your questions are already answered, and even the most senior people spent a great amount of time searching for solutions in the project's documentation, Github issues or Stackoverflow. When you start, navigating this knowledge sources is hard and consumes a lot of your life, but it will get easier with the time. Learn how to use Git. If you can, host your own Gitea, if not, use an existing service such as Gitlab or Github. Learn how to install and maintain services, (that is why I suggested hosting your own Gitea). If you don't know what to install, take a look at the awesome self-hosted list. Learn how to use Ansible, from now on try to deploy every machine with it. Build a small project inside AWS so you can get used to the most common services (VPC, EC2, S3, Route53, RDS), most of them have free-tier resources so you don't need to pay anything. You can try also with Google Cloud or Azure, but I recommend against it. Once you are comfortable with AWS, learn how to use Terraform. You could for example deploy the previous project. From now on only use Terraform to provision AWS resources. Get into the CI/CD world hosting your own Drone, if not, use Gitlab runners or Github Actions . To get the basic knowledge of the Dev side I would: Learn the basics of a programming language, for example Python. There are thousand sources there on how to do it, books, articles, videos, forums or courses, choose the one that suits you best. As with the Ops path, get comfortable with git and searching for things you don't know. As soon as you can, start doing small programming projects that make your life easier. Coding your stuff is what's going to make you internalize the learned concepts, by finding solutions to the blocks you encounter. Publish those projects into a public git server, don't be afraid if you code is good enough, it works for you, you did your best and you should be happy about it. That's all that matters. By doing so, you'll start collaborating to the open source world and it will probably force yourself to make your code better. Step into the TDD world, learn why, how and when to test your code. For those projects that you want to maintain, create CI/CD pipelines that enhance the quality of your code, by for example running your tests or some linters . Once you're comfortable, try to collaborate with existent projects (right now you may not now where to look for projects to collaborate, but when you reach this point, I promise you will).", "title": "DevOps"}, {"location": "devops/devops/#devops-pitfalls", "text": "I've found that the DevOps word leads to some pitfalls that we should try to avoid.", "title": "DevOps pitfalls"}, {"location": "devops/devops/#getting-lost-in-the-label", "text": "Labels are a language tool used to speed up communication by describing someone or something in a word or short phrase. However, there are times when labels achieve the complete oposite, as it's the case with DevOps, where there are different views on what the label represents and usually one of the communication parties strongly feels they belong to the label while the other doesn't agree. These discussions can fall into an unproductive, agitated semantic debate where each part tries to convince each other. So instead of starting a twitter thread telling people why they aren't a DevOps team, we could invest those energies in creating resources that close the gap between both parties. Similarly, instead of starting an internal discussion of what do we understand as DevOps? , we could discuss how to improve our existent processes so that the team members feel more comfortable contributing to the application or infrastructure code.", "title": "Getting lost in the label"}, {"location": "devops/devops/#you-need-to-do-it-all-to-be-awarded-the-devops-pin", "text": "I find specially harmful the idea that to be qualified as DevOps you need to develop and maintain the application at the same time as the infrastructure that holds it. To be able to do that in a typical company product you'll need to know (between another thousand more things): How to operate the cloud infrastructure where the project lives, which can be AWS , Google Cloud, Azure or/and baremetal servers. Deploy new resources in that infrastructure, which probably would mean knowing Terraform, Ansible, Docker or/and Kubernetes . How to integrate the new resources with the operations processes, for example: The monitoring system, so you'll need to know how to use Prometheus , Nagios, Zabbix or the existent solution. The continuous integration or delivery system, that you'll need to know how to maintain, so you have to know how it works and how is it built. The backup system. The log centralizer system. Infrastructure architecture to know what you need to deploy, how and where. To code efficiently in the language that the application is developed in, for example Java, Python, Rust, Go, PHP or Javascript, in a way that meets the quality requirements (code style, linters, coverage and documentation). Knowing how to test your code in that language. Software architecture to structure complex code projects in a maintainable way. The product you're developing to be able to suggest features and fixtures when the product owner or the stakeholders show their needs. How to make the application user friendly so anyone wants to use it. And don't forget that you also need to do that in a secure way, so you should also have to know about pentesting, static and dynamic security tools, common security vulnerabilities... And I could keep on going forever. Even if you could get there (and you won't), it wouldn't matter, because when you did, the technologies will have changed so much that you will already be outdated and would need to start over. It's the sickness of the fullstack developer. If you make job openings with this mindset you're going to end up with a team of cis white males in their thirties or forties that are used to earn 3 or 4 times the minimum salary. No other kind of people can reach that point and hold it in time. But bare with me a little longer, even if you make there. What happens when the project changes so that you need to: Change the programming language of your application. Change the cloud provider. Change the deployment system. Change the program architecture. Change the language framework. Or any other thousand of possible changes? You would need to be able to keep up with them. Noooo way. Luckily you are not alone. You are a cog in a team, that as a whole is able to overcome these changes. That is why we have developers, sysadmins, security, user experience, quality, scrum master and product owners. So each profile can design, create, investigate and learn it's best in their area of expertise. In the merge of all that personal knowledge is where the team thrives. DevOps then, as I understand it, is the philosophy where the developers and sysadmins try their best to break the barriers that separate them. That idea can be brought to earth by for example: Open discussions on how to: Improve the development workflow. Make developers or sysadmins life easier. Make it easier to use the sysadmin tools. Make it easier to understand the developers code. Formations on the technologies or architecture used by either side. A clear documentation that allows either side to catch up with new changes. Periodic meetings to update each other with the changes. Periodic meetings to release the tension that have appeared between them. Joined design sessions to decide how to solve problems.", "title": "You need to do it all to be awarded the DevOps pin"}, {"location": "devops/devops/#learn-path", "text": "DevOps has become a juicy work, if you want to become one, I think you first need to get the basic knowledge of each of them (developing and operating) before being able to unlock the benefits of the combination of both. You can try to learn both at the same time, but I think it can be a daunting task. To get the basic knowledge of the Ops side I would: Learn basic Linux administration, otherwise you'll be lost. Learn how to be comfortable searching for anything you don't know, most of your questions are already answered, and even the most senior people spent a great amount of time searching for solutions in the project's documentation, Github issues or Stackoverflow. When you start, navigating this knowledge sources is hard and consumes a lot of your life, but it will get easier with the time. Learn how to use Git. If you can, host your own Gitea, if not, use an existing service such as Gitlab or Github. Learn how to install and maintain services, (that is why I suggested hosting your own Gitea). If you don't know what to install, take a look at the awesome self-hosted list. Learn how to use Ansible, from now on try to deploy every machine with it. Build a small project inside AWS so you can get used to the most common services (VPC, EC2, S3, Route53, RDS), most of them have free-tier resources so you don't need to pay anything. You can try also with Google Cloud or Azure, but I recommend against it. Once you are comfortable with AWS, learn how to use Terraform. You could for example deploy the previous project. From now on only use Terraform to provision AWS resources. Get into the CI/CD world hosting your own Drone, if not, use Gitlab runners or Github Actions . To get the basic knowledge of the Dev side I would: Learn the basics of a programming language, for example Python. There are thousand sources there on how to do it, books, articles, videos, forums or courses, choose the one that suits you best. As with the Ops path, get comfortable with git and searching for things you don't know. As soon as you can, start doing small programming projects that make your life easier. Coding your stuff is what's going to make you internalize the learned concepts, by finding solutions to the blocks you encounter. Publish those projects into a public git server, don't be afraid if you code is good enough, it works for you, you did your best and you should be happy about it. That's all that matters. By doing so, you'll start collaborating to the open source world and it will probably force yourself to make your code better. Step into the TDD world, learn why, how and when to test your code. For those projects that you want to maintain, create CI/CD pipelines that enhance the quality of your code, by for example running your tests or some linters . Once you're comfortable, try to collaborate with existent projects (right now you may not now where to look for projects to collaborate, but when you reach this point, I promise you will).", "title": "Learn path"}, {"location": "devops/flake8/", "text": "DEPRECATION: Use Flakehell instead Flake8 doesn't support pyproject.toml , which is becoming the standard, so I suggest using Flakehell instead. Flake8 is a style guide enforcement tool. Its configuration is stored in setup.cfg , tox.ini or .flake8 . File: .flake8 [flake8] # ignore = E203, E266, E501, W503, F403, F401 max-line-length = 88 # max-complexity = 18 # select = B,C,E,F,W,T4,B9 You can use it both with: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://gitlab.com/pycqa/flake8 rev : master hooks : - id : flake8 Github Actions: File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Flake8 : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 - name : Flake8 uses : cclauss/GitHub-Action-for-Flake8@v0.5.0 References \u2691 Docs", "title": "Flake8"}, {"location": "devops/flake8/#references", "text": "Docs", "title": "References"}, {"location": "devops/helmfile/", "text": "Helmfile is a declarative spec for deploying Helm charts. It lets you: Keep a directory of chart value files and maintain changes in version control. Apply CI/CD to configuration changes. Environmental chart promotion. Periodically sync to avoid skew in environments. To avoid upgrades for each iteration of helm, the helmfile executable delegates to helm - as a result, helm must be installed. All information is saved in the helmfile.yaml file. In case we need custom yamls, we'll use kustomize . Installation \u2691 Helmfile is not yet in the distribution package managers, so you'll need to install it manually. Gather the latest release number . wget {{ bin_url }} -O helmfile_linux_amd64 chmod +x helmfile_linux_amd64 mv helmfile_linux_amd64 ~/.local/bin/helmfile Usage \u2691 How to deploy a new chart \u2691 When we want to add a new chart, the workflow would be: Run helmfile deps && helmfile diff to check that your existing charts are updated, if they are not, run helmfile apply . Configure the release in helmfile.yaml specifying: name : Deployment name. namespace : K8s namespace to deploy. chart : Chart release. values : path pointing to the values file created above. Create a directory with the {{ chart_name }} . mkdir {{ chart_name }} Get a copy of the chart values inside that directory. helm inspect values {{ package_name }} > {{ chart_name }} /values.yaml Edit the values.yaml file according to the chart documentation. Be careful becase some charts specify the docker image version in the name. Comment out that line because upgrading the chart version without upgrading the image tag can break the service. Run helmfile deps to update the lock file. Run helmfile diff to check the changes. Run helmfile apply to apply the changes. Keep charts updated \u2691 Updating charts with helmfile is easy as long as you don't use environments, you run helmfile deps , then helmfile diff and finally helmfile apply . The tricky business comes when you want to use environments to reuse your helmfile code and don't repeat yourself. This is my suggested workflow, I've opened an issue to see if the developers agree with it: As of today, helmfile doesn't support lock files per environment , that means that the lock file needs to be shared by all of them. At a first sight this is a good idea, because it forces us to have the same versions of the charts in all the environments. The problem comes when you want to upgrade the charts of staging, test that they work and then apply the same changes in production. You'd start the process by running helmfile deps , which will read the helmfiles and update the lock file to the latest version. From this point on you need to be careful on executing the next steps in order so as not to break production. Tell your team that you're going to do the update operation, so that they don't try to run helmfile against any environment of the cluster. Run helmfile --environment=staging diff to review the changes to be introduced. To be able to see the differences of long diff files, you can filter it with egrep . helmfile diff | egrep -A20 -B20 \"^.{5}(\\-|\\+)\" It will show you all the changed lines with the 20 previous and next ones. * Once you agree on them, run helmfile --environment=staging apply to apply them. * Check that all the helm deployments are well deployed with helm list -A | grep -v deployed * Wait 20 minutes to see if the monitoring system or your fellow partners start yelling at you. * If something breaks up, try to fix it up, if you see it's going to delay you to the point that you're not going to be able to finish the upgrade in your working day, it's better to revert back to the working version of that chart and move on with the next steps. Keep in mind that since you run the apply to the last of the steps of this long process, the team is blocked by you. So prioritize to commit the next stable version to the version control repository. * Once you've checked that all the desired upgrades are working, change the context to the production cluster and run helmfile --environment=production diff . This review should be quick, as it should be the same as the staging one. * Now upgrade the production environment with helmfile --environment=production apply . * Check that all the helm deployments are well deployed with helm list -A | grep -v deployed * Wait another 20 minutes and check that everything is working. * Make a commit with the new lockfile and upload it to the version control repository. If you want the team to be involved in the review process, you can open a PR with the lock file updated with the WIP state, and upload the relevant diff of staging and production, let the discussion end and then run the apply on staging and then on production if everything goes well. Another ugly solution that I thought was to have a lockfile per environment, and let a Makefile manage them, for example, copying it to helmfile.lock before running any command. Uninstall charts \u2691 Helmfile still doesn't remove charts if you remove them from your helmfile.yaml . To remove them you have to either set installed: false in the release candidate and execute helmfile apply or delete the release definition from your helmfile and remove it using standard helm commands. Force the reinstallation of everything \u2691 If you manually changed the deployed resources and want to reset the cluster state to the helmfile one, use helmfile sync which will reinstall all the releases. Multi-environment project structure \u2691 helmfile can handle environments with many different project structures. Such as the next one: \u251c\u2500\u2500 README.md \u251c\u2500\u2500 helmfile.yaml \u251c\u2500\u2500 vars \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2514\u2500\u2500 default_values.yaml \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 local_defined_chart_1 \u2502 \u2514\u2500\u2500 local_defined_chart_2 \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 environments.yaml \u2502 \u2514\u2500\u2500 templates.yaml \u251c\u2500\u2500 base \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 helmfile.yaml \u2502 \u251c\u2500\u2500 helmfile.lock \u2502 \u251c\u2500\u2500 repos.yaml \u2502 \u251c\u2500\u2500 chart_1 \u2502 \u2502 \u251c\u2500\u2500 secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 values.yaml \u2502 \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2502 \u2514\u2500\u2500 default_values.yaml \u2502 \u2514\u2500\u2500 chart_2 \u2502 \u251c\u2500\u2500 secrets.yaml \u2502 \u251c\u2500\u2500 values.yaml \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2514\u2500\u2500 default_values.yaml \u2514\u2500\u2500 service_1 \u251c\u2500\u2500 README.md \u251c\u2500\u2500 helmfile.yaml \u251c\u2500\u2500 helmfile.lock \u251c\u2500\u2500 repos.yaml \u251c\u2500\u2500 chart_1 \u2502 \u251c\u2500\u2500 secrets.yaml \u2502 \u251c\u2500\u2500 values.yaml \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2514\u2500\u2500 default_values.yaml \u2514\u2500\u2500 chart_2 \u251c\u2500\u2500 secrets.yaml \u251c\u2500\u2500 values.yaml \u251c\u2500\u2500 production_secrets.yaml \u251c\u2500\u2500 production_values.yaml \u251c\u2500\u2500 default_secrets.yaml \u2514\u2500\u2500 default_values.yaml Where: There is a general README.md that introduces the repository. Optionally there could be a helmfile.yaml file at the root with a glob pattern so that it's easy to run commands on all children helmfiles. helmfiles : - ./*/helmfile.yaml * There is a vars directory to store the variables and secrets shared by the charts that belong to different services. * There is a templates directory to store the helmfile code to reuse through templates and layering . * The project structure is defined by the services hosted in the Kubernetes cluster. Each service contains: A README.md to document the service implementation. A helmfile.yaml file to configure the service charts. A helmfile.lock to lock the versions of the service charts. A repos.yaml to define the repositories to fetch the charts from. One or more chart directories that contain the environment specific and shared chart values and secrets. There is a base service that manages all the charts required to keep the cluster running, such as the ingress, csi, cni or the cluster-autoscaler. Using helmfile environments \u2691 To customize the contents of a helmfile.yaml or values.yaml file per environment, add them under the environments key in the helmfile.yaml : environments : default : production : The environment name defaults to default , that is, helmfile sync implies the default environment. So it's a good idea to use staging as default to be more robust against human errors. If you want to specify a non-default environment, provide a --environment NAME flag to helmfile like helmfile --environment production sync . In the environments definition we'll load the values and secrets from the vars directory with the next snippet. environments : default : secrets : - ../vars/default_secrets.yaml values : - ../vars/default_values.yaml production : secrets : - ../vars/production_secrets.yaml values : - ../vars/production_values.yaml As this snippet is going to be repeated on every helmfile.yaml we'll use a state layering for it . To install a release only in one environment use: environments : default : production : --- releases : - name : newrelic-agent installed : {{ eq .Environment.Name \"production\" | toYaml }} # snip Using environment specific variables \u2691 Environment Values allows you to inject a set of values specific to the selected environment, into values.yaml templates or helmfile.yaml files. Use it to inject common values from the environment to multiple values files, to make your configuration DRY. Suppose you have three files helmfile.yaml, production.yaml and values.yaml.gotmpl File: helmfile.yaml environments : production : values : - production.yaml --- releases : - name : myapp values : - values.yaml.gotmpl File: production.yaml domain : prod.example.com File: values.yaml.gotmpl domain : {{ .Values | get \"domain\" \"dev.example.com\" }} Sadly you can't use templates in the secrets files , so you'll need to repeat the code. Loading the chart variables and secrets \u2691 For each chart definition in the helmfile.yaml we need to load it's secrets and values. We could use the next snippet: - name : chart_1 values : - ./chart_1/values.yaml - ./chart_1/{{ Environment.Name }}_values.yaml secrets : - ./chart_1/secrets.yaml - ./chart_1/{{ Environment.Name }}_secrets.yaml This assumes that the environment variable is set, as it's going to be shared by all the helmfiles.yaml you can add it to the vars files: File: vars/production_values.yaml environment : production File: vars/default_values.yaml environment : staging Instead of .Environment.Name , in theory you could have used .Vars | get \"environment\" , which could have prevented the variables and secrets of the default environment will need to be called default_values.yaml , and default_secrets.yaml , which is misleading. But you can't use .Values in the helmfile.yaml as it's not loaded when the file is parsed, and you get an error. A solution would be to layer the helmfile state files but I wasn't able to make it work. Avoiding code repetition \u2691 Besides environments, helmfile gives other useful tricks to prevent the illness of code repetition. Using release templates \u2691 For each chart in a helmfile.yaml we're going to repeat the values and secrets sections, to avoid it, we can use release templates: templates : default : &default # This prevents helmfile exiting when it encounters a missing file # Valid values are \"Error\", \"Warn\", \"Info\", \"Debug\". The default is \"Error\" # Use \"Debug\" to make missing files errors invisible at the default log level(--log-level=INFO) missingFileHandler : Warn values : - {{ ` {{ .Release.Name }} ` }} /values.yaml - {{ ` {{ .Release.Name }} ` }} /{{`{{ .Values | get \"environment\" }}`}}.yaml secrets : - config/{{`{{ .Release.Name }}`}}/secrets.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}-secrets.yaml releases : - name : chart_1 chart : stable/chart_1 << : *default - name : chart_2 chart : stable/chart_2 << : *default If you're not familiar with YAML anchors, &default names the block, then *default references it. The <<: syntax says to \"extend\" (merge) that reference into the current tree. The missingFileHandler: Warn field is necessary if you don't need all the values and secret files, but want to use the same definition for all charts. {{` {{ .Release.Name }} `}} is surrounded by {{` and }}` so as not to be executed on the loading time of helmfile.yaml . We need to defer it until each release is actually processed by the helmfile command, such as diff or apply . For more information see this issue . Layering the state \u2691 You may occasionally end up with many helmfiles that shares common parts like which repositories to use, and which release to be bundled by default. Use Layering to extract the common parts into a dedicated library helmfiles, so that each helmfile becomes DRY. Let's assume that your code looks like: File: helmfile.yaml bases : - environments.yaml releases : - name : metricbeat chart : stable/metricbeat - name : myapp chart : mychart File: environments.yaml environments : development : production : At run time, bases in your helmfile.yaml are evaluated to produce: --- # environments.yaml environments : development : production : --- # helmfile.yaml releases : - name : myapp chart : mychart - name : metricbeat chart : stable/metricbeat Finally the resulting YAML documents are merged in the order of occurrence, so that your helmfile.yaml becomes: environments : development : production : releases : - name : metricbeat chart : stable/metricbeat - name : myapp chart : mychart Using this concept, we can reuse the environments section as: File: vars/environments.yaml environments : default : secrets : - ../vars/staging-secrets.yaml values : - ../vars/staging-values.yaml production : secrets : - ../vars/production-secrets.yaml values : - ../vars/production-values.yaml And the default release templates as: File: templates/templates.yaml templates : default : &default values : - {{ ` {{ .Release.Name }} ` }} /values.yaml - {{ ` {{ .Release.Name }} ` }} /{{`{{ .Values | get \"environment\" }}`}}.yaml secrets : - config/{{`{{ .Release.Name }}`}}/secrets.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}-secrets.yaml So the service's helmfile.yaml turns out to be: bases : - ../templates/environments.yaml - ../templates/templates.yaml releases : - name : chart_1 chart : stable/chart_1 << : *default - name : chart_2 chart : stable/chart_2 << : *default Much shorter and simple. Managing dependencies \u2691 Helmfile support concurrency with the option --concurrency=N so we can take advantage of it and improve our deployment speed, but to ensure it works as expected we have to define the dependencies among charts. For example, if an application needs a database, it has to be deployed before hand. releases : - name : vpn-dashboard chart : incubator/raw needs : - monitoring/prometheus-operator - name : prometheus-operator namespace : monitoring chart : prometheus-community/kube-prometheus-stack Debugging helmfile \u2691 Error: \"release-name\" has no deployed releases \u2691 This may happen when you try to install a chart and it fails. The best solution until this issue is resolved is to use helm delete --purge {{ release-name }} and then apply again. Error: failed to download \"stable/metrics-server\" (hint: running helm repo update may help) \u2691 I had this issue if verify: true in the helmfile.yaml file. Comment it or set it to false. Cannot patch X field is immutable \u2691 You may think that deleting the resource, usually a deployment or daemonset will fix it, but helmfile apply will end without any error, the resource won't be recreated , and if you do a helm list , the deployment will be marked as failed. The solution we've found is disabling the resource in the chart's values so that it's uninstalled an install it again. This can be a problem with the resources that have persistence. To patch it, edit the volume resource with kubectl edit pv -n namespace volume_pvc , change the persistentVolumeReclaimPolicy to Retain , apply the changes to uninstall, and when reinstalling configure the chart to use that volume (easier said than done). Links \u2691 Git", "title": "Helmfile"}, {"location": "devops/helmfile/#installation", "text": "Helmfile is not yet in the distribution package managers, so you'll need to install it manually. Gather the latest release number . wget {{ bin_url }} -O helmfile_linux_amd64 chmod +x helmfile_linux_amd64 mv helmfile_linux_amd64 ~/.local/bin/helmfile", "title": "Installation"}, {"location": "devops/helmfile/#usage", "text": "", "title": "Usage"}, {"location": "devops/helmfile/#how-to-deploy-a-new-chart", "text": "When we want to add a new chart, the workflow would be: Run helmfile deps && helmfile diff to check that your existing charts are updated, if they are not, run helmfile apply . Configure the release in helmfile.yaml specifying: name : Deployment name. namespace : K8s namespace to deploy. chart : Chart release. values : path pointing to the values file created above. Create a directory with the {{ chart_name }} . mkdir {{ chart_name }} Get a copy of the chart values inside that directory. helm inspect values {{ package_name }} > {{ chart_name }} /values.yaml Edit the values.yaml file according to the chart documentation. Be careful becase some charts specify the docker image version in the name. Comment out that line because upgrading the chart version without upgrading the image tag can break the service. Run helmfile deps to update the lock file. Run helmfile diff to check the changes. Run helmfile apply to apply the changes.", "title": "How to deploy a new chart"}, {"location": "devops/helmfile/#keep-charts-updated", "text": "Updating charts with helmfile is easy as long as you don't use environments, you run helmfile deps , then helmfile diff and finally helmfile apply . The tricky business comes when you want to use environments to reuse your helmfile code and don't repeat yourself. This is my suggested workflow, I've opened an issue to see if the developers agree with it: As of today, helmfile doesn't support lock files per environment , that means that the lock file needs to be shared by all of them. At a first sight this is a good idea, because it forces us to have the same versions of the charts in all the environments. The problem comes when you want to upgrade the charts of staging, test that they work and then apply the same changes in production. You'd start the process by running helmfile deps , which will read the helmfiles and update the lock file to the latest version. From this point on you need to be careful on executing the next steps in order so as not to break production. Tell your team that you're going to do the update operation, so that they don't try to run helmfile against any environment of the cluster. Run helmfile --environment=staging diff to review the changes to be introduced. To be able to see the differences of long diff files, you can filter it with egrep . helmfile diff | egrep -A20 -B20 \"^.{5}(\\-|\\+)\" It will show you all the changed lines with the 20 previous and next ones. * Once you agree on them, run helmfile --environment=staging apply to apply them. * Check that all the helm deployments are well deployed with helm list -A | grep -v deployed * Wait 20 minutes to see if the monitoring system or your fellow partners start yelling at you. * If something breaks up, try to fix it up, if you see it's going to delay you to the point that you're not going to be able to finish the upgrade in your working day, it's better to revert back to the working version of that chart and move on with the next steps. Keep in mind that since you run the apply to the last of the steps of this long process, the team is blocked by you. So prioritize to commit the next stable version to the version control repository. * Once you've checked that all the desired upgrades are working, change the context to the production cluster and run helmfile --environment=production diff . This review should be quick, as it should be the same as the staging one. * Now upgrade the production environment with helmfile --environment=production apply . * Check that all the helm deployments are well deployed with helm list -A | grep -v deployed * Wait another 20 minutes and check that everything is working. * Make a commit with the new lockfile and upload it to the version control repository. If you want the team to be involved in the review process, you can open a PR with the lock file updated with the WIP state, and upload the relevant diff of staging and production, let the discussion end and then run the apply on staging and then on production if everything goes well. Another ugly solution that I thought was to have a lockfile per environment, and let a Makefile manage them, for example, copying it to helmfile.lock before running any command.", "title": "Keep charts updated"}, {"location": "devops/helmfile/#uninstall-charts", "text": "Helmfile still doesn't remove charts if you remove them from your helmfile.yaml . To remove them you have to either set installed: false in the release candidate and execute helmfile apply or delete the release definition from your helmfile and remove it using standard helm commands.", "title": "Uninstall charts"}, {"location": "devops/helmfile/#force-the-reinstallation-of-everything", "text": "If you manually changed the deployed resources and want to reset the cluster state to the helmfile one, use helmfile sync which will reinstall all the releases.", "title": "Force the reinstallation of everything"}, {"location": "devops/helmfile/#multi-environment-project-structure", "text": "helmfile can handle environments with many different project structures. Such as the next one: \u251c\u2500\u2500 README.md \u251c\u2500\u2500 helmfile.yaml \u251c\u2500\u2500 vars \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2514\u2500\u2500 default_values.yaml \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 local_defined_chart_1 \u2502 \u2514\u2500\u2500 local_defined_chart_2 \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 environments.yaml \u2502 \u2514\u2500\u2500 templates.yaml \u251c\u2500\u2500 base \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 helmfile.yaml \u2502 \u251c\u2500\u2500 helmfile.lock \u2502 \u251c\u2500\u2500 repos.yaml \u2502 \u251c\u2500\u2500 chart_1 \u2502 \u2502 \u251c\u2500\u2500 secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 values.yaml \u2502 \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2502 \u2514\u2500\u2500 default_values.yaml \u2502 \u2514\u2500\u2500 chart_2 \u2502 \u251c\u2500\u2500 secrets.yaml \u2502 \u251c\u2500\u2500 values.yaml \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2514\u2500\u2500 default_values.yaml \u2514\u2500\u2500 service_1 \u251c\u2500\u2500 README.md \u251c\u2500\u2500 helmfile.yaml \u251c\u2500\u2500 helmfile.lock \u251c\u2500\u2500 repos.yaml \u251c\u2500\u2500 chart_1 \u2502 \u251c\u2500\u2500 secrets.yaml \u2502 \u251c\u2500\u2500 values.yaml \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2514\u2500\u2500 default_values.yaml \u2514\u2500\u2500 chart_2 \u251c\u2500\u2500 secrets.yaml \u251c\u2500\u2500 values.yaml \u251c\u2500\u2500 production_secrets.yaml \u251c\u2500\u2500 production_values.yaml \u251c\u2500\u2500 default_secrets.yaml \u2514\u2500\u2500 default_values.yaml Where: There is a general README.md that introduces the repository. Optionally there could be a helmfile.yaml file at the root with a glob pattern so that it's easy to run commands on all children helmfiles. helmfiles : - ./*/helmfile.yaml * There is a vars directory to store the variables and secrets shared by the charts that belong to different services. * There is a templates directory to store the helmfile code to reuse through templates and layering . * The project structure is defined by the services hosted in the Kubernetes cluster. Each service contains: A README.md to document the service implementation. A helmfile.yaml file to configure the service charts. A helmfile.lock to lock the versions of the service charts. A repos.yaml to define the repositories to fetch the charts from. One or more chart directories that contain the environment specific and shared chart values and secrets. There is a base service that manages all the charts required to keep the cluster running, such as the ingress, csi, cni or the cluster-autoscaler.", "title": "Multi-environment project structure"}, {"location": "devops/helmfile/#using-helmfile-environments", "text": "To customize the contents of a helmfile.yaml or values.yaml file per environment, add them under the environments key in the helmfile.yaml : environments : default : production : The environment name defaults to default , that is, helmfile sync implies the default environment. So it's a good idea to use staging as default to be more robust against human errors. If you want to specify a non-default environment, provide a --environment NAME flag to helmfile like helmfile --environment production sync . In the environments definition we'll load the values and secrets from the vars directory with the next snippet. environments : default : secrets : - ../vars/default_secrets.yaml values : - ../vars/default_values.yaml production : secrets : - ../vars/production_secrets.yaml values : - ../vars/production_values.yaml As this snippet is going to be repeated on every helmfile.yaml we'll use a state layering for it . To install a release only in one environment use: environments : default : production : --- releases : - name : newrelic-agent installed : {{ eq .Environment.Name \"production\" | toYaml }} # snip", "title": "Using helmfile environments"}, {"location": "devops/helmfile/#using-environment-specific-variables", "text": "Environment Values allows you to inject a set of values specific to the selected environment, into values.yaml templates or helmfile.yaml files. Use it to inject common values from the environment to multiple values files, to make your configuration DRY. Suppose you have three files helmfile.yaml, production.yaml and values.yaml.gotmpl File: helmfile.yaml environments : production : values : - production.yaml --- releases : - name : myapp values : - values.yaml.gotmpl File: production.yaml domain : prod.example.com File: values.yaml.gotmpl domain : {{ .Values | get \"domain\" \"dev.example.com\" }} Sadly you can't use templates in the secrets files , so you'll need to repeat the code.", "title": "Using environment specific variables"}, {"location": "devops/helmfile/#loading-the-chart-variables-and-secrets", "text": "For each chart definition in the helmfile.yaml we need to load it's secrets and values. We could use the next snippet: - name : chart_1 values : - ./chart_1/values.yaml - ./chart_1/{{ Environment.Name }}_values.yaml secrets : - ./chart_1/secrets.yaml - ./chart_1/{{ Environment.Name }}_secrets.yaml This assumes that the environment variable is set, as it's going to be shared by all the helmfiles.yaml you can add it to the vars files: File: vars/production_values.yaml environment : production File: vars/default_values.yaml environment : staging Instead of .Environment.Name , in theory you could have used .Vars | get \"environment\" , which could have prevented the variables and secrets of the default environment will need to be called default_values.yaml , and default_secrets.yaml , which is misleading. But you can't use .Values in the helmfile.yaml as it's not loaded when the file is parsed, and you get an error. A solution would be to layer the helmfile state files but I wasn't able to make it work.", "title": "Loading the chart variables and secrets"}, {"location": "devops/helmfile/#avoiding-code-repetition", "text": "Besides environments, helmfile gives other useful tricks to prevent the illness of code repetition.", "title": "Avoiding code repetition"}, {"location": "devops/helmfile/#using-release-templates", "text": "For each chart in a helmfile.yaml we're going to repeat the values and secrets sections, to avoid it, we can use release templates: templates : default : &default # This prevents helmfile exiting when it encounters a missing file # Valid values are \"Error\", \"Warn\", \"Info\", \"Debug\". The default is \"Error\" # Use \"Debug\" to make missing files errors invisible at the default log level(--log-level=INFO) missingFileHandler : Warn values : - {{ ` {{ .Release.Name }} ` }} /values.yaml - {{ ` {{ .Release.Name }} ` }} /{{`{{ .Values | get \"environment\" }}`}}.yaml secrets : - config/{{`{{ .Release.Name }}`}}/secrets.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}-secrets.yaml releases : - name : chart_1 chart : stable/chart_1 << : *default - name : chart_2 chart : stable/chart_2 << : *default If you're not familiar with YAML anchors, &default names the block, then *default references it. The <<: syntax says to \"extend\" (merge) that reference into the current tree. The missingFileHandler: Warn field is necessary if you don't need all the values and secret files, but want to use the same definition for all charts. {{` {{ .Release.Name }} `}} is surrounded by {{` and }}` so as not to be executed on the loading time of helmfile.yaml . We need to defer it until each release is actually processed by the helmfile command, such as diff or apply . For more information see this issue .", "title": "Using release templates"}, {"location": "devops/helmfile/#layering-the-state", "text": "You may occasionally end up with many helmfiles that shares common parts like which repositories to use, and which release to be bundled by default. Use Layering to extract the common parts into a dedicated library helmfiles, so that each helmfile becomes DRY. Let's assume that your code looks like: File: helmfile.yaml bases : - environments.yaml releases : - name : metricbeat chart : stable/metricbeat - name : myapp chart : mychart File: environments.yaml environments : development : production : At run time, bases in your helmfile.yaml are evaluated to produce: --- # environments.yaml environments : development : production : --- # helmfile.yaml releases : - name : myapp chart : mychart - name : metricbeat chart : stable/metricbeat Finally the resulting YAML documents are merged in the order of occurrence, so that your helmfile.yaml becomes: environments : development : production : releases : - name : metricbeat chart : stable/metricbeat - name : myapp chart : mychart Using this concept, we can reuse the environments section as: File: vars/environments.yaml environments : default : secrets : - ../vars/staging-secrets.yaml values : - ../vars/staging-values.yaml production : secrets : - ../vars/production-secrets.yaml values : - ../vars/production-values.yaml And the default release templates as: File: templates/templates.yaml templates : default : &default values : - {{ ` {{ .Release.Name }} ` }} /values.yaml - {{ ` {{ .Release.Name }} ` }} /{{`{{ .Values | get \"environment\" }}`}}.yaml secrets : - config/{{`{{ .Release.Name }}`}}/secrets.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}-secrets.yaml So the service's helmfile.yaml turns out to be: bases : - ../templates/environments.yaml - ../templates/templates.yaml releases : - name : chart_1 chart : stable/chart_1 << : *default - name : chart_2 chart : stable/chart_2 << : *default Much shorter and simple.", "title": "Layering the state"}, {"location": "devops/helmfile/#managing-dependencies", "text": "Helmfile support concurrency with the option --concurrency=N so we can take advantage of it and improve our deployment speed, but to ensure it works as expected we have to define the dependencies among charts. For example, if an application needs a database, it has to be deployed before hand. releases : - name : vpn-dashboard chart : incubator/raw needs : - monitoring/prometheus-operator - name : prometheus-operator namespace : monitoring chart : prometheus-community/kube-prometheus-stack", "title": "Managing dependencies"}, {"location": "devops/helmfile/#debugging-helmfile", "text": "", "title": "Debugging helmfile"}, {"location": "devops/helmfile/#error-release-name-has-no-deployed-releases", "text": "This may happen when you try to install a chart and it fails. The best solution until this issue is resolved is to use helm delete --purge {{ release-name }} and then apply again.", "title": "Error: \"release-name\" has no deployed releases"}, {"location": "devops/helmfile/#error-failed-to-download-stablemetrics-server-hint-running-helm-repo-update-may-help", "text": "I had this issue if verify: true in the helmfile.yaml file. Comment it or set it to false.", "title": "Error: failed to download \"stable/metrics-server\" (hint: running helm repo update may help)"}, {"location": "devops/helmfile/#cannot-patch-x-field-is-immutable", "text": "You may think that deleting the resource, usually a deployment or daemonset will fix it, but helmfile apply will end without any error, the resource won't be recreated , and if you do a helm list , the deployment will be marked as failed. The solution we've found is disabling the resource in the chart's values so that it's uninstalled an install it again. This can be a problem with the resources that have persistence. To patch it, edit the volume resource with kubectl edit pv -n namespace volume_pvc , change the persistentVolumeReclaimPolicy to Retain , apply the changes to uninstall, and when reinstalling configure the chart to use that volume (easier said than done).", "title": "Cannot patch X field is immutable"}, {"location": "devops/helmfile/#links", "text": "Git", "title": "Links"}, {"location": "devops/jwt/", "text": "JWT (JSON Web Token) is a proposed Internet standard for creating data with optional signature and/or optional encryption whose payload holds JSON that asserts some number of claims. The tokens are signed either using a private secret or a public/private key. References \u2691 Hasura.io best practices using jwt", "title": "JWT"}, {"location": "devops/jwt/#references", "text": "Hasura.io best practices using jwt", "title": "References"}, {"location": "devops/markdownlint/", "text": "markdownlint-cli is a command line interface for the markdownlint Node.js style checker and lint tool for Markdown/CommonMark files. I've evaluated these other projects ( 1 , 2 , but their configuration is less user friendly and are less maintained. You can use this cookiecutter template to create a python project with markdownlint already configured. Installation \u2691 npm install -g markdownlint-cli Configuration \u2691 To configure your project , add a .markdownlint.json in your project root directory, or in any parent. I've opened an issue to see if they are going to support pyproject.toml to save the configuration. Check the styles examples . Go to the rules document if you ever need to check more information on a specific rule. You can use it both with: The Vim through the ALE plugin . Pre-commit : File: .pre-commit-config.yaml - repo : https://github.com/igorshubovych/markdownlint-cli rev : v0.23.2 hooks : - id : markdownlint Troubleshooting \u2691 Until the #2926 PR is merged you need to change the let l:pattern=.* file to make the linting work to: File: ~/.vim/bundle/ale/autoload/ale/handlers let l :pattern = ': \\?\\(\\d*\\):\\? \\(MD\\d\\{3}\\)\\(\\/\\)\\([A-Za-z0-9-\\/]\\+\\)\\(.*\\)$' References \u2691 Git", "title": "Markdownlint"}, {"location": "devops/markdownlint/#installation", "text": "npm install -g markdownlint-cli", "title": "Installation"}, {"location": "devops/markdownlint/#configuration", "text": "To configure your project , add a .markdownlint.json in your project root directory, or in any parent. I've opened an issue to see if they are going to support pyproject.toml to save the configuration. Check the styles examples . Go to the rules document if you ever need to check more information on a specific rule. You can use it both with: The Vim through the ALE plugin . Pre-commit : File: .pre-commit-config.yaml - repo : https://github.com/igorshubovych/markdownlint-cli rev : v0.23.2 hooks : - id : markdownlint", "title": "Configuration"}, {"location": "devops/markdownlint/#troubleshooting", "text": "Until the #2926 PR is merged you need to change the let l:pattern=.* file to make the linting work to: File: ~/.vim/bundle/ale/autoload/ale/handlers let l :pattern = ': \\?\\(\\d*\\):\\? \\(MD\\d\\{3}\\)\\(\\/\\)\\([A-Za-z0-9-\\/]\\+\\)\\(.*\\)$'", "title": "Troubleshooting"}, {"location": "devops/markdownlint/#references", "text": "Git", "title": "References"}, {"location": "devops/mypy/", "text": "Mypy is an optional static type checker for Python that aims to combine the benefits of dynamic (or \"duck\") typing and static typing. Mypy combines the expressive power and convenience of Python with a powerful type system and compile-time type checking. You can use this cookiecutter template to create a python project with mypy already configured. Installation \u2691 pip install mypy Configuration \u2691 Mypy configuration is saved in the mypy.ini file, and they don't yet support pyproject.toml . File: mypy.ini [mypy] show_error_codes = True follow_imports = silent strict_optional = True warn_redundant_casts = True warn_unused_ignores = True disallow_any_generics = True check_untyped_defs = True no_implicit_reexport = True warn_unused_configs = True disallow_subclassing_any = True disallow_incomplete_defs = True disallow_untyped_decorators = True disallow_untyped_calls = True # for strict mypy: (this is the tricky one :-)) disallow_untyped_defs = True You can use it both with: Pre-commit : File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/mirrors-mypy rev : v0.782 hooks : - name : Run mypy static analysis tool id : mypy Github Actions: File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Mypy : runs-on : ubuntu-latest name : Mypy steps : - uses : actions/checkout@v1 - name : Set up Python 3.7 uses : actions/setup-python@v1 with : python-version : 3.7 - name : Install Dependencies run : pip install mypy - name : mypy run : mypy Ignore one line \u2691 Add # type: ignore to the line you want to skip. Troubleshooting \u2691 Module X has no attribute Y \u2691 If you're importing objects from your own module, you need to tell mypy that those objects are available. To do so in the __init__.py of your module, list them under the __all__ variable . File: init .py from .model import Entity __all__ = [ \"Entity\" , ] [W0707: Consider explicitly re-raising using the 'from' \u2691 keyword]( https://blog.ram.rachum.com/post/621791438475296768/improving-python-exception-chaining-with ) The error can be raised by two cases. An exception was raised, we were handling it, and something went wrong in the process of handling it. An exception was raised, and we decided to replace it with a different exception that will make more sense to whoever called this code. try : self . connection , _ = self . sock . accept () except socket . timeout as error : raise IPCException ( 'The socket timed out' ) from error The error bit at the end tells Python: The IPCException that we\u2019re raising is just a friendlier version of the socket.timeout that we just caught. When we run that code and reach that exception, the traceback is going to look like this: Traceback (most recent call last): File \"foo.py\", line 19, in self.connection, _ = self.sock.accept() File \"foo.py\", line 7, in accept raise socket.timeout socket.timeout The above exception was the direct cause of the following exception: Traceback (most recent call last): File \"foo.py\", line 21, in raise IPCException('The socket timed out') from e IPCException: The socket timed out The The above exception was the direct cause of the following exception: part tells us that we are in the second case. If you were dealing with the first one, the message between the two tracebacks would be: During handling of the above exception, another exception occurred: Issues \u2691 Incompatible return value with TypeVar : search for 10003 in repository-pattern and fix the type: ignore . References \u2691 Docs Git Homepage", "title": "Mypy"}, {"location": "devops/mypy/#installation", "text": "pip install mypy", "title": "Installation"}, {"location": "devops/mypy/#configuration", "text": "Mypy configuration is saved in the mypy.ini file, and they don't yet support pyproject.toml . File: mypy.ini [mypy] show_error_codes = True follow_imports = silent strict_optional = True warn_redundant_casts = True warn_unused_ignores = True disallow_any_generics = True check_untyped_defs = True no_implicit_reexport = True warn_unused_configs = True disallow_subclassing_any = True disallow_incomplete_defs = True disallow_untyped_decorators = True disallow_untyped_calls = True # for strict mypy: (this is the tricky one :-)) disallow_untyped_defs = True You can use it both with: Pre-commit : File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/mirrors-mypy rev : v0.782 hooks : - name : Run mypy static analysis tool id : mypy Github Actions: File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Mypy : runs-on : ubuntu-latest name : Mypy steps : - uses : actions/checkout@v1 - name : Set up Python 3.7 uses : actions/setup-python@v1 with : python-version : 3.7 - name : Install Dependencies run : pip install mypy - name : mypy run : mypy", "title": "Configuration"}, {"location": "devops/mypy/#ignore-one-line", "text": "Add # type: ignore to the line you want to skip.", "title": "Ignore one line"}, {"location": "devops/mypy/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "devops/mypy/#module-x-has-no-attribute-y", "text": "If you're importing objects from your own module, you need to tell mypy that those objects are available. To do so in the __init__.py of your module, list them under the __all__ variable . File: init .py from .model import Entity __all__ = [ \"Entity\" , ]", "title": "Module X has no attribute Y"}, {"location": "devops/mypy/#w0707-consider-explicitly-re-raising-using-the-from", "text": "keyword]( https://blog.ram.rachum.com/post/621791438475296768/improving-python-exception-chaining-with ) The error can be raised by two cases. An exception was raised, we were handling it, and something went wrong in the process of handling it. An exception was raised, and we decided to replace it with a different exception that will make more sense to whoever called this code. try : self . connection , _ = self . sock . accept () except socket . timeout as error : raise IPCException ( 'The socket timed out' ) from error The error bit at the end tells Python: The IPCException that we\u2019re raising is just a friendlier version of the socket.timeout that we just caught. When we run that code and reach that exception, the traceback is going to look like this: Traceback (most recent call last): File \"foo.py\", line 19, in self.connection, _ = self.sock.accept() File \"foo.py\", line 7, in accept raise socket.timeout socket.timeout The above exception was the direct cause of the following exception: Traceback (most recent call last): File \"foo.py\", line 21, in raise IPCException('The socket timed out') from e IPCException: The socket timed out The The above exception was the direct cause of the following exception: part tells us that we are in the second case. If you were dealing with the first one, the message between the two tracebacks would be: During handling of the above exception, another exception occurred:", "title": "[W0707: Consider explicitly re-raising using the 'from'"}, {"location": "devops/mypy/#issues", "text": "Incompatible return value with TypeVar : search for 10003 in repository-pattern and fix the type: ignore .", "title": "Issues"}, {"location": "devops/mypy/#references", "text": "Docs Git Homepage", "title": "References"}, {"location": "devops/pip_tools/", "text": "A modern alternative: poetry Pip-tools is a set of command line tools to help you keep your pip-based packages fresh, even when you've pinned them. For stability reasons it's a good idea to hardcode the dependencies versions. Furthermore, safety needs them to work properly. You can use this cookiecutter template to create a python project with pip-tools already configured. We've got three places where the dependencies are defined: setup.py should declare the loosest possible dependency versions that are still workable. Its job is to say what a particular package can work with. requirements.txt is a deployment manifest that defines an entire installation job, and shouldn't be thought of as tied to any one package. Its job is to declare an exhaustive list of all the necessary packages to make a deployment work. requirements-dev.txt Adds the dependencies required for the development of the program. Content of examples may be outdated An updated version of setup.py and requirements-dev.in can be found in the cookiecutter template . With pip-tools, the dependency management is trivial. Install the tool: pip install pip-tools Set the general dependencies in the setup.py install_requires . Generate the requirements.txt file: pip-compile -U --allow-unsafe ` The -U flag will try to upgrade the dependencies, and --allow-unsafe will let you manage the setuptools and pip dependencies. Add the additional testing dependencies in the requirements-dev.in file. File: requirements-dev.in -c requirements.txt pip-tools factory_boy pytest pytest-cov The -c line will make pip-compile look at that file for compatibility, but it won't duplicate those requirements in the requirements-dev.txt . Compile the development requirements requirements-dev.txt with pip-compile dev-requirements.in . If you have another requirements.txt for the mkdocs documentation, run pip-compile docs/requirements.txt . To install packages pip sync requirements . txt requirements - dev . txt To uninstall all pip packages use pip freeze | xargs pip uninstall -y Trigger hooks: Pre-commit: File: .pre-commit-config.yaml - repo : https://github.com/jazzband/pip-tools rev : 5.0.0 hooks : - name : Build requirements.txt id : pip-compile - name : Build dev-requirements.txt id : pip-compile args : [ 'dev-requirements.in' ] - name : Build mkdocs requirements.txt id : pip-compile args : [ 'docs/requirements.txt' ] pip-tools generates different results in the CI than in the development environment breaking the CI without an easy way to fix it. Therefore it should be run by the developers periodically. References \u2691 Git", "title": "Pip-tools"}, {"location": "devops/pip_tools/#references", "text": "Git", "title": "References"}, {"location": "devops/proselint/", "text": "Proselint is another linter for prose. Installation \u2691 pip install proselint Configuration \u2691 It can be configured through the ~/.config/proselint/config file, such as: { \"checks\" : { \"typography.diacritical_marks\" : false } } The Vim through the ALE plugin . Pre-commit : - repo : https://github.com/amperser/proselint/ rev : 0.10.2 hooks : - id : proselint exclude : LICENSE|requirements files : \\.(md|mdown|markdown)$ References \u2691 Git", "title": "Proselint"}, {"location": "devops/proselint/#installation", "text": "pip install proselint", "title": "Installation"}, {"location": "devops/proselint/#configuration", "text": "It can be configured through the ~/.config/proselint/config file, such as: { \"checks\" : { \"typography.diacritical_marks\" : false } } The Vim through the ALE plugin . Pre-commit : - repo : https://github.com/amperser/proselint/ rev : 0.10.2 hooks : - id : proselint exclude : LICENSE|requirements files : \\.(md|mdown|markdown)$", "title": "Configuration"}, {"location": "devops/proselint/#references", "text": "Git", "title": "References"}, {"location": "devops/safety/", "text": "Safety checks your installed dependencies for known security vulnerabilities. You can use this cookiecutter template to create a python project with safety already configured. Installation \u2691 pip install safety Configuration \u2691 Safety can be used through: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-safety rev : v1.1.3 hooks : - id : python-safety-dependencies-check Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : Safety : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install safety - name : Execute safety run : safety check References \u2691 Git", "title": "Safety"}, {"location": "devops/safety/#installation", "text": "pip install safety", "title": "Installation"}, {"location": "devops/safety/#configuration", "text": "Safety can be used through: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-safety rev : v1.1.3 hooks : - id : python-safety-dependencies-check Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : Safety : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install safety - name : Execute safety run : safety check", "title": "Configuration"}, {"location": "devops/safety/#references", "text": "Git", "title": "References"}, {"location": "devops/write_good/", "text": "write-good is a naive linter for English prose. Installation \u2691 npm install -g write-good There is no way to configure it through a configuration file, but it accepts command line arguments. The ALE vim implementation supports the specification of such flags with the ale_writegood_options variable: let g :ale_writegood_options = \"--no-passive\" Use write-good --help to see the available flags. As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter. References \u2691 Git", "title": "Write Good"}, {"location": "devops/write_good/#installation", "text": "npm install -g write-good There is no way to configure it through a configuration file, but it accepts command line arguments. The ALE vim implementation supports the specification of such flags with the ale_writegood_options variable: let g :ale_writegood_options = \"--no-passive\" Use write-good --help to see the available flags. As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter.", "title": "Installation"}, {"location": "devops/write_good/#references", "text": "Git", "title": "References"}, {"location": "devops/yamllint/", "text": "Yamllint is a linter for YAML files. yamllint does not only check for syntax validity, but for weirdnesses like key repetition and cosmetic problems such as lines length, trailing spaces or indentation. You can use it both with: The Vim through the ALE plugin . Pre-commit : File: .pre-commit-config.yaml - repo : https://github.com/adrienverge/yamllint rev : v1.21.0 hooks : - id : yamllint References \u2691 Git Docs", "title": "Yamllint"}, {"location": "devops/yamllint/#references", "text": "Git Docs", "title": "References"}, {"location": "devops/aws/aws/", "tags": ["WIP"], "text": "Amazon Web Services (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis. In aggregate, these cloud computing web services provide a set of primitive abstract technical infrastructure and distributed computing building blocks and tools. Learn path \u2691 TBD", "title": "AWS"}, {"location": "devops/aws/aws/#learn-path", "text": "TBD", "title": "Learn path"}, {"location": "devops/aws/eks/", "text": "Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that makes it easy for you to run Kubernetes on AWS without needing to stand up or maintain your own Kubernetes control plane. Pod limit per node \u2691 AWS EKS supports native VPC networking with the Amazon VPC Container Network Interface (CNI) plugin for Kubernetes. Using this plugin allows Kubernetes Pods to have the same IP address inside the pod as they do on the VPC network. This is a great feature but it introduces a limitation in the number of Pods per EC2 Node instance. Whenever you deploy a Pod in the EKS worker Node, EKS creates a new IP address from VPC subnet and attach to the instance. The formula for defining the maximum number of pods per instance is as follows: N * (M-1) + 2 Where: N is the number of Elastic Network Interfaces (ENI) of the instance type. M is the number of IP addresses of a single ENI. So, for t3.small , this calculation is 3 * (4-1) + 2 = 11 . For a list of all the instance types and their limits see this document Upgrade an EKS cluster \u2691 New Kubernetes versions introduce significant changes, so it's recommended that you test the behavior of your applications against a new Kubernetes version before performing the update on your production clusters. The update process consists of Amazon EKS launching new API server nodes with the updated Kubernetes version to replace the existing ones. Amazon EKS performs standard infrastructure and readiness health checks for network traffic on these new nodes to verify that they are working as expected. If any of these checks fail, Amazon EKS reverts the infrastructure deployment, and your cluster remains on the prior Kubernetes version. Running applications are not affected, and your cluster is never left in a non-deterministic or unrecoverable state. Amazon EKS regularly backs up all managed clusters, and mechanisms exist to recover clusters if necessary. We are constantly evaluating and improving our Kubernetes infrastructure management processes. To upgrade a cluster follow these steps: Upgrade all your charts to the latest version with helmfile . helmfile deps helmfile apply Check your current version and compare it with the one you want to upgrade. kubectl version --short kubectl get nodes Check the docs to see if the version you want to upgrade requires some special steps. If your worker nodes aren't at the same version as the cluster control plane upgrade them to the control plane version (never higher). Edit the cluster_version attribute of the eks terraform module and apply the changes (reviewing them first). terraform apply This is a long step (approximately 40 minutes) * Upgrade your charts again. References \u2691 Docs", "title": "EKS"}, {"location": "devops/aws/eks/#pod-limit-per-node", "text": "AWS EKS supports native VPC networking with the Amazon VPC Container Network Interface (CNI) plugin for Kubernetes. Using this plugin allows Kubernetes Pods to have the same IP address inside the pod as they do on the VPC network. This is a great feature but it introduces a limitation in the number of Pods per EC2 Node instance. Whenever you deploy a Pod in the EKS worker Node, EKS creates a new IP address from VPC subnet and attach to the instance. The formula for defining the maximum number of pods per instance is as follows: N * (M-1) + 2 Where: N is the number of Elastic Network Interfaces (ENI) of the instance type. M is the number of IP addresses of a single ENI. So, for t3.small , this calculation is 3 * (4-1) + 2 = 11 . For a list of all the instance types and their limits see this document", "title": "Pod limit per node"}, {"location": "devops/aws/eks/#upgrade-an-eks-cluster", "text": "New Kubernetes versions introduce significant changes, so it's recommended that you test the behavior of your applications against a new Kubernetes version before performing the update on your production clusters. The update process consists of Amazon EKS launching new API server nodes with the updated Kubernetes version to replace the existing ones. Amazon EKS performs standard infrastructure and readiness health checks for network traffic on these new nodes to verify that they are working as expected. If any of these checks fail, Amazon EKS reverts the infrastructure deployment, and your cluster remains on the prior Kubernetes version. Running applications are not affected, and your cluster is never left in a non-deterministic or unrecoverable state. Amazon EKS regularly backs up all managed clusters, and mechanisms exist to recover clusters if necessary. We are constantly evaluating and improving our Kubernetes infrastructure management processes. To upgrade a cluster follow these steps: Upgrade all your charts to the latest version with helmfile . helmfile deps helmfile apply Check your current version and compare it with the one you want to upgrade. kubectl version --short kubectl get nodes Check the docs to see if the version you want to upgrade requires some special steps. If your worker nodes aren't at the same version as the cluster control plane upgrade them to the control plane version (never higher). Edit the cluster_version attribute of the eks terraform module and apply the changes (reviewing them first). terraform apply This is a long step (approximately 40 minutes) * Upgrade your charts again.", "title": "Upgrade an EKS cluster"}, {"location": "devops/aws/eks/#references", "text": "Docs", "title": "References"}, {"location": "devops/aws/s3/", "text": "S3 is the secure, durable, and scalable object storage infrastructure of AWS. Often used for serving static website content or holding backups or data. Commands \u2691 Bucket management \u2691 List buckets \u2691 aws s3 ls Create bucket \u2691 aws s3api create-bucket \\ --bucket {{ bucket_name }} \\ --create-bucket-configuration LocationConstraint = us-east-1 Enable versioning \u2691 aws s3api put-bucket-versioning --bucket {{ bucket_name }} --versioning-configuration Status = Enabled Enable encryption \u2691 aws s3api put-bucket-encryption --bucket {{ bucket_name }} \\ --server-side-encryption-configuration = '{ \"Rules\": [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" } } ] }' Download bucket \u2691 aws s3 cp --recursive s3:// {{ bucket_name }} . Audit the S3 bucket policy \u2691 IFS = $( echo -en \"\\n\\b\" ) for bucket in ` aws s3 ls | awk '{ print $3 }' ` do echo \"Bucket $bucket :\" aws s3api get-bucket-acl --bucket \" $bucket \" done Add cache header to all items in a bucket \u2691 Log in to AWS Management Console. Go into S3 bucket. Select all files by route. Choose \"More\" from the menu. Select \"Change metadata\". In the \"Key\" field, select \"Cache-Control\" from the drop down menu max-age=604800Enter (7 days in seconds) for Value. Press \"Save\" button. Object management \u2691 Remove an object \u2691 aws s3 rm s3:// {{ bucket_name }} / {{ path_to_file }} Upload \u2691 Upload a local file with the cli \u2691 aws s3 cp {{ path_to_file }} s3:// {{ bucket_name }} / {{ upload_path }} Upload a file unauthenticated \u2691 curl --request PUT --upload-file test.txt https:// {{ bucket_name }} .s3.amazonaws.com/uploads/ Restore an object \u2691 First you need to get the version of the object aws s3api list-object-versions \\ --bucket {{ bucket_name }} \\ --prefix {{ bucket_path_to_file }} Fetch the VersionId and download the file aws s3api get-object \\ --bucket {{ bucket_name }} \\ --key {{ bucket_path_to_file }} \\ --version-id {{ versionid }} Once you have it, overwrite the same object in the same path aws s3 cp \\ {{ local_path_to_restored_file }} \\ s3:// {{ bucket_name }} / {{ upload_path }} Copy objects between buckets \u2691 aws s3 sync s3://SOURCE_BUCKET_NAME s3://NEW_BUCKET_NAME Troubleshooting \u2691 get_environ_proxies() missing 1 required positional argument: 'no_proxy' \u2691 sudo pip3 install --upgrade boto3 Links \u2691 User guide", "title": "S3"}, {"location": "devops/aws/s3/#commands", "text": "", "title": "Commands"}, {"location": "devops/aws/s3/#bucket-management", "text": "", "title": "Bucket management"}, {"location": "devops/aws/s3/#list-buckets", "text": "aws s3 ls", "title": "List buckets"}, {"location": "devops/aws/s3/#create-bucket", "text": "aws s3api create-bucket \\ --bucket {{ bucket_name }} \\ --create-bucket-configuration LocationConstraint = us-east-1", "title": "Create bucket"}, {"location": "devops/aws/s3/#enable-versioning", "text": "aws s3api put-bucket-versioning --bucket {{ bucket_name }} --versioning-configuration Status = Enabled", "title": "Enable versioning"}, {"location": "devops/aws/s3/#enable-encryption", "text": "aws s3api put-bucket-encryption --bucket {{ bucket_name }} \\ --server-side-encryption-configuration = '{ \"Rules\": [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" } } ] }'", "title": "Enable encryption"}, {"location": "devops/aws/s3/#download-bucket", "text": "aws s3 cp --recursive s3:// {{ bucket_name }} .", "title": "Download bucket"}, {"location": "devops/aws/s3/#audit-the-s3-bucket-policy", "text": "IFS = $( echo -en \"\\n\\b\" ) for bucket in ` aws s3 ls | awk '{ print $3 }' ` do echo \"Bucket $bucket :\" aws s3api get-bucket-acl --bucket \" $bucket \" done", "title": "Audit the S3 bucket policy"}, {"location": "devops/aws/s3/#add-cache-header-to-all-items-in-a-bucket", "text": "Log in to AWS Management Console. Go into S3 bucket. Select all files by route. Choose \"More\" from the menu. Select \"Change metadata\". In the \"Key\" field, select \"Cache-Control\" from the drop down menu max-age=604800Enter (7 days in seconds) for Value. Press \"Save\" button.", "title": "Add cache header to all items in a bucket"}, {"location": "devops/aws/s3/#object-management", "text": "", "title": "Object management"}, {"location": "devops/aws/s3/#remove-an-object", "text": "aws s3 rm s3:// {{ bucket_name }} / {{ path_to_file }}", "title": "Remove an object"}, {"location": "devops/aws/s3/#upload", "text": "", "title": "Upload"}, {"location": "devops/aws/s3/#upload-a-local-file-with-the-cli", "text": "aws s3 cp {{ path_to_file }} s3:// {{ bucket_name }} / {{ upload_path }}", "title": "Upload a local file with the cli"}, {"location": "devops/aws/s3/#upload-a-file-unauthenticated", "text": "curl --request PUT --upload-file test.txt https:// {{ bucket_name }} .s3.amazonaws.com/uploads/", "title": "Upload a file unauthenticated"}, {"location": "devops/aws/s3/#restore-an-object", "text": "First you need to get the version of the object aws s3api list-object-versions \\ --bucket {{ bucket_name }} \\ --prefix {{ bucket_path_to_file }} Fetch the VersionId and download the file aws s3api get-object \\ --bucket {{ bucket_name }} \\ --key {{ bucket_path_to_file }} \\ --version-id {{ versionid }} Once you have it, overwrite the same object in the same path aws s3 cp \\ {{ local_path_to_restored_file }} \\ s3:// {{ bucket_name }} / {{ upload_path }}", "title": "Restore an object"}, {"location": "devops/aws/s3/#copy-objects-between-buckets", "text": "aws s3 sync s3://SOURCE_BUCKET_NAME s3://NEW_BUCKET_NAME", "title": "Copy objects between buckets"}, {"location": "devops/aws/s3/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "devops/aws/s3/#get_environ_proxies-missing-1-required-positional-argument-no_proxy", "text": "sudo pip3 install --upgrade boto3", "title": "get_environ_proxies() missing 1 required positional argument: 'no_proxy'"}, {"location": "devops/aws/s3/#links", "text": "User guide", "title": "Links"}, {"location": "devops/aws/security_groups/", "text": "Security groups are the AWS way of defining firewall rules between the resources. If not handled properly they can soon become hard to read, which can lead to an insecure infrastructure. It has helped me to use four types of security groups: Default security groups: Security groups created by AWS per VPC and region, they can't be deleted. Naming security groups: Used to identify an aws resource. They are usually referenced in other security groups. Ingress security groups: Used to define the rules of ingress traffic to the resource. Egress security groups: Used to define the rules of egress traffic to the resource. But what helped most has been using clinv while refactoring all the security groups. With clinv unused I got rid of all the security groups that weren't used by any AWS resource (beware of #16 , 17 , #18 and #19 ), then used the clinv unassigned security_groups to methodically decide if they were correct and add them to my inventory or if I needed to refactor them. Best practices \u2691 Follow a naming convention . Avoid as much as you can the use of CIDRs in the definition of security groups. Instead, use naming security groups as much as you can. This will probably mean that you'll need to create security rules for each service that is going to use the security group. It is cumbersome but from a security point of view we gain traceability. Follow the principle of least privileges. Open the least number of ports required for the service to work. Reuse existing security groups. If there is a security group for web servers that uses port 80, don't create the new service using port 8080. Remove all rules from the default security groups and don't use them. Don't define the rules in the aws_security_group terraform resource. Use aws_security_group_rules for each security group to avoid creation dependency loops. Add descriptions to each security group and security group rule. Avoid using port ranges in the security group rule definitions, as you probably won't need them. Naming convention \u2691 A naming convention is required once the number of security groups starts to grow, both to understand them and to be able to search in them. Note It is assumed that terraform is used to create the resources Default security groups \u2691 There are going to be two kinds of default security groups: VPC default security groups. Region default security groups. For the first one we'll use: resource \"aws_default_security_group\" \"{{ region_id }}_{{ vpc_friendly_identifier }}\" { vpc_id = \"{{ vpc_id }}\" } Where: region_id is the region identifier with underscores, for example us_east_1 vpc_friendly_identifier is a human understandable identifier, such as publicdmz . vpc_id is the VPC id such as vpc-xxxxxxxxxxxxxxxxx . For the second one: resource \"aws_default_security_group\" \"{{ region_id }}\" { provider = aws .{{ region_id }} } Where the provider must be configured in the ` terraform_config.tf ` file , for example : ```terraform provider \"aws\" { alias = \"us_west_2\" region = \"us-west-2\" } Naming security groups \u2691 For the naming security groups I've created an UltiSnips template. snippet naming \"naming security group rule\" b resource \"aws_security_group\" \"${1:resource_name}_${2:resource_type}\" { name = \"$1-$2\" description = \"Identify the $1 $2.\" vpc_id = data.terraform_remote_state.vpc.outputs.vpc_ $ { 3 : vpc } _id tags = { Name = \"$1 $2\" } } output \"$1_$2_id\" { value = aws_security_group . $ 1 _$ 2.id } $ 0 endsnippet Where: instance_name is a human friendly identifier of the resource that the security group is going to identify, for example gitea , ci or bastion . resource_type identifies the type of resource, such as instance for EC2, or load_balancer for ELBs. vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. Once you've finished defining the security group, move the output resource to the outputs.tf file. Ingress security groups \u2691 For the ingress security groups I've created another UltiSnips template. snippet ingress \"ingress security group rule\" b resource \"aws_security_group\" \"ingress_${1:protocol}_from_${2:destination}_at_${3:vpc}\" { name = \"ingress-$1-from-$2-at-$3\" description = \"Allow the ingress of $1 traffic from the $2 instances at $3\" vpc_id = data.terraform_remote_state.vpc.outputs.vpc_ $ 3 _id tags = { Name = \"Ingress $1 from $2 at $3\" } } resource \"aws_security_group_rule\" \"ingress_$1_from_$2_at_$3\" { type = \"ingress\" from_port = $ { 4 : port } to_port = $ { 5 : $ 4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group.ingress_ $ 1 _from_$ 2 _at_$ 3.id source_security_group_id = aws_security_group . $ 7.id description = \"Allow the ingress $1 traffic on port $4 from the $2 instances at $3.\" } output \"ingress_$1_from_$2_at_$3_id\" { value = aws_security_group.ingress_ $ 1 _from_$ 2 _at_$ 3.id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to use the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will have access to the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file. Egress security groups \u2691 For the egress security groups I've created another UltiSnips template. snippet egress \"egress security group rule\" b resource \"aws_security_group\" \"egress_${1:protocol}_to_${2:destination}_from_${3:vpc}\" { name = \"egress-$1-to-$2-from-$3\" description = \"Allow the egress of $1 traffic to the $2 instances at $3\" vpc_id = data.terraform_remote_state.vpc.outputs.vpc_ $ 3 _id tags = { Name = \"Egress $1 to $2 at $3\" } } resource \"aws_security_group_rule\" \"egress_$1_to_$2_from_$3\" { type = \"egress\" from_port = $ { 4 : port } to_port = $ { 5 : $ 4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group.egress_ $ 1 _to_$ 2 _from_$ 3.id source_security_group_id = aws_security_group . $ 7.id description = \"Allow the egress of $1 traffic on port $4 to the $2 instances at $3.\" } output \"egress_$1_to_$2_from_$3_id\" { value = aws_security_group.egress_ $ 1 _to_$ 2 _from_$ 3.id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to be accessed by the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will be accessed by the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file. Instance security group definition \u2691 When defining the security groups in the aws_instance resources, define them in this order: Naming security groups. Ingress security groups. Egress security groups. For example resource \"aws_instance\" \"gitea_production\" { ami = ... availability_zone = ... subnet_id = ... vpc_security_group_ids = [ data.terraform_remote_state.security_groups.outputs.gitea_instance_id , data.terraform_remote_state.security_groups.outputs.ingress_http_from_gitea_loadbalancer_at_publicdmz_id , data.terraform_remote_state.security_groups.outputs.ingress_http_from_monitoring_at_privatedmz_id , data.terraform_remote_state.security_groups.outputs.ingress_administration_from_bastion_at_connectiondmz_id , data.terraform_remote_state.security_groups.outputs.egress_ldap_to_ldap_instance_from_publicdmz_id , data.terraform_remote_state.security_groups.outputs.egress_https_to_debian_repositories_from_publicdmz_id , ]", "title": "Security groups workflow"}, {"location": "devops/aws/security_groups/#best-practices", "text": "Follow a naming convention . Avoid as much as you can the use of CIDRs in the definition of security groups. Instead, use naming security groups as much as you can. This will probably mean that you'll need to create security rules for each service that is going to use the security group. It is cumbersome but from a security point of view we gain traceability. Follow the principle of least privileges. Open the least number of ports required for the service to work. Reuse existing security groups. If there is a security group for web servers that uses port 80, don't create the new service using port 8080. Remove all rules from the default security groups and don't use them. Don't define the rules in the aws_security_group terraform resource. Use aws_security_group_rules for each security group to avoid creation dependency loops. Add descriptions to each security group and security group rule. Avoid using port ranges in the security group rule definitions, as you probably won't need them.", "title": "Best practices"}, {"location": "devops/aws/security_groups/#naming-convention", "text": "A naming convention is required once the number of security groups starts to grow, both to understand them and to be able to search in them. Note It is assumed that terraform is used to create the resources", "title": "Naming convention"}, {"location": "devops/aws/security_groups/#default-security-groups", "text": "There are going to be two kinds of default security groups: VPC default security groups. Region default security groups. For the first one we'll use: resource \"aws_default_security_group\" \"{{ region_id }}_{{ vpc_friendly_identifier }}\" { vpc_id = \"{{ vpc_id }}\" } Where: region_id is the region identifier with underscores, for example us_east_1 vpc_friendly_identifier is a human understandable identifier, such as publicdmz . vpc_id is the VPC id such as vpc-xxxxxxxxxxxxxxxxx . For the second one: resource \"aws_default_security_group\" \"{{ region_id }}\" { provider = aws .{{ region_id }} } Where the provider must be configured in the ` terraform_config.tf ` file , for example : ```terraform provider \"aws\" { alias = \"us_west_2\" region = \"us-west-2\" }", "title": "Default security groups"}, {"location": "devops/aws/security_groups/#naming-security-groups", "text": "For the naming security groups I've created an UltiSnips template. snippet naming \"naming security group rule\" b resource \"aws_security_group\" \"${1:resource_name}_${2:resource_type}\" { name = \"$1-$2\" description = \"Identify the $1 $2.\" vpc_id = data.terraform_remote_state.vpc.outputs.vpc_ $ { 3 : vpc } _id tags = { Name = \"$1 $2\" } } output \"$1_$2_id\" { value = aws_security_group . $ 1 _$ 2.id } $ 0 endsnippet Where: instance_name is a human friendly identifier of the resource that the security group is going to identify, for example gitea , ci or bastion . resource_type identifies the type of resource, such as instance for EC2, or load_balancer for ELBs. vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. Once you've finished defining the security group, move the output resource to the outputs.tf file.", "title": "Naming security groups"}, {"location": "devops/aws/security_groups/#ingress-security-groups", "text": "For the ingress security groups I've created another UltiSnips template. snippet ingress \"ingress security group rule\" b resource \"aws_security_group\" \"ingress_${1:protocol}_from_${2:destination}_at_${3:vpc}\" { name = \"ingress-$1-from-$2-at-$3\" description = \"Allow the ingress of $1 traffic from the $2 instances at $3\" vpc_id = data.terraform_remote_state.vpc.outputs.vpc_ $ 3 _id tags = { Name = \"Ingress $1 from $2 at $3\" } } resource \"aws_security_group_rule\" \"ingress_$1_from_$2_at_$3\" { type = \"ingress\" from_port = $ { 4 : port } to_port = $ { 5 : $ 4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group.ingress_ $ 1 _from_$ 2 _at_$ 3.id source_security_group_id = aws_security_group . $ 7.id description = \"Allow the ingress $1 traffic on port $4 from the $2 instances at $3.\" } output \"ingress_$1_from_$2_at_$3_id\" { value = aws_security_group.ingress_ $ 1 _from_$ 2 _at_$ 3.id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to use the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will have access to the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file.", "title": "Ingress security groups"}, {"location": "devops/aws/security_groups/#egress-security-groups", "text": "For the egress security groups I've created another UltiSnips template. snippet egress \"egress security group rule\" b resource \"aws_security_group\" \"egress_${1:protocol}_to_${2:destination}_from_${3:vpc}\" { name = \"egress-$1-to-$2-from-$3\" description = \"Allow the egress of $1 traffic to the $2 instances at $3\" vpc_id = data.terraform_remote_state.vpc.outputs.vpc_ $ 3 _id tags = { Name = \"Egress $1 to $2 at $3\" } } resource \"aws_security_group_rule\" \"egress_$1_to_$2_from_$3\" { type = \"egress\" from_port = $ { 4 : port } to_port = $ { 5 : $ 4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group.egress_ $ 1 _to_$ 2 _from_$ 3.id source_security_group_id = aws_security_group . $ 7.id description = \"Allow the egress of $1 traffic on port $4 to the $2 instances at $3.\" } output \"egress_$1_to_$2_from_$3_id\" { value = aws_security_group.egress_ $ 1 _to_$ 2 _from_$ 3.id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to be accessed by the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will be accessed by the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file.", "title": "Egress security groups"}, {"location": "devops/aws/security_groups/#instance-security-group-definition", "text": "When defining the security groups in the aws_instance resources, define them in this order: Naming security groups. Ingress security groups. Egress security groups. For example resource \"aws_instance\" \"gitea_production\" { ami = ... availability_zone = ... subnet_id = ... vpc_security_group_ids = [ data.terraform_remote_state.security_groups.outputs.gitea_instance_id , data.terraform_remote_state.security_groups.outputs.ingress_http_from_gitea_loadbalancer_at_publicdmz_id , data.terraform_remote_state.security_groups.outputs.ingress_http_from_monitoring_at_privatedmz_id , data.terraform_remote_state.security_groups.outputs.ingress_administration_from_bastion_at_connectiondmz_id , data.terraform_remote_state.security_groups.outputs.egress_ldap_to_ldap_instance_from_publicdmz_id , data.terraform_remote_state.security_groups.outputs.egress_https_to_debian_repositories_from_publicdmz_id , ]", "title": "Instance security group definition"}, {"location": "devops/aws/iam/iam/", "text": "AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources for your users. You use IAM to control who can use your AWS resources (authentication) and what resources they can use and in what ways (authorization). Configurable AWS access controls: Grant access to AWS Management console, APIs Create individual users Manage permissions with groups Configure a strong password policy Enable Multi-Factor Authentication for privileged users Use IAM roles for EC2 instances Use IAM roles to share access Rotate security credentials regularly Restrict privileged access further with conditions Use your corporate directory system or a third party authentication Links \u2691 Docs", "title": "IAM"}, {"location": "devops/aws/iam/iam/#links", "text": "Docs", "title": "Links"}, {"location": "devops/aws/iam/iam_commands/", "text": "Information gathering \u2691 List roles \u2691 aws iam list-roles --query 'Roles[*].{RoleName: RoleName, RoleId: RoleId}' --output table List policies \u2691 aws iam list-policies --query 'Policies[*].{PolicyName: PolicyName, PolicyId: PolicyId}' --output table List attached policies \u2691 aws iam list-attached-role-policies --role-name {{ role_name }} Get role configuration \u2691 aws iam get-role --role-name {{ role_name }} Get role policies \u2691 aws iam list-role-policies --role-name {{ role_name }}", "title": "IAM Commands"}, {"location": "devops/aws/iam/iam_commands/#information-gathering", "text": "", "title": "Information gathering"}, {"location": "devops/aws/iam/iam_commands/#list-roles", "text": "aws iam list-roles --query 'Roles[*].{RoleName: RoleName, RoleId: RoleId}' --output table", "title": "List roles"}, {"location": "devops/aws/iam/iam_commands/#list-policies", "text": "aws iam list-policies --query 'Policies[*].{PolicyName: PolicyName, PolicyId: PolicyId}' --output table", "title": "List policies"}, {"location": "devops/aws/iam/iam_commands/#list-attached-policies", "text": "aws iam list-attached-role-policies --role-name {{ role_name }}", "title": "List attached policies"}, {"location": "devops/aws/iam/iam_commands/#get-role-configuration", "text": "aws iam get-role --role-name {{ role_name }}", "title": "Get role configuration"}, {"location": "devops/aws/iam/iam_commands/#get-role-policies", "text": "aws iam list-role-policies --role-name {{ role_name }}", "title": "Get role policies"}, {"location": "devops/aws/iam/iam_debug/", "text": "MFADevice entity at the same path and name already exists \u2691 It happens when a user receives an error while creating her MFA authentication. To solve it: List the existing MFA physical or virtual devices aws iam list-mfa-devices aws iam list-virtual-mfa-devices Delete the conflictive one aws iam delete-virtual-mfa-device --serial-number {{ mfa_arn }}", "title": "IAM Debugging"}, {"location": "devops/aws/iam/iam_debug/#mfadevice-entity-at-the-same-path-and-name-already-exists", "text": "It happens when a user receives an error while creating her MFA authentication. To solve it: List the existing MFA physical or virtual devices aws iam list-mfa-devices aws iam list-virtual-mfa-devices Delete the conflictive one aws iam delete-virtual-mfa-device --serial-number {{ mfa_arn }}", "title": "MFADevice entity at the same path and name already exists"}, {"location": "devops/helm/helm/", "text": "Helm is the package manager for Kubernetes. Through charts it helps you define, install and upgrade even the most complex Kubernetes applications. The advantages of using helm over kubectl apply are the easiness of: Repeatable application installation. CI integration. Versioning and sharing. Charts are a group of Go templates of kubernetes yaml resource manifests, they are easy to create, version, share, and publish. Helm alone lacks some features, that are satisfied through some external programs: Helmfile is used to declaratively configure your charts, so they can be versioned through git. Helm-secrets is used to remove hardcoded credentials from values.yaml files. Helm has an open issue to integrate it into it's codebase. Helm-git is used to install helm charts directly from Git repositories. Links \u2691 Homepage Docs Git Chart hub Git charts repositories", "title": "Helm"}, {"location": "devops/helm/helm/#links", "text": "Homepage Docs Git Chart hub Git charts repositories", "title": "Links"}, {"location": "devops/helm/helm_commands/", "text": "Small cheatsheet on how to use the helm command. List charts \u2691 helm ls Get information of chart \u2691 helm inspect {{ package_name }} List all the available versions of a chart \u2691 helm search -l {{ package_name }} Download a chart \u2691 helm fetch {{ package_name }} Download and extract helm fetch --untar {{ package_name }} Search charts \u2691 helm search {{ package_name }} Operations you should do with helmfile \u2691 The following operations can be done with helm, but consider using helmfile instead. Install chart \u2691 Helm deploys all the pods, replication controllers and services. The pod will be in a pending state while the Docker Image is downloaded and until a Persistent Volume is available. Once complete it will transition into a running state. helm install {{ package_name }} Give it a name \u2691 helm install --name {{ release_name }} {{ package_name }} Give it a namespace \u2691 helm install --namespace {{ namespace }} {{ package_name }} Customize the chart before installing \u2691 helm inspect values {{ package_name }} > values.yml Edit the values.yml helm install -f values.yml {{ package_name }} Upgrade a release \u2691 If a new version of the chart is released or you want to change the configuration use helm upgrade -f {{ config_file }} {{ release_name }} {{ package_name }} Rollback an upgrade \u2691 First check the revisions helm history {{ release_name }} Then rollback helm rollback {{ release_name }} {{ revision }} Delete a release \u2691 helm delete --purge {{ release_name }} Working with repositories \u2691 List repositories \u2691 helm repo list Add repository \u2691 helm repo add {{ repo_name }} {{ repo_url }} Update repositories \u2691 helm repo update", "title": "Helm Commands"}, {"location": "devops/helm/helm_commands/#list-charts", "text": "helm ls", "title": "List charts"}, {"location": "devops/helm/helm_commands/#get-information-of-chart", "text": "helm inspect {{ package_name }}", "title": "Get information of chart"}, {"location": "devops/helm/helm_commands/#list-all-the-available-versions-of-a-chart", "text": "helm search -l {{ package_name }}", "title": "List all the available versions of a chart"}, {"location": "devops/helm/helm_commands/#download-a-chart", "text": "helm fetch {{ package_name }} Download and extract helm fetch --untar {{ package_name }}", "title": "Download a chart"}, {"location": "devops/helm/helm_commands/#search-charts", "text": "helm search {{ package_name }}", "title": "Search charts"}, {"location": "devops/helm/helm_commands/#operations-you-should-do-with-helmfile", "text": "The following operations can be done with helm, but consider using helmfile instead.", "title": "Operations you should do with helmfile"}, {"location": "devops/helm/helm_commands/#install-chart", "text": "Helm deploys all the pods, replication controllers and services. The pod will be in a pending state while the Docker Image is downloaded and until a Persistent Volume is available. Once complete it will transition into a running state. helm install {{ package_name }}", "title": "Install chart"}, {"location": "devops/helm/helm_commands/#give-it-a-name", "text": "helm install --name {{ release_name }} {{ package_name }}", "title": "Give it a name"}, {"location": "devops/helm/helm_commands/#give-it-a-namespace", "text": "helm install --namespace {{ namespace }} {{ package_name }}", "title": "Give it a namespace"}, {"location": "devops/helm/helm_commands/#customize-the-chart-before-installing", "text": "helm inspect values {{ package_name }} > values.yml Edit the values.yml helm install -f values.yml {{ package_name }}", "title": "Customize the chart before installing"}, {"location": "devops/helm/helm_commands/#upgrade-a-release", "text": "If a new version of the chart is released or you want to change the configuration use helm upgrade -f {{ config_file }} {{ release_name }} {{ package_name }}", "title": "Upgrade a release"}, {"location": "devops/helm/helm_commands/#rollback-an-upgrade", "text": "First check the revisions helm history {{ release_name }} Then rollback helm rollback {{ release_name }} {{ revision }}", "title": "Rollback an upgrade"}, {"location": "devops/helm/helm_commands/#delete-a-release", "text": "helm delete --purge {{ release_name }}", "title": "Delete a release"}, {"location": "devops/helm/helm_commands/#working-with-repositories", "text": "", "title": "Working with repositories"}, {"location": "devops/helm/helm_commands/#list-repositories", "text": "helm repo list", "title": "List repositories"}, {"location": "devops/helm/helm_commands/#add-repository", "text": "helm repo add {{ repo_name }} {{ repo_url }}", "title": "Add repository"}, {"location": "devops/helm/helm_commands/#update-repositories", "text": "helm repo update", "title": "Update repositories"}, {"location": "devops/helm/helm_installation/", "text": "There are two usable versions of Helm, v2 and v3, the latter is quite new so some of the things we need to install as of 2020-01-27 are not yet supported (Prometheus operator), so we are going to stick to the version 2. Helm has a client-server architecture, the server is installed in the Kubernetes cluster and the client is a Go executable installed in the user computer. Helm client \u2691 You'll first need to configure kubectl. The binary is still not available in the distribution package managers, so check the latest version in the releases of their git repository , and get the download link of the tar.gz . wget {{ url_to_tar_gz }} -O helm.tar.gz tar -xvf helm.tar.gz mv linux-amd64/helm ~/.local/bin/ We are going to use some plugins inside Helmfile , so install them with: helm plugin install https://github.com/futuresimple/helm-secrets helm plugin install https://github.com/databus23/helm-diff Finally initialize the environment helm init Now that you've got Helm installed, you'll probably want to install Helmfile .", "title": "Helm Installation"}, {"location": "devops/helm/helm_installation/#helm-client", "text": "You'll first need to configure kubectl. The binary is still not available in the distribution package managers, so check the latest version in the releases of their git repository , and get the download link of the tar.gz . wget {{ url_to_tar_gz }} -O helm.tar.gz tar -xvf helm.tar.gz mv linux-amd64/helm ~/.local/bin/ We are going to use some plugins inside Helmfile , so install them with: helm plugin install https://github.com/futuresimple/helm-secrets helm plugin install https://github.com/databus23/helm-diff Finally initialize the environment helm init Now that you've got Helm installed, you'll probably want to install Helmfile .", "title": "Helm client"}, {"location": "devops/helm/helm_secrets/", "text": "Helm-secrets is a helm plugin that manages secrets with Git workflow and stores them anywhere. It delegates the cryptographic operations to Mozilla's Sops tool, which supports PGP, AWS KMS and GCP KMS. The configuration is stored in .sops.yaml files. You can find in Mozilla's documentation a detailed configuration guide. For my use case, I'm only going to use a list of PGP keys, so the following contents should be in the .sops.yaml file at the project root directory. creation_rules : - pgp : >- {{ gpg_key_1 }}, {{ gpg_key_2}} Installation \u2691 Weirdly, helm plugin install https://github.com/jkroepke/helm-secrets --version v3.9.1 asks for your github user :S so I'd rather install it by hand. wget https://github.com/jkroepke/helm-secrets/releases/download/v3.9.1/helm-secrets.tar.gz tar xvzf helm-secrets.tar.gz -C \" $( helm env HELM_PLUGINS ) \" rm helm-secrets.tar.gz If you're going to use GPG as backend you need to install sops . It's in your distribution repositories, but probably not in the latest version, therefore I suggest you install the binary directly: Grab the latest release Download, chmod +x and move it somewhere in your $PATH . Prevent committing decrypted files to git \u2691 From the docs : If you like to secure situation when decrypted file is committed by mistake to git you can add your secrets.yaml.dec files to you charts project repository .gitignore. A second level of security is to add for example a .sopscommithook file inside your chart repository local commit hook. This will prevent committing decrypted files without sops metadata. .sopscommithook content example: #!/bin/sh for FILE in $(git diff-index HEAD --name-only | grep <your vars dir> | grep \"secrets.y\"); do if [ -f \"$FILE\" ] && ! grep -C10000 \"sops:\" $FILE | grep -q \"version:\"; then echo \"!!!!! $FILE\" 'File is not encrypted !!!!!' echo \"Run : helm secrets enc <file path>\" exit 1 fi done exit Usage \u2691 Encrypt secret files \u2691 Imagine you've got a values.yaml with the following information: grafana : enabled : true adminPassword : admin If you want to encrypt adminPassword , remove that line from the values.yaml and create a secrets.yaml file with: grafana : adminPassword : supersecretpassword And encrypt the file. helm secrets enc secrets.yaml If you use Helmfile , you'll need to add the secrets file to your helmfile.yaml. values : - values.yaml secrets : - secrets.yaml From that point on, helmfile will automatically decrypt the credentials. Edit secret files \u2691 helm secrets edit secrets.yaml Decrypt secret files \u2691 helm secrets dec secrets.yaml It will generate a secrets.yaml.dec file that it's not decrypted. Be careful not to add these files to git . Clean all the decrypted files \u2691 helm secrets clean . Add or remove keys \u2691 Until this helm-secrets issue has been solved, if you want to add or remove PGP keys from .sops.yaml , you need to execute sops updatekeys -y for each secrets.yaml file in the repository. Check sops documentation for more options. Links \u2691 Git", "title": "Helm Secrets"}, {"location": "devops/helm/helm_secrets/#installation", "text": "Weirdly, helm plugin install https://github.com/jkroepke/helm-secrets --version v3.9.1 asks for your github user :S so I'd rather install it by hand. wget https://github.com/jkroepke/helm-secrets/releases/download/v3.9.1/helm-secrets.tar.gz tar xvzf helm-secrets.tar.gz -C \" $( helm env HELM_PLUGINS ) \" rm helm-secrets.tar.gz If you're going to use GPG as backend you need to install sops . It's in your distribution repositories, but probably not in the latest version, therefore I suggest you install the binary directly: Grab the latest release Download, chmod +x and move it somewhere in your $PATH .", "title": "Installation"}, {"location": "devops/helm/helm_secrets/#prevent-committing-decrypted-files-to-git", "text": "From the docs : If you like to secure situation when decrypted file is committed by mistake to git you can add your secrets.yaml.dec files to you charts project repository .gitignore. A second level of security is to add for example a .sopscommithook file inside your chart repository local commit hook. This will prevent committing decrypted files without sops metadata. .sopscommithook content example: #!/bin/sh for FILE in $(git diff-index HEAD --name-only | grep <your vars dir> | grep \"secrets.y\"); do if [ -f \"$FILE\" ] && ! grep -C10000 \"sops:\" $FILE | grep -q \"version:\"; then echo \"!!!!! $FILE\" 'File is not encrypted !!!!!' echo \"Run : helm secrets enc <file path>\" exit 1 fi done exit", "title": "Prevent committing decrypted files to git"}, {"location": "devops/helm/helm_secrets/#usage", "text": "", "title": "Usage"}, {"location": "devops/helm/helm_secrets/#encrypt-secret-files", "text": "Imagine you've got a values.yaml with the following information: grafana : enabled : true adminPassword : admin If you want to encrypt adminPassword , remove that line from the values.yaml and create a secrets.yaml file with: grafana : adminPassword : supersecretpassword And encrypt the file. helm secrets enc secrets.yaml If you use Helmfile , you'll need to add the secrets file to your helmfile.yaml. values : - values.yaml secrets : - secrets.yaml From that point on, helmfile will automatically decrypt the credentials.", "title": "Encrypt secret files"}, {"location": "devops/helm/helm_secrets/#edit-secret-files", "text": "helm secrets edit secrets.yaml", "title": "Edit secret files"}, {"location": "devops/helm/helm_secrets/#decrypt-secret-files", "text": "helm secrets dec secrets.yaml It will generate a secrets.yaml.dec file that it's not decrypted. Be careful not to add these files to git .", "title": "Decrypt secret files"}, {"location": "devops/helm/helm_secrets/#clean-all-the-decrypted-files", "text": "helm secrets clean .", "title": "Clean all the decrypted files"}, {"location": "devops/helm/helm_secrets/#add-or-remove-keys", "text": "Until this helm-secrets issue has been solved, if you want to add or remove PGP keys from .sops.yaml , you need to execute sops updatekeys -y for each secrets.yaml file in the repository. Check sops documentation for more options.", "title": "Add or remove keys"}, {"location": "devops/helm/helm_secrets/#links", "text": "Git", "title": "Links"}, {"location": "devops/kong/kong/", "text": "Kong is a lua application API platform running in Nginx. Installation \u2691 Kong supports several platforms of which we'll use Kubernetes with the helm chart , as it gives the following advantages: Kong is configured dynamically and responds to the changes in your infrastructure. Kong is deployed onto Kubernetes with a Controller, which is responsible for configuring Kong. All of Kong\u2019s configuration is done using Kubernetes resources, stored in Kubernetes\u2019 data-store (etcd). Use the power of kubectl (or any custom tooling around kubectl) to configure Kong and get benefits of all Kubernetes, such as declarative configuration, cloud-provider agnostic deployments, RBAC, reconciliation of desired state, and elastic scalability. Kong is configured using a combination of Ingress Resource and Custom Resource Definitions(CRDs). DB-less by default, meaning Kong has the capability of running without a database and using only memory storage for entities. In the helmfile.yaml add the repository and the release: repositories : - name : kong url : https://charts.konghq.com releases : - name : kong namespace : api-manager chart : kong/kong values : - kong/values.yaml secrets : - kong/secrets.yaml While particularizing the values.yaml keep in mind that: If you don't want the ingress controller set up ingressController.enabled: false , and in proxy set service: ClusterIP and ingress.enabled: true . Kong can be run with or without a database. By default the chart installs it without database. If you deploy it without database and without the ingress controller, you have to provide a declarative configuration for Kong to run. It can be provided using an existing ConfigMap dblessConfig.configMap or the whole configuration can be put into the values.yaml file for deployment itself, under the dblessConfig.config parameter. Although kong supports it's own Kubernetes resources (CRD) for plugins and consumers , I've found now way of integrating them into the helm chart, therefore I'm going to specify everything in the dblessConfig.config . So the general kong configuration values.yaml would be: dblessConfig : config : _format_version : \"1.1\" services : - name : example.com url : https://api.example.com plugins : - name : key-auth - name : rate-limiting config : second : 10 hour : 1000 policy : local routes : - name : example paths : - /example And the secrets.yaml : consumers : - username : lyz keyauth_credentials : - key : vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4 To test that everything works use curl -I https://api.example.com/example -H 'apikey: vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4' To add the prometheus monitorization, enable the serviceMonitor.enabled: true and make sure you set the correct labels . There is a grafana official dashboard you can also use. Links \u2691 Homepage Docs", "title": "Kong"}, {"location": "devops/kong/kong/#installation", "text": "Kong supports several platforms of which we'll use Kubernetes with the helm chart , as it gives the following advantages: Kong is configured dynamically and responds to the changes in your infrastructure. Kong is deployed onto Kubernetes with a Controller, which is responsible for configuring Kong. All of Kong\u2019s configuration is done using Kubernetes resources, stored in Kubernetes\u2019 data-store (etcd). Use the power of kubectl (or any custom tooling around kubectl) to configure Kong and get benefits of all Kubernetes, such as declarative configuration, cloud-provider agnostic deployments, RBAC, reconciliation of desired state, and elastic scalability. Kong is configured using a combination of Ingress Resource and Custom Resource Definitions(CRDs). DB-less by default, meaning Kong has the capability of running without a database and using only memory storage for entities. In the helmfile.yaml add the repository and the release: repositories : - name : kong url : https://charts.konghq.com releases : - name : kong namespace : api-manager chart : kong/kong values : - kong/values.yaml secrets : - kong/secrets.yaml While particularizing the values.yaml keep in mind that: If you don't want the ingress controller set up ingressController.enabled: false , and in proxy set service: ClusterIP and ingress.enabled: true . Kong can be run with or without a database. By default the chart installs it without database. If you deploy it without database and without the ingress controller, you have to provide a declarative configuration for Kong to run. It can be provided using an existing ConfigMap dblessConfig.configMap or the whole configuration can be put into the values.yaml file for deployment itself, under the dblessConfig.config parameter. Although kong supports it's own Kubernetes resources (CRD) for plugins and consumers , I've found now way of integrating them into the helm chart, therefore I'm going to specify everything in the dblessConfig.config . So the general kong configuration values.yaml would be: dblessConfig : config : _format_version : \"1.1\" services : - name : example.com url : https://api.example.com plugins : - name : key-auth - name : rate-limiting config : second : 10 hour : 1000 policy : local routes : - name : example paths : - /example And the secrets.yaml : consumers : - username : lyz keyauth_credentials : - key : vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4 To test that everything works use curl -I https://api.example.com/example -H 'apikey: vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4' To add the prometheus monitorization, enable the serviceMonitor.enabled: true and make sure you set the correct labels . There is a grafana official dashboard you can also use.", "title": "Installation"}, {"location": "devops/kong/kong/#links", "text": "Homepage Docs", "title": "Links"}, {"location": "devops/kubectl/kubectl/", "text": "Kubectl Definition Kubectl is a command line tool for controlling Kubernetes clusters. kubectl looks for a file named config in the $HOME/.kube directory. You can specify other kubeconfig files by setting the KUBECONFIG environment variable or by setting the --kubeconfig flag. Resource types and it's aliases \u2691 Resource Name Short Name clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies netpol nodes no statefulsets sts persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses thirdpartyresources Usage \u2691 Port forward / Tunnel to an internal service \u2691 If you have a service running in kubernetes and you want to directly access it instead of going through the usual path, you can use kubectl port-forward . kubectl port-forward allows using resource name, such as a pod name, service replica set or deployment, to select the matching resource to port forward to. For example, the next commands are equivalent: kubectl port-forward mongo-75f59d57f4-4nd6q 28015 :27017 kubectl port-forward deployment/mongo 28015 :27017 kubectl port-forward replicaset/mongo-75f59d57f4 28015 :27017 kubectl port-forward service/mongo 28015 :27017 The output is similar to this: Forwarding from 127 .0.0.1:28015 -> 27017 Forwarding from [ ::1 ] :28015 -> 27017 If you don't need a specific local port, you can let kubectl choose and allocate the local port and thus relieve you from having to manage local port conflicts, with the slightly simpler syntax: $: kubectl port-forward deployment/mongo :27017 Forwarding from 127 .0.0.1:63753 -> 27017 Forwarding from [ ::1 ] :63753 -> 27017 Run a command against a specific context \u2691 If you have multiple contexts and you want to be able to run commands against a context that you have access to but is not your active context you can use the --context global option for all kubectl commands: kubectl get pods --context <context_B> To get a list of available contexts use kubectl config get-contexts Links \u2691 Overview . Cheatsheet . Kbenv : Virtualenv for kubectl.", "title": "Kubectl"}, {"location": "devops/kubectl/kubectl/#resource-types-and-its-aliases", "text": "Resource Name Short Name clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies netpol nodes no statefulsets sts persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses thirdpartyresources", "title": "Resource types and it's aliases"}, {"location": "devops/kubectl/kubectl/#usage", "text": "", "title": "Usage"}, {"location": "devops/kubectl/kubectl/#port-forward-tunnel-to-an-internal-service", "text": "If you have a service running in kubernetes and you want to directly access it instead of going through the usual path, you can use kubectl port-forward . kubectl port-forward allows using resource name, such as a pod name, service replica set or deployment, to select the matching resource to port forward to. For example, the next commands are equivalent: kubectl port-forward mongo-75f59d57f4-4nd6q 28015 :27017 kubectl port-forward deployment/mongo 28015 :27017 kubectl port-forward replicaset/mongo-75f59d57f4 28015 :27017 kubectl port-forward service/mongo 28015 :27017 The output is similar to this: Forwarding from 127 .0.0.1:28015 -> 27017 Forwarding from [ ::1 ] :28015 -> 27017 If you don't need a specific local port, you can let kubectl choose and allocate the local port and thus relieve you from having to manage local port conflicts, with the slightly simpler syntax: $: kubectl port-forward deployment/mongo :27017 Forwarding from 127 .0.0.1:63753 -> 27017 Forwarding from [ ::1 ] :63753 -> 27017", "title": "Port forward / Tunnel to an internal service"}, {"location": "devops/kubectl/kubectl/#run-a-command-against-a-specific-context", "text": "If you have multiple contexts and you want to be able to run commands against a context that you have access to but is not your active context you can use the --context global option for all kubectl commands: kubectl get pods --context <context_B> To get a list of available contexts use kubectl config get-contexts", "title": "Run a command against a specific context"}, {"location": "devops/kubectl/kubectl/#links", "text": "Overview . Cheatsheet . Kbenv : Virtualenv for kubectl.", "title": "Links"}, {"location": "devops/kubectl/kubectl_commands/", "text": "Configuration and context \u2691 Add a new cluster to your kubeconf that supports basic auth \u2691 kubectl config set-credentials {{ username }} / {{ cluster_dns }} --username ={{ username }} --password ={{ password }} Create new context \u2691 kubectl config set-context {{ context_name }} --user ={{ username }} --namespace ={{ namespace }} Get current context \u2691 kubectl config current-context List contexts \u2691 kubectl config get-contexts Switch context \u2691 kubectl config use-context {{ context_name }} Creating objects \u2691 Create Resource \u2691 kubectl create -f {{ resource.definition.yaml | dir.where.yamls.live | url.to.yaml }} --record Create a configmap from a file \u2691 kubectl create configmap {{ configmap_name }} --from-file {{ path/to/file }} Deleting resources \u2691 Delete the pod using the type and name specified in a file \u2691 kubectl delete -f {{ path_to_file }} Delete pods and services by name \u2691 kubectl delete pod,service {{ pod_names }} {{ service_names }} Delete pods and services by label \u2691 kubectl delete pod,services -l {{ label_name }}={{ label_value }} Delete all pods and services in namespace \u2691 kubectl -n {{ namespace_name }} delete po,svc --all Delete all evicted pods \u2691 while read i ; do kubectl delete pod \" $i \" ; done < < ( kubectl get pods | grep -i evicted | sed 's/ .*//g' ) Editing resources \u2691 Edit a service \u2691 kubectl edit svc/ {{ service_name }} Information gathering \u2691 Get credentials \u2691 Get credentials kubectl config view --minify Deployments \u2691 Restart pods without taking the service down \u2691 kubectl rollout deployment {{ deployment_name }} View status of deployments \u2691 kubectl get deployments Describe Deployments \u2691 kubectl describe deployment {{ deployment_name }} Get images of deployment \u2691 kubectl get pods --selector = app ={{ deployment_name }} -o json | \\ jq '.items[] | .metadata.name + \": \" + .spec.containers[0].image' Nodes \u2691 List all nodes \u2691 kubectl get nodes Check which nodes are ready \u2691 JSONPATH = '{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\ && kubectl get nodes -o jsonpath = $JSONPATH | grep \"Ready=True\" External IPs of all nodes \u2691 kubectl get nodes -o jsonpath = '{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}' Get exposed ports of node \u2691 export NODE_PORT = $( kubectl get services/kubernetes-bootcamp -o go-template = '{{(index .spec.ports 0).nodePort}}' ) Pods \u2691 List all pods in the current namespace \u2691 kubectl get pods List all pods in all namespaces \u2691 kubectl get pods --all-namespaces List all pods of a selected namespace \u2691 kubectl get pods -n {{ namespace }} List with more detail \u2691 kubectl get pods -o wide Get pods of a selected deployment \u2691 kubectl get pods --selector = \"name={{ name }}\" Get pods of a given label \u2691 kubectl get pods -l {{ label_name }} Get pods by IP \u2691 kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE alpine-3835730047-ggn2v 1 /1 Running 0 5d 10 .22.19.69 ip-10-35-80-221.ec2.internal Sort pods by restart count \u2691 kubectl get pods --sort-by = '.status.containerStatuses[0].restartCount' Describe Pods \u2691 kubectl describe pods {{ pod_name }} Get name of pod \u2691 pod = $( kubectl get pod --selector ={{ selector_label }}={{ selector_value }} -o jsonpath ={ .items..metadata.name } ) List pods that belong to a particular RC \u2691 sel = ${ $( kubectl get rc my-rc --output = json | jq -j '.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"' ) %? } echo $( kubectl get pods --selector = $sel --output = jsonpath ={ .items..metadata.name } ) Services \u2691 List services in namespace \u2691 kubectl get services List services sorted by name kubectl get services --sort-by = .metadata.name Describe Services \u2691 kubectl describe services {{ service_name }} kubectl describe svc {{ service_name }} Replication controller \u2691 List all replication controller \u2691 kubectl get rc Secrets \u2691 View status of secrets \u2691 kubectl get secrets Namespaces \u2691 View namespaces \u2691 kubectl get namespaces Limits \u2691 kubectl get limitrange kubectl describe limitrange limits Jobs and cronjobs \u2691 Get cronjobs of a namespace \u2691 kubectl get cronjobs -n {{ namespace }} Get jobs of a namespace \u2691 kubectl get jobs -n {{ namespace }} You can then describe a specific job to get the pod it created. kubectl describe job -n {{ namespace }} {{ job_name }} And now you can see the evolution of the job with: kubectl logs -n {{ namespace }} {{ pod_name }} Interacting with nodes and cluster \u2691 Mark node as unschedulable \u2691 kubectl cordon {{ node_name }} Mark node as schedulable \u2691 kubectl uncordon {{ node_name }} Drain node in preparation for maintenance \u2691 kubectl drain {{ node_name }} Show metrics of all node \u2691 kubectl top node Show metrics of a node \u2691 kubectl top node {{ node_name }} Display addresses of the master and servies \u2691 kubectl cluster-info Dump current cluster state to stdout \u2691 kubectl cluster-info dump Dump current cluster state to directory \u2691 kubectl cluster-info dump --output-directory ={{ path_to_directory }} Interacting with pods \u2691 Dump logs of pod \u2691 kubectl logs {{ pod_name }} Dump logs of pod and specified container \u2691 kubectl logs {{ pod_name }} -c {{ container_name }} Stream logs of pod \u2691 kubectl logs -f {{ pod_name }} kubectl logs -f {{ pod_name }} -c {{ container_name }} Another option is to use the kubetail program. Attach to running container \u2691 kubectl attach {{ pod_name }} -i Get a shell of a running container \u2691 kubectl exec {{ pod_name }} -it bash Get a debian container inside kubernetes \u2691 kubectl run --generator = run-pod/v1 -i --tty debian --image = debian -- bash Get a root shell of a running container \u2691 Get the Node where the pod is and the docker ID kubectl describe pod {{ pod_name }} SSH into the node ssh {{ node }} Get into docker docker exec -it -u root {{ docker_id }} bash Forward port of pod to your local machine \u2691 kubectl port-forward {{ pod_name }} {{ pod_port }} : {{ local_port }} Expose port \u2691 kubectl expose {{ deployment_name }} --type = \"{{ expose_type }}\" --port {{ port_number }} Where expose_type is one of: ['NodePort', 'ClusterIP', 'LoadBalancer', 'externalName'] Run command on existing pod \u2691 kubectl exec {{ pod_name }} -- ls / kubectl exec {{ pod_name }} -c {{ container_name }} -- ls / Show metrics for a given pod and it's containers \u2691 kubectl top pod {{ pod_name }} --containers Extract file from pod \u2691 kubectl cp {{ container_id }} : {{ path_to_file }} {{ path_to_local_file }} Scaling resources \u2691 Scale a deployment with a specified size \u2691 kubectl scale deployment {{ deployment_name }} --replicas {{ replicas_number }} Scale a replicaset \u2691 kubectl scale --replicas ={{ replicas_number }} rs/ {{ replicaset_name }} Scale a resource specified in a file \u2691 kubectl scale --replicas ={{ replicas_number }} -f {{ path_to_yaml }} Updating resources \u2691 Namespaces \u2691 Temporary set the namespace for a request \u2691 kubectl -n {{ namespace_name }} {{ command_to_execute }} kubectl --namespace ={{ namespace_name }} {{ command_to_execute }} Permanently set the namespace for a request \u2691 kubectl config set-context $( kubectl config current-context ) --namespace ={{ namespace_name }} Deployment \u2691 Modify the image of a deployment \u2691 kubectl set image {{ deployment_name }} {{ label }} : {{ label_value }} for example kubectl set image deployment/nginx-deployment nginx = nginx:1.9.1 Or edit it by hand kubectl edit {{ deployment_name }} Get the status of the rolling update \u2691 kubectl rollout status {{ deployment_name }} Get the history of the deployment \u2691 kubectl rollout history deployment {{ deployment_name }} To get more details of a selected revision: kubectl rollout history deployment {{ deployment_name }} --revision ={{ revision_number }} Get back to a specified revision \u2691 To get to the last version kubectl rollout undo deployment {{ deployment_name }} To go to a specific version kubectl rollout undo {{ deployment_name }} --to-revision ={{ revision_number }} Pods \u2691 Rolling update of pods \u2691 Is prefered to use the deployment rollout kubectl rolling-update {{ pod_name }} -f {{ new_pod_definition_yaml }} Change the name of the resource and update the image \u2691 kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --image = image: {{ new_pod_version }} Abort existing rollout in progress \u2691 kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --rollback Force replace, delete and then re-create the resource \u2691 ** Will cause a service outage ** kubectl replace --force -f {{ new_pod_yaml }} Add a label \u2691 kubectl label pods {{ pod_name }} new-label ={{ new_label }} Autoscale a deployment \u2691 kubectl autoscale deployment {{ deployment_name }} --min ={{ min_instances }} --max ={{ max_instances }} [ --cpu-percent ={{ cpu_percent }}] Copy resources between namespaces \u2691 kubectl get rs,secrets -o json --namespace old | jq '.items[].metadata.namespace = \"new\"' | kubectl create-f - Formatting output \u2691 Print a table using a comma separated list of custom columns \u2691 -o = custom-columns = <spec> Print a table using the custom columns template in the file \u2691 -o = custom-columns-file = <filename> Output a JSON formatted API object \u2691 -o = json Print the fields defined in a jsonpath expression \u2691 -o = jsonpath = <template> Print the fields defined by the jsonpath expression in the file \u2691 -o = jsonpath-file = <filename> Print only the resource name and nothing else \u2691 -o = name Output in the plain-text format with any additional information, and for pods, the node name is included \u2691 -o = wide Output a YAML formatted API object \u2691 -o = yaml", "title": "Kubectl Commands"}, {"location": "devops/kubectl/kubectl_commands/#configuration-and-context", "text": "", "title": "Configuration and context"}, {"location": "devops/kubectl/kubectl_commands/#add-a-new-cluster-to-your-kubeconf-that-supports-basic-auth", "text": "kubectl config set-credentials {{ username }} / {{ cluster_dns }} --username ={{ username }} --password ={{ password }}", "title": "Add a new cluster to your kubeconf that supports basic auth"}, {"location": "devops/kubectl/kubectl_commands/#create-new-context", "text": "kubectl config set-context {{ context_name }} --user ={{ username }} --namespace ={{ namespace }}", "title": "Create new context"}, {"location": "devops/kubectl/kubectl_commands/#get-current-context", "text": "kubectl config current-context", "title": "Get current context"}, {"location": "devops/kubectl/kubectl_commands/#list-contexts", "text": "kubectl config get-contexts", "title": "List contexts"}, {"location": "devops/kubectl/kubectl_commands/#switch-context", "text": "kubectl config use-context {{ context_name }}", "title": "Switch context"}, {"location": "devops/kubectl/kubectl_commands/#creating-objects", "text": "", "title": "Creating objects"}, {"location": "devops/kubectl/kubectl_commands/#create-resource", "text": "kubectl create -f {{ resource.definition.yaml | dir.where.yamls.live | url.to.yaml }} --record", "title": "Create Resource"}, {"location": "devops/kubectl/kubectl_commands/#create-a-configmap-from-a-file", "text": "kubectl create configmap {{ configmap_name }} --from-file {{ path/to/file }}", "title": "Create a configmap from a file"}, {"location": "devops/kubectl/kubectl_commands/#deleting-resources", "text": "", "title": "Deleting resources"}, {"location": "devops/kubectl/kubectl_commands/#delete-the-pod-using-the-type-and-name-specified-in-a-file", "text": "kubectl delete -f {{ path_to_file }}", "title": "Delete the pod using the type and name specified in a file"}, {"location": "devops/kubectl/kubectl_commands/#delete-pods-and-services-by-name", "text": "kubectl delete pod,service {{ pod_names }} {{ service_names }}", "title": "Delete pods and services by name"}, {"location": "devops/kubectl/kubectl_commands/#delete-pods-and-services-by-label", "text": "kubectl delete pod,services -l {{ label_name }}={{ label_value }}", "title": "Delete pods and services by label"}, {"location": "devops/kubectl/kubectl_commands/#delete-all-pods-and-services-in-namespace", "text": "kubectl -n {{ namespace_name }} delete po,svc --all", "title": "Delete all pods and services in namespace"}, {"location": "devops/kubectl/kubectl_commands/#delete-all-evicted-pods", "text": "while read i ; do kubectl delete pod \" $i \" ; done < < ( kubectl get pods | grep -i evicted | sed 's/ .*//g' )", "title": "Delete all evicted pods"}, {"location": "devops/kubectl/kubectl_commands/#editing-resources", "text": "", "title": "Editing resources"}, {"location": "devops/kubectl/kubectl_commands/#edit-a-service", "text": "kubectl edit svc/ {{ service_name }}", "title": "Edit a service"}, {"location": "devops/kubectl/kubectl_commands/#information-gathering", "text": "", "title": "Information gathering"}, {"location": "devops/kubectl/kubectl_commands/#get-credentials", "text": "Get credentials kubectl config view --minify", "title": "Get credentials"}, {"location": "devops/kubectl/kubectl_commands/#deployments", "text": "", "title": "Deployments"}, {"location": "devops/kubectl/kubectl_commands/#restart-pods-without-taking-the-service-down", "text": "kubectl rollout deployment {{ deployment_name }}", "title": "Restart pods without taking the service down"}, {"location": "devops/kubectl/kubectl_commands/#view-status-of-deployments", "text": "kubectl get deployments", "title": "View status of deployments"}, {"location": "devops/kubectl/kubectl_commands/#describe-deployments", "text": "kubectl describe deployment {{ deployment_name }}", "title": "Describe Deployments"}, {"location": "devops/kubectl/kubectl_commands/#get-images-of-deployment", "text": "kubectl get pods --selector = app ={{ deployment_name }} -o json | \\ jq '.items[] | .metadata.name + \": \" + .spec.containers[0].image'", "title": "Get images of deployment"}, {"location": "devops/kubectl/kubectl_commands/#nodes", "text": "", "title": "Nodes"}, {"location": "devops/kubectl/kubectl_commands/#list-all-nodes", "text": "kubectl get nodes", "title": "List all nodes"}, {"location": "devops/kubectl/kubectl_commands/#check-which-nodes-are-ready", "text": "JSONPATH = '{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\ && kubectl get nodes -o jsonpath = $JSONPATH | grep \"Ready=True\"", "title": "Check which nodes are ready"}, {"location": "devops/kubectl/kubectl_commands/#external-ips-of-all-nodes", "text": "kubectl get nodes -o jsonpath = '{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}'", "title": "External IPs of all nodes"}, {"location": "devops/kubectl/kubectl_commands/#get-exposed-ports-of-node", "text": "export NODE_PORT = $( kubectl get services/kubernetes-bootcamp -o go-template = '{{(index .spec.ports 0).nodePort}}' )", "title": "Get exposed ports of node"}, {"location": "devops/kubectl/kubectl_commands/#pods", "text": "", "title": "Pods"}, {"location": "devops/kubectl/kubectl_commands/#list-all-pods-in-the-current-namespace", "text": "kubectl get pods", "title": "List all pods in the current namespace"}, {"location": "devops/kubectl/kubectl_commands/#list-all-pods-in-all-namespaces", "text": "kubectl get pods --all-namespaces", "title": "List all pods in all namespaces"}, {"location": "devops/kubectl/kubectl_commands/#list-all-pods-of-a-selected-namespace", "text": "kubectl get pods -n {{ namespace }}", "title": "List all pods of a selected namespace"}, {"location": "devops/kubectl/kubectl_commands/#list-with-more-detail", "text": "kubectl get pods -o wide", "title": "List with more detail"}, {"location": "devops/kubectl/kubectl_commands/#get-pods-of-a-selected-deployment", "text": "kubectl get pods --selector = \"name={{ name }}\"", "title": "Get pods of a selected deployment"}, {"location": "devops/kubectl/kubectl_commands/#get-pods-of-a-given-label", "text": "kubectl get pods -l {{ label_name }}", "title": "Get pods of a given label"}, {"location": "devops/kubectl/kubectl_commands/#get-pods-by-ip", "text": "kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE alpine-3835730047-ggn2v 1 /1 Running 0 5d 10 .22.19.69 ip-10-35-80-221.ec2.internal", "title": "Get pods by IP"}, {"location": "devops/kubectl/kubectl_commands/#sort-pods-by-restart-count", "text": "kubectl get pods --sort-by = '.status.containerStatuses[0].restartCount'", "title": "Sort pods by restart count"}, {"location": "devops/kubectl/kubectl_commands/#describe-pods", "text": "kubectl describe pods {{ pod_name }}", "title": "Describe Pods"}, {"location": "devops/kubectl/kubectl_commands/#get-name-of-pod", "text": "pod = $( kubectl get pod --selector ={{ selector_label }}={{ selector_value }} -o jsonpath ={ .items..metadata.name } )", "title": "Get name of pod"}, {"location": "devops/kubectl/kubectl_commands/#list-pods-that-belong-to-a-particular-rc", "text": "sel = ${ $( kubectl get rc my-rc --output = json | jq -j '.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"' ) %? } echo $( kubectl get pods --selector = $sel --output = jsonpath ={ .items..metadata.name } )", "title": "List pods that belong to a particular RC"}, {"location": "devops/kubectl/kubectl_commands/#services", "text": "", "title": "Services"}, {"location": "devops/kubectl/kubectl_commands/#list-services-in-namespace", "text": "kubectl get services List services sorted by name kubectl get services --sort-by = .metadata.name", "title": "List services in namespace"}, {"location": "devops/kubectl/kubectl_commands/#describe-services", "text": "kubectl describe services {{ service_name }} kubectl describe svc {{ service_name }}", "title": "Describe Services"}, {"location": "devops/kubectl/kubectl_commands/#replication-controller", "text": "", "title": "Replication controller"}, {"location": "devops/kubectl/kubectl_commands/#list-all-replication-controller", "text": "kubectl get rc", "title": "List all replication controller"}, {"location": "devops/kubectl/kubectl_commands/#secrets", "text": "", "title": "Secrets"}, {"location": "devops/kubectl/kubectl_commands/#view-status-of-secrets", "text": "kubectl get secrets", "title": "View status of secrets"}, {"location": "devops/kubectl/kubectl_commands/#namespaces", "text": "", "title": "Namespaces"}, {"location": "devops/kubectl/kubectl_commands/#view-namespaces", "text": "kubectl get namespaces", "title": "View namespaces"}, {"location": "devops/kubectl/kubectl_commands/#limits", "text": "kubectl get limitrange kubectl describe limitrange limits", "title": "Limits"}, {"location": "devops/kubectl/kubectl_commands/#jobs-and-cronjobs", "text": "", "title": "Jobs and cronjobs"}, {"location": "devops/kubectl/kubectl_commands/#get-cronjobs-of-a-namespace", "text": "kubectl get cronjobs -n {{ namespace }}", "title": "Get cronjobs of a namespace"}, {"location": "devops/kubectl/kubectl_commands/#get-jobs-of-a-namespace", "text": "kubectl get jobs -n {{ namespace }} You can then describe a specific job to get the pod it created. kubectl describe job -n {{ namespace }} {{ job_name }} And now you can see the evolution of the job with: kubectl logs -n {{ namespace }} {{ pod_name }}", "title": "Get jobs of a namespace"}, {"location": "devops/kubectl/kubectl_commands/#interacting-with-nodes-and-cluster", "text": "", "title": "Interacting with nodes and cluster"}, {"location": "devops/kubectl/kubectl_commands/#mark-node-as-unschedulable", "text": "kubectl cordon {{ node_name }}", "title": "Mark node as unschedulable"}, {"location": "devops/kubectl/kubectl_commands/#mark-node-as-schedulable", "text": "kubectl uncordon {{ node_name }}", "title": "Mark node as schedulable"}, {"location": "devops/kubectl/kubectl_commands/#drain-node-in-preparation-for-maintenance", "text": "kubectl drain {{ node_name }}", "title": "Drain node in preparation for maintenance"}, {"location": "devops/kubectl/kubectl_commands/#show-metrics-of-all-node", "text": "kubectl top node", "title": "Show metrics of all node"}, {"location": "devops/kubectl/kubectl_commands/#show-metrics-of-a-node", "text": "kubectl top node {{ node_name }}", "title": "Show metrics of a node"}, {"location": "devops/kubectl/kubectl_commands/#display-addresses-of-the-master-and-servies", "text": "kubectl cluster-info", "title": "Display addresses of the master and servies"}, {"location": "devops/kubectl/kubectl_commands/#dump-current-cluster-state-to-stdout", "text": "kubectl cluster-info dump", "title": "Dump current cluster state to stdout"}, {"location": "devops/kubectl/kubectl_commands/#dump-current-cluster-state-to-directory", "text": "kubectl cluster-info dump --output-directory ={{ path_to_directory }}", "title": "Dump current cluster state to directory"}, {"location": "devops/kubectl/kubectl_commands/#interacting-with-pods", "text": "", "title": "Interacting with pods"}, {"location": "devops/kubectl/kubectl_commands/#dump-logs-of-pod", "text": "kubectl logs {{ pod_name }}", "title": "Dump logs of pod"}, {"location": "devops/kubectl/kubectl_commands/#dump-logs-of-pod-and-specified-container", "text": "kubectl logs {{ pod_name }} -c {{ container_name }}", "title": "Dump logs of pod and specified container"}, {"location": "devops/kubectl/kubectl_commands/#stream-logs-of-pod", "text": "kubectl logs -f {{ pod_name }} kubectl logs -f {{ pod_name }} -c {{ container_name }} Another option is to use the kubetail program.", "title": "Stream logs of pod"}, {"location": "devops/kubectl/kubectl_commands/#attach-to-running-container", "text": "kubectl attach {{ pod_name }} -i", "title": "Attach to running container"}, {"location": "devops/kubectl/kubectl_commands/#get-a-shell-of-a-running-container", "text": "kubectl exec {{ pod_name }} -it bash", "title": "Get a shell of a running container"}, {"location": "devops/kubectl/kubectl_commands/#get-a-debian-container-inside-kubernetes", "text": "kubectl run --generator = run-pod/v1 -i --tty debian --image = debian -- bash", "title": "Get a debian container inside kubernetes"}, {"location": "devops/kubectl/kubectl_commands/#get-a-root-shell-of-a-running-container", "text": "Get the Node where the pod is and the docker ID kubectl describe pod {{ pod_name }} SSH into the node ssh {{ node }} Get into docker docker exec -it -u root {{ docker_id }} bash", "title": "Get a root shell of a running container"}, {"location": "devops/kubectl/kubectl_commands/#forward-port-of-pod-to-your-local-machine", "text": "kubectl port-forward {{ pod_name }} {{ pod_port }} : {{ local_port }}", "title": "Forward port of pod to your local machine"}, {"location": "devops/kubectl/kubectl_commands/#expose-port", "text": "kubectl expose {{ deployment_name }} --type = \"{{ expose_type }}\" --port {{ port_number }} Where expose_type is one of: ['NodePort', 'ClusterIP', 'LoadBalancer', 'externalName']", "title": "Expose port"}, {"location": "devops/kubectl/kubectl_commands/#run-command-on-existing-pod", "text": "kubectl exec {{ pod_name }} -- ls / kubectl exec {{ pod_name }} -c {{ container_name }} -- ls /", "title": "Run command on existing pod"}, {"location": "devops/kubectl/kubectl_commands/#show-metrics-for-a-given-pod-and-its-containers", "text": "kubectl top pod {{ pod_name }} --containers", "title": "Show metrics for a given pod and it's containers"}, {"location": "devops/kubectl/kubectl_commands/#extract-file-from-pod", "text": "kubectl cp {{ container_id }} : {{ path_to_file }} {{ path_to_local_file }}", "title": "Extract file from pod"}, {"location": "devops/kubectl/kubectl_commands/#scaling-resources", "text": "", "title": "Scaling resources"}, {"location": "devops/kubectl/kubectl_commands/#scale-a-deployment-with-a-specified-size", "text": "kubectl scale deployment {{ deployment_name }} --replicas {{ replicas_number }}", "title": "Scale a deployment with a specified size"}, {"location": "devops/kubectl/kubectl_commands/#scale-a-replicaset", "text": "kubectl scale --replicas ={{ replicas_number }} rs/ {{ replicaset_name }}", "title": "Scale a replicaset"}, {"location": "devops/kubectl/kubectl_commands/#scale-a-resource-specified-in-a-file", "text": "kubectl scale --replicas ={{ replicas_number }} -f {{ path_to_yaml }}", "title": "Scale a resource specified in a file"}, {"location": "devops/kubectl/kubectl_commands/#updating-resources", "text": "", "title": "Updating resources"}, {"location": "devops/kubectl/kubectl_commands/#namespaces_1", "text": "", "title": "Namespaces"}, {"location": "devops/kubectl/kubectl_commands/#temporary-set-the-namespace-for-a-request", "text": "kubectl -n {{ namespace_name }} {{ command_to_execute }} kubectl --namespace ={{ namespace_name }} {{ command_to_execute }}", "title": "Temporary set the namespace for a request"}, {"location": "devops/kubectl/kubectl_commands/#permanently-set-the-namespace-for-a-request", "text": "kubectl config set-context $( kubectl config current-context ) --namespace ={{ namespace_name }}", "title": "Permanently set the namespace for a request"}, {"location": "devops/kubectl/kubectl_commands/#deployment", "text": "", "title": "Deployment"}, {"location": "devops/kubectl/kubectl_commands/#modify-the-image-of-a-deployment", "text": "kubectl set image {{ deployment_name }} {{ label }} : {{ label_value }} for example kubectl set image deployment/nginx-deployment nginx = nginx:1.9.1 Or edit it by hand kubectl edit {{ deployment_name }}", "title": "Modify the image of a deployment"}, {"location": "devops/kubectl/kubectl_commands/#get-the-status-of-the-rolling-update", "text": "kubectl rollout status {{ deployment_name }}", "title": "Get the status of the rolling update"}, {"location": "devops/kubectl/kubectl_commands/#get-the-history-of-the-deployment", "text": "kubectl rollout history deployment {{ deployment_name }} To get more details of a selected revision: kubectl rollout history deployment {{ deployment_name }} --revision ={{ revision_number }}", "title": "Get the history of the deployment"}, {"location": "devops/kubectl/kubectl_commands/#get-back-to-a-specified-revision", "text": "To get to the last version kubectl rollout undo deployment {{ deployment_name }} To go to a specific version kubectl rollout undo {{ deployment_name }} --to-revision ={{ revision_number }}", "title": "Get back to a specified revision"}, {"location": "devops/kubectl/kubectl_commands/#pods_1", "text": "", "title": "Pods"}, {"location": "devops/kubectl/kubectl_commands/#rolling-update-of-pods", "text": "Is prefered to use the deployment rollout kubectl rolling-update {{ pod_name }} -f {{ new_pod_definition_yaml }}", "title": "Rolling update of pods"}, {"location": "devops/kubectl/kubectl_commands/#change-the-name-of-the-resource-and-update-the-image", "text": "kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --image = image: {{ new_pod_version }}", "title": "Change the name of the resource and update the image"}, {"location": "devops/kubectl/kubectl_commands/#abort-existing-rollout-in-progress", "text": "kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --rollback", "title": "Abort existing rollout in progress"}, {"location": "devops/kubectl/kubectl_commands/#force-replace-delete-and-then-re-create-the-resource", "text": "** Will cause a service outage ** kubectl replace --force -f {{ new_pod_yaml }}", "title": "Force replace, delete and then re-create the resource"}, {"location": "devops/kubectl/kubectl_commands/#add-a-label", "text": "kubectl label pods {{ pod_name }} new-label ={{ new_label }}", "title": "Add a label"}, {"location": "devops/kubectl/kubectl_commands/#autoscale-a-deployment", "text": "kubectl autoscale deployment {{ deployment_name }} --min ={{ min_instances }} --max ={{ max_instances }} [ --cpu-percent ={{ cpu_percent }}]", "title": "Autoscale a deployment"}, {"location": "devops/kubectl/kubectl_commands/#copy-resources-between-namespaces", "text": "kubectl get rs,secrets -o json --namespace old | jq '.items[].metadata.namespace = \"new\"' | kubectl create-f -", "title": "Copy resources between namespaces"}, {"location": "devops/kubectl/kubectl_commands/#formatting-output", "text": "", "title": "Formatting output"}, {"location": "devops/kubectl/kubectl_commands/#print-a-table-using-a-comma-separated-list-of-custom-columns", "text": "-o = custom-columns = <spec>", "title": "Print a table using a comma separated list of custom columns"}, {"location": "devops/kubectl/kubectl_commands/#print-a-table-using-the-custom-columns-template-in-the-file", "text": "-o = custom-columns-file = <filename>", "title": "Print a table using the custom columns template in the  file"}, {"location": "devops/kubectl/kubectl_commands/#output-a-json-formatted-api-object", "text": "-o = json", "title": "Output a JSON formatted API object"}, {"location": "devops/kubectl/kubectl_commands/#print-the-fields-defined-in-a-jsonpath-expression", "text": "-o = jsonpath = <template>", "title": "Print the fields defined in a jsonpath expression"}, {"location": "devops/kubectl/kubectl_commands/#print-the-fields-defined-by-the-jsonpath-expression-in-the-file", "text": "-o = jsonpath-file = <filename>", "title": "Print the fields defined by the jsonpath expression in the  file"}, {"location": "devops/kubectl/kubectl_commands/#print-only-the-resource-name-and-nothing-else", "text": "-o = name", "title": "Print only the resource name and nothing else"}, {"location": "devops/kubectl/kubectl_commands/#output-in-the-plain-text-format-with-any-additional-information-and-for-pods-the-node-name-is-included", "text": "-o = wide", "title": "Output in the plain-text format with any additional information, and for pods, the node name is included"}, {"location": "devops/kubectl/kubectl_commands/#output-a-yaml-formatted-api-object", "text": "-o = yaml", "title": "Output a YAML formatted API object"}, {"location": "devops/kubectl/kubectl_installation/", "text": "Kubectl is not yet in the distribution package managers, so we'll need to install it manually. curl -LO \"https://storage.googleapis.com/kubernetes-release/release/ $( \\ curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt \\ ) /bin/linux/amd64/kubectl\" chmod +x kubectl mv kubectl ~/.local/bin/kubectl Configure kubectl \u2691 Set editor \u2691 # File ~/.bashrc KUBE_EDITOR = \"vim\" Set auto completion \u2691 # File ~/.bashrc source < ( kubectl completion bash ) Configure EKS cluster \u2691 To configure the access to an existing cluster, we'll let aws-cli create the required files: aws eks update-kubeconfig --name {{ cluster_name }}", "title": "Kubectl Installation"}, {"location": "devops/kubectl/kubectl_installation/#configure-kubectl", "text": "", "title": "Configure kubectl"}, {"location": "devops/kubectl/kubectl_installation/#set-editor", "text": "# File ~/.bashrc KUBE_EDITOR = \"vim\"", "title": "Set editor"}, {"location": "devops/kubectl/kubectl_installation/#set-auto-completion", "text": "# File ~/.bashrc source < ( kubectl completion bash )", "title": "Set auto completion"}, {"location": "devops/kubectl/kubectl_installation/#configure-eks-cluster", "text": "To configure the access to an existing cluster, we'll let aws-cli create the required files: aws eks update-kubeconfig --name {{ cluster_name }}", "title": "Configure EKS cluster"}, {"location": "devops/kubernetes/kubernetes/", "text": "Kubernetes (commonly stylized as k8s) is an open-source container-orchestration system for automating application deployment, scaling, and management. Developed by Google in Go under the Apache 2.0 license, it was first released on June 7, 2014 reaching 1.0 by July 21, 2015. It works with a range of container tools, including Docker. Many cloud services offer a Kubernetes-based platform or infrastructure as a service ( PaaS or IaaS ) on which Kubernetes can be deployed as a platform-providing service. Many vendors also provide their own branded Kubernetes distributions. It has become the standard infrastructure to manage containers in production environments. Docker Swarm would be an alternative but it falls short in features compared with Kubernetes. These are some of the advantages of using Kubernetes: Widely used in production and actively developed. Ensure high availability of your services with autohealing and autoscaling. Easy, quickly and predictable deployment and promotion of applications. Seamless roll out of features. Optimize hardware use while guaranteeing resource isolation. Easiest way to build multi-cloud and baremetal environments. Several companies have used Kubernetes to release their own PaaS : OpenShift by Red Hat. Tectonic by CoreOS. Rancher labs by Rancher. Learn roadmap \u2691 K8s is huge, and growing at a pace that most mortals can't stay updated unless you work with it daily. This is how I learnt, but probably there are better resources now : Read containing container chaos kubernetes . Test the katacoda lab . Install Kubernetes in laptop with minikube . Read K8s concepts . Then K8s tasks . I didn't like the book Getting started with kubernetes I'd personally avoid the book Getting started with kubernetes , I didn't like it \u00af\\(\u00b0_o)/\u00af . Tools to test \u2691 Velero : To backup and migrate Kubernetes resources and persistent volumes. Popeye is a utility that scans live Kubernetes cluster and reports potential issues with deployed resources and configurations. It sanitizes your cluster based on what's deployed and not what's sitting on disk. By scanning your cluster, it detects misconfigurations and helps you to ensure that best practices are in place, thus preventing future headaches. It aims at reducing the cognitive overload one faces when operating a Kubernetes cluster in the wild. Furthermore, if your cluster employs a metric-server, it reports potential resources over/under allocations and attempts to warn you should your cluster run out of capacity. Popeye is a readonly tool, it does not alter any of your Kubernetes resources in any way! Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging. The query is a regular expression so the pod name can easily be filtered and you don't need to specify the exact id (for instance omitting the deployment id). If a pod is deleted it gets removed from tail and if a new pod is added it automatically gets tailed. When a pod contains multiple containers Stern can tail all of them too without having to do this manually for each one. Simply specify the container flag to limit what containers to show. By default all containers are listened to. Fairwinds' Polaris keeps your clusters sailing smoothly. It runs a variety of checks to ensure that Kubernetes pods and controllers are configured using best practices, helping you avoid problems in the future. kube-hunter hunts for security weaknesses in Kubernetes clusters. The tool was developed to increase awareness and visibility for security issues in Kubernetes environments. References \u2691 Docs Awesome K8s Katacoda playground Comic Diving deeper \u2691 Architecture Resources Kubectl Additional Components Networking Helm Tools Debugging Reference \u2691 References API conventions", "title": "Kubernetes"}, {"location": "devops/kubernetes/kubernetes/#learn-roadmap", "text": "K8s is huge, and growing at a pace that most mortals can't stay updated unless you work with it daily. This is how I learnt, but probably there are better resources now : Read containing container chaos kubernetes . Test the katacoda lab . Install Kubernetes in laptop with minikube . Read K8s concepts . Then K8s tasks . I didn't like the book Getting started with kubernetes I'd personally avoid the book Getting started with kubernetes , I didn't like it \u00af\\(\u00b0_o)/\u00af .", "title": "Learn roadmap"}, {"location": "devops/kubernetes/kubernetes/#tools-to-test", "text": "Velero : To backup and migrate Kubernetes resources and persistent volumes. Popeye is a utility that scans live Kubernetes cluster and reports potential issues with deployed resources and configurations. It sanitizes your cluster based on what's deployed and not what's sitting on disk. By scanning your cluster, it detects misconfigurations and helps you to ensure that best practices are in place, thus preventing future headaches. It aims at reducing the cognitive overload one faces when operating a Kubernetes cluster in the wild. Furthermore, if your cluster employs a metric-server, it reports potential resources over/under allocations and attempts to warn you should your cluster run out of capacity. Popeye is a readonly tool, it does not alter any of your Kubernetes resources in any way! Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging. The query is a regular expression so the pod name can easily be filtered and you don't need to specify the exact id (for instance omitting the deployment id). If a pod is deleted it gets removed from tail and if a new pod is added it automatically gets tailed. When a pod contains multiple containers Stern can tail all of them too without having to do this manually for each one. Simply specify the container flag to limit what containers to show. By default all containers are listened to. Fairwinds' Polaris keeps your clusters sailing smoothly. It runs a variety of checks to ensure that Kubernetes pods and controllers are configured using best practices, helping you avoid problems in the future. kube-hunter hunts for security weaknesses in Kubernetes clusters. The tool was developed to increase awareness and visibility for security issues in Kubernetes environments.", "title": "Tools to test"}, {"location": "devops/kubernetes/kubernetes/#references", "text": "Docs Awesome K8s Katacoda playground Comic", "title": "References"}, {"location": "devops/kubernetes/kubernetes/#diving-deeper", "text": "Architecture Resources Kubectl Additional Components Networking Helm Tools Debugging", "title": "Diving deeper"}, {"location": "devops/kubernetes/kubernetes/#reference", "text": "References API conventions", "title": "Reference"}, {"location": "devops/kubernetes/kubernetes_annotations/", "text": "Annotations are non-identifying metadata key/value pairs attached to objects, such as pods. Annotations are intended to give meaningful and relevant information to libraries and tools. Annotations, like labels, are key/value maps: \"annotations\" : { \"key1\" : \"value1\" , \"key2\" : \"value2\" } Here are some examples of information that could be recorded in annotations: Fields managed by a declarative configuration layer. Attaching these fields as annotations distinguishes them from default values set by clients or servers, and from auto generated fields and fields set by auto sizing or auto scaling systems. Build, release, or image information like timestamps, release IDs, git branch, PR numbers, image hashes, and registry address. Pointers to logging, monitoring, analytics, or audit repositories. Client library or tool information that can be used for debugging purposes, for example, name, version, and build information. User or tool/system provenance information, such as URLs of related objects from other ecosystem components. Lightweight rollout tool metadata: for example, config or checkpoints.", "title": "Annotations"}, {"location": "devops/kubernetes/kubernetes_architecture/", "text": "Kubernetes is a combination of components distributed between two kind of nodes, Masters and Workers . Master Nodes \u2691 Master nodes or Kubernetes Control Plane are the controlling unit for the cluster. They manage scheduling of new containers, configure networking and provide health information. To do so it uses: kube-api-server exposes the Kubernetes control plane API validating and configuring data for the different API objects. It's used by all the components to interact between themselves. etcd is a \"Distributed reliable key-value store for the most critical data of a distributed system\". Kubernetes uses Etcd to store state about the cluster and service discovery between nodes. This state includes what nodes exist in the cluster, which nodes they are running on and what containers should be running. kube-scheduler watches for newly created pods with no assigned node, and selects a node for them to run on. Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines. kube-controller-manager runs the following controllers: Node Controller : Responsible for noticing and responding when nodes go down. Replication Controller : Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : Populates the Endpoints object (that is, joins Services & Pods). Service Account & Token Controllers : Create default accounts and API access tokens for new namespaces. cloud-controller-manager runs controllers that interact with the underlying cloud providers. Node Controller : For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding. Route Controller : For setting up routes in the underlying cloud infrastructure. Service Controller : For creating, updating and deleting cloud provider load balancers. Volume Controller : For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes. Worker Nodes \u2691 Worker nodes are the units that hold the application data of the cluster. There can be different types of nodes: CPU architecture, amount of resources (CPU, RAM, GPU) or cloud provider. Each node has the services necessary to run pods: Container Runtime : The software responsible for running containers (Docker, rkt, containerd, CRI-O). kubelet : The primary \u201cnode agent\u201d. It works in terms of a PodSpec (a YAML or JSON object that describes it). kubelet takes a set of PodSpecs from the masters kube-api-server and ensures that the containers described are running and healthy. kube-proxy is the network proxy that runs on each node. This reflects services as defined in the Kubernetes API on each node and can do simple TCP and UDP stream forwarding or round robin across a set of backends. kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, it will forward the traffic itself. kube-proxy operation modes \u2691 kube-proxy currently supports three different operation modes: User space : This mode gets its name because the service routing takes place in kube-proxy in the user process space instead of in the kernel network stack. It is not commonly used as it is slow and outdated. iptables : This mode uses Linux kernel-level Netfilter rules to configure all routing for Kubernetes Services. This mode is the default for kube-proxy on most platforms. When load balancing for multiple backend pods, it uses unweighted round-robin scheduling. IPVS (IP Virtual Server) : Built on the Netfilter framework, IPVS implements Layer-4 load balancing in the Linux kernel, supporting multiple load-balancing algorithms, including least connections and shortest expected delay. This kube-proxy mode became generally available in Kubernetes 1.11, but it requires the Linux kernel to have the IPVS modules loaded. It is also not as widely supported by various Kubernetes networking projects as the iptables mode. Kubectl \u2691 The kubectl is the command line client used to communicate with the Masters. Number of clusters \u2691 You can run a given set of workloads either on few large clusters (with many workloads in each cluster) or on many clusters (with few workloads in each cluster). Here's a table that summarizes the pros and cons of various approaches: Figure: Possibilities of number of clusters from learnk8s.io article Reference to the original article for a full read (it's really good!). I'm going to analyze only the Large shared cluster and Cluster per environment options, as they are the closest to my use case. One Large shared cluster \u2691 With this option, you run all your workloads in the same cluster. Kubernetes provides namespaces to logically separate portions of a cluster from each other, and in the above case, you could use a separate namespace for each application instance. Pros: Efficient resource usage : You need to have only one copy of all the resources that are needed to run and manage a Kubernetes cluster (master nodes, load balancers, Ingress controllers, authentication, logging, and monitoring). Cheap : As you avoid the duplication of resources, you require less hardware. Efficient administration : Administering a Kubernetes cluster requires: Upgrading the Kubernetes version Setting up a CI/CD pipeline Installing a CNI plugin Setting up the user authentication system Installing an admission controller If you have only a single cluster, you need to do all of this only once. If you have many clusters, then you need to apply everything multiple times, which probably requires you to develop some automated processes and tools for being able to do this consistently. Cons: Single point of failure : If you have only one cluster and if that cluster breaks, then all your workloads are down. There are many ways that something can go wrong: A Kubernetes upgrade produces unexpected side effects. An cluster-wide component (such as a CNI plugin) doesn't work as expected. An erroneous configuration is made to one of the cluster components. An outage occurs in the underlying infrastructure. A single incident like this can produce major damage across all your workloads if you have only a single shared cluster. No hard security isolation : If multiple apps run in the same Kubernetes cluster, this means that these apps share the hardware, network, and operating system on the nodes of the cluster. Concretely, two containers of two different apps running on the same node are technically two processes running on the same hardware and operating system kernel. Linux containers provide some form of isolation, but this isolation is not as strong as the one provided by, for example, virtual machines (VMs). Under the hood, a process in a container is still just a process running on the host's operating system. This may be an issue from a security point of view \u2014 it theoretically allows unrelated apps to interact with each other in undesired ways (intentionally and unintentionally). Furthermore, all the workloads in a Kubernetes cluster share certain cluster-wide services, such as DNS \u2014 this allows apps to discover the Services of other apps in the cluster. It's important to keep in mind that Kubernetes is designed for sharing, and not for isolation and security. No hard multi-tenancy : Given the many shared resources in a Kubernetes cluster, there are many ways that different apps can \"step on each other's toes\". For example, an app may monopolize a certain shared resource, such as the CPU or memory, and thus starve other apps running on the same node. Kubernetes provides various ways to control this behaviour, however it's not trivial to tweak these tools in exactly the right way, and they cannot prevent every unwanted side effect either. Many users : If you have only a single cluster, then many people in your organisation must have access to this cluster. The more people have access to a system, the higher the risk that they break something. Within the cluster, you can control who can do what with role-based access control (RBAC) \u2014 however, this still can't prevent that users break something within their area of authorisation. Cluster per environment \u2691 With this option you have a separate cluster for each environment. For example, you can have a dev , test , and prod cluster where you run all the application instances of a specific environment. Isolation of the *prod environment*: In general, this approach isolates all the environments from each other \u2014 but, in practice, this especially matters for the prod environment. The production versions of your app are now not affected by whatever happens in any of the other clusters and application environments. So, if some misconfiguration creates havoc in your dev cluster, the prod versions of your apps keep running as if nothing had happened. Cluster can be customised for an environment : You can optimise each cluster for its environment \u2014 for example, you can: * Install development and debugging tools in the dev cluster. * Install testing frameworks and tools in the test cluster. * Use more powerful hardware and network connections for the prod cluster. This may improve the efficiency of both the development and operation of your apps. * Lock down access to prod cluster : Nobody really needs to do work on the prod cluster, so you can make access to it very restrictive. Cons: More administration and resources : In comparison with the single cluster. Lack of isolation between apps : The main disadvantage of this approach is the missing hardware and resource isolation between apps. Unrelated apps share cluster resources, such as the operating system kernel, CPU, memory, and several other services. As already mentioned, this may be a security issue. App requirements are not localised : If an app has special requirements, then these requirements must be satisfied in all clusters. Conclusion \u2691 If you don't need environment isolation and are not afraid of upgrading the Kubernetes cluster, go for the single cluster, otherwise use a cluster per environment. References \u2691 Kubernetes components overview", "title": "Architecture"}, {"location": "devops/kubernetes/kubernetes_architecture/#master-nodes", "text": "Master nodes or Kubernetes Control Plane are the controlling unit for the cluster. They manage scheduling of new containers, configure networking and provide health information. To do so it uses: kube-api-server exposes the Kubernetes control plane API validating and configuring data for the different API objects. It's used by all the components to interact between themselves. etcd is a \"Distributed reliable key-value store for the most critical data of a distributed system\". Kubernetes uses Etcd to store state about the cluster and service discovery between nodes. This state includes what nodes exist in the cluster, which nodes they are running on and what containers should be running. kube-scheduler watches for newly created pods with no assigned node, and selects a node for them to run on. Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines. kube-controller-manager runs the following controllers: Node Controller : Responsible for noticing and responding when nodes go down. Replication Controller : Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : Populates the Endpoints object (that is, joins Services & Pods). Service Account & Token Controllers : Create default accounts and API access tokens for new namespaces. cloud-controller-manager runs controllers that interact with the underlying cloud providers. Node Controller : For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding. Route Controller : For setting up routes in the underlying cloud infrastructure. Service Controller : For creating, updating and deleting cloud provider load balancers. Volume Controller : For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes.", "title": "Master Nodes"}, {"location": "devops/kubernetes/kubernetes_architecture/#worker-nodes", "text": "Worker nodes are the units that hold the application data of the cluster. There can be different types of nodes: CPU architecture, amount of resources (CPU, RAM, GPU) or cloud provider. Each node has the services necessary to run pods: Container Runtime : The software responsible for running containers (Docker, rkt, containerd, CRI-O). kubelet : The primary \u201cnode agent\u201d. It works in terms of a PodSpec (a YAML or JSON object that describes it). kubelet takes a set of PodSpecs from the masters kube-api-server and ensures that the containers described are running and healthy. kube-proxy is the network proxy that runs on each node. This reflects services as defined in the Kubernetes API on each node and can do simple TCP and UDP stream forwarding or round robin across a set of backends. kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, it will forward the traffic itself.", "title": "Worker Nodes"}, {"location": "devops/kubernetes/kubernetes_architecture/#kube-proxy-operation-modes", "text": "kube-proxy currently supports three different operation modes: User space : This mode gets its name because the service routing takes place in kube-proxy in the user process space instead of in the kernel network stack. It is not commonly used as it is slow and outdated. iptables : This mode uses Linux kernel-level Netfilter rules to configure all routing for Kubernetes Services. This mode is the default for kube-proxy on most platforms. When load balancing for multiple backend pods, it uses unweighted round-robin scheduling. IPVS (IP Virtual Server) : Built on the Netfilter framework, IPVS implements Layer-4 load balancing in the Linux kernel, supporting multiple load-balancing algorithms, including least connections and shortest expected delay. This kube-proxy mode became generally available in Kubernetes 1.11, but it requires the Linux kernel to have the IPVS modules loaded. It is also not as widely supported by various Kubernetes networking projects as the iptables mode.", "title": "kube-proxy operation modes"}, {"location": "devops/kubernetes/kubernetes_architecture/#kubectl", "text": "The kubectl is the command line client used to communicate with the Masters.", "title": "Kubectl"}, {"location": "devops/kubernetes/kubernetes_architecture/#number-of-clusters", "text": "You can run a given set of workloads either on few large clusters (with many workloads in each cluster) or on many clusters (with few workloads in each cluster). Here's a table that summarizes the pros and cons of various approaches: Figure: Possibilities of number of clusters from learnk8s.io article Reference to the original article for a full read (it's really good!). I'm going to analyze only the Large shared cluster and Cluster per environment options, as they are the closest to my use case.", "title": "Number of clusters"}, {"location": "devops/kubernetes/kubernetes_architecture/#one-large-shared-cluster", "text": "With this option, you run all your workloads in the same cluster. Kubernetes provides namespaces to logically separate portions of a cluster from each other, and in the above case, you could use a separate namespace for each application instance. Pros: Efficient resource usage : You need to have only one copy of all the resources that are needed to run and manage a Kubernetes cluster (master nodes, load balancers, Ingress controllers, authentication, logging, and monitoring). Cheap : As you avoid the duplication of resources, you require less hardware. Efficient administration : Administering a Kubernetes cluster requires: Upgrading the Kubernetes version Setting up a CI/CD pipeline Installing a CNI plugin Setting up the user authentication system Installing an admission controller If you have only a single cluster, you need to do all of this only once. If you have many clusters, then you need to apply everything multiple times, which probably requires you to develop some automated processes and tools for being able to do this consistently. Cons: Single point of failure : If you have only one cluster and if that cluster breaks, then all your workloads are down. There are many ways that something can go wrong: A Kubernetes upgrade produces unexpected side effects. An cluster-wide component (such as a CNI plugin) doesn't work as expected. An erroneous configuration is made to one of the cluster components. An outage occurs in the underlying infrastructure. A single incident like this can produce major damage across all your workloads if you have only a single shared cluster. No hard security isolation : If multiple apps run in the same Kubernetes cluster, this means that these apps share the hardware, network, and operating system on the nodes of the cluster. Concretely, two containers of two different apps running on the same node are technically two processes running on the same hardware and operating system kernel. Linux containers provide some form of isolation, but this isolation is not as strong as the one provided by, for example, virtual machines (VMs). Under the hood, a process in a container is still just a process running on the host's operating system. This may be an issue from a security point of view \u2014 it theoretically allows unrelated apps to interact with each other in undesired ways (intentionally and unintentionally). Furthermore, all the workloads in a Kubernetes cluster share certain cluster-wide services, such as DNS \u2014 this allows apps to discover the Services of other apps in the cluster. It's important to keep in mind that Kubernetes is designed for sharing, and not for isolation and security. No hard multi-tenancy : Given the many shared resources in a Kubernetes cluster, there are many ways that different apps can \"step on each other's toes\". For example, an app may monopolize a certain shared resource, such as the CPU or memory, and thus starve other apps running on the same node. Kubernetes provides various ways to control this behaviour, however it's not trivial to tweak these tools in exactly the right way, and they cannot prevent every unwanted side effect either. Many users : If you have only a single cluster, then many people in your organisation must have access to this cluster. The more people have access to a system, the higher the risk that they break something. Within the cluster, you can control who can do what with role-based access control (RBAC) \u2014 however, this still can't prevent that users break something within their area of authorisation.", "title": "One Large shared cluster"}, {"location": "devops/kubernetes/kubernetes_architecture/#cluster-per-environment", "text": "With this option you have a separate cluster for each environment. For example, you can have a dev , test , and prod cluster where you run all the application instances of a specific environment. Isolation of the *prod environment*: In general, this approach isolates all the environments from each other \u2014 but, in practice, this especially matters for the prod environment. The production versions of your app are now not affected by whatever happens in any of the other clusters and application environments. So, if some misconfiguration creates havoc in your dev cluster, the prod versions of your apps keep running as if nothing had happened. Cluster can be customised for an environment : You can optimise each cluster for its environment \u2014 for example, you can: * Install development and debugging tools in the dev cluster. * Install testing frameworks and tools in the test cluster. * Use more powerful hardware and network connections for the prod cluster. This may improve the efficiency of both the development and operation of your apps. * Lock down access to prod cluster : Nobody really needs to do work on the prod cluster, so you can make access to it very restrictive. Cons: More administration and resources : In comparison with the single cluster. Lack of isolation between apps : The main disadvantage of this approach is the missing hardware and resource isolation between apps. Unrelated apps share cluster resources, such as the operating system kernel, CPU, memory, and several other services. As already mentioned, this may be a security issue. App requirements are not localised : If an app has special requirements, then these requirements must be satisfied in all clusters.", "title": "Cluster per environment"}, {"location": "devops/kubernetes/kubernetes_architecture/#conclusion", "text": "If you don't need environment isolation and are not afraid of upgrading the Kubernetes cluster, go for the single cluster, otherwise use a cluster per environment.", "title": "Conclusion"}, {"location": "devops/kubernetes/kubernetes_architecture/#references", "text": "Kubernetes components overview", "title": "References"}, {"location": "devops/kubernetes/kubernetes_cluster_autoscaler/", "text": "While Horizontal pod autoscaling allows a deployment to scale given the resources needed, they are limited to the kubernetes existing working nodes. To autoscale the number of working nodes we need the cluster autoscaler . For AWS, there are the Amazon guidelines to enable it . But I'd use the cluster-autoscaler helm chart.", "title": "Cluster Autoscaler"}, {"location": "devops/kubernetes/kubernetes_dashboard/", "text": "Dashboard definition Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard. Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred. Deployment \u2691 The best way to install it is with the stable/kubernetes-dashboard chart with helmfile . Links \u2691 Git Documentation Kubernetes introduction to the dashboard Hasham Haider guide", "title": "Dashboard"}, {"location": "devops/kubernetes/kubernetes_dashboard/#deployment", "text": "The best way to install it is with the stable/kubernetes-dashboard chart with helmfile .", "title": "Deployment"}, {"location": "devops/kubernetes/kubernetes_dashboard/#links", "text": "Git Documentation Kubernetes introduction to the dashboard Hasham Haider guide", "title": "Links"}, {"location": "devops/kubernetes/kubernetes_deployments/", "text": "The different types of deployments configure a ReplicaSet and a PodSchema for your application. Depending on the type of application we'll use one of the following types. Deployments \u2691 Deployments are the controller for stateless applications, therefore it favors availability over consistency. It provides availability by creating multiple copies of the same pod. Those pods are disposable\u200a\u2014\u200aif they become unhealthy, Deployment will just create new replacements. What\u2019s more, you can update Deployment at a controlled rate, without a service outage. When an incident like the AWS outage happens, your workloads will automatically recover. Pods created by Deployment won't have the same identity after being killed and recreated, and they don't have unique persistent storage either. Concrete examples: Nginx, Tomcat A typical use case is: Create a Deployment to bring up a Replica Set and Pods. Check the status of a Deployment to see if it succeeds or not. Later, update that Deployment to recreate the Pods (for example, to use a new image). Rollback to an earlier Deployment revision if the current Deployment isn't stable. Pause and resume a Deployment. Deployment example apiVersion : apps/v1beta1 kind : Deployment metadata : name : nginx-deployment spec : replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.7.9 ports : - containerPort : 80 StatefulSets \u2691 StatefulSets are the controller for stateful applications, therefore it favors consistency over availability. If your applications need to store data, like databases, cache, and message queues, each of your stateful pod will need a stronger notion of identity. Unlike Deployment , StatefulSets creates pods with stable, unique, and sticky identity and storage. You can deploy, scale, and delete pods in order. Which is safer, and makes it easier for you to reason about your stateful applications. Concrete examples: Zookeeper, MongoDB, MySQL The tricky part of the StatefulSets is that in multi node cluster on different regions in AWS, as the persistence is achieved with EBS volumes and they are fixed to a region. Your pod will always spawn in the same node. If there is a failure in that node, the pod will be marked as unschedulable and the service will be down. So as of 2020-03-02 is still not advised to host your databases inside kubernetes unless you have an operator . DaemonSet \u2691 DaemonSets are the controller for applications that need to be run in each node. It's useful for recollecting log or monitoring purposes. DaemonSet makes sure that every node runs a copy of a pod. If you add or remove nodes, pods will be created or removed on them automatically. If you just want to run the daemons on some of the nodes, use node labels to control it. Concrete examples: fluentd, linkerd Job \u2691 Jobs are the controller to run batch processing workloads. It creates multiple pods running in parallel to process independent but related work items. This can be the emails to be sent or frames to be rendered.", "title": "Deployments"}, {"location": "devops/kubernetes/kubernetes_deployments/#deployments", "text": "Deployments are the controller for stateless applications, therefore it favors availability over consistency. It provides availability by creating multiple copies of the same pod. Those pods are disposable\u200a\u2014\u200aif they become unhealthy, Deployment will just create new replacements. What\u2019s more, you can update Deployment at a controlled rate, without a service outage. When an incident like the AWS outage happens, your workloads will automatically recover. Pods created by Deployment won't have the same identity after being killed and recreated, and they don't have unique persistent storage either. Concrete examples: Nginx, Tomcat A typical use case is: Create a Deployment to bring up a Replica Set and Pods. Check the status of a Deployment to see if it succeeds or not. Later, update that Deployment to recreate the Pods (for example, to use a new image). Rollback to an earlier Deployment revision if the current Deployment isn't stable. Pause and resume a Deployment. Deployment example apiVersion : apps/v1beta1 kind : Deployment metadata : name : nginx-deployment spec : replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.7.9 ports : - containerPort : 80", "title": "Deployments"}, {"location": "devops/kubernetes/kubernetes_deployments/#statefulsets", "text": "StatefulSets are the controller for stateful applications, therefore it favors consistency over availability. If your applications need to store data, like databases, cache, and message queues, each of your stateful pod will need a stronger notion of identity. Unlike Deployment , StatefulSets creates pods with stable, unique, and sticky identity and storage. You can deploy, scale, and delete pods in order. Which is safer, and makes it easier for you to reason about your stateful applications. Concrete examples: Zookeeper, MongoDB, MySQL The tricky part of the StatefulSets is that in multi node cluster on different regions in AWS, as the persistence is achieved with EBS volumes and they are fixed to a region. Your pod will always spawn in the same node. If there is a failure in that node, the pod will be marked as unschedulable and the service will be down. So as of 2020-03-02 is still not advised to host your databases inside kubernetes unless you have an operator .", "title": "StatefulSets"}, {"location": "devops/kubernetes/kubernetes_deployments/#daemonset", "text": "DaemonSets are the controller for applications that need to be run in each node. It's useful for recollecting log or monitoring purposes. DaemonSet makes sure that every node runs a copy of a pod. If you add or remove nodes, pods will be created or removed on them automatically. If you just want to run the daemons on some of the nodes, use node labels to control it. Concrete examples: fluentd, linkerd", "title": "DaemonSet"}, {"location": "devops/kubernetes/kubernetes_deployments/#job", "text": "Jobs are the controller to run batch processing workloads. It creates multiple pods running in parallel to process independent but related work items. This can be the emails to be sent or frames to be rendered.", "title": "Job"}, {"location": "devops/kubernetes/kubernetes_external_dns/", "text": "The external-dns resource allows the creation of DNS records from within kubernetes inside the definition of service and ingress resources. It currently supports the following providers: Provider Status Google Cloud DNS Stable AWS Route 53 Stable AWS Cloud Map Beta AzureDNS Beta CloudFlare Beta RcodeZero Alpha DigitalOcean Alpha DNSimple Alpha Infoblox Alpha Dyn Alpha OpenStack Designate Alpha PowerDNS Alpha CoreDNS Alpha Exoscale Alpha Oracle Cloud Infrastructure DNS Alpha Linode DNS Alpha RFC2136 Alpha NS1 Alpha TransIP Alpha VinylDNS Alpha RancherDNS Alpha Akamai FastDNS Alpha There are two reasons to enable it: If there is any change in the ingress or service load balancer endpoint, due to a deployment, the dns records are automatically changed. It's easier for developers to connect their applications. Deployment in AWS \u2691 To install it inside EKS, create the ExternalDNSEKSIAMPolicy . ExternalDNSEKSIAMPolicy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ListHostedZones\" , \"route53:ListResourceRecordSets\" ], \"Resource\" : [ \"*\" ] } ] } and the associated eks-external-dns role that will be attached to the pod service account. When defining iam_role resources that are going to be used inside EKS, you need to use the EKS OIDC provider as trusted entity, so instead of using the default empty role policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" } } ] } We'll use, as stated in the Creating an IAM Role and Policy for Service Account and Specifying an IAM Role for your Service Account documents: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringEquals\" : { \"oidc.eks.us-east-1.amazonaws.com/id/{{ cluster_fingerprint }}:sub\" : \"system:serviceaccount:{{ service_account_namespace }}:{{ service_account_name }}\" } }, \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::{{ aws_account_id }}:oidc-provider/oidc.eks.{{ aws_zone }}.amazonaws.com/id/{{ cluster_fingerprint }}\" } } ] } Then particularize the external-dns helm chart. There are two ways of attaching the IAM role to external-dns , using the asumeRoleArn attribute on the aws values.yaml key or under the rbac serviceAccountAnnotations . I've tried the first but it gave an error because the cluster IAM role didn't have permissions to execute the assume role on that specific role. The second worked flawlessly. For more information visit the official external-dns aws documentation .", "title": "External DNS"}, {"location": "devops/kubernetes/kubernetes_external_dns/#deployment-in-aws", "text": "To install it inside EKS, create the ExternalDNSEKSIAMPolicy . ExternalDNSEKSIAMPolicy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ListHostedZones\" , \"route53:ListResourceRecordSets\" ], \"Resource\" : [ \"*\" ] } ] } and the associated eks-external-dns role that will be attached to the pod service account. When defining iam_role resources that are going to be used inside EKS, you need to use the EKS OIDC provider as trusted entity, so instead of using the default empty role policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" } } ] } We'll use, as stated in the Creating an IAM Role and Policy for Service Account and Specifying an IAM Role for your Service Account documents: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringEquals\" : { \"oidc.eks.us-east-1.amazonaws.com/id/{{ cluster_fingerprint }}:sub\" : \"system:serviceaccount:{{ service_account_namespace }}:{{ service_account_name }}\" } }, \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::{{ aws_account_id }}:oidc-provider/oidc.eks.{{ aws_zone }}.amazonaws.com/id/{{ cluster_fingerprint }}\" } } ] } Then particularize the external-dns helm chart. There are two ways of attaching the IAM role to external-dns , using the asumeRoleArn attribute on the aws values.yaml key or under the rbac serviceAccountAnnotations . I've tried the first but it gave an error because the cluster IAM role didn't have permissions to execute the assume role on that specific role. The second worked flawlessly. For more information visit the official external-dns aws documentation .", "title": "Deployment in AWS"}, {"location": "devops/kubernetes/kubernetes_hpa/", "text": "With Horizontal pod autoscaling , Kubernetes automatically scales the number of pods in a deployment or replication controller based on observed CPU utilization or on some other application provided metrics. The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user. To make it work, the definition of pod resource consumption needs to be specified.", "title": "Horizontal Pod Autoscaling"}, {"location": "devops/kubernetes/kubernetes_ingress/", "text": "An Ingress is An API object that manages external access to the services in a cluster, typically HTTP. Ingress provide a centralized way to: Load balancing. SSL termination. Dynamic service discovery. Traffic routing. Authentication. Traffic distribution: canary deployments, A/B testing, mirroring/shadowing. Graphical user interface. JWT validation. WAF and DDOS protection. Requests tracing. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type NodePort or LoadBalancer .", "title": "Ingress"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/", "text": "Ingress controllers monitor the cluster events for the creation or modification of Ingress resources, modifying accordingly the underlying load balancers. They are not part of the master kube-controller-manager , so you'll need to install them manually. There are different Ingress controllers, such as AWS ALB, Nginx, HAProxy or Traefik, using one or other depends on your needs. Almost all controllers are open sourced and support dynamic service discovery, SSL termination or WebSockets. But they differ in: Supported protocols : HTTP, HTTPS, gRPC, HTTP/2.0, TCP (with SNI) or UDP. Underlying software : NGINX, Traefik, HAProxy or Envoy. Traffic routing : host and path, regular expression support. Namespace limitations : supported or not. Upstream probes : active checks, passive checks, retries, circuit breakers, custom health checks... Load balancing algorithms : round-robin, sticky sessions, rdp-cookie... Authentication : Basic, digest, Oauth, external auth, SSL certificate... Traffic distribution : canary deployments, A/B testing, mirroring/shadowing. Paid subscription : extended functionality or technical support. Graphical user interface : JWT validation : Customization of configuration : Basic DDOS protection mechanisms : rate limit, traffic filtering. WAF : Requests tracing : monitor, trace and debug requests via OpenTracing or other options. Both ITNext and Flant provide good ingress controller comparisons, a synoptical resume of both articles follows. Kubernetes Ingress controller \u2691 The \u201cofficial\u201d Kubernetes controller. Not to be confused with the one offered by the NGINX company. Developed by the community, it's based on the Nginx web server with a set of Lua plugins to implement extra features. Thanks to the popularity of NGINX and minimal modifications over it when using as a controller, it can be the simplest and most straightforward option for an average engineer dealing with K8s. Traefik \u2691 Originally, this proxy was created for the routing of requests for microservices and their dynamic environment, hence many of its useful features: Continuous update of configuration (no restarts) . Support for multiple load balancing algorithms. Web UI. Metrics export. Support for various protocols. REST API. Canary releases. Let\u2019s Encrypt certificates support. TCP/SSL with SNI. Traffic mirroring/shadowing. The main disadvantage is that in order to organize the high availability of the controller you have to install and connect its own KV-storage. In 2019, the same developers have developed Maesh . Another service mesh solution built on top of Traefik. HAProxy \u2691 HAProxy is well known as a proxy server and load balancer. As part of the Kubernetes cluster, it offers: * \u201csoft\u201d configuration update (without traffic loss) * DNS-based service discovery * Dynamic configuration through API. * Full customization of a config-files template (via replacing a ConfigMap). * Using Spring Boot functions. * Great number of supported balancing algorithms. In general, developers put emphasis on high speed, optimization, and efficiency in consumed resources. It\u2019s worth mentioning a lot of new features have appeared in a recent (June\u201919) v2.0 release, and even more (including OpenTracing support) is expected with upcoming v2.1. Istio Ingress \u2691 Istio is a comprehensive service mesh solution. It can manage not just all incoming outside traffic (as an Ingress controller) but control all traffic inside the cluster as well. Under the hood, Istio uses Envoy as a sidecar-proxy for each service. In essence, it is a large processor that can do almost anything. Its central idea is maximum control, extensibility, security, and transparency. With Istio Ingress, you can fine tune traffic routing, access authorization between services, balancing, monitoring, canary releases and much more. \u201c Back to microservices with Istio \u201d is a great intro to learn about Istio. ALB Ingress controller \u2691 The AWS ALB Ingress Controller satisfies Kubernetes ingress resources by provisioning Application Load Balancers . It's advantages are: AWS managed loadbalancer. Authentication with OIDC or Cognito. AWS WAF support. Natively redirect HTTP to HTTPS. Supports fixed response without forwarding to the application.. It has also the potential advantage of using IP traffic mode. ALB support two types of traffic: instance mode : Ingress traffic starts from the ALB and reaches the NodePort opened for your service. Traffic is then routed to the container Pods within the cluster. The number of hops for the packet to reach its destination in this mode is always two. IP mode : Ingress traffic starts from the ALB and reaches the container Pods within cluster directly. In order to use this mode, the networking plugin for the K8s cluster must use a secondary IP address on ENI as pod IP, aka AWS CNI plugin for K8s. The number of hops for the packet to reach its destination is always one. The IP mode gives the following advantages: The load balancer can be pod location-aware: reduce the chance to route traffic to an irrelevant node and then rely on kube-proxy and network agent. The number of hops for the packet to reach its destination is always one No extra overlay network comparing to using Network plugins (Calico, Flannel) directly int he cloud (AWS). It also has it's disadvantages: Even though AWS guides you on it's deployment , after two months of AWS Support cases, I wasn't able to deploy it using terraform and helm . You can't reuse existing ALBs instead of creating new ALB per ingress . Therefore ingress: false needs to be specified in the service helm chart and manually edit the ALB ingress helm chart to add each new service. ALB ingress deployment \u2691 This section is a defunct work in progress. I wasn't able to make it work, but it can be useful if you want to deploy it yourself. If you success, please make a PR . I've used the AWS Guide , in conjunction with the AWS general ALB controller documentation and the AWS general ALB documentation to define the properties of the incubator helm chart . Before that you need to an IAM policy ALBIngressControllerIAMPolicy to give the required permissions to manage the certificates, ALB creation and WAF integration. That IAM policy needs to be attached to the eks-alb-ingress-controller IAM role. You also had to create an IAM OIDC provider, which are entities in IAM that describe an external identity provider (IdP) service that supports the OpenID Connect (OIDC) standard, such as Google or Github. You use an IAM OIDC identity provider when you want to establish trust between an OIDC compatible IdP and your AWS account. This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign in code or manage your own user identities. The IAM OIDC provider is going to be used by the AWS EKS pod identity admission controller to attach IAM roles to specific service accounts. This will prevent the attachment of the IAM role to the whole node group. So instead of allowing every pod inside the worker groups to create the ALB, only the ones attached to the eks-alb-ingress-controller service account will be able to do so. Links \u2691 ITNext ingress controller comparison Flant ingress controller comparison", "title": "Ingress Controller"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#kubernetes-ingress-controller", "text": "The \u201cofficial\u201d Kubernetes controller. Not to be confused with the one offered by the NGINX company. Developed by the community, it's based on the Nginx web server with a set of Lua plugins to implement extra features. Thanks to the popularity of NGINX and minimal modifications over it when using as a controller, it can be the simplest and most straightforward option for an average engineer dealing with K8s.", "title": "Kubernetes Ingress controller"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#traefik", "text": "Originally, this proxy was created for the routing of requests for microservices and their dynamic environment, hence many of its useful features: Continuous update of configuration (no restarts) . Support for multiple load balancing algorithms. Web UI. Metrics export. Support for various protocols. REST API. Canary releases. Let\u2019s Encrypt certificates support. TCP/SSL with SNI. Traffic mirroring/shadowing. The main disadvantage is that in order to organize the high availability of the controller you have to install and connect its own KV-storage. In 2019, the same developers have developed Maesh . Another service mesh solution built on top of Traefik.", "title": "Traefik"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#haproxy", "text": "HAProxy is well known as a proxy server and load balancer. As part of the Kubernetes cluster, it offers: * \u201csoft\u201d configuration update (without traffic loss) * DNS-based service discovery * Dynamic configuration through API. * Full customization of a config-files template (via replacing a ConfigMap). * Using Spring Boot functions. * Great number of supported balancing algorithms. In general, developers put emphasis on high speed, optimization, and efficiency in consumed resources. It\u2019s worth mentioning a lot of new features have appeared in a recent (June\u201919) v2.0 release, and even more (including OpenTracing support) is expected with upcoming v2.1.", "title": "HAProxy"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#istio-ingress", "text": "Istio is a comprehensive service mesh solution. It can manage not just all incoming outside traffic (as an Ingress controller) but control all traffic inside the cluster as well. Under the hood, Istio uses Envoy as a sidecar-proxy for each service. In essence, it is a large processor that can do almost anything. Its central idea is maximum control, extensibility, security, and transparency. With Istio Ingress, you can fine tune traffic routing, access authorization between services, balancing, monitoring, canary releases and much more. \u201c Back to microservices with Istio \u201d is a great intro to learn about Istio.", "title": "Istio Ingress"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#alb-ingress-controller", "text": "The AWS ALB Ingress Controller satisfies Kubernetes ingress resources by provisioning Application Load Balancers . It's advantages are: AWS managed loadbalancer. Authentication with OIDC or Cognito. AWS WAF support. Natively redirect HTTP to HTTPS. Supports fixed response without forwarding to the application.. It has also the potential advantage of using IP traffic mode. ALB support two types of traffic: instance mode : Ingress traffic starts from the ALB and reaches the NodePort opened for your service. Traffic is then routed to the container Pods within the cluster. The number of hops for the packet to reach its destination in this mode is always two. IP mode : Ingress traffic starts from the ALB and reaches the container Pods within cluster directly. In order to use this mode, the networking plugin for the K8s cluster must use a secondary IP address on ENI as pod IP, aka AWS CNI plugin for K8s. The number of hops for the packet to reach its destination is always one. The IP mode gives the following advantages: The load balancer can be pod location-aware: reduce the chance to route traffic to an irrelevant node and then rely on kube-proxy and network agent. The number of hops for the packet to reach its destination is always one No extra overlay network comparing to using Network plugins (Calico, Flannel) directly int he cloud (AWS). It also has it's disadvantages: Even though AWS guides you on it's deployment , after two months of AWS Support cases, I wasn't able to deploy it using terraform and helm . You can't reuse existing ALBs instead of creating new ALB per ingress . Therefore ingress: false needs to be specified in the service helm chart and manually edit the ALB ingress helm chart to add each new service.", "title": "ALB Ingress controller"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#alb-ingress-deployment", "text": "This section is a defunct work in progress. I wasn't able to make it work, but it can be useful if you want to deploy it yourself. If you success, please make a PR . I've used the AWS Guide , in conjunction with the AWS general ALB controller documentation and the AWS general ALB documentation to define the properties of the incubator helm chart . Before that you need to an IAM policy ALBIngressControllerIAMPolicy to give the required permissions to manage the certificates, ALB creation and WAF integration. That IAM policy needs to be attached to the eks-alb-ingress-controller IAM role. You also had to create an IAM OIDC provider, which are entities in IAM that describe an external identity provider (IdP) service that supports the OpenID Connect (OIDC) standard, such as Google or Github. You use an IAM OIDC identity provider when you want to establish trust between an OIDC compatible IdP and your AWS account. This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign in code or manage your own user identities. The IAM OIDC provider is going to be used by the AWS EKS pod identity admission controller to attach IAM roles to specific service accounts. This will prevent the attachment of the IAM role to the whole node group. So instead of allowing every pod inside the worker groups to create the ALB, only the ones attached to the eks-alb-ingress-controller service account will be able to do so.", "title": "ALB ingress deployment"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#links", "text": "ITNext ingress controller comparison Flant ingress controller comparison", "title": "Links"}, {"location": "devops/kubernetes/kubernetes_jobs/", "text": "Kubernetes jobs creates one or more Pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created. Cronjobs creates Jobs on a repeating schedule. This example CronJob manifest prints the current time and a hello message every minute: apiVersion : batch/v1beta1 kind : CronJob metadata : name : hello spec : schedule : \"*/1 * * * *\" jobTemplate : spec : template : spec : containers : - name : hello image : busybox args : - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy : OnFailure To deploy cronjobs you can use the bambash helm chart . Check the kubectl commands to interact with jobs . Debugging job logs \u2691 To obtain the logs of a completed or failed job, you need to: Locate the cronjob you want to debug: kubectl get cronjobs -n cronjobs . Locate the associated job: kubectl get jobs -n cronjobs . Locate the associated pod: kubectl get pods -n cronjobs . If the pod still exists, you can execute kubectl logs -n cronjobs {{ pod_name }} . If the pod doesn't exist anymore, you need to search the pod in your log centralizer solution. Rerunning failed jobs \u2691 If you have a job that has failed after the 6 default retries, it will show up in your monitorization forever, to fix it, you can manually trigger the job yourself with: kubectl get job \"your-job\" -o json \\ | jq 'del(.spec.selector)' \\ | jq 'del(.spec.template.metadata.labels)' \\ | kubectl replace --force -f - Manually creating a job from a cronjob \u2691 kubectl create job {{ job_name }} -n {{ namespace }} \\ --from = cronjobs/ {{ cronjob_name }} Monitorization of cronjobs \u2691 Alerting of traditional Unix cronjobs meant sending an email if the job failed. Most job scheduling systems that have followed have provided the same experience, Kubernetes does not. One approach to alerting jobs is to use the Prometheus push gateway, allowing us to push richer metrics than the success/failure status. This approach has it\u2019s downsides; we have to update the code for our jobs, we also have to explicitly configure a push gateway location and update it if it changes (a burden alleviated by the pull based metrics for long lived workloads). You can use tools such as Sentry, but it will also require changes to the jobs. Jobs are powerful things allowing us to implement several different workflows, the combination of options can be overwhelming compared to a traditional Unix cron job. This variety makes it difficult to establish one simple rule for alerting failed jobs. Things get easier if we restrict ourselves to a subset of possible options. We will focus on non-concurrent jobs. The relationship between cronjobs and jobs makes the task ahead difficult. To make our life easier we will put one requirement on the jobs we create, they will have to include a label that associates them with the original cronjob. Below we present an example of our ideal cronjob (which matches what the helm chart deploys): apiVersion : batch/v1beta1 kind : CronJob metadata : name : our-task spec : schedule : \"*/5 * * * *\" successfulJobsHistoryLimit : 3 concurrencyPolicy : Forbid jobTemplate : metadata : labels : cron : our-task # <-- match created jobs with the cronjob spec : backoffLimit : 3 template : metadata : labels : cronjob : our-task spec : containers : - name : our-task command : - /user/bin/false image : alpine restartPolicy : Never Building our alert \u2691 We are also going to need some metrics to get us started. K8s does not provide us any by default, but fortunately kube-state-metrics is installed with the Prometheus operator chart, so we have the following metrics: kube_cronjob_labels{ cronjob=\"our-task\", namespace=\"default\"} 1 kube_job_created{ job=\"our-task-1520165700\", namespace=\"default\"} 1.520165707e+09 kube_job_failed{ condition=\"false\", job=\"our-task-1520165700\", namespace=\"default\"} 0 kube_job_failed{ condition=\"true\", job=\"our-task-1520165700\", namespace=\"default\"} 1 kube_job_labels{ job=\"our-task-1520165700\", label_cron=\"our-task\", namespace=\"default\"} 1 This shows the primary set of metrics we will be using to construct our alert. What is not shown above is the status of the cronjob. The big challenge with K8s cronjob alerting is that cronjobs themselves do not possess any status information, beyond the last time the cronjob created a job. The status information only exists on the job that the cronjob creates. In order to determine if our cronjob is failing, our first order of business is to find which jobs we should be looking at. A K8s cronjob creates new job objects and keeps a number of them around to help us debug the runs of our jobs. We have to be determine which job corresponds to the last run of our cronjob. If we have added the cron label to the jobs as above, we can find the last run time of the jobs for a given cronjob as follows: max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( job_name , label_cron ) This query demonstrates an important technique when working with kube-state-metrics . For each API object it exported data on, it exports a time series including all the labels for that object. These time series have a value of 1. As such we can join the set of labels for an object onto the metrics about that object by multiplication. Depending on how your Prometheus instance is configured, the value of the job label on your metrics will likely be kube-state-metrics . kube-state-metrics adds a job label itself with the name of the job object. Prometheus resolves this collision of label names by including the raw metric\u2019s label as an job_name label. Since we are querying the start time of jobs, and there should only ever be one job with a given name. You may wonder why we need the max aggregation. Manually plugging the query into Prometheus may convince you that it is unnecessary. Consider though that you may have multiple instances of kube-state-metrics running for redundancy. Using max ensures our query is valid even if we have multiple instances of kube-state-metrics running. Issues of duplicate metrics are common when constructing production recording rules and alerts. We can find the start time of the most recent job for a given cronjob by finding the maximum of all job start times as follows: max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( label_cron ) The only difference between this and the previous query is in the labels used for the aggregation. Now that we have the start time of each job, and the start time of the most recent job, we can do a simple equality match to find the start time of the most recent job for a given cronjob. We will create a metric for this: - record : job_cronjob : kube_job_status_start_time : max expr : | sum without ( label_cron , job_name ) ( label_replace ( label_replace ( max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( job_name , label_cron ) == ON ( label_cron ) GROUP_LEFT () max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( label_cron ) , \" job \", \" $1 \", \" job_name \", \" (.+) \" ) , \" cronjob \", \" $1 \", \" label_cron \", \" (.+) \" ) ) We have also taken the opportunity to adjust the labels to be a little more aesthetically pleasing. By copying job_name to job , label_cron to cronjob and removing job_name and label_cron . Now that we have the most recently started job for a given cronjob, we can find which, if any, have failed attempts: - record : job_cronjob : kube_job_status_failed : sum expr : | sum without ( label_cron , job_name ) ( clamp_max ( job_cronjob : kube_job_status_start_time : max , 1 ) * ON ( job ) GROUP_LEFT () label_replace ( label_replace ( ( kube_job_status_failed != 0 and kube_job_status_succeeded == 0 ) , \" job \", \" $1 \", \" job_name \", \" (.+) \" ) , \" cronjob \", \" $1 \", \" label_cron \", \" (.+) \" ) ) The initial clamp_max clause is used to transform our start times metric into a set of time series to perform label matching to filter another set of metrics. Multiplication by 1 (or addition of 0), is a useful means of filter and merging time series and it is well worth taking the time to understand the technique. We get those cronjobs that have a failed job and no successful ones with the query: ( kube_job_status_failed != 0 and kube_job_status_succeeded == 0 ) The kube_job_status_succeeded == 0 it's important, otherwise once a job has a failed instance, it doesn't matter if there's a posterior one that succeeded, we're going to keep on receiving the alert that it failed. We adjust the labels on the previous query to match our start time metric so ensure the labels have the same meaning as those on our job_cronjob:kube_job_status_start_time:max metric. The label matching on the multiplication will then perform our filtering. We now have a metric containing the set of most recently failed jobs, labeled by their parent cronjob, so we can now construct the alert: - alert : CronJobStatusFailed expr : job_cronjob : kube_job_status_failed : sum > 0 for : 1m annotations : description : ' {{ $labels.cronjob }} last run has failed {{ $value }} times. ' We use the kube_cronjob_labels here to merge in labels from the original cronjob.", "title": "Jobs"}, {"location": "devops/kubernetes/kubernetes_jobs/#debugging-job-logs", "text": "To obtain the logs of a completed or failed job, you need to: Locate the cronjob you want to debug: kubectl get cronjobs -n cronjobs . Locate the associated job: kubectl get jobs -n cronjobs . Locate the associated pod: kubectl get pods -n cronjobs . If the pod still exists, you can execute kubectl logs -n cronjobs {{ pod_name }} . If the pod doesn't exist anymore, you need to search the pod in your log centralizer solution.", "title": "Debugging job logs"}, {"location": "devops/kubernetes/kubernetes_jobs/#rerunning-failed-jobs", "text": "If you have a job that has failed after the 6 default retries, it will show up in your monitorization forever, to fix it, you can manually trigger the job yourself with: kubectl get job \"your-job\" -o json \\ | jq 'del(.spec.selector)' \\ | jq 'del(.spec.template.metadata.labels)' \\ | kubectl replace --force -f -", "title": "Rerunning failed jobs"}, {"location": "devops/kubernetes/kubernetes_jobs/#manually-creating-a-job-from-a-cronjob", "text": "kubectl create job {{ job_name }} -n {{ namespace }} \\ --from = cronjobs/ {{ cronjob_name }}", "title": "Manually creating a job from a cronjob"}, {"location": "devops/kubernetes/kubernetes_jobs/#monitorization-of-cronjobs", "text": "Alerting of traditional Unix cronjobs meant sending an email if the job failed. Most job scheduling systems that have followed have provided the same experience, Kubernetes does not. One approach to alerting jobs is to use the Prometheus push gateway, allowing us to push richer metrics than the success/failure status. This approach has it\u2019s downsides; we have to update the code for our jobs, we also have to explicitly configure a push gateway location and update it if it changes (a burden alleviated by the pull based metrics for long lived workloads). You can use tools such as Sentry, but it will also require changes to the jobs. Jobs are powerful things allowing us to implement several different workflows, the combination of options can be overwhelming compared to a traditional Unix cron job. This variety makes it difficult to establish one simple rule for alerting failed jobs. Things get easier if we restrict ourselves to a subset of possible options. We will focus on non-concurrent jobs. The relationship between cronjobs and jobs makes the task ahead difficult. To make our life easier we will put one requirement on the jobs we create, they will have to include a label that associates them with the original cronjob. Below we present an example of our ideal cronjob (which matches what the helm chart deploys): apiVersion : batch/v1beta1 kind : CronJob metadata : name : our-task spec : schedule : \"*/5 * * * *\" successfulJobsHistoryLimit : 3 concurrencyPolicy : Forbid jobTemplate : metadata : labels : cron : our-task # <-- match created jobs with the cronjob spec : backoffLimit : 3 template : metadata : labels : cronjob : our-task spec : containers : - name : our-task command : - /user/bin/false image : alpine restartPolicy : Never", "title": "Monitorization of cronjobs"}, {"location": "devops/kubernetes/kubernetes_jobs/#building-our-alert", "text": "We are also going to need some metrics to get us started. K8s does not provide us any by default, but fortunately kube-state-metrics is installed with the Prometheus operator chart, so we have the following metrics: kube_cronjob_labels{ cronjob=\"our-task\", namespace=\"default\"} 1 kube_job_created{ job=\"our-task-1520165700\", namespace=\"default\"} 1.520165707e+09 kube_job_failed{ condition=\"false\", job=\"our-task-1520165700\", namespace=\"default\"} 0 kube_job_failed{ condition=\"true\", job=\"our-task-1520165700\", namespace=\"default\"} 1 kube_job_labels{ job=\"our-task-1520165700\", label_cron=\"our-task\", namespace=\"default\"} 1 This shows the primary set of metrics we will be using to construct our alert. What is not shown above is the status of the cronjob. The big challenge with K8s cronjob alerting is that cronjobs themselves do not possess any status information, beyond the last time the cronjob created a job. The status information only exists on the job that the cronjob creates. In order to determine if our cronjob is failing, our first order of business is to find which jobs we should be looking at. A K8s cronjob creates new job objects and keeps a number of them around to help us debug the runs of our jobs. We have to be determine which job corresponds to the last run of our cronjob. If we have added the cron label to the jobs as above, we can find the last run time of the jobs for a given cronjob as follows: max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( job_name , label_cron ) This query demonstrates an important technique when working with kube-state-metrics . For each API object it exported data on, it exports a time series including all the labels for that object. These time series have a value of 1. As such we can join the set of labels for an object onto the metrics about that object by multiplication. Depending on how your Prometheus instance is configured, the value of the job label on your metrics will likely be kube-state-metrics . kube-state-metrics adds a job label itself with the name of the job object. Prometheus resolves this collision of label names by including the raw metric\u2019s label as an job_name label. Since we are querying the start time of jobs, and there should only ever be one job with a given name. You may wonder why we need the max aggregation. Manually plugging the query into Prometheus may convince you that it is unnecessary. Consider though that you may have multiple instances of kube-state-metrics running for redundancy. Using max ensures our query is valid even if we have multiple instances of kube-state-metrics running. Issues of duplicate metrics are common when constructing production recording rules and alerts. We can find the start time of the most recent job for a given cronjob by finding the maximum of all job start times as follows: max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( label_cron ) The only difference between this and the previous query is in the labels used for the aggregation. Now that we have the start time of each job, and the start time of the most recent job, we can do a simple equality match to find the start time of the most recent job for a given cronjob. We will create a metric for this: - record : job_cronjob : kube_job_status_start_time : max expr : | sum without ( label_cron , job_name ) ( label_replace ( label_replace ( max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( job_name , label_cron ) == ON ( label_cron ) GROUP_LEFT () max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( label_cron ) , \" job \", \" $1 \", \" job_name \", \" (.+) \" ) , \" cronjob \", \" $1 \", \" label_cron \", \" (.+) \" ) ) We have also taken the opportunity to adjust the labels to be a little more aesthetically pleasing. By copying job_name to job , label_cron to cronjob and removing job_name and label_cron . Now that we have the most recently started job for a given cronjob, we can find which, if any, have failed attempts: - record : job_cronjob : kube_job_status_failed : sum expr : | sum without ( label_cron , job_name ) ( clamp_max ( job_cronjob : kube_job_status_start_time : max , 1 ) * ON ( job ) GROUP_LEFT () label_replace ( label_replace ( ( kube_job_status_failed != 0 and kube_job_status_succeeded == 0 ) , \" job \", \" $1 \", \" job_name \", \" (.+) \" ) , \" cronjob \", \" $1 \", \" label_cron \", \" (.+) \" ) ) The initial clamp_max clause is used to transform our start times metric into a set of time series to perform label matching to filter another set of metrics. Multiplication by 1 (or addition of 0), is a useful means of filter and merging time series and it is well worth taking the time to understand the technique. We get those cronjobs that have a failed job and no successful ones with the query: ( kube_job_status_failed != 0 and kube_job_status_succeeded == 0 ) The kube_job_status_succeeded == 0 it's important, otherwise once a job has a failed instance, it doesn't matter if there's a posterior one that succeeded, we're going to keep on receiving the alert that it failed. We adjust the labels on the previous query to match our start time metric so ensure the labels have the same meaning as those on our job_cronjob:kube_job_status_start_time:max metric. The label matching on the multiplication will then perform our filtering. We now have a metric containing the set of most recently failed jobs, labeled by their parent cronjob, so we can now construct the alert: - alert : CronJobStatusFailed expr : job_cronjob : kube_job_status_failed : sum > 0 for : 1m annotations : description : ' {{ $labels.cronjob }} last run has failed {{ $value }} times. ' We use the kube_cronjob_labels here to merge in labels from the original cronjob.", "title": "Building our alert"}, {"location": "devops/kubernetes/kubernetes_labels/", "text": "Labels are identifying metadata key/value pairs attached to objects, such as pods. Labels are intended to give meaningful and relevant information to users, but which do not directly imply semantics to the core system. \"labels\" : { \"key1\" : \"value1\" , \"key2\" : \"value2\" }", "title": "Labels"}, {"location": "devops/kubernetes/kubernetes_metric_server/", "text": "The metrics server monitors the resource consumption inside the cluster. It populates the information in kubectl top nodes to get the node status and gives the information to automatically autoscale deployments with Horizontal pod autoscaling . To install it, you can use the metrics-server helm chart. To test that the horizontal pod autoscaling is working, follow the AWS EKS guide .", "title": "Metrics Server"}, {"location": "devops/kubernetes/kubernetes_namespaces/", "text": "Namespaces are virtual clusters backed by the same physical cluster. It's the first level of isolation between applications. When to Use Multiple Namespaces \u2691 Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide. Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces are a way to divide cluster resources between multiple uses (via resource quota). It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.", "title": "Namespaces"}, {"location": "devops/kubernetes/kubernetes_namespaces/#when-to-use-multiple-namespaces", "text": "Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide. Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces are a way to divide cluster resources between multiple uses (via resource quota). It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.", "title": "When to Use Multiple Namespaces"}, {"location": "devops/kubernetes/kubernetes_networking/", "text": "Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. If you want to get a quickly grasp on how k8s networking works, I suggest you to read StackRox's Kubernetes networking demystified article . CNI comparison \u2691 Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. There are different container networking plugins you can use for your cluster. To ensure that you choose the best one for your use case, I've made a summary based on the following resources: Rancher k8s CNI comparison . ITnext k8s CNI comparison . Mark Ramm-Christensen AWS CNI analysis . TL;DR \u2691 When using EKS, if you have no networking experience and understand and accept their disadvantages I'd use the AWS VPC CNI as it's installed by default. Nevertheless, the pod limit per node and the vendor locking makes interesting to migrate in the future to Calico. To make the transition smoother, you can enable Calico Network Policies with the AWS VPC CNI and get used to them before fully migrating to Calico. Calico seems to be the best solution when you need a greater control of the networking inside k8s, as it supports security features (NetworkPolicies or encryption), that can be improved even more with the integration with Istio. It's known to be easy to debug and has commercial support. If you aren't using EKS, you could evaluate to first use Canal as it uses Flannel simple network overlay but with Calico's Network Policies. If you do this, you could first focus in getting used to Network Policies and then dive further into the network configuration with Calico. But a deeper analysis should be done to assess if the compared difficulty justifies the need of this step. I don't recommend to use Flannel alone even though it's simple as you'll probably need security features such as NetworkPolicies, although they say, it's the best insecure solution for low resource or several architecture nodes. I wouldn't use Weave either unless you need encryption throughout all the internal network and multicast. Flannel \u2691 Flannel , a project developed by the CoreOS, is perhaps the most straightforward and popular CNI plugin available. It is one of the most mature examples of networking fabric for container orchestration systems, intended to allow for better inter-container and inter-host networking. As the CNI concept took off, a CNI plugin for Flannel was an early entry. Flannel is relatively easy to install and configure. It is packaged as a single binary called flanneld and can be installed by default by many common Kubernetes cluster deployment tools and in many Kubernetes distributions. Flannel can use the Kubernetes cluster\u2019s existing etcd cluster to store its state information using the API to avoid having to provision a dedicated data store. Flannel configures a layer 3 IPv4 overlay network. A large internal network is created that spans across every node within the cluster. Within this overlay network, each node is given a subnet to allocate IP addresses internally. As pods are provisioned, the Docker bridge interface on each node allocates an address for each new container. Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsulated in UDP packets by flanneld for routing to the appropriate destination. Flannel has several different types of backends available for encapsulation and routing. The default and recommended approach is to use VXLAN, as it offers both good performance and is less manual intervention than other options. Overall, Flannel is a good choice for most users. From an administrative perspective, it offers a simple networking model that sets up an environment that\u2019s suitable for most use cases when you only need the basics. In general, it\u2019s a safe bet to start out with Flannel until you need something that it cannot provide or you need security features, such as NetworkPolicies or encryption. It's also recommended if you have low resource nodes in your cluster (only a few GB of RAM, a few cores). Moreover, it is compatible with a very large number of architectures (amd64, arm, arm64, etc.). It is the only one, along with Cilium, that is able to correctly auto-detect your MTU, so you don\u2019t have to configure anything to let it work. Calico \u2691 Calico , is another popular networking option in the Kubernetes ecosystem. While Flannel is positioned as the simple choice, Calico is best known for its performance, flexibility, and power. Calico takes a more holistic view of networking, concerning itself not only with providing network connectivity between hosts and pods, but also with network security and administration. On a freshly provisioned Kubernetes cluster that meets the system requirements, Calico can be deployed quickly by applying a single manifest file. If you are interested in Calico\u2019s optional network policy capabilities, you can enable them by applying an additional manifest to your cluster. Unlike Flannel, Calico does not use an overlay network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. This means that packets do not need to be wrapped in an extra layer of encapsulation when moving between hosts. The BGP routing mechanism can direct packets natively without an extra step of wrapping traffic in an additional layer of traffic. Besides the performance that this offers, one side effect of this is that it allows for more conventional troubleshooting when network problems arise. While encapsulated solutions using technologies like VXLAN work well, the process manipulates packets in a way that can make tracing difficult. With Calico, the standard debugging tools have access to the same information they would in simple environments, making it easier for a wider range of developers and administrators to understand behavior. In addition to networking connectivity, Calico is well known for its advanced network features. Network policy is one of its most sought after capabilities. In addition, Calico can also integrate with Istio , a service mesh, to interpret and enforce policy for workloads within the cluster both at the service mesh layer and the network infrastructure layer. This means that you can configure powerful rules describing how pods should be able to send and accept traffic, improving security and control over your networking environment. Project Calico is a good choice for environments that support its requirements and when performance and features like network policy are important. Additionally, Calico offers commercial support if you\u2019re seeking a support contract or want to keep that option open for the future. In general, it\u2019s a good choice for when you want to be able to control your network instead of just configuring it once and forgetting about it. If you are looking on how to install Calico, you can start with Calico guide for clusters with less than 50 nodes or if you use EKS with AWS guide . Canal \u2691 Canal is an interesting option for quite a few reasons. First of all, Canal was the name for a project that sought to integrate the networking layer provided by flannel with the networking policy capabilities of Calico. As the contributors worked through the details however, it became apparent that a full integration was not necessarily needed if work was done on both projects to ensure standardization and flexibility. As a result, the official project became somewhat defunct, but the intended ability to deploy the two technology together was achieved. For this reason, it\u2019s still sometimes easiest to refer to the combination as \u201cCanal\u201d even if the project no longer exists. Because Canal is a combination of Flannel and Calico, its benefits are also at the intersection of these two technologies. The networking layer is the simple overlay provided by Flannel that works across many different deployment environments without much additional configuration. The network policy capabilities layered on top supplement the base network with Calico\u2019s powerful networking rule evaluation to provide additional security and control. After ensuring that the cluster fulfills the necessary system requirements, Canal can be deployed by applying two manifests, making it no more difficult to configure than either of the projects on their own. Canal is a good way for teams to start to experiment and gain experience with network policy before they\u2019re ready to experiment with changing their actual networking. In general, Canal is a good choice if you like the networking model that Flannel provides but find some of Calico\u2019s features enticing. The ability define network policy rules is a huge advantage from a security perspective and is, in many ways, Calico\u2019s killer feature. Being able to apply that technology onto a familiar networking layer means that you can get a more capable environment without having to go through much of a transition. Weave Net \u2691 Weave Net by Weaveworks is a CNI-capable networking option for Kubernetes that offers a different paradigm than the others we\u2019ve discussed so far. Weave creates a mesh overlay network between each of the nodes in the cluster, allowing for flexible routing between participants. This, coupled with a few other unique features, allows Weave to intelligently route in situations that might otherwise cause problems. To create its network, Weave relies on a routing component installed on each host in the network. These routers then exchange topology information to maintain an up-to-date view of the available network landscape. When looking to send traffic to a pod located on a different node, the weave router makes an automatic decision whether to send it via \u201cfast datapath\u201d or to fall back on the \u201csleeve\u201d packet forwarding method. Fast datapath is an approach that relies on the kernel\u2019s native Open vSwitch datapath module to forward packets to the appropriate pod without moving in and out of userspace multiple times. The Weave router updates the Open vSwitch configuration to ensure that the kernel layer has accurate information about how to route incoming packets. In contrast, sleeve mode is available as a backup when the networking topology isn\u2019t suitable for fast datapath routing. It is a slower encapsulation mode that can route packets in instances where fast datapath does not have the necessary routing information or connectivity. As traffic flows through the routers, they learn which peers are associated with which MAC addresses, allowing them to route more intelligently with fewer hops for subsequent traffic. This same mechanism helps each node self-correct when a network change alters the available routes. Like Calico, Weave also provides network policy capabilities for your cluster. This is automatically installed and configured when you set up Weave, so no additional configuration is necessary beyond adding your network rules. One thing that Weave provides that the other options do not is easy encryption for the entire network. While it adds quite a bit of network overhead, Weave can be configured to automatically encrypt all routed traffic by using NaCl encryption for sleeve traffic and, since it needs to encrypt VXLAN traffic in the kernel, IPsec ESP for fast datapath traffic. Another advantage of Weave is that it's the only CNI that supports multicast. In case you need it. Weave is a great option for those looking for feature rich encrypted networking without adding a large amount of complexity or management at the expense of worse overall performance. It is relatively easy to set up, offers many built in and automatically configured features, and can provide routing in scenarios where other solutions might fail. The mesh topography does put a limit on the size of the network that can be reasonably accommodated, but for most users, this won\u2019t be a problem. Additionally, Weave offers paid support for organizations that prefer to be able to have someone to contact for help and troubleshooting. AWS CNI \u2691 AWS developed their own CNI that uses Elastic Network Interfaces for pod networking. It's the default CNI if you use EKS. Advantages of the AWS CNI \u2691 Amazon native networking provides a number of significant advantages over some of the more traditional overlay network solutions for Kubernetes: Raw AWS network performance. Integration of tools familiar to AWS developers and admins, like AWS VPC flow logs and Security Groups \u2014 allowing users with existing VPC networks and networking best practices to carry those over directly to Kubernetes. The ability to enforce network policy decisions at the Kubernetes layer if you install Calico. If your team has significant experience with AWS networking, and/or your application is sensitive to network performance all of this makes VPC CNI very attractive. Disadvantages of the AWS CNI \u2691 On the other hand, there are a couple of limitations that may be significant to you. There are three primary reasons why you might instead choose an overlay network. Makes the multi-cloud k8s advantage more difficult. the CNI limits the number of pods that can be scheduled on each k8s node according to the number of IP Addresses available to each EC2 instance type so that each pod can be allocated an IP. Doesn't support encryption on the network. Multicast requirements. It eats up the number of IP Addresses available within your VPC unless you give it an alternate subnet. VPC CNI Pod Density Limitations \u2691 First, the VPC CNI plugin is designed to use/abuse ENI interfaces to get each pod in your cluster its own IP address from Amazon directly. This means that you will be network limited in the number of pods that you can run on any given worker node in the cluster. The primary IP for each ENI is used for cluster communication purposes. New pods are assigned to one of the secondary IPs for that ENI. VPC CNI has a custom DaemonSet that manages the assignment of IPs to pods. Because ENI and IP allocation requests can take time, this l-ipam daemon creates a warm pool of ENIs and IPs on each node and uses one of the available IPs for each new pod as it is assigned. This yields the following formula for maximum pod density for any given instance: ENIs * (IPs_per_ENI - 1) Each instance type in Amazon has unique limitations in the number of ENIs and IPs per ENI allowed. For example, an m5.large worker node allows 25 pod IP addresses per node at an approximate cost of $2.66/month per pod. Stepping up to an m5.xlarge allows for a theoretical maximum of 55 pods, for a monthly cost of $2.62, making the m5.large the more cost effective node choice by a small amount for clusters bound by IP address limitations. And if that set of calculations is not enough, there are a few other factors to consider. Kubernetes clusters generally run a set of services on each node. VPC CNI itself uses an l-ipam DaemonSet, and if you want Kubernetes network policies, calico requires another. Furthermore, production clusters generally also have DaemonSets for metrics collection, log aggregation, and other cluster-wide services. Each of these uses an IP address per node. So now the formula is: (ENIs * (IPs_per_ENI - 1) - 1 * DaemonSets This makes some of the cheaper instances on Amazon completely unusable because there are no IP addresses left for application pods. On the other hand, Kubernetes itself has a supported limit of 100 pods per node, making some of the larger instances with lots of available addresses less attractive. However, the pod per node limit IS configurable, and in my experience, this limit can be increased without much increase in Kubernetes overhead. Weave people made a quick Google sheet with each of the Amazon instance types, and the maximum pod densities for each based on the VPC CNI network plugin restrictions: A 100 pod/node limit setting in Kubernetes, A default of 4 DaemonSets (2 for AWS networking, 1 for log aggregation, and 1 for metric collection), A simple cost calculation for per-pod pricing. This sheet is not intended to provide a definitive answer on pod economics for AWS VPC based Kubernetes clusters. There are a number of important caveats to the use of this sheet: CPU and memory requirements will often dictate lower pod density than the theoretical maximum here. Beyond DaemonSets, Kubernetes System pods, and other \u201csystem level\u201d operational tools used in your cluster will consume Pod IP\u2019s and limit the number of application pods that you can run. Each instance type also has network performance limitations which may impact performance often far before theoretical pod limits are reached. Cloud Portability \u2691 Hybrid cloud, disaster recovery, and other requirements often push users away from custom cloud vendor solutions and towards open solutions. However, in this case the level of lock-in is quite low. The CNI layer provides a level of abstraction on top of the underlying network, and it is possible to deploy the same workloads using the same deployment configurations with different CNI backends. Which from my point of view, seems a bad and ugly idea. Links \u2691 StackRox Kubernetes networking demystified article . Writing your own simple CNI plug in .", "title": "Networking"}, {"location": "devops/kubernetes/kubernetes_networking/#cni-comparison", "text": "Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. There are different container networking plugins you can use for your cluster. To ensure that you choose the best one for your use case, I've made a summary based on the following resources: Rancher k8s CNI comparison . ITnext k8s CNI comparison . Mark Ramm-Christensen AWS CNI analysis .", "title": "CNI comparison"}, {"location": "devops/kubernetes/kubernetes_networking/#tldr", "text": "When using EKS, if you have no networking experience and understand and accept their disadvantages I'd use the AWS VPC CNI as it's installed by default. Nevertheless, the pod limit per node and the vendor locking makes interesting to migrate in the future to Calico. To make the transition smoother, you can enable Calico Network Policies with the AWS VPC CNI and get used to them before fully migrating to Calico. Calico seems to be the best solution when you need a greater control of the networking inside k8s, as it supports security features (NetworkPolicies or encryption), that can be improved even more with the integration with Istio. It's known to be easy to debug and has commercial support. If you aren't using EKS, you could evaluate to first use Canal as it uses Flannel simple network overlay but with Calico's Network Policies. If you do this, you could first focus in getting used to Network Policies and then dive further into the network configuration with Calico. But a deeper analysis should be done to assess if the compared difficulty justifies the need of this step. I don't recommend to use Flannel alone even though it's simple as you'll probably need security features such as NetworkPolicies, although they say, it's the best insecure solution for low resource or several architecture nodes. I wouldn't use Weave either unless you need encryption throughout all the internal network and multicast.", "title": "TL;DR"}, {"location": "devops/kubernetes/kubernetes_networking/#flannel", "text": "Flannel , a project developed by the CoreOS, is perhaps the most straightforward and popular CNI plugin available. It is one of the most mature examples of networking fabric for container orchestration systems, intended to allow for better inter-container and inter-host networking. As the CNI concept took off, a CNI plugin for Flannel was an early entry. Flannel is relatively easy to install and configure. It is packaged as a single binary called flanneld and can be installed by default by many common Kubernetes cluster deployment tools and in many Kubernetes distributions. Flannel can use the Kubernetes cluster\u2019s existing etcd cluster to store its state information using the API to avoid having to provision a dedicated data store. Flannel configures a layer 3 IPv4 overlay network. A large internal network is created that spans across every node within the cluster. Within this overlay network, each node is given a subnet to allocate IP addresses internally. As pods are provisioned, the Docker bridge interface on each node allocates an address for each new container. Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsulated in UDP packets by flanneld for routing to the appropriate destination. Flannel has several different types of backends available for encapsulation and routing. The default and recommended approach is to use VXLAN, as it offers both good performance and is less manual intervention than other options. Overall, Flannel is a good choice for most users. From an administrative perspective, it offers a simple networking model that sets up an environment that\u2019s suitable for most use cases when you only need the basics. In general, it\u2019s a safe bet to start out with Flannel until you need something that it cannot provide or you need security features, such as NetworkPolicies or encryption. It's also recommended if you have low resource nodes in your cluster (only a few GB of RAM, a few cores). Moreover, it is compatible with a very large number of architectures (amd64, arm, arm64, etc.). It is the only one, along with Cilium, that is able to correctly auto-detect your MTU, so you don\u2019t have to configure anything to let it work.", "title": "Flannel"}, {"location": "devops/kubernetes/kubernetes_networking/#calico", "text": "Calico , is another popular networking option in the Kubernetes ecosystem. While Flannel is positioned as the simple choice, Calico is best known for its performance, flexibility, and power. Calico takes a more holistic view of networking, concerning itself not only with providing network connectivity between hosts and pods, but also with network security and administration. On a freshly provisioned Kubernetes cluster that meets the system requirements, Calico can be deployed quickly by applying a single manifest file. If you are interested in Calico\u2019s optional network policy capabilities, you can enable them by applying an additional manifest to your cluster. Unlike Flannel, Calico does not use an overlay network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. This means that packets do not need to be wrapped in an extra layer of encapsulation when moving between hosts. The BGP routing mechanism can direct packets natively without an extra step of wrapping traffic in an additional layer of traffic. Besides the performance that this offers, one side effect of this is that it allows for more conventional troubleshooting when network problems arise. While encapsulated solutions using technologies like VXLAN work well, the process manipulates packets in a way that can make tracing difficult. With Calico, the standard debugging tools have access to the same information they would in simple environments, making it easier for a wider range of developers and administrators to understand behavior. In addition to networking connectivity, Calico is well known for its advanced network features. Network policy is one of its most sought after capabilities. In addition, Calico can also integrate with Istio , a service mesh, to interpret and enforce policy for workloads within the cluster both at the service mesh layer and the network infrastructure layer. This means that you can configure powerful rules describing how pods should be able to send and accept traffic, improving security and control over your networking environment. Project Calico is a good choice for environments that support its requirements and when performance and features like network policy are important. Additionally, Calico offers commercial support if you\u2019re seeking a support contract or want to keep that option open for the future. In general, it\u2019s a good choice for when you want to be able to control your network instead of just configuring it once and forgetting about it. If you are looking on how to install Calico, you can start with Calico guide for clusters with less than 50 nodes or if you use EKS with AWS guide .", "title": "Calico"}, {"location": "devops/kubernetes/kubernetes_networking/#canal", "text": "Canal is an interesting option for quite a few reasons. First of all, Canal was the name for a project that sought to integrate the networking layer provided by flannel with the networking policy capabilities of Calico. As the contributors worked through the details however, it became apparent that a full integration was not necessarily needed if work was done on both projects to ensure standardization and flexibility. As a result, the official project became somewhat defunct, but the intended ability to deploy the two technology together was achieved. For this reason, it\u2019s still sometimes easiest to refer to the combination as \u201cCanal\u201d even if the project no longer exists. Because Canal is a combination of Flannel and Calico, its benefits are also at the intersection of these two technologies. The networking layer is the simple overlay provided by Flannel that works across many different deployment environments without much additional configuration. The network policy capabilities layered on top supplement the base network with Calico\u2019s powerful networking rule evaluation to provide additional security and control. After ensuring that the cluster fulfills the necessary system requirements, Canal can be deployed by applying two manifests, making it no more difficult to configure than either of the projects on their own. Canal is a good way for teams to start to experiment and gain experience with network policy before they\u2019re ready to experiment with changing their actual networking. In general, Canal is a good choice if you like the networking model that Flannel provides but find some of Calico\u2019s features enticing. The ability define network policy rules is a huge advantage from a security perspective and is, in many ways, Calico\u2019s killer feature. Being able to apply that technology onto a familiar networking layer means that you can get a more capable environment without having to go through much of a transition.", "title": "Canal"}, {"location": "devops/kubernetes/kubernetes_networking/#weave-net", "text": "Weave Net by Weaveworks is a CNI-capable networking option for Kubernetes that offers a different paradigm than the others we\u2019ve discussed so far. Weave creates a mesh overlay network between each of the nodes in the cluster, allowing for flexible routing between participants. This, coupled with a few other unique features, allows Weave to intelligently route in situations that might otherwise cause problems. To create its network, Weave relies on a routing component installed on each host in the network. These routers then exchange topology information to maintain an up-to-date view of the available network landscape. When looking to send traffic to a pod located on a different node, the weave router makes an automatic decision whether to send it via \u201cfast datapath\u201d or to fall back on the \u201csleeve\u201d packet forwarding method. Fast datapath is an approach that relies on the kernel\u2019s native Open vSwitch datapath module to forward packets to the appropriate pod without moving in and out of userspace multiple times. The Weave router updates the Open vSwitch configuration to ensure that the kernel layer has accurate information about how to route incoming packets. In contrast, sleeve mode is available as a backup when the networking topology isn\u2019t suitable for fast datapath routing. It is a slower encapsulation mode that can route packets in instances where fast datapath does not have the necessary routing information or connectivity. As traffic flows through the routers, they learn which peers are associated with which MAC addresses, allowing them to route more intelligently with fewer hops for subsequent traffic. This same mechanism helps each node self-correct when a network change alters the available routes. Like Calico, Weave also provides network policy capabilities for your cluster. This is automatically installed and configured when you set up Weave, so no additional configuration is necessary beyond adding your network rules. One thing that Weave provides that the other options do not is easy encryption for the entire network. While it adds quite a bit of network overhead, Weave can be configured to automatically encrypt all routed traffic by using NaCl encryption for sleeve traffic and, since it needs to encrypt VXLAN traffic in the kernel, IPsec ESP for fast datapath traffic. Another advantage of Weave is that it's the only CNI that supports multicast. In case you need it. Weave is a great option for those looking for feature rich encrypted networking without adding a large amount of complexity or management at the expense of worse overall performance. It is relatively easy to set up, offers many built in and automatically configured features, and can provide routing in scenarios where other solutions might fail. The mesh topography does put a limit on the size of the network that can be reasonably accommodated, but for most users, this won\u2019t be a problem. Additionally, Weave offers paid support for organizations that prefer to be able to have someone to contact for help and troubleshooting.", "title": "Weave Net"}, {"location": "devops/kubernetes/kubernetes_networking/#aws-cni", "text": "AWS developed their own CNI that uses Elastic Network Interfaces for pod networking. It's the default CNI if you use EKS.", "title": "AWS CNI"}, {"location": "devops/kubernetes/kubernetes_networking/#advantages-of-the-aws-cni", "text": "Amazon native networking provides a number of significant advantages over some of the more traditional overlay network solutions for Kubernetes: Raw AWS network performance. Integration of tools familiar to AWS developers and admins, like AWS VPC flow logs and Security Groups \u2014 allowing users with existing VPC networks and networking best practices to carry those over directly to Kubernetes. The ability to enforce network policy decisions at the Kubernetes layer if you install Calico. If your team has significant experience with AWS networking, and/or your application is sensitive to network performance all of this makes VPC CNI very attractive.", "title": "Advantages of the AWS CNI"}, {"location": "devops/kubernetes/kubernetes_networking/#disadvantages-of-the-aws-cni", "text": "On the other hand, there are a couple of limitations that may be significant to you. There are three primary reasons why you might instead choose an overlay network. Makes the multi-cloud k8s advantage more difficult. the CNI limits the number of pods that can be scheduled on each k8s node according to the number of IP Addresses available to each EC2 instance type so that each pod can be allocated an IP. Doesn't support encryption on the network. Multicast requirements. It eats up the number of IP Addresses available within your VPC unless you give it an alternate subnet.", "title": "Disadvantages of the AWS CNI"}, {"location": "devops/kubernetes/kubernetes_networking/#vpc-cni-pod-density-limitations", "text": "First, the VPC CNI plugin is designed to use/abuse ENI interfaces to get each pod in your cluster its own IP address from Amazon directly. This means that you will be network limited in the number of pods that you can run on any given worker node in the cluster. The primary IP for each ENI is used for cluster communication purposes. New pods are assigned to one of the secondary IPs for that ENI. VPC CNI has a custom DaemonSet that manages the assignment of IPs to pods. Because ENI and IP allocation requests can take time, this l-ipam daemon creates a warm pool of ENIs and IPs on each node and uses one of the available IPs for each new pod as it is assigned. This yields the following formula for maximum pod density for any given instance: ENIs * (IPs_per_ENI - 1) Each instance type in Amazon has unique limitations in the number of ENIs and IPs per ENI allowed. For example, an m5.large worker node allows 25 pod IP addresses per node at an approximate cost of $2.66/month per pod. Stepping up to an m5.xlarge allows for a theoretical maximum of 55 pods, for a monthly cost of $2.62, making the m5.large the more cost effective node choice by a small amount for clusters bound by IP address limitations. And if that set of calculations is not enough, there are a few other factors to consider. Kubernetes clusters generally run a set of services on each node. VPC CNI itself uses an l-ipam DaemonSet, and if you want Kubernetes network policies, calico requires another. Furthermore, production clusters generally also have DaemonSets for metrics collection, log aggregation, and other cluster-wide services. Each of these uses an IP address per node. So now the formula is: (ENIs * (IPs_per_ENI - 1) - 1 * DaemonSets This makes some of the cheaper instances on Amazon completely unusable because there are no IP addresses left for application pods. On the other hand, Kubernetes itself has a supported limit of 100 pods per node, making some of the larger instances with lots of available addresses less attractive. However, the pod per node limit IS configurable, and in my experience, this limit can be increased without much increase in Kubernetes overhead. Weave people made a quick Google sheet with each of the Amazon instance types, and the maximum pod densities for each based on the VPC CNI network plugin restrictions: A 100 pod/node limit setting in Kubernetes, A default of 4 DaemonSets (2 for AWS networking, 1 for log aggregation, and 1 for metric collection), A simple cost calculation for per-pod pricing. This sheet is not intended to provide a definitive answer on pod economics for AWS VPC based Kubernetes clusters. There are a number of important caveats to the use of this sheet: CPU and memory requirements will often dictate lower pod density than the theoretical maximum here. Beyond DaemonSets, Kubernetes System pods, and other \u201csystem level\u201d operational tools used in your cluster will consume Pod IP\u2019s and limit the number of application pods that you can run. Each instance type also has network performance limitations which may impact performance often far before theoretical pod limits are reached.", "title": "VPC CNI Pod Density Limitations"}, {"location": "devops/kubernetes/kubernetes_networking/#cloud-portability", "text": "Hybrid cloud, disaster recovery, and other requirements often push users away from custom cloud vendor solutions and towards open solutions. However, in this case the level of lock-in is quite low. The CNI layer provides a level of abstraction on top of the underlying network, and it is possible to deploy the same workloads using the same deployment configurations with different CNI backends. Which from my point of view, seems a bad and ugly idea.", "title": "Cloud Portability"}, {"location": "devops/kubernetes/kubernetes_networking/#links", "text": "StackRox Kubernetes networking demystified article . Writing your own simple CNI plug in .", "title": "Links"}, {"location": "devops/kubernetes/kubernetes_operators/", "text": "Operators are Kubernetes specific applications (pods) that configure, manage and optimize other Kubernetes deployments automatically. A Kubernetes Operator might be able to: Install and provide sane initial configuration and sizing for your deployment, according to the specs of your Kubernetes cluster. Perform live reloading of deployments and pods to accommodate for any user requested parameter modification (hot config reloading). Safe coordination of application upgrades. Automatically scale up or down according to performance metrics. Service discovery via native Kubernetes APIs Application TLS certificate configuration Disaster recovery. Perform backups to offsite storage, integrity checks or any other maintenance task. How do they work? \u2691 An Operator encodes this domain knowledge and extends the Kubernetes API through the third party resources mechanism, enabling users to create, configure, and manage applications. Like Kubernetes's built-in resources, an Operator doesn't manage just a single instance of the application, but multiple instances across the cluster. Operators build upon two central Kubernetes concepts: Resources and Controllers. As an example, the built-in ReplicaSet resource lets users set a desired number number of Pods to run, and controllers inside Kubernetes ensure the desired state set in the ReplicaSet resource remains true by creating or removing running Pods. There are many fundamental controllers and resources in Kubernetes that work in this manner, including Services, Deployments, and Daemon Sets. An Operator builds upon the basic Kubernetes resource and controller concepts and adds a set of knowledge or configuration that allows the Operator to execute common application tasks. For example, when scaling an etcd cluster manually, a user has to perform a number of steps: create a DNS name for the new etcd member, launch the new etcd instance, and then use the etcd administrative tools (etcdctl member add) to tell the existing cluster about this new member. Instead with the etcd Operator a user can simply increase the etcd cluster size field by 1. Links \u2691 CoreOS introduction to Operators Sysdig Prometheus Operator guide part 3", "title": "Operators"}, {"location": "devops/kubernetes/kubernetes_operators/#how-do-they-work", "text": "An Operator encodes this domain knowledge and extends the Kubernetes API through the third party resources mechanism, enabling users to create, configure, and manage applications. Like Kubernetes's built-in resources, an Operator doesn't manage just a single instance of the application, but multiple instances across the cluster. Operators build upon two central Kubernetes concepts: Resources and Controllers. As an example, the built-in ReplicaSet resource lets users set a desired number number of Pods to run, and controllers inside Kubernetes ensure the desired state set in the ReplicaSet resource remains true by creating or removing running Pods. There are many fundamental controllers and resources in Kubernetes that work in this manner, including Services, Deployments, and Daemon Sets. An Operator builds upon the basic Kubernetes resource and controller concepts and adds a set of knowledge or configuration that allows the Operator to execute common application tasks. For example, when scaling an etcd cluster manually, a user has to perform a number of steps: create a DNS name for the new etcd member, launch the new etcd instance, and then use the etcd administrative tools (etcdctl member add) to tell the existing cluster about this new member. Instead with the etcd Operator a user can simply increase the etcd cluster size field by 1.", "title": "How do they work?"}, {"location": "devops/kubernetes/kubernetes_operators/#links", "text": "CoreOS introduction to Operators Sysdig Prometheus Operator guide part 3", "title": "Links"}, {"location": "devops/kubernetes/kubernetes_pods/", "text": "Pods are the basic building block of Kubernetes, the smallest and simplest unit in the object model that you create or deploy. A Pod represents a running process on your cluster. A Pod represents a unit of deployment. It encapsulates: An application container (or, in some cases, multiple tightly coupled containers). Storage resources. A unique network IP. Options that govern how the container(s) should run.", "title": "Pods"}, {"location": "devops/kubernetes/kubernetes_replicasets/", "text": "ReplicaSet maintains a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. You'll probably never manually use these resources, as they are defined inside the deployments . The older version of this resource are the Replication controllers .", "title": "ReplicaSets"}, {"location": "devops/kubernetes/kubernetes_services/", "text": "A Service defines a policy to access a logical set of Pods using a reliable endpoint. Users and other programs can access pods running on your cluster seamlessly. Therefore allowing a loose coupling between dependent Pods. When a request arrives the endpoint, the kube-proxy pod of the node forwards the request to the Pods that match the service LabelSelector. Services can be exposed in different ways by specifying a type in the ServiceSpec: ClusterIP (default): Exposes the Service on an internal IP in the cluster. This type makes the Service only reachable from within the cluster. NodePort : Exposes the Service on the same port of each selected Node in the cluster using NAT to the outside. LoadBalancer : Creates an external load balancer in the current cloud and assigns a fixed, external IP to the Service. To create an internal ELB of AWs add to the annotations: annotations : service.beta.kubernetes.io/aws-load-balancer-internal : 0.0.0.0/0 ExternalName : Exposes the Service using an arbitrary name by returning a CNAME record with the name. No proxy is used. If no RBAC or NetworkPolicies are applied, you can call a service of another namespace with the following nomenclature. curl {{ service_name }} . {{ service_namespace }} .svc.cluster.local", "title": "Services"}, {"location": "devops/kubernetes/kubernetes_storage_driver/", "text": "Storage drivers are pods that through the Container Storage Interface or CSI provide an interface to use external storage services from within Kubernetes. Amazon EBS CSI storage driver \u2691 Allows Kubernetes clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes with the awsElasticBlockStore volume type . To install it, you first need to attach the Amazon_EBS_CSI_Driver IAM policy to the worker nodes. Then you can use the aws-ebs-csi-driver helm chart. To test it worked follow the steps under To deploy a sample application and verify that the CSI driver is working .", "title": "Storage Driver"}, {"location": "devops/kubernetes/kubernetes_storage_driver/#amazon-ebs-csi-storage-driver", "text": "Allows Kubernetes clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes with the awsElasticBlockStore volume type . To install it, you first need to attach the Amazon_EBS_CSI_Driver IAM policy to the worker nodes. Then you can use the aws-ebs-csi-driver helm chart. To test it worked follow the steps under To deploy a sample application and verify that the CSI driver is working .", "title": "Amazon EBS CSI storage driver"}, {"location": "devops/kubernetes/kubernetes_tools/", "text": "There are several tools built to enhance the operation, installation and use of Kubernetes. Tried \u2691 K3s : Recommended small kubernetes, like hyperkube. To try \u2691 crossplane : Crossplane is an open source multicloud control plane. It introduces workload and resource abstractions on-top of existing managed services that enables a high degree of workload portability across cloud providers. A single crossplane enables the provisioning and full-lifecycle management of services and infrastructure across a wide range of providers, offerings, vendors, regions, and clusters. Crossplane offers a universal API for cloud computing, a workload scheduler, and a set of smart controllers that can automate work across clouds. razee : A multi-cluster continuous delivery tool for Kubernetes Automate the rollout process of Kubernetes resources across multiple clusters, environments, and cloud providers, and gain insight into what applications and versions run in your cluster. kube-ops-view : it shows how are the ops on the nodes. kubediff : a tool for Kubernetes to show differences between running state and version controlled configuration. ksniff : A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster. kubeview : Visualize dependencies kubernetes.", "title": "Tools"}, {"location": "devops/kubernetes/kubernetes_tools/#tried", "text": "K3s : Recommended small kubernetes, like hyperkube.", "title": "Tried"}, {"location": "devops/kubernetes/kubernetes_tools/#to-try", "text": "crossplane : Crossplane is an open source multicloud control plane. It introduces workload and resource abstractions on-top of existing managed services that enables a high degree of workload portability across cloud providers. A single crossplane enables the provisioning and full-lifecycle management of services and infrastructure across a wide range of providers, offerings, vendors, regions, and clusters. Crossplane offers a universal API for cloud computing, a workload scheduler, and a set of smart controllers that can automate work across clouds. razee : A multi-cluster continuous delivery tool for Kubernetes Automate the rollout process of Kubernetes resources across multiple clusters, environments, and cloud providers, and gain insight into what applications and versions run in your cluster. kube-ops-view : it shows how are the ops on the nodes. kubediff : a tool for Kubernetes to show differences between running state and version controlled configuration. ksniff : A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster. kubeview : Visualize dependencies kubernetes.", "title": "To try"}, {"location": "devops/kubernetes/kubernetes_vertical_pod_autoscaler/", "text": "Kubernetes knows the amount of resources a pod needs to operate through some metadata specified in the deployment. Generally this values change and manually maintaining all the resources requested and limits is a nightmare. The Vertical pod autoscaler does data analysis on the pod metrics to automatically adjust these values. Nevertheless it's still not suggested to use it in conjunction with the horizontal pod autoscaler , so we'll need to watch out for future improvements.", "title": "Vertical Pod Autoscaler"}, {"location": "devops/kubernetes/kubernetes_volumes/", "text": "On disk files in a Container are ephemeral by default, which presents the following issues: When a Container crashes, kubelet will restart it, but the files will be lost. When running Containers together in a Pod it is often necessary to share files between those Containers. The Kubernetes Volume abstraction solves both of these problems with several types . configMap \u2691 The configMap resource provides a way to inject configuration data into Pods. The data stored in a ConfigMap object can be referenced in a volume of type configMap and then consumed by containerized applications running in a Pod. emptyDir \u2691 An emptyDir volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the Pod can all read and write the same files in the emptyDir volume. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever. hostPath \u2691 A hostPath volume mounts a file or directory from the host node\u2019s filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications. For example, some uses for a hostPath are: Running a Container that needs access to Docker internals; use a hostPath of /var/lib/docker . Running cAdvisor in a Container; use a hostPath of /sys . secret \u2691 A secret volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by Pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage. awsElasticBlockStore \u2691 An awsElasticBlockStore volume mounts an Amazon Web Services (AWS) EBS Volume into your Pod. Unlike emptyDir , which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. There are some restrictions when using an awsElasticBlockStore volume: The nodes on which Pods are running must be AWS EC2 instances. Those instances need to be in the same region and availability-zone as the EBS volume. EBS only supports a single EC2 instance mounting a volume. nfs \u2691 An nfs volume allows an existing NFS (Network File System) share to be mounted into your Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. NFS can be mounted by multiple writers simultaneously. local \u2691 A local volume represents a mounted local storage device such as a disk, partition or directory. Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet. Compared to hostPath volumes, local volumes can be used in a durable and portable manner without manually scheduling Pods to nodes, as the system is aware of the volume\u2019s node constraints by looking at the node affinity on the PersistentVolume. However, local volumes are still subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume will also become inaccessible, and a Pod using it will not be able to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk. Others \u2691 glusterfs cephfs", "title": "Volumes"}, {"location": "devops/kubernetes/kubernetes_volumes/#configmap", "text": "The configMap resource provides a way to inject configuration data into Pods. The data stored in a ConfigMap object can be referenced in a volume of type configMap and then consumed by containerized applications running in a Pod.", "title": "configMap"}, {"location": "devops/kubernetes/kubernetes_volumes/#emptydir", "text": "An emptyDir volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the Pod can all read and write the same files in the emptyDir volume. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever.", "title": "emptyDir"}, {"location": "devops/kubernetes/kubernetes_volumes/#hostpath", "text": "A hostPath volume mounts a file or directory from the host node\u2019s filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications. For example, some uses for a hostPath are: Running a Container that needs access to Docker internals; use a hostPath of /var/lib/docker . Running cAdvisor in a Container; use a hostPath of /sys .", "title": "hostPath"}, {"location": "devops/kubernetes/kubernetes_volumes/#secret", "text": "A secret volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by Pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage.", "title": "secret"}, {"location": "devops/kubernetes/kubernetes_volumes/#awselasticblockstore", "text": "An awsElasticBlockStore volume mounts an Amazon Web Services (AWS) EBS Volume into your Pod. Unlike emptyDir , which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. There are some restrictions when using an awsElasticBlockStore volume: The nodes on which Pods are running must be AWS EC2 instances. Those instances need to be in the same region and availability-zone as the EBS volume. EBS only supports a single EC2 instance mounting a volume.", "title": "awsElasticBlockStore"}, {"location": "devops/kubernetes/kubernetes_volumes/#nfs", "text": "An nfs volume allows an existing NFS (Network File System) share to be mounted into your Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. NFS can be mounted by multiple writers simultaneously.", "title": "nfs"}, {"location": "devops/kubernetes/kubernetes_volumes/#local", "text": "A local volume represents a mounted local storage device such as a disk, partition or directory. Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet. Compared to hostPath volumes, local volumes can be used in a durable and portable manner without manually scheduling Pods to nodes, as the system is aware of the volume\u2019s node constraints by looking at the node affinity on the PersistentVolume. However, local volumes are still subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume will also become inaccessible, and a Pod using it will not be able to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk.", "title": "local"}, {"location": "devops/kubernetes/kubernetes_volumes/#others", "text": "glusterfs cephfs", "title": "Others"}, {"location": "devops/prometheus/alertmanager/", "text": "The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts. It is configured through the alertmanager.config key of the values.yaml of the helm chart. As stated in the configuration file , it has four main keys (as templates is handled in alertmanager.config.templateFiles ): global : SMTP and API main configuration, it will be inherited by the other elements. route : Route tree definition. receivers : Notification integrations configuration. inhibit_rules : Alert inhibition configuration. Route \u2691 A route block defines a node in a routing tree and its children. Its optional configuration parameters are inherited from its parent node if not set. Every alert enters the routing tree at the configured top-level route, which must match all alerts (i.e. not have any configured matchers). It then traverses the child nodes. If continue is set to false, it stops after the first matching child. If continue is true on a matching node, the alert will continue matching against subsequent siblings. If an alert does not match any children of a node (no matching child nodes, or none exist), the alert is handled based on the configuration parameters of the current node. Receivers \u2691 Notification receivers are the named configurations of one or more notification integrations. Email notifications \u2691 To configure email notifications, set up the following in your config : config : global : smtp_from : {{ from_email_address }} smtp_smarthost : {{ smtp_server_endpoint }} :{{ smtp_server_port }} smtp_auth_username : {{ smpt_authentication_username }} smtp_auth_password : {{ smpt_authentication_password }} receivers : - name : 'email' email_configs : - to : {{ receiver_email }} send_resolved : true If you need to set smtp_auth_username and smtp_auth_password you should value using helm secrets . send_resolved , set to False by default, defines whether or not to notify about resolved alerts. Rocketchat Notifications \u2691 Go to pavel-kazhavets AlertmanagerRocketChat repo for the updated rules. In RocketChat: Login as admin user and go to: Administration => Integrations => New Integration => Incoming WebHook. Set \"Enabled\" and \"Script Enabled\" to \"True\". Set all channel, icons, etc. as you need. Paste contents of the official AlertmanagerIntegrations.js or my version into Script field. AlertmanagerIntegrations.js class Script { process_incoming_request ({ request }) { console . log ( request . content ); var alertColor = \"warning\" ; if ( request . content . status == \"resolved\" ) { alertColor = \"good\" ; } else if ( request . content . status == \"firing\" ) { alertColor = \"danger\" ; } let finFields = []; for ( i = 0 ; i < request . content . alerts . length ; i ++ ) { var endVal = request . content . alerts [ i ]; var elem = { title : \"alertname: \" + endVal . labels . alertname , value : \"*instance:* \" + endVal . labels . instance , short : false }; finFields . push ( elem ); if ( !! endVal . annotations . summary ) { finFields . push ({ title : \"summary\" , value : endVal . annotations . summary }); } if ( !! endVal . annotations . severity ) { finFields . push ({ title : \"severity\" , value : endVal . labels . severity }); } if ( !! endVal . annotations . grafana ) { finFields . push ({ title : \"grafana\" , value : endVal . annotations . grafana }); } if ( !! endVal . annotations . prometheus ) { finFields . push ({ title : \"prometheus\" , value : endVal . annotations . prometheus }); } if ( !! endVal . annotations . message ) { finFields . push ({ title : \"message\" , value : endVal . annotations . message }); } if ( !! endVal . annotations . description ) { finFields . push ({ title : \"description\" , value : endVal . annotations . description }); } } return { content : { username : \"Prometheus Alert\" , attachments : [{ color : alertColor , title_link : request . content . externalURL , title : \"Prometheus notification\" , fields : finFields }] } }; return { error : { success : false } }; } } Create Integration. The field Webhook URL will appear in the Integration configuration. In Alertmanager: Create new receiver or modify config of existing one. You'll need to add webhooks_config to it. Small example: route : repeat_interval : 30m group_interval : 30m receiver : 'rocketchat' receivers : - name : 'rocketchat' webhook_configs : - send_resolved : false url : '${WEBHOOK_URL}' Reload/restart alertmanager. In order to test the webhook you can use the following curl (replace {{ webhook-url }} ): curl -X POST -H 'Content-Type: application/json' --data ' { \"text\": \"Example message\", \"attachments\": [ { \"title\": \"Rocket.Chat\", \"title_link\": \"https://rocket.chat\", \"text\": \"Rocket.Chat, the best open source chat\", \"image_url\": \"https://rocket.cha t/images/mockup.png\", \"color\": \"#764FA5\" } ], \"status\": \"firing\", \"alerts\": [ { \"labels\": { \"alertname\": \"high_load\", \"severity\": \"major\", \"instance\": \"node-exporter:9100\" }, \"annotations\": { \"message\": \"node-exporter:9100 of job xxxx is under high load.\", \"summary\": \"node-exporter:9100 under high load.\" } } ] } ' {{ webhook-url }} Inhibit rules \u2691 Inhibit rules define which alerts triggered by Prometheus shouldn't be forwarded to the notification integrations. For example the Watchdog alert is meant to test that everything works as expected, but is not meant to be used by the users. Similarly, if you are using EKS, you'll probably have an KubeVersionMismatch , because Kubernetes allows a certain version skew between their components. So the alert is more strict than the Kubernetes policy. To disable both alerts, set a match rule in config.inhibit_rules : config : inhibit_rules : - target_match : alertname : Watchdog - target_match : alertname : KubeVersionMismatch Alert rules \u2691 Alert rules are a special kind of Prometheus Rules that trigger alerts based on PromQL expressions. People have gathered several examples under Awesome prometheus alert rules Alerts must be configured in the Prometheus operator helm chart, under the additionalPrometheusRulesMap . For example: additionalPrometheusRulesMap : - groups : - name : alert-rules rules : - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" description : \"Probe failed\\n VALUE = {{ $value }}\\n LABELS: {{ $labels }}\" Other examples of rules are: Blackbox Exporter rules Silences \u2691 To silence an alert with a regular expression use the matcher alertname=~\".*Condition\" . References \u2691 Awesome prometheus alert rules", "title": "AlertManager"}, {"location": "devops/prometheus/alertmanager/#route", "text": "A route block defines a node in a routing tree and its children. Its optional configuration parameters are inherited from its parent node if not set. Every alert enters the routing tree at the configured top-level route, which must match all alerts (i.e. not have any configured matchers). It then traverses the child nodes. If continue is set to false, it stops after the first matching child. If continue is true on a matching node, the alert will continue matching against subsequent siblings. If an alert does not match any children of a node (no matching child nodes, or none exist), the alert is handled based on the configuration parameters of the current node.", "title": "Route"}, {"location": "devops/prometheus/alertmanager/#receivers", "text": "Notification receivers are the named configurations of one or more notification integrations.", "title": "Receivers"}, {"location": "devops/prometheus/alertmanager/#email-notifications", "text": "To configure email notifications, set up the following in your config : config : global : smtp_from : {{ from_email_address }} smtp_smarthost : {{ smtp_server_endpoint }} :{{ smtp_server_port }} smtp_auth_username : {{ smpt_authentication_username }} smtp_auth_password : {{ smpt_authentication_password }} receivers : - name : 'email' email_configs : - to : {{ receiver_email }} send_resolved : true If you need to set smtp_auth_username and smtp_auth_password you should value using helm secrets . send_resolved , set to False by default, defines whether or not to notify about resolved alerts.", "title": "Email notifications"}, {"location": "devops/prometheus/alertmanager/#rocketchat-notifications", "text": "Go to pavel-kazhavets AlertmanagerRocketChat repo for the updated rules. In RocketChat: Login as admin user and go to: Administration => Integrations => New Integration => Incoming WebHook. Set \"Enabled\" and \"Script Enabled\" to \"True\". Set all channel, icons, etc. as you need. Paste contents of the official AlertmanagerIntegrations.js or my version into Script field. AlertmanagerIntegrations.js class Script { process_incoming_request ({ request }) { console . log ( request . content ); var alertColor = \"warning\" ; if ( request . content . status == \"resolved\" ) { alertColor = \"good\" ; } else if ( request . content . status == \"firing\" ) { alertColor = \"danger\" ; } let finFields = []; for ( i = 0 ; i < request . content . alerts . length ; i ++ ) { var endVal = request . content . alerts [ i ]; var elem = { title : \"alertname: \" + endVal . labels . alertname , value : \"*instance:* \" + endVal . labels . instance , short : false }; finFields . push ( elem ); if ( !! endVal . annotations . summary ) { finFields . push ({ title : \"summary\" , value : endVal . annotations . summary }); } if ( !! endVal . annotations . severity ) { finFields . push ({ title : \"severity\" , value : endVal . labels . severity }); } if ( !! endVal . annotations . grafana ) { finFields . push ({ title : \"grafana\" , value : endVal . annotations . grafana }); } if ( !! endVal . annotations . prometheus ) { finFields . push ({ title : \"prometheus\" , value : endVal . annotations . prometheus }); } if ( !! endVal . annotations . message ) { finFields . push ({ title : \"message\" , value : endVal . annotations . message }); } if ( !! endVal . annotations . description ) { finFields . push ({ title : \"description\" , value : endVal . annotations . description }); } } return { content : { username : \"Prometheus Alert\" , attachments : [{ color : alertColor , title_link : request . content . externalURL , title : \"Prometheus notification\" , fields : finFields }] } }; return { error : { success : false } }; } } Create Integration. The field Webhook URL will appear in the Integration configuration. In Alertmanager: Create new receiver or modify config of existing one. You'll need to add webhooks_config to it. Small example: route : repeat_interval : 30m group_interval : 30m receiver : 'rocketchat' receivers : - name : 'rocketchat' webhook_configs : - send_resolved : false url : '${WEBHOOK_URL}' Reload/restart alertmanager. In order to test the webhook you can use the following curl (replace {{ webhook-url }} ): curl -X POST -H 'Content-Type: application/json' --data ' { \"text\": \"Example message\", \"attachments\": [ { \"title\": \"Rocket.Chat\", \"title_link\": \"https://rocket.chat\", \"text\": \"Rocket.Chat, the best open source chat\", \"image_url\": \"https://rocket.cha t/images/mockup.png\", \"color\": \"#764FA5\" } ], \"status\": \"firing\", \"alerts\": [ { \"labels\": { \"alertname\": \"high_load\", \"severity\": \"major\", \"instance\": \"node-exporter:9100\" }, \"annotations\": { \"message\": \"node-exporter:9100 of job xxxx is under high load.\", \"summary\": \"node-exporter:9100 under high load.\" } } ] } ' {{ webhook-url }}", "title": "Rocketchat Notifications"}, {"location": "devops/prometheus/alertmanager/#inhibit-rules", "text": "Inhibit rules define which alerts triggered by Prometheus shouldn't be forwarded to the notification integrations. For example the Watchdog alert is meant to test that everything works as expected, but is not meant to be used by the users. Similarly, if you are using EKS, you'll probably have an KubeVersionMismatch , because Kubernetes allows a certain version skew between their components. So the alert is more strict than the Kubernetes policy. To disable both alerts, set a match rule in config.inhibit_rules : config : inhibit_rules : - target_match : alertname : Watchdog - target_match : alertname : KubeVersionMismatch", "title": "Inhibit rules"}, {"location": "devops/prometheus/alertmanager/#alert-rules", "text": "Alert rules are a special kind of Prometheus Rules that trigger alerts based on PromQL expressions. People have gathered several examples under Awesome prometheus alert rules Alerts must be configured in the Prometheus operator helm chart, under the additionalPrometheusRulesMap . For example: additionalPrometheusRulesMap : - groups : - name : alert-rules rules : - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" description : \"Probe failed\\n VALUE = {{ $value }}\\n LABELS: {{ $labels }}\" Other examples of rules are: Blackbox Exporter rules", "title": "Alert rules"}, {"location": "devops/prometheus/alertmanager/#silences", "text": "To silence an alert with a regular expression use the matcher alertname=~\".*Condition\" .", "title": "Silences"}, {"location": "devops/prometheus/alertmanager/#references", "text": "Awesome prometheus alert rules", "title": "References"}, {"location": "devops/prometheus/blackbox_exporter/", "text": "The blackbox exporter allows blackbox probing of endpoints over HTTP, HTTPS, DNS, TCP and ICMP. It can be used to test: Website accessibility . Both for availability and security purposes. Website loading time . DNS response times to diagnose network latency issues. SSL certificates expiration . ICMP requests to gather network health information . Security protections such as if and endpoint stops being protected by VPN, WAF or SSL client certificate. Unauthorized read or write S3 buckets . When running, the Blackbox exporter is going to expose a HTTP endpoint that can be used in order to monitor targets over the network. By default, the Blackbox exporter exposes the /probe endpoint that is used to retrieve those metrics. The blackbox exporter is configured with a YAML configuration file made of modules . Installation \u2691 To install the exporter we'll use helmfile to install the stable/prometheus-blackbox-exporter chart . Add the following lines to your helmfile.yaml . - name : prometheus-blackbox-exporter namespace : monitoring chart : stable/prometheus-blackbox-exporter values : - prometheus-blackbox-exporter/values.yaml Edit the chart values. mkdir prometheus-blackbox-exporter helm inspect values stable/prometheus-blackbox-exporter > prometheus-blackbox-exporter/values.yaml vi prometheus-blackbox-exporter/values.yaml Make sure to enable the serviceMonitor in the values and target at least one page: serviceMonitor : enabled : true # Default values that will be used for all ServiceMonitors created by `targets` defaults : labels : release : prometheus-operator interval : 30s scrapeTimeout : 30s module : http_2xx targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book The label release: prometheus-operator must be the one your prometheus instance is searching for . If you want to use the icmp probe, make sure to allow allowIcmp: true . If you want to probe endpoints protected behind client SSL certificates, until this chart issue is solved, you need to create them manually as the Prometheus blackbox exporter helm chart doesn't yet create the required secrets. kubectl create secret generic monitor-certificates \\ --from-file = monitor.crt.pem \\ --from-file = monitor.key.pem \\ -n monitoring Where monitor.crt.pem and monitor.key.pem are the SSL certificate and key for the monitor account. I've found two grafana dashboards for the blackbox exporter. 7587 didn't work straight out of the box while 5345 did. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : blackbox-exporter : # Ref: https://grafana.com/dashboards/5345 gnetId : 5345 revision : 3 datasource : Prometheus And install. helmfile diff helmfile apply Blackbox exporter probes \u2691 Modules define how blackbox exporter is going to query the endpoint, therefore one needs to be created for each request type under the config.modules section of the chart. The modules are then used in the targets section for the desired endpoints. targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book module : https_2xx HTTP endpoint working correctly \u2691 http_2xx : prober : http timeout : 5s http : valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2.0\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" HTTPS endpoint working correctly \u2691 https_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2.0\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" HTTPS endpoint behind client SSL certificate \u2691 https_client_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2.0\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" tls_config : cert_file : /etc/secrets/monitor.crt.pem key_file : /etc/secrets/monitor.key.pem Where the secrets have been created throughout the installation. HTTPS endpoint with an specific error \u2691 If you don't want to configure the authentication for example for an API, you can fetch the expected error. https_client_api : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2.0\" ] valid_status_codes : [ 404 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" fail_if_body_not_matches_regexp : - '.*ERROR route not.*' HTTP endpoint returning an error \u2691 http_4xx : prober : http timeout : 5s http : method : HEAD valid_status_codes : [ 404 , 403 ] valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2.0\" ] no_follow_redirects : false HTTPS endpoint through an HTTP proxy \u2691 https_external_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.0\" , \"HTTP/1.1\" , \"HTTP/2.0\" ] valid_status_codes : [ 200 ] no_follow_redirects : false proxy_url : \"http://{{ proxy_url }}:{{ proxy_port }}\" preferred_ip_protocol : \"ip4\" HTTPS endpoint with basic auth \u2691 https_basic_auth_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2.0 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 basic_auth : username : {{ username }} password : {{ password }} HTTPs endpoint with API key \u2691 https_api_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2.0 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 headers : apikey : {{ api_key }} HTTPS Put file \u2691 Test if the probe can upload a file. https_put_file_2xx : prober : http timeout : 5s http : method : PUT body : hi fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2.0\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" Check open port \u2691 tcp_connect : prober : tcp The port is specified when using the module. - name : lyz-code.github.io url : lyz-code.github.io:389 module : tcp_connect Ping to the resource \u2691 Test if the target is alive. It's useful When you don't know what port to check or if it uses UDP. ping : prober : icmp timeout : 5s icmp : preferred_ip_protocol : \"ip4\" Blackbox exporter alerts \u2691 Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. To make security tests Availability alerts \u2691 The most basic probes, test if the service is up and returning. Blackbox probe failed \u2691 Blackbox probe failed. - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" message : \"Probe failed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_success+%3D%3D+0&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : probe_success{target!~\".*-fail-.*$\"} == 0 Blackbox probe HTTP failure \u2691 HTTP status code is not 200-399. - alert : BlackboxProbeHttpFailure expr : probe_http_status_code <= 199 OR probe_http_status_code >= 400 for : 5m labels : severity : error annotations : summary : \"Blackbox probe HTTP failure (instance {{ $labels.target }})\" message : \"HTTP status code is not 200-399\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\" Performance alerts \u2691 Blackbox slow probe \u2691 Blackbox probe took more than 1s to complete. - alert : BlackboxSlowProbe expr : avg_over_time(probe_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox slow probe (target {{ $labels.target }})\" message : \"Blackbox probe took more than 1s to complete\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1 Blackbox probe slow HTTP \u2691 HTTP request took more than 1s. - alert : BlackboxProbeSlowHttp expr : avg_over_time(probe_http_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow HTTP (instance {{ $labels.target }})\" message : \"HTTP request took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_http_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_http_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1 Blackbox probe slow ping \u2691 Blackbox ping took more than 1s. - alert : BlackboxProbeSlowPing expr : avg_over_time(probe_icmp_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow ping (instance {{ $labels.target }})\" message : \"Blackbox ping took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_icmp_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" SSL certificate alerts \u2691 Blackbox SSL certificate will expire in a month \u2691 SSL certificate expires in 30 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 30 for : 5m labels : severity : warning annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 30 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\" Blackbox SSL certificate will expire in a few days \u2691 SSL certificate expires in 3 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 3 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 3 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+3&g0.tab=1\" Blackbox SSL certificate expired \u2691 SSL certificate has expired already. - alert : BlackboxSslCertificateExpired expr : probe_ssl_earliest_cert_expiry - time() <= 0 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate expired (instance {{ $labels.target }})\" message : \"SSL certificate has expired already\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Security alerts \u2691 To define the security alerts, I've found easier to create a probe with the action I want to prevent and make sure that the probe fails. This probes contain the -fail- key in the target name, followed by the test it's performing. This convention allows the concatenation of tests. For example, when testing if and endpoint is accessible without basic auth and without vpn we'd use: - name : protected.endpoint.org-fail-without-ssl-and-without-credentials url : protected.endpoint.org module : https_external_2xx Test endpoints protected with network policies \u2691 Assuming that the blackbox exporter is in the internal network and that there is an http proxy on the external network we want to test. Create a working probe with the https_external_2xx module containing the -fail-without-vpn key in the target name. - alert : BlackboxVPNProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-vpn.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"VPN protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint from outside the internal network\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with SSL client certificate \u2691 Create a working probe with a module without the SSL client certificate configured, such as https_2xx and set the -fail-without-ssl key in the target name. - alert : BlackboxClientSSLProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-ssl.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"SSL client certificate protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without SSL certificate\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with credentials. \u2691 Create a working probe with a module without the basic auth credentials configured, such as https_2xx and set the -fail-without-credentials key in the target name. - alert : BlackboxCredentialsProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-credentials.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Credentials protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without credentials\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with WAF. \u2691 Create a working probe with a module bypassing the WAF, for example directly attacking the service and set the -fail-without-waf key in the target name. - alert : BlackboxWAFProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-waf.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"WAF protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the haproxy endpoint from the internal network (bypassed the WAF)\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Unauthorized read of S3 buckets \u2691 Create a working probe to an existent private object in an S3 bucket and set the -fail-read-object key in the target name. - alert : BlackboxS3BucketWrongReadPermissions expr : probe_success{target=~\".*-fail-.*read-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong read permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful read of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Unauthorized write of S3 buckets \u2691 Create a working probe using the https_put_file_2xx module to try to create a file in an S3 bucket and set the -fail-write-object key in the target name. - alert : BlackboxS3BucketWrongWritePermissions expr : probe_success{target=~\".*-fail-.*write-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong write permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful write of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Monitoring external access to internal services \u2691 There are two possible solutions to simulate traffic from outside your infrastructure to the internal services. Both require the installation of an agent outside of your internal infrastructure, it can be: An HTTP proxy. A blackbox exporter instance. Using the proxy you have following advantages: It's really easy to set up a transparent http proxy . All probe configuration goes in the same blackbox exporter instance values.yaml . With the following disadvantages: When using an external http proxy, the probe runs the DNS resolution locally . Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. * There is no way to do tcp or ping probes to simulate external traffic. * The latency between the blackbox exporter and the proxy is added to all the external probes. While using an external blackbox exporter gives the following advantages: Traffic is completely external to the infrastructure, so the proxy disadvantages would be solved. And the following disadvantages: Simulation of external traffic in AWS could be done by spawning the blackbox exporter instance in another region, but as there is no way of using EKS worker nodes in different regions, there is no way of managing the exporter from within Kubernetes. This means: The loose of the advantages of the Prometheus operator , so we have to write the configuration manually. Configuration can't be managed with Helm , so two solutions should be used to manage the monitorization (Ansible could be used). Even if it's possible to host the second external blackbox exporter within Kubernetes, two independent Helm charts are needed, with the consequent configuration management burden. In conclusion, when using a Kubernetes cluster that allows the creation of worker nodes outside the main infrastructure, or if several non HTTP/HTTPS endpoints need to be probed with the tcp or ping modules, install an external blackbox exporter instance. Otherwise install an HTTP proxy and assume that you can only simulate external HTTP/HTTPS traffic. Troubleshooting \u2691 To get more debugging information of the blackbox probes, add &debug=true to the probe url, for example http://localhost:9115/probe?module=http_2xx&target=https://www.prometheus.io/&debug=true . Service monitors are not being created \u2691 When running helmfile apply several times to update the resources, some are not being correctly created. Until the bug is solved, a workaround is to remove the chart release helm delete --purge prometeus-blackbox-exporter and running helmfile apply again. probe_success == 0 when using an http proxy \u2691 Even when using an external http proxy, the probe runs the DNS resolution locally. Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. Links \u2691 Git . Blackbox exporter modules configuration . Devconnected introduction to blackbox exporter .", "title": "Blackbox Exporter"}, {"location": "devops/prometheus/blackbox_exporter/#installation", "text": "To install the exporter we'll use helmfile to install the stable/prometheus-blackbox-exporter chart . Add the following lines to your helmfile.yaml . - name : prometheus-blackbox-exporter namespace : monitoring chart : stable/prometheus-blackbox-exporter values : - prometheus-blackbox-exporter/values.yaml Edit the chart values. mkdir prometheus-blackbox-exporter helm inspect values stable/prometheus-blackbox-exporter > prometheus-blackbox-exporter/values.yaml vi prometheus-blackbox-exporter/values.yaml Make sure to enable the serviceMonitor in the values and target at least one page: serviceMonitor : enabled : true # Default values that will be used for all ServiceMonitors created by `targets` defaults : labels : release : prometheus-operator interval : 30s scrapeTimeout : 30s module : http_2xx targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book The label release: prometheus-operator must be the one your prometheus instance is searching for . If you want to use the icmp probe, make sure to allow allowIcmp: true . If you want to probe endpoints protected behind client SSL certificates, until this chart issue is solved, you need to create them manually as the Prometheus blackbox exporter helm chart doesn't yet create the required secrets. kubectl create secret generic monitor-certificates \\ --from-file = monitor.crt.pem \\ --from-file = monitor.key.pem \\ -n monitoring Where monitor.crt.pem and monitor.key.pem are the SSL certificate and key for the monitor account. I've found two grafana dashboards for the blackbox exporter. 7587 didn't work straight out of the box while 5345 did. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : blackbox-exporter : # Ref: https://grafana.com/dashboards/5345 gnetId : 5345 revision : 3 datasource : Prometheus And install. helmfile diff helmfile apply", "title": "Installation"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-exporter-probes", "text": "Modules define how blackbox exporter is going to query the endpoint, therefore one needs to be created for each request type under the config.modules section of the chart. The modules are then used in the targets section for the desired endpoints. targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book module : https_2xx", "title": "Blackbox exporter probes"}, {"location": "devops/prometheus/blackbox_exporter/#http-endpoint-working-correctly", "text": "http_2xx : prober : http timeout : 5s http : valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2.0\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"", "title": "HTTP endpoint working correctly"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-working-correctly", "text": "https_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2.0\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"", "title": "HTTPS endpoint working correctly"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-behind-client-ssl-certificate", "text": "https_client_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2.0\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" tls_config : cert_file : /etc/secrets/monitor.crt.pem key_file : /etc/secrets/monitor.key.pem Where the secrets have been created throughout the installation.", "title": "HTTPS endpoint behind client SSL certificate"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-with-an-specific-error", "text": "If you don't want to configure the authentication for example for an API, you can fetch the expected error. https_client_api : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2.0\" ] valid_status_codes : [ 404 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" fail_if_body_not_matches_regexp : - '.*ERROR route not.*'", "title": "HTTPS endpoint with an specific error"}, {"location": "devops/prometheus/blackbox_exporter/#http-endpoint-returning-an-error", "text": "http_4xx : prober : http timeout : 5s http : method : HEAD valid_status_codes : [ 404 , 403 ] valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2.0\" ] no_follow_redirects : false", "title": "HTTP endpoint returning an error"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-through-an-http-proxy", "text": "https_external_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.0\" , \"HTTP/1.1\" , \"HTTP/2.0\" ] valid_status_codes : [ 200 ] no_follow_redirects : false proxy_url : \"http://{{ proxy_url }}:{{ proxy_port }}\" preferred_ip_protocol : \"ip4\"", "title": "HTTPS endpoint through an HTTP proxy"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-with-basic-auth", "text": "https_basic_auth_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2.0 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 basic_auth : username : {{ username }} password : {{ password }}", "title": "HTTPS endpoint with basic auth"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-with-api-key", "text": "https_api_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2.0 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 headers : apikey : {{ api_key }}", "title": "HTTPs endpoint with API key"}, {"location": "devops/prometheus/blackbox_exporter/#https-put-file", "text": "Test if the probe can upload a file. https_put_file_2xx : prober : http timeout : 5s http : method : PUT body : hi fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2.0\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"", "title": "HTTPS Put file"}, {"location": "devops/prometheus/blackbox_exporter/#check-open-port", "text": "tcp_connect : prober : tcp The port is specified when using the module. - name : lyz-code.github.io url : lyz-code.github.io:389 module : tcp_connect", "title": "Check open port"}, {"location": "devops/prometheus/blackbox_exporter/#ping-to-the-resource", "text": "Test if the target is alive. It's useful When you don't know what port to check or if it uses UDP. ping : prober : icmp timeout : 5s icmp : preferred_ip_protocol : \"ip4\"", "title": "Ping to the resource"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-exporter-alerts", "text": "Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. To make security tests", "title": "Blackbox exporter alerts"}, {"location": "devops/prometheus/blackbox_exporter/#availability-alerts", "text": "The most basic probes, test if the service is up and returning.", "title": "Availability alerts"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-probe-failed", "text": "Blackbox probe failed. - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" message : \"Probe failed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_success+%3D%3D+0&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : probe_success{target!~\".*-fail-.*$\"} == 0", "title": "Blackbox probe failed"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-probe-http-failure", "text": "HTTP status code is not 200-399. - alert : BlackboxProbeHttpFailure expr : probe_http_status_code <= 199 OR probe_http_status_code >= 400 for : 5m labels : severity : error annotations : summary : \"Blackbox probe HTTP failure (instance {{ $labels.target }})\" message : \"HTTP status code is not 200-399\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\"", "title": "Blackbox probe HTTP failure"}, {"location": "devops/prometheus/blackbox_exporter/#performance-alerts", "text": "", "title": "Performance alerts"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-slow-probe", "text": "Blackbox probe took more than 1s to complete. - alert : BlackboxSlowProbe expr : avg_over_time(probe_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox slow probe (target {{ $labels.target }})\" message : \"Blackbox probe took more than 1s to complete\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1", "title": "Blackbox slow probe"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-probe-slow-http", "text": "HTTP request took more than 1s. - alert : BlackboxProbeSlowHttp expr : avg_over_time(probe_http_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow HTTP (instance {{ $labels.target }})\" message : \"HTTP request took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_http_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_http_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1", "title": "Blackbox probe slow HTTP"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-probe-slow-ping", "text": "Blackbox ping took more than 1s. - alert : BlackboxProbeSlowPing expr : avg_over_time(probe_icmp_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow ping (instance {{ $labels.target }})\" message : \"Blackbox ping took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_icmp_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\"", "title": "Blackbox probe slow ping"}, {"location": "devops/prometheus/blackbox_exporter/#ssl-certificate-alerts", "text": "", "title": "SSL certificate alerts"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-will-expire-in-a-month", "text": "SSL certificate expires in 30 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 30 for : 5m labels : severity : warning annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 30 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\"", "title": "Blackbox SSL certificate will expire in a month"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-will-expire-in-a-few-days", "text": "SSL certificate expires in 3 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 3 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 3 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+3&g0.tab=1\"", "title": "Blackbox SSL certificate will expire in a few days"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-expired", "text": "SSL certificate has expired already. - alert : BlackboxSslCertificateExpired expr : probe_ssl_earliest_cert_expiry - time() <= 0 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate expired (instance {{ $labels.target }})\" message : \"SSL certificate has expired already\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"", "title": "Blackbox SSL certificate expired"}, {"location": "devops/prometheus/blackbox_exporter/#security-alerts", "text": "To define the security alerts, I've found easier to create a probe with the action I want to prevent and make sure that the probe fails. This probes contain the -fail- key in the target name, followed by the test it's performing. This convention allows the concatenation of tests. For example, when testing if and endpoint is accessible without basic auth and without vpn we'd use: - name : protected.endpoint.org-fail-without-ssl-and-without-credentials url : protected.endpoint.org module : https_external_2xx", "title": "Security alerts"}, {"location": "devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-network-policies", "text": "Assuming that the blackbox exporter is in the internal network and that there is an http proxy on the external network we want to test. Create a working probe with the https_external_2xx module containing the -fail-without-vpn key in the target name. - alert : BlackboxVPNProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-vpn.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"VPN protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint from outside the internal network\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"", "title": "Test endpoints protected with network policies"}, {"location": "devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-ssl-client-certificate", "text": "Create a working probe with a module without the SSL client certificate configured, such as https_2xx and set the -fail-without-ssl key in the target name. - alert : BlackboxClientSSLProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-ssl.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"SSL client certificate protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without SSL certificate\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"", "title": "Test endpoints protected with SSL client certificate"}, {"location": "devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-credentials", "text": "Create a working probe with a module without the basic auth credentials configured, such as https_2xx and set the -fail-without-credentials key in the target name. - alert : BlackboxCredentialsProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-credentials.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Credentials protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without credentials\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"", "title": "Test endpoints protected with credentials."}, {"location": "devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-waf", "text": "Create a working probe with a module bypassing the WAF, for example directly attacking the service and set the -fail-without-waf key in the target name. - alert : BlackboxWAFProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-waf.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"WAF protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the haproxy endpoint from the internal network (bypassed the WAF)\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"", "title": "Test endpoints protected with WAF."}, {"location": "devops/prometheus/blackbox_exporter/#unauthorized-read-of-s3-buckets", "text": "Create a working probe to an existent private object in an S3 bucket and set the -fail-read-object key in the target name. - alert : BlackboxS3BucketWrongReadPermissions expr : probe_success{target=~\".*-fail-.*read-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong read permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful read of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"", "title": "Unauthorized read of S3 buckets"}, {"location": "devops/prometheus/blackbox_exporter/#unauthorized-write-of-s3-buckets", "text": "Create a working probe using the https_put_file_2xx module to try to create a file in an S3 bucket and set the -fail-write-object key in the target name. - alert : BlackboxS3BucketWrongWritePermissions expr : probe_success{target=~\".*-fail-.*write-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong write permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful write of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"", "title": "Unauthorized write of S3 buckets"}, {"location": "devops/prometheus/blackbox_exporter/#monitoring-external-access-to-internal-services", "text": "There are two possible solutions to simulate traffic from outside your infrastructure to the internal services. Both require the installation of an agent outside of your internal infrastructure, it can be: An HTTP proxy. A blackbox exporter instance. Using the proxy you have following advantages: It's really easy to set up a transparent http proxy . All probe configuration goes in the same blackbox exporter instance values.yaml . With the following disadvantages: When using an external http proxy, the probe runs the DNS resolution locally . Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. * There is no way to do tcp or ping probes to simulate external traffic. * The latency between the blackbox exporter and the proxy is added to all the external probes. While using an external blackbox exporter gives the following advantages: Traffic is completely external to the infrastructure, so the proxy disadvantages would be solved. And the following disadvantages: Simulation of external traffic in AWS could be done by spawning the blackbox exporter instance in another region, but as there is no way of using EKS worker nodes in different regions, there is no way of managing the exporter from within Kubernetes. This means: The loose of the advantages of the Prometheus operator , so we have to write the configuration manually. Configuration can't be managed with Helm , so two solutions should be used to manage the monitorization (Ansible could be used). Even if it's possible to host the second external blackbox exporter within Kubernetes, two independent Helm charts are needed, with the consequent configuration management burden. In conclusion, when using a Kubernetes cluster that allows the creation of worker nodes outside the main infrastructure, or if several non HTTP/HTTPS endpoints need to be probed with the tcp or ping modules, install an external blackbox exporter instance. Otherwise install an HTTP proxy and assume that you can only simulate external HTTP/HTTPS traffic.", "title": "Monitoring external access to internal services"}, {"location": "devops/prometheus/blackbox_exporter/#troubleshooting", "text": "To get more debugging information of the blackbox probes, add &debug=true to the probe url, for example http://localhost:9115/probe?module=http_2xx&target=https://www.prometheus.io/&debug=true .", "title": "Troubleshooting"}, {"location": "devops/prometheus/blackbox_exporter/#service-monitors-are-not-being-created", "text": "When running helmfile apply several times to update the resources, some are not being correctly created. Until the bug is solved, a workaround is to remove the chart release helm delete --purge prometeus-blackbox-exporter and running helmfile apply again.", "title": "Service monitors are not being created"}, {"location": "devops/prometheus/blackbox_exporter/#probe_success-0-when-using-an-http-proxy", "text": "Even when using an external http proxy, the probe runs the DNS resolution locally. Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist.", "title": "probe_success == 0 when using an http proxy"}, {"location": "devops/prometheus/blackbox_exporter/#links", "text": "Git . Blackbox exporter modules configuration . Devconnected introduction to blackbox exporter .", "title": "Links"}, {"location": "devops/prometheus/instance_sizing_analysis/", "text": "Once we gather the instance metrics with the Node exporter , we can do statistical analysis on the evolution of time to detect the instances that are undersized or oversized. RAM analysis \u2691 Instance RAM percent usage metric can be calculated with the following Prometheus rule : - record : instance_path:node_memory_MemAvailable_percent expr : (1 - node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes ) * 100 The average , standard deviation and the standard score of the last two weeks would be: - record : instance_path:node_memory_MemAvailable_percent:avg_over_time_2w expr : avg_over_time(instance_path:node_memory_MemAvailable_percent[2w]) - record : instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w expr : stddev_over_time(instance_path:node_memory_MemAvailable_percent[2w]) - record : instance_path:node_memory_MemAvailable_percent:z_score expr : > ( instance_path:node_memory_MemAvailable_percent - instance_path:node_memory_MemAvailable_percent:avg_over_time_2w ) / instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w With that data we can define that an instance is oversized if the average plus the standard deviation is less than 60% and undersized if its greater than 90%. With the average we take into account the nominal RAM consumption, and with the standard deviation we take into account the spikes. Tweak this rule to your use case The criteria of undersized and oversized is just a first approximation I'm going to use. You can use it as a base criteria, but don't go through with it blindly. See the disclaimer below for more information. # RAM - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : RAM problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w > 90 labels : type : EC2 metric : RAM problem : undersized Where avg_plus_stddev_over_time_2w is: - record : instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w expr : > instance_path:node_memory_MemAvailable_percent:avg_over_time_2w + instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w CPU analysis \u2691 Instance CPU percent usage metric can be calculated with the following Prometheus rule : - record : instance_path:node_cpu_percent:rate1m expr : > (1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100 The node_cpu_seconds_total doesn't give us the percent of usage, that is why we need to do the average of the rate of the last minute. The average , standard deviation , the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only. CPU usage rules # --------------------------------------- # -- Resource consumption calculations -- # --------------------------------------- # CPU - record : instance_path:node_cpu_percent:rate1m expr : > (1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100 - record : instance_path:node_cpu_percent:rate1m:avg_over_time_2w expr : avg_over_time(instance_path:node_cpu_percent:rate1m[2w]) - record : instance_path:node_cpu_percent:rate1m:stddev_over_time_2w expr : stddev_over_time(instance_path:node_cpu_percent:rate1m[2w]) - record : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_cpu_percent:rate1m:avg_over_time_2w + instance_path:node_cpu_percent:rate1m:stddev_over_time_2w - record : instance_path:node_cpu_percent:rate1m:z_score expr : > ( instance_path:node_cpu_percent:rate1m - instance_path:node_cpu_percent:rate1m:avg_over_time_2w ) / instance_path:node_cpu_percent:rate1m:stddev_over_time_2w # ---------------------------------- # -- Resource sizing calculations -- # ---------------------------------- # CPU - record : instance_path:wrong_resource_size expr : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : CPU problem : oversized - record : instance_path:wrong_resource_size expr : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w > 80 labels : type : EC2 metric : CPU problem : undersized Network analysis \u2691 We can deduce the network usage from the node_network_receive_bytes_total and node_network_transmit_bytes_total metrics. For example for the transmit, the Gigabits per second transmitted can be calculated with the following Prometheus rule : - record : instance_path:node_network_transmit_gigabits_per_second:rate5m expr : > increase( node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 Where we: Filter the traffic only to the external network interfaces node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"} . Those are the ones used by AWS , but you'll need to tweak that for your case. Convert the increase of Kilobytes per minute [1m] to Gigabits per second by multiplying it by 7.450580596923828 * 10^-9 / 60 . The average , standard deviation , the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only. Network usage rules # --------------------------------------- # -- Resource consumption calculations -- # --------------------------------------- # NetworkReceive - record : instance_path:node_network_receive_gigabits_per_second:rate1m expr : > increase( node_network_receive_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 - record : instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w expr : > avg_over_time( instance_path:node_network_receive_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w expr : > stddev_over_time( instance_path:node_network_receive_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w + instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w - record : instance_path:node_network_receive_gigabits_per_second:rate1m:z_score expr : > ( instance_path:node_network_receive_gigabits_per_second:rate1m - instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w ) / instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w # NetworkTransmit - record : instance_path:node_network_transmit_gigabits_per_second:rate1m expr : > increase( node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w expr : > avg_over_time( instance_path:node_network_transmit_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w expr : > stddev_over_time( instance_path:node_network_transmit_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w + instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score expr : > ( instance_path:node_network_transmit_gigabits_per_second:rate1m - instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w ) / instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w # ---------------------------------- # -- Resource sizing calculations -- # ---------------------------------- # NetworkReceive - record : instance_path:wrong_resource_size expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w < 0.5 labels : type : EC2 metric : NetworkReceive problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w > 3 labels : type : EC2 metric : NetworkReceive problem : undersized # NetworkTransmit - record : instance_path:wrong_resource_size expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w < 0.5 labels : type : EC2 metric : NetworkTransmit problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w > 3 labels : type : EC2 metric : NetworkTransmit problem : undersized The difference with network is that we don't have a percent of the total instance bandwidth, In my case, my instances support from 0.5 to 5 Gbps which is more than I need, so most of my instances are marked as oversized with the < 0.5 rule. I will manually study the ones that go over 3 Gbps. The correct way to do it, is to tag the baseline, burst or/and maximum network performance by instance type. In the AWS case, the data can be extracted using the AWS docs or external benchmarks . Once you know the network performance per instance type, you can use relabeling in the Node exporter service monitor to add a label like max_network_performance and use it later in the rules. If you do follow this path, please contact me or do a pull request so I can test your solution. Overall analysis \u2691 Now that we have all the analysis under the metric instance_path:wrong_resource_size with labels, we can aggregate them to see the number of rules each instance is breaking with the following rule: # Mark the number of oversize rules matched by each instance - record : instance_path:wrong_instance_size expr : count by (instance) (sum by (metric, instance) (instance_path:wrong_resource_size)) By executing sort_desc(instance_path:wrong_instance_size) in the Prometheus web application, we'll be able to see such instances. instance_path:wrong_instance_size{instance=\"frontend-production:192.168.1.2\"} 4 instance_path:wrong_instance_size{instance=\"backend-production-instance:172.30.0.195\"} 2 ... To see the detail of what rules is our instance breaking we can use something like instance_path:wrong_resource_size{instance =~'frontend.*'} instance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"RAM\",problem=\"oversized\",type=\"EC2\"} 5.126602454544287 instance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",metric=\"CPU\",problem=\"oversized\",type=\"EC2\"} 0.815639209497615 instance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkReceive\",problem=\"oversized\",type=\"EC2\"} 0.02973250128744766 instance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkTransmit\",problem=\"oversized\",type=\"EC2\"} 0.01586461503849804 Here we see that the frontend-production is a c4.2xlarge instance that is consuming an average plus standard deviation of CPU of 0.81%, RAM 5.12%, NetworkTransmit 0.015Gbps and NetworkReceive 0.029Gbps, which results in an oversized alert on all four metrics. If you want to see the evolution over the time, instead of Console click on Graph under the text box where you have entered the query. With this information, we can decide which is the correct instance for each application. Once all instances are migrated to their ideal size, we can add alerts on these metrics so we can have a continuous analysis of our instances. Once I've done it, I'll add the alerts here. Disclaimer \u2691 We haven't tested this rules yet in production to resize our infrastructure (will do soon), so use all the information in this document cautiously. What I can expect to fail is that the assumption of average plus a standard deviation criteria can not be enough, maybe I need to increase the resolution of the standard deviation so it can be more sensible to the spikes, or we need to use a safety factor of 2 or 3. We'll see :) Read throughly the Gitlab post on anomaly detection using Prometheus , it's awesome and it may give you insights on why this approach is not working with you, as well as other algorithms that for example take into account the seasonality of the metrics. In particular it's interesting to analyze your resources z-score evolution over time, if all values fall in the +4 to -4 range, you can statistically assert that your metric similarly follows the normal distribution, and can assume that any value of z_score above 3 is an anomaly. If your results return with a range of +20 to -20 , the tail is too long and your results will be skewed. To test it you can use the following queries to test the RAM behaviour, adapt them for the rest of the resources: # Minimum z_score value sort_desc ( abs (( min_over_time ( instance_path : node_memory_MemAvailable_percent [ 1w ] ) - instance_path : node_memory_MemAvailable_percent : avg_over_time_2w ) / instance_path : node_memory_MemAvailable_percent : stddev_over_time_2w )) # Maximum z_score value sort_desc ( abs (( max_over_time ( instance_path : node_memory_MemAvailable_percent [ 1w ] ) - instance_path : node_memory_MemAvailable_percent : avg_over_time_2w ) / instance_path : node_memory_MemAvailable_percent : stddev_over_time_2w )) For a less exhaustive but more graphical analysis, execute instance_path:node_memory_MemAvailable_percent:z_score in Graph mode. In my case the RAM is in the +-5 interval, with some peaks of 20, but after reviewing instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w in those periods, I feel it's still safe to use the assumption. Same criteria applies to instance_path:node_cpu_percent:rate1m:z_score , instance_path:node_network_receive_gigabits_per_second:rate1m:z_score , and instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score , metrics. References \u2691 Gitlab post on anomaly detection using Prometheus .", "title": "Instance sizing analysis"}, {"location": "devops/prometheus/instance_sizing_analysis/#ram-analysis", "text": "Instance RAM percent usage metric can be calculated with the following Prometheus rule : - record : instance_path:node_memory_MemAvailable_percent expr : (1 - node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes ) * 100 The average , standard deviation and the standard score of the last two weeks would be: - record : instance_path:node_memory_MemAvailable_percent:avg_over_time_2w expr : avg_over_time(instance_path:node_memory_MemAvailable_percent[2w]) - record : instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w expr : stddev_over_time(instance_path:node_memory_MemAvailable_percent[2w]) - record : instance_path:node_memory_MemAvailable_percent:z_score expr : > ( instance_path:node_memory_MemAvailable_percent - instance_path:node_memory_MemAvailable_percent:avg_over_time_2w ) / instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w With that data we can define that an instance is oversized if the average plus the standard deviation is less than 60% and undersized if its greater than 90%. With the average we take into account the nominal RAM consumption, and with the standard deviation we take into account the spikes. Tweak this rule to your use case The criteria of undersized and oversized is just a first approximation I'm going to use. You can use it as a base criteria, but don't go through with it blindly. See the disclaimer below for more information. # RAM - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : RAM problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w > 90 labels : type : EC2 metric : RAM problem : undersized Where avg_plus_stddev_over_time_2w is: - record : instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w expr : > instance_path:node_memory_MemAvailable_percent:avg_over_time_2w + instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w", "title": "RAM analysis"}, {"location": "devops/prometheus/instance_sizing_analysis/#cpu-analysis", "text": "Instance CPU percent usage metric can be calculated with the following Prometheus rule : - record : instance_path:node_cpu_percent:rate1m expr : > (1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100 The node_cpu_seconds_total doesn't give us the percent of usage, that is why we need to do the average of the rate of the last minute. The average , standard deviation , the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only. CPU usage rules # --------------------------------------- # -- Resource consumption calculations -- # --------------------------------------- # CPU - record : instance_path:node_cpu_percent:rate1m expr : > (1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100 - record : instance_path:node_cpu_percent:rate1m:avg_over_time_2w expr : avg_over_time(instance_path:node_cpu_percent:rate1m[2w]) - record : instance_path:node_cpu_percent:rate1m:stddev_over_time_2w expr : stddev_over_time(instance_path:node_cpu_percent:rate1m[2w]) - record : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_cpu_percent:rate1m:avg_over_time_2w + instance_path:node_cpu_percent:rate1m:stddev_over_time_2w - record : instance_path:node_cpu_percent:rate1m:z_score expr : > ( instance_path:node_cpu_percent:rate1m - instance_path:node_cpu_percent:rate1m:avg_over_time_2w ) / instance_path:node_cpu_percent:rate1m:stddev_over_time_2w # ---------------------------------- # -- Resource sizing calculations -- # ---------------------------------- # CPU - record : instance_path:wrong_resource_size expr : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : CPU problem : oversized - record : instance_path:wrong_resource_size expr : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w > 80 labels : type : EC2 metric : CPU problem : undersized", "title": "CPU analysis"}, {"location": "devops/prometheus/instance_sizing_analysis/#network-analysis", "text": "We can deduce the network usage from the node_network_receive_bytes_total and node_network_transmit_bytes_total metrics. For example for the transmit, the Gigabits per second transmitted can be calculated with the following Prometheus rule : - record : instance_path:node_network_transmit_gigabits_per_second:rate5m expr : > increase( node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 Where we: Filter the traffic only to the external network interfaces node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"} . Those are the ones used by AWS , but you'll need to tweak that for your case. Convert the increase of Kilobytes per minute [1m] to Gigabits per second by multiplying it by 7.450580596923828 * 10^-9 / 60 . The average , standard deviation , the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only. Network usage rules # --------------------------------------- # -- Resource consumption calculations -- # --------------------------------------- # NetworkReceive - record : instance_path:node_network_receive_gigabits_per_second:rate1m expr : > increase( node_network_receive_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 - record : instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w expr : > avg_over_time( instance_path:node_network_receive_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w expr : > stddev_over_time( instance_path:node_network_receive_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w + instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w - record : instance_path:node_network_receive_gigabits_per_second:rate1m:z_score expr : > ( instance_path:node_network_receive_gigabits_per_second:rate1m - instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w ) / instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w # NetworkTransmit - record : instance_path:node_network_transmit_gigabits_per_second:rate1m expr : > increase( node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w expr : > avg_over_time( instance_path:node_network_transmit_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w expr : > stddev_over_time( instance_path:node_network_transmit_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w + instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score expr : > ( instance_path:node_network_transmit_gigabits_per_second:rate1m - instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w ) / instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w # ---------------------------------- # -- Resource sizing calculations -- # ---------------------------------- # NetworkReceive - record : instance_path:wrong_resource_size expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w < 0.5 labels : type : EC2 metric : NetworkReceive problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w > 3 labels : type : EC2 metric : NetworkReceive problem : undersized # NetworkTransmit - record : instance_path:wrong_resource_size expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w < 0.5 labels : type : EC2 metric : NetworkTransmit problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w > 3 labels : type : EC2 metric : NetworkTransmit problem : undersized The difference with network is that we don't have a percent of the total instance bandwidth, In my case, my instances support from 0.5 to 5 Gbps which is more than I need, so most of my instances are marked as oversized with the < 0.5 rule. I will manually study the ones that go over 3 Gbps. The correct way to do it, is to tag the baseline, burst or/and maximum network performance by instance type. In the AWS case, the data can be extracted using the AWS docs or external benchmarks . Once you know the network performance per instance type, you can use relabeling in the Node exporter service monitor to add a label like max_network_performance and use it later in the rules. If you do follow this path, please contact me or do a pull request so I can test your solution.", "title": "Network analysis"}, {"location": "devops/prometheus/instance_sizing_analysis/#overall-analysis", "text": "Now that we have all the analysis under the metric instance_path:wrong_resource_size with labels, we can aggregate them to see the number of rules each instance is breaking with the following rule: # Mark the number of oversize rules matched by each instance - record : instance_path:wrong_instance_size expr : count by (instance) (sum by (metric, instance) (instance_path:wrong_resource_size)) By executing sort_desc(instance_path:wrong_instance_size) in the Prometheus web application, we'll be able to see such instances. instance_path:wrong_instance_size{instance=\"frontend-production:192.168.1.2\"} 4 instance_path:wrong_instance_size{instance=\"backend-production-instance:172.30.0.195\"} 2 ... To see the detail of what rules is our instance breaking we can use something like instance_path:wrong_resource_size{instance =~'frontend.*'} instance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"RAM\",problem=\"oversized\",type=\"EC2\"} 5.126602454544287 instance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",metric=\"CPU\",problem=\"oversized\",type=\"EC2\"} 0.815639209497615 instance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkReceive\",problem=\"oversized\",type=\"EC2\"} 0.02973250128744766 instance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkTransmit\",problem=\"oversized\",type=\"EC2\"} 0.01586461503849804 Here we see that the frontend-production is a c4.2xlarge instance that is consuming an average plus standard deviation of CPU of 0.81%, RAM 5.12%, NetworkTransmit 0.015Gbps and NetworkReceive 0.029Gbps, which results in an oversized alert on all four metrics. If you want to see the evolution over the time, instead of Console click on Graph under the text box where you have entered the query. With this information, we can decide which is the correct instance for each application. Once all instances are migrated to their ideal size, we can add alerts on these metrics so we can have a continuous analysis of our instances. Once I've done it, I'll add the alerts here.", "title": "Overall analysis"}, {"location": "devops/prometheus/instance_sizing_analysis/#disclaimer", "text": "We haven't tested this rules yet in production to resize our infrastructure (will do soon), so use all the information in this document cautiously. What I can expect to fail is that the assumption of average plus a standard deviation criteria can not be enough, maybe I need to increase the resolution of the standard deviation so it can be more sensible to the spikes, or we need to use a safety factor of 2 or 3. We'll see :) Read throughly the Gitlab post on anomaly detection using Prometheus , it's awesome and it may give you insights on why this approach is not working with you, as well as other algorithms that for example take into account the seasonality of the metrics. In particular it's interesting to analyze your resources z-score evolution over time, if all values fall in the +4 to -4 range, you can statistically assert that your metric similarly follows the normal distribution, and can assume that any value of z_score above 3 is an anomaly. If your results return with a range of +20 to -20 , the tail is too long and your results will be skewed. To test it you can use the following queries to test the RAM behaviour, adapt them for the rest of the resources: # Minimum z_score value sort_desc ( abs (( min_over_time ( instance_path : node_memory_MemAvailable_percent [ 1w ] ) - instance_path : node_memory_MemAvailable_percent : avg_over_time_2w ) / instance_path : node_memory_MemAvailable_percent : stddev_over_time_2w )) # Maximum z_score value sort_desc ( abs (( max_over_time ( instance_path : node_memory_MemAvailable_percent [ 1w ] ) - instance_path : node_memory_MemAvailable_percent : avg_over_time_2w ) / instance_path : node_memory_MemAvailable_percent : stddev_over_time_2w )) For a less exhaustive but more graphical analysis, execute instance_path:node_memory_MemAvailable_percent:z_score in Graph mode. In my case the RAM is in the +-5 interval, with some peaks of 20, but after reviewing instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w in those periods, I feel it's still safe to use the assumption. Same criteria applies to instance_path:node_cpu_percent:rate1m:z_score , instance_path:node_network_receive_gigabits_per_second:rate1m:z_score , and instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score , metrics.", "title": "Disclaimer"}, {"location": "devops/prometheus/instance_sizing_analysis/#references", "text": "Gitlab post on anomaly detection using Prometheus .", "title": "References"}, {"location": "devops/prometheus/node_exporter/", "text": "Node Exporter is a Prometheus exporter for hardware and OS metrics exposed by *NIX kernels, written in Go with pluggable metric collectors. Install \u2691 To install in kubernetes nodes, use this chart . Elsewhere use this ansible role . If you use node exporter agents outside kubernetes, you need to configure a prometheus service discovery to scrap the information from them. To auto discover EC2 instances use the ec2_sd_config configuration. It can be added in the helm chart values.yaml under the key prometheus.prometheusSpec.additionalScrapeConfigs - job_name : node_exporter ec2_sd_configs : - region : us-east-1 port : 9100 refresh_interval : 1m relabel_configs : - source_labels : [ '__meta_ec2_tag_Name' , '__meta_ec2_private_ip' ] separator : ':' target_label : instance - source_labels : - __meta_ec2_instance_type target_label : instance_type The relabel_configs part will substitute the instance label of each target from {{ instance_ip }}:9100 to {{ instance_name }}:{{ instance_ip }} . If the worker nodes already have an IAM role with the ec2:DescribeInstances permission there is no need to specify the role_arn or access_keys and secret_key . If you have stopped instances, the node exporter will raise an alert because it won't be able to scrape the metrics from them. To only fetch data from running instances add a filter: ec2_sd_configs : - region : us-east-1 filters : - name : instance-state-name values : - running To monitor only the instances of a list of VPCs use this filter: ec2_sd_configs : - region : us-east-1 filters : - name : vpc-id values : - vpc-xxxxxxxxxxxxxxxxx - vpc-yyyyyyyyyyyyyyyyy By default, prometheus will try to scrape the private instance ip. To use the public one you need to relabel it with the following snippet: ec2_sd_configs : - region : us-east-1 relabel_configs : - source_labels : [ '__meta_ec2_public_ip' ] regex : ^(.*)$ target_label : __address__ replacement : ${1}:9100 I'm using the 11074 grafana dashboards for the blackbox exporter, which worked straight out of the box. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : node_exporter : # Ref: https://grafana.com/dashboards/11074 gnetId : 11074 revision : 4 datasource : Prometheus And install. helmfile diff helmfile apply Node exporter size analysis \u2691 Once the instance metrics are being ingested, we can do a periodic analysis to deduce which instances are undersized or oversized. Node exporter alerts \u2691 Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. Host out of memory \u2691 Node memory is filling up ( < 10% left). - alert : HostOutOfMemory expr : node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of memory (instance {{ $labels.instance }})\" message : \"Node memory is filling up (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host memory under memory pressure \u2691 The node is under heavy memory pressure. High rate of major page faults. - alert : HostMemoryUnderMemoryPressure expr : rate(node_vmstat_pgmajfault[1m]) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host memory under memory pressure (instance {{ $labels.instance }})\" message : \"The node is under heavy memory pressure. High rate of major page faults.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual network throughput in \u2691 Host network interfaces are probably receiving too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputIn expr : sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput in (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably receiving too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual network throughput out \u2691 Host network interfaces are probably sending too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputOut expr : sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput out (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably sending too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk read rate \u2691 Disk is probably reading too much data (> 50 MB/s) - alert : HostUnusualDiskReadRate expr : sum by (instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read rate (instance {{ $labels.instance }})\" message : \"Disk is probably reading too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk write rate \u2691 Disk is probably writing too much data (> 50 MB/s) - alert : HostUnusualDiskWriteRate expr : sum by (instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write rate (instance {{ $labels.instance }})\" message : \"Disk is probably writing too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host out of disk space \u2691 Disk is worryingly almost full ( < 10% left ). - alert : HostOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 10 for : 5m labels : severity : critical annotations : summary : \"Host out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Disk is almost full ( < 20% left ) - alert : HostReachingOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 20 for : 5m labels : severity : warning annotations : summary : \"Host reaching out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 20% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host disk will fill in 4 hours \u2691 Disk will fill in 4 hours at current write rate - alert : HostDiskWillFillIn4Hours expr : predict_linear(node_filesystem_free_bytes{fstype!~\"tmpfs\"}[1h], 4 * 3600) < 0 for : 5m labels : severity : critical annotations : summary : \"Host disk will fill in 4 hours (instance {{ $labels.instance }})\" message : \"Disk will fill in 4 hours at current write rate\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host out of inodes \u2691 Disk is almost running out of available inodes ( < 10% left ). - alert : HostOutOfInodes expr : node_filesystem_files_free{fstype!~\"tmpfs\"} / node_filesystem_files{fstype!~\"tmpfs\"} * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of inodes (instance {{ $labels.instance }})\" message : \"Disk is almost running out of available inodes (< 10% left)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk read latency \u2691 Disk latency is growing (read operations > 100ms). - alert : HostUnusualDiskReadLatency expr : rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (read operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk write latency \u2691 Disk latency is growing (write operations > 100ms) - alert : HostUnusualDiskWriteLatency expr : rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (write operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host high CPU load \u2691 CPU load is > 80% - alert : HostHighCpuLoad expr : 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 80 for : 5m labels : severity : warning annotations : summary : \"Host high CPU load (instance {{ $labels.instance }})\" message : \"CPU load is > 80%\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host context switching \u2691 Context switching is growing on node (> 1000 / s) # 1000 context switches is an arbitrary number. # Alert threshold depends on nature of application. # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58 - alert : HostContextSwitching expr : (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode=\"idle\"})) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host context switching (instance {{ $labels.instance }})\" message : \"Context switching is growing on node (> 1000 / s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host swap is filling up \u2691 Swap is filling up (>80%) - alert : HostSwapIsFillingUp expr : (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80 for : 5m labels : severity : warning annotations : summary : \"Host swap is filling up (instance {{ $labels.instance }})\" message : \"Swap is filling up (>80%)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host SystemD service crashed \u2691 SystemD service crashed - alert : HostSystemdServiceCrashed expr : node_systemd_unit_state{state=\"failed\"} == 1 for : 5m labels : severity : warning annotations : summary : \"Host SystemD service crashed (instance {{ $labels.instance }})\" message : \"SystemD service crashed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host physical component too hot \u2691 Physical hardware component too hot - alert : HostPhysicalComponentTooHot expr : node_hwmon_temp_celsius > 75 for : 5m labels : severity : warning annotations : summary : \"Host physical component too hot (instance {{ $labels.instance }})\" message : \"Physical hardware component too hot\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host node overtemperature alarm \u2691 Physical node temperature alarm triggered - alert : HostNodeOvertemperatureAlarm expr : node_hwmon_temp_alarm == 1 for : 5m labels : severity : critical annotations : summary : \"Host node overtemperature alarm (instance {{ $labels.instance }})\" message : \"Physical node temperature alarm triggered\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host RAID array got inactive \u2691 RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically. - alert : HostRaidArrayGotInactive expr : node_md_state{state=\"inactive\"} > 0 for : 5m labels : severity : critical annotations : summary : \"Host RAID array got inactive (instance {{ $labels.instance }})\" message : \"RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host RAID disk failure \u2691 At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap. - alert : HostRaidDiskFailure expr : node_md_disks{state=\"fail\"} > 0 for : 5m labels : severity : warning annotations : summary : \"Host RAID disk failure (instance {{ $labels.instance }})\" message : \"At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host kernel version deviations \u2691 Different kernel versions are running. - alert : HostKernelVersionDeviations expr : count(sum(label_replace(node_uname_info, \"kernel\", \"$1\", \"release\", \"([0-9]+.[0-9]+.[0-9]+).*\")) by (kernel)) > 1 for : 5m labels : severity : warning annotations : summary : \"Host kernel version deviations (instance {{ $labels.instance }})\" message : \"Different kernel versions are running\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host OOM kill detected \u2691 OOM kill detected - alert : HostOomKillDetected expr : increase(node_vmstat_oom_kill[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host OOM kill detected (instance {{ $labels.instance }})\" message : \"OOM kill detected\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host Network Receive Errors \u2691 {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last five minutes. - alert : HostNetworkReceiveErrors expr : increase(node_network_receive_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Receive Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} receive errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host Network Transmit Errors \u2691 {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last five minutes. - alert : HostNetworkTransmitErrors expr : increase(node_network_transmit_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Transmit Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} transmit errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" References \u2691 Git Prometheus node exporter guide Node exporter alerts", "title": "Node Exporter"}, {"location": "devops/prometheus/node_exporter/#install", "text": "To install in kubernetes nodes, use this chart . Elsewhere use this ansible role . If you use node exporter agents outside kubernetes, you need to configure a prometheus service discovery to scrap the information from them. To auto discover EC2 instances use the ec2_sd_config configuration. It can be added in the helm chart values.yaml under the key prometheus.prometheusSpec.additionalScrapeConfigs - job_name : node_exporter ec2_sd_configs : - region : us-east-1 port : 9100 refresh_interval : 1m relabel_configs : - source_labels : [ '__meta_ec2_tag_Name' , '__meta_ec2_private_ip' ] separator : ':' target_label : instance - source_labels : - __meta_ec2_instance_type target_label : instance_type The relabel_configs part will substitute the instance label of each target from {{ instance_ip }}:9100 to {{ instance_name }}:{{ instance_ip }} . If the worker nodes already have an IAM role with the ec2:DescribeInstances permission there is no need to specify the role_arn or access_keys and secret_key . If you have stopped instances, the node exporter will raise an alert because it won't be able to scrape the metrics from them. To only fetch data from running instances add a filter: ec2_sd_configs : - region : us-east-1 filters : - name : instance-state-name values : - running To monitor only the instances of a list of VPCs use this filter: ec2_sd_configs : - region : us-east-1 filters : - name : vpc-id values : - vpc-xxxxxxxxxxxxxxxxx - vpc-yyyyyyyyyyyyyyyyy By default, prometheus will try to scrape the private instance ip. To use the public one you need to relabel it with the following snippet: ec2_sd_configs : - region : us-east-1 relabel_configs : - source_labels : [ '__meta_ec2_public_ip' ] regex : ^(.*)$ target_label : __address__ replacement : ${1}:9100 I'm using the 11074 grafana dashboards for the blackbox exporter, which worked straight out of the box. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : node_exporter : # Ref: https://grafana.com/dashboards/11074 gnetId : 11074 revision : 4 datasource : Prometheus And install. helmfile diff helmfile apply", "title": "Install"}, {"location": "devops/prometheus/node_exporter/#node-exporter-size-analysis", "text": "Once the instance metrics are being ingested, we can do a periodic analysis to deduce which instances are undersized or oversized.", "title": "Node exporter size analysis"}, {"location": "devops/prometheus/node_exporter/#node-exporter-alerts", "text": "Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection.", "title": "Node exporter alerts"}, {"location": "devops/prometheus/node_exporter/#host-out-of-memory", "text": "Node memory is filling up ( < 10% left). - alert : HostOutOfMemory expr : node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of memory (instance {{ $labels.instance }})\" message : \"Node memory is filling up (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host out of memory"}, {"location": "devops/prometheus/node_exporter/#host-memory-under-memory-pressure", "text": "The node is under heavy memory pressure. High rate of major page faults. - alert : HostMemoryUnderMemoryPressure expr : rate(node_vmstat_pgmajfault[1m]) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host memory under memory pressure (instance {{ $labels.instance }})\" message : \"The node is under heavy memory pressure. High rate of major page faults.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host memory under memory pressure"}, {"location": "devops/prometheus/node_exporter/#host-unusual-network-throughput-in", "text": "Host network interfaces are probably receiving too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputIn expr : sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput in (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably receiving too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host unusual network throughput in"}, {"location": "devops/prometheus/node_exporter/#host-unusual-network-throughput-out", "text": "Host network interfaces are probably sending too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputOut expr : sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput out (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably sending too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host unusual network throughput out"}, {"location": "devops/prometheus/node_exporter/#host-unusual-disk-read-rate", "text": "Disk is probably reading too much data (> 50 MB/s) - alert : HostUnusualDiskReadRate expr : sum by (instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read rate (instance {{ $labels.instance }})\" message : \"Disk is probably reading too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host unusual disk read rate"}, {"location": "devops/prometheus/node_exporter/#host-unusual-disk-write-rate", "text": "Disk is probably writing too much data (> 50 MB/s) - alert : HostUnusualDiskWriteRate expr : sum by (instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write rate (instance {{ $labels.instance }})\" message : \"Disk is probably writing too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host unusual disk write rate"}, {"location": "devops/prometheus/node_exporter/#host-out-of-disk-space", "text": "Disk is worryingly almost full ( < 10% left ). - alert : HostOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 10 for : 5m labels : severity : critical annotations : summary : \"Host out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Disk is almost full ( < 20% left ) - alert : HostReachingOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 20 for : 5m labels : severity : warning annotations : summary : \"Host reaching out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 20% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host out of disk space"}, {"location": "devops/prometheus/node_exporter/#host-disk-will-fill-in-4-hours", "text": "Disk will fill in 4 hours at current write rate - alert : HostDiskWillFillIn4Hours expr : predict_linear(node_filesystem_free_bytes{fstype!~\"tmpfs\"}[1h], 4 * 3600) < 0 for : 5m labels : severity : critical annotations : summary : \"Host disk will fill in 4 hours (instance {{ $labels.instance }})\" message : \"Disk will fill in 4 hours at current write rate\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host disk will fill in 4 hours"}, {"location": "devops/prometheus/node_exporter/#host-out-of-inodes", "text": "Disk is almost running out of available inodes ( < 10% left ). - alert : HostOutOfInodes expr : node_filesystem_files_free{fstype!~\"tmpfs\"} / node_filesystem_files{fstype!~\"tmpfs\"} * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of inodes (instance {{ $labels.instance }})\" message : \"Disk is almost running out of available inodes (< 10% left)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host out of inodes"}, {"location": "devops/prometheus/node_exporter/#host-unusual-disk-read-latency", "text": "Disk latency is growing (read operations > 100ms). - alert : HostUnusualDiskReadLatency expr : rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (read operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host unusual disk read latency"}, {"location": "devops/prometheus/node_exporter/#host-unusual-disk-write-latency", "text": "Disk latency is growing (write operations > 100ms) - alert : HostUnusualDiskWriteLatency expr : rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (write operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host unusual disk write latency"}, {"location": "devops/prometheus/node_exporter/#host-high-cpu-load", "text": "CPU load is > 80% - alert : HostHighCpuLoad expr : 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 80 for : 5m labels : severity : warning annotations : summary : \"Host high CPU load (instance {{ $labels.instance }})\" message : \"CPU load is > 80%\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host high CPU load"}, {"location": "devops/prometheus/node_exporter/#host-context-switching", "text": "Context switching is growing on node (> 1000 / s) # 1000 context switches is an arbitrary number. # Alert threshold depends on nature of application. # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58 - alert : HostContextSwitching expr : (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode=\"idle\"})) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host context switching (instance {{ $labels.instance }})\" message : \"Context switching is growing on node (> 1000 / s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host context switching"}, {"location": "devops/prometheus/node_exporter/#host-swap-is-filling-up", "text": "Swap is filling up (>80%) - alert : HostSwapIsFillingUp expr : (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80 for : 5m labels : severity : warning annotations : summary : \"Host swap is filling up (instance {{ $labels.instance }})\" message : \"Swap is filling up (>80%)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host swap is filling up"}, {"location": "devops/prometheus/node_exporter/#host-systemd-service-crashed", "text": "SystemD service crashed - alert : HostSystemdServiceCrashed expr : node_systemd_unit_state{state=\"failed\"} == 1 for : 5m labels : severity : warning annotations : summary : \"Host SystemD service crashed (instance {{ $labels.instance }})\" message : \"SystemD service crashed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host SystemD service crashed"}, {"location": "devops/prometheus/node_exporter/#host-physical-component-too-hot", "text": "Physical hardware component too hot - alert : HostPhysicalComponentTooHot expr : node_hwmon_temp_celsius > 75 for : 5m labels : severity : warning annotations : summary : \"Host physical component too hot (instance {{ $labels.instance }})\" message : \"Physical hardware component too hot\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host physical component too hot"}, {"location": "devops/prometheus/node_exporter/#host-node-overtemperature-alarm", "text": "Physical node temperature alarm triggered - alert : HostNodeOvertemperatureAlarm expr : node_hwmon_temp_alarm == 1 for : 5m labels : severity : critical annotations : summary : \"Host node overtemperature alarm (instance {{ $labels.instance }})\" message : \"Physical node temperature alarm triggered\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host node overtemperature alarm"}, {"location": "devops/prometheus/node_exporter/#host-raid-array-got-inactive", "text": "RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically. - alert : HostRaidArrayGotInactive expr : node_md_state{state=\"inactive\"} > 0 for : 5m labels : severity : critical annotations : summary : \"Host RAID array got inactive (instance {{ $labels.instance }})\" message : \"RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host RAID array got inactive"}, {"location": "devops/prometheus/node_exporter/#host-raid-disk-failure", "text": "At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap. - alert : HostRaidDiskFailure expr : node_md_disks{state=\"fail\"} > 0 for : 5m labels : severity : warning annotations : summary : \"Host RAID disk failure (instance {{ $labels.instance }})\" message : \"At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host RAID disk failure"}, {"location": "devops/prometheus/node_exporter/#host-kernel-version-deviations", "text": "Different kernel versions are running. - alert : HostKernelVersionDeviations expr : count(sum(label_replace(node_uname_info, \"kernel\", \"$1\", \"release\", \"([0-9]+.[0-9]+.[0-9]+).*\")) by (kernel)) > 1 for : 5m labels : severity : warning annotations : summary : \"Host kernel version deviations (instance {{ $labels.instance }})\" message : \"Different kernel versions are running\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host kernel version deviations"}, {"location": "devops/prometheus/node_exporter/#host-oom-kill-detected", "text": "OOM kill detected - alert : HostOomKillDetected expr : increase(node_vmstat_oom_kill[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host OOM kill detected (instance {{ $labels.instance }})\" message : \"OOM kill detected\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host OOM kill detected"}, {"location": "devops/prometheus/node_exporter/#host-network-receive-errors", "text": "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last five minutes. - alert : HostNetworkReceiveErrors expr : increase(node_network_receive_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Receive Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} receive errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host Network Receive Errors"}, {"location": "devops/prometheus/node_exporter/#host-network-transmit-errors", "text": "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last five minutes. - alert : HostNetworkTransmitErrors expr : increase(node_network_transmit_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Transmit Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} transmit errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"", "title": "Host Network Transmit Errors"}, {"location": "devops/prometheus/node_exporter/#references", "text": "Git Prometheus node exporter guide Node exporter alerts", "title": "References"}, {"location": "devops/prometheus/prometheus/", "text": "Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy. A quick overview of Prometheus would be, as stated in the coreos article : At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets. The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications. There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented. Go to the Prometheus architecture post for more details. We are living a shift to the DevOps culture, containers and Kubernetes. So nowadays: Developers need to integrate app and business related metrics as an organic part of the infrastructure. So monitoring needs to be democratized, made more accessible and cover additional layers of the stack. Container based infrastructures are changing how we monitor the resources. Now we have a huge number of volatile software entities, services, virtual network addresses, exposed metrics that suddenly appear or vanish. Traditional monitoring tools are not designed to handle this. These reasons pushed Soundcloud to build a new monitoring system that had the following features Multi-dimensional data model : The model is based on key-value pairs, similar to how Kubernetes itself organizes infrastructure metadata using labels. It allows for flexible and accurate time series data, powering its Prometheus query language. Accessible format and protocols : Exposing prometheus metrics is a pretty straightforward task. Metrics are human readable, are in a self-explanatory format, and are published using a standard HTTP transport. You can check that the metrics are correctly exposed just using your web browser. Service discovery : The Prometheus server is in charge of periodically scraping the targets, so that applications and services don\u2019t need to worry about emitting data (metrics are pulled, not pushed). These Prometheus servers have several methods to auto-discover scrape targets, some of them can be configured to filter and match container metadata, making it an excellent fit for ephemeral Kubernetes workloads. Modular and highly available components : Metric collection, alerting, graphical visualization, etc, are performed by different composable services. All these services are designed to support redundancy and sharding. Pull based metrics : Most monitoring systems are pushing metrics to a centralized collection platform. Prometheus flips this model on it's head with the following advantages: No need to install custom software in the physical servers or containers. Doesn't require applications to use CPU cycles pushing metrics. Handles service failure/unavailability gracefully. If a target goes down, Prometheus can record it was unable to retrieve data. You can use the Pushgateway if pulling metrics is not feasible. Installation \u2691 There are several ways to install prometheus , but I'd recommend using the Kubernetes or Docker Prometheus operator . Exposing your metrics \u2691 Prometheus defines a very nice text-based format for its metrics: # HELP prometheus_engine_query_duration_seconds Query timings # TYPE prometheus_engine_query_duration_seconds summary prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.5\"} 7.0442e-05 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.9\"} 0.0084092 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.99\"} 0.389403939 The data is relatively human readable and we even have TYPE and HELP decorators to increase the readability. To expose application metrics to the Prometheus server, use one of the client libraries and follow the suggested naming and units conventions for metrics . Metric types \u2691 There are these metric types: Counter : A simple monotonically incrementing type; basically use this for situations where you want to know \u201chow many times has x happened\u201d. Gauge : A representation of a metric that can go both up and down. Think of a speedometer in a car, this type provides a snapshot of \u201cwhat is the current value of x now\u201d. Histogram : It represents observed metrics sharded into distinct buckets. Think of this as a mechanism to track \u201chow long something took\u201d or \u201chow big something was\u201d. Summary : Similar to a histogram, except the bins are converted into an aggregate immediately. Using labels \u2691 Prometheus metrics support the concept of Labels to provide extra dimensions to your data. By using Labels efficiently we can essentially provide more insights into our data whilst having to manage less actual metrics. Prometheus rules \u2691 Prometheus supports two types of rules which may be configured and then evaluated at regular intervals: recording rules and alerting rules. Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed. A simple example rules file would be: groups : - name : example rules : - record : job:http_inprogress_requests:sum expr : sum by (job) (http_inprogress_requests) Regarding naming and aggregation conventions , Recording rules should be of the general form level:metric:operations . level represents the aggregation level and labels of the rule output. metric is the metric name and should be unchanged other than stripping _total off counters when using rate() or irate() . operations is a list of operations (splitted by : ) that were applied to the metric, newest operation first. If you want to add extra labels to the calculated rule use the labels tag like the following example: groups : - name : example rules : - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : RAM problem : oversized Finding a metric \u2691 You can use {__name__=~\".*deploy.*\"} to find the metrics that have deploy somewhere in the name. Accessing Prometheus metrics through python \u2691 import requests response = requests . get ( \"http://127.0.0.1:9090/api/v1/query\" , params = { \"query\" : \"container_cpu_user_seconds_total\" }, ) Links \u2691 Homepage . Docs . Awesome Prometheus . Prometheus rules best practices and configuration . Diving deeper \u2691 Architecture Prometheus Operator Prometheus Installation Blackbox Exporter Node Exporter Prometheus Troubleshooting Introduction posts \u2691 Soundcloud introduction . Sysdig guide . Prometheus monitoring solutions comparison . ITNEXT overview Books \u2691 Prometheus Up & Running . Monitoring With Prometheus .", "title": "Prometheus"}, {"location": "devops/prometheus/prometheus/#installation", "text": "There are several ways to install prometheus , but I'd recommend using the Kubernetes or Docker Prometheus operator .", "title": "Installation"}, {"location": "devops/prometheus/prometheus/#exposing-your-metrics", "text": "Prometheus defines a very nice text-based format for its metrics: # HELP prometheus_engine_query_duration_seconds Query timings # TYPE prometheus_engine_query_duration_seconds summary prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.5\"} 7.0442e-05 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.9\"} 0.0084092 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.99\"} 0.389403939 The data is relatively human readable and we even have TYPE and HELP decorators to increase the readability. To expose application metrics to the Prometheus server, use one of the client libraries and follow the suggested naming and units conventions for metrics .", "title": "Exposing your metrics"}, {"location": "devops/prometheus/prometheus/#metric-types", "text": "There are these metric types: Counter : A simple monotonically incrementing type; basically use this for situations where you want to know \u201chow many times has x happened\u201d. Gauge : A representation of a metric that can go both up and down. Think of a speedometer in a car, this type provides a snapshot of \u201cwhat is the current value of x now\u201d. Histogram : It represents observed metrics sharded into distinct buckets. Think of this as a mechanism to track \u201chow long something took\u201d or \u201chow big something was\u201d. Summary : Similar to a histogram, except the bins are converted into an aggregate immediately.", "title": "Metric types"}, {"location": "devops/prometheus/prometheus/#using-labels", "text": "Prometheus metrics support the concept of Labels to provide extra dimensions to your data. By using Labels efficiently we can essentially provide more insights into our data whilst having to manage less actual metrics.", "title": "Using labels"}, {"location": "devops/prometheus/prometheus/#prometheus-rules", "text": "Prometheus supports two types of rules which may be configured and then evaluated at regular intervals: recording rules and alerting rules. Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed. A simple example rules file would be: groups : - name : example rules : - record : job:http_inprogress_requests:sum expr : sum by (job) (http_inprogress_requests) Regarding naming and aggregation conventions , Recording rules should be of the general form level:metric:operations . level represents the aggregation level and labels of the rule output. metric is the metric name and should be unchanged other than stripping _total off counters when using rate() or irate() . operations is a list of operations (splitted by : ) that were applied to the metric, newest operation first. If you want to add extra labels to the calculated rule use the labels tag like the following example: groups : - name : example rules : - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : RAM problem : oversized", "title": "Prometheus rules"}, {"location": "devops/prometheus/prometheus/#finding-a-metric", "text": "You can use {__name__=~\".*deploy.*\"} to find the metrics that have deploy somewhere in the name.", "title": "Finding a metric"}, {"location": "devops/prometheus/prometheus/#accessing-prometheus-metrics-through-python", "text": "import requests response = requests . get ( \"http://127.0.0.1:9090/api/v1/query\" , params = { \"query\" : \"container_cpu_user_seconds_total\" }, )", "title": "Accessing Prometheus metrics through python"}, {"location": "devops/prometheus/prometheus/#links", "text": "Homepage . Docs . Awesome Prometheus . Prometheus rules best practices and configuration .", "title": "Links"}, {"location": "devops/prometheus/prometheus/#diving-deeper", "text": "Architecture Prometheus Operator Prometheus Installation Blackbox Exporter Node Exporter Prometheus Troubleshooting", "title": "Diving deeper"}, {"location": "devops/prometheus/prometheus/#introduction-posts", "text": "Soundcloud introduction . Sysdig guide . Prometheus monitoring solutions comparison . ITNEXT overview", "title": "Introduction posts"}, {"location": "devops/prometheus/prometheus/#books", "text": "Prometheus Up & Running . Monitoring With Prometheus .", "title": "Books"}, {"location": "devops/prometheus/prometheus_architecture/", "text": "Prometheus Server \u2691 Prometheus servers have the following assignments: Periodically scrape and store metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. Run rules over scraped data to either record new timeseries from existing data or generate alerts. Discovers new targets from the Service discovery. Push alerts to the Alertmanager. Executes PromQL queries. Prometheus Targets \u2691 Prometheus Targets define how does prometheus extract the metrics from the different sources. If the services expose the metrics themselves such as Kubernetes , Prometheus fetch them directly. On the other cases, exporters are used. Exporters are modules that extract information and translate it into the Prometheus format, which the server can then ingest. There are several exporters available, for example: Hardware: Node/system HTTP: HAProxy , NGINX , Apache . APIs: Github , Docker Hub . Other monitoring systems: Cloudwatch . Databases: MySQL , Elasticsearch . Messaging systems: RabbitMQ , Kafka . Miscellaneous: Blackbox , JMX . Pushgateway \u2691 In case the nodes are not exposing an endpoint from which the Prometheus server can collect the metrics, the Prometheus ecosystem has a push gateway. This gateway API is useful for one-off jobs that run, capture the data, transform that data into the Prometheus data format and then push that data into the Prometheus server. Service discovery \u2691 Prometheus is designed to require very little configuration when first setup, and was designed from the ground up to run in dynamic environments such Kubernetes. It therefore performs automatic discovery of services running to try and make a \u201cbest guess\u201d of what it should be monitoring. Alertmanager \u2691 The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts. Data visualization and export \u2691 There are several ways to visualize or export data from Prometheus. Prometheus web UI \u2691 Prometheus comes with its own user interface that you can use to: Run PromQL queries. Check the Alertmanager rules. Check the configuration. Check the Targets. Check the service discovery. Grafana \u2691 Grafana is the best way to visually analyze the evolution of the metrics throughout time. API clients \u2691 Prometheus also exposes an API, so in case you are interested in writing your own clients, you can do that too. Links \u2691 Prometheus Overview Open Source for U architecture overview", "title": "Architecture"}, {"location": "devops/prometheus/prometheus_architecture/#prometheus-server", "text": "Prometheus servers have the following assignments: Periodically scrape and store metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. Run rules over scraped data to either record new timeseries from existing data or generate alerts. Discovers new targets from the Service discovery. Push alerts to the Alertmanager. Executes PromQL queries.", "title": "Prometheus Server"}, {"location": "devops/prometheus/prometheus_architecture/#prometheus-targets", "text": "Prometheus Targets define how does prometheus extract the metrics from the different sources. If the services expose the metrics themselves such as Kubernetes , Prometheus fetch them directly. On the other cases, exporters are used. Exporters are modules that extract information and translate it into the Prometheus format, which the server can then ingest. There are several exporters available, for example: Hardware: Node/system HTTP: HAProxy , NGINX , Apache . APIs: Github , Docker Hub . Other monitoring systems: Cloudwatch . Databases: MySQL , Elasticsearch . Messaging systems: RabbitMQ , Kafka . Miscellaneous: Blackbox , JMX .", "title": "Prometheus Targets"}, {"location": "devops/prometheus/prometheus_architecture/#pushgateway", "text": "In case the nodes are not exposing an endpoint from which the Prometheus server can collect the metrics, the Prometheus ecosystem has a push gateway. This gateway API is useful for one-off jobs that run, capture the data, transform that data into the Prometheus data format and then push that data into the Prometheus server.", "title": "Pushgateway"}, {"location": "devops/prometheus/prometheus_architecture/#service-discovery", "text": "Prometheus is designed to require very little configuration when first setup, and was designed from the ground up to run in dynamic environments such Kubernetes. It therefore performs automatic discovery of services running to try and make a \u201cbest guess\u201d of what it should be monitoring.", "title": "Service discovery"}, {"location": "devops/prometheus/prometheus_architecture/#alertmanager", "text": "The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.", "title": "Alertmanager"}, {"location": "devops/prometheus/prometheus_architecture/#data-visualization-and-export", "text": "There are several ways to visualize or export data from Prometheus.", "title": "Data visualization and export"}, {"location": "devops/prometheus/prometheus_architecture/#prometheus-web-ui", "text": "Prometheus comes with its own user interface that you can use to: Run PromQL queries. Check the Alertmanager rules. Check the configuration. Check the Targets. Check the service discovery.", "title": "Prometheus web UI"}, {"location": "devops/prometheus/prometheus_architecture/#grafana", "text": "Grafana is the best way to visually analyze the evolution of the metrics throughout time.", "title": "Grafana"}, {"location": "devops/prometheus/prometheus_architecture/#api-clients", "text": "Prometheus also exposes an API, so in case you are interested in writing your own clients, you can do that too.", "title": "API clients"}, {"location": "devops/prometheus/prometheus_architecture/#links", "text": "Prometheus Overview Open Source for U architecture overview", "title": "Links"}, {"location": "devops/prometheus/prometheus_installation/", "text": "Kubernetes \u2691 Helm 2 is not supported anymore. Later versions of the chart return an Error: apiVersion 'v2' is not valid. The value must be \"v1\" when using helm 2. Diving deeper , it seems that from 11.1.7 support for helm 2 was dropped. To install the operator we'll use helmfile to install the stable/prometheus-operator chart . Add the following lines to your helmfile.yaml . - name : prometheus-operator namespace : monitoring chart : stable/prometheus-operator values : - prometheus-operator/values.yaml Edit the chart values. mkdir prometheus-operator helm inspect values stable/prometheus-operator > prometheus-operator/values.yaml vi prometheus-operator/values.yaml I've implemented the following changes: If you are using a managed solution like EKS, the provider will hide kube-scheduler and kube-controller-manager so those metrics will fail. Therefore you need to disable: defaultRules.rules.kubeScheduler: false . kubeScheduler.enabled: false . kubeControllerManager.enabled: false . Enabled the ingress of alertmanager , grafana and prometheus . Set up the storage of alertmanager and prometheus with storageClassName: gp2 (for AWS). Change additionalPrometheusRules to additionalPrometheusRulesMap as the former is going to be deprecated in future releases. For private clusters, disable the admission webhook . prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false And install. helmfile diff helmfile apply Once it's installed you can check everything is working by accessing the grafana dashboard. First of all get the pod name (we'll asume you've used the monitoring namespace). kubectl get pods -n monitoring | grep grafana Then set up the proxies kubectl port-forward {{ grafana_pod }} -n monitoring 3000 :3000 kubectl port-forward -n monitoring \\ prometheus-prometheus-operator-prometheus-0 9090 :9090 To access grafana, go to http://localhost:3000 through your browser and at the top left, click on Home and select any dashboard. To access prometheus, go to http://localhost:9090 . If you're using the EKS helm chart, you'll need to manually edit the kube-proxy-config configmap until this bug has been solved. Edit the 127.0.0.1 value to 0.0.0.0 for the key metricsBindAddress in kubectl -n kube-system edit cm kube-proxy-config And restart the DaemonSet: kubectl rollout restart -n kube-system daemonset.apps/kube-proxy Upgrading notes \u2691 10.x -> 11.1.7 \u2691 If you have a private cluster in EKS, you are not able to use the admission webhooks as the nodes are not able to reach the master. Between those versions, something changed and you need to disable tls too with: prometheusOperator : tls : enabled : false admissionWebhooks : enabled : false If you run helmfile apply without that flag, the deployment gets tainted, and you may need to edit the deployment to remove the tls-secret volume. Docker \u2691 To install it outside Kubernetes, use the cloudalchemy ansible role for host installations or the prom/prometheus docker with the following command: /usr/bin/docker run --rm \\ --name prometheus \\ -v /data/prometheus:/etc/prometheus \\ prom/prometheus:latest \\ --storage.tsdb.retention.time = 30d \\ --config.file = /etc/prometheus/prometheus.yml \\ With a basic prometheus configuration: File: /data/prometheus/prometheus.yml ```yaml \u2691 http://prometheus.io/docs/operating/configuration/ \u2691 global: evaluation_interval: 1m scrape_interval: 1m scrape_timeout: 10s external_labels: environment: helm rule_files: - /etc/prometheus/rules/*.rules scrape_configs: - job_name: prometheus metrics_path: /metrics static_configs: - targets: - prometheus:9090 ``` And some basic rules: File: /data/prometheus/rules/ groups : - name : ansible managed alert rules rules : - alert : Watchdog annotations : description : |- This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the \"DeadMansSnitch\" integration in PagerDuty. summary : Ensure entire alerting pipeline is functional expr : vector(1) for : 10m labels : severity : warning - alert : InstanceDown annotations : description : '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.' summary : Instance {{ $labels.instance }} down expr : up == 0 for : 5m labels : severity : critical - alert : RebootRequired annotations : description : '{{ $labels.instance }} requires a reboot.' summary : Instance {{ $labels.instance }} - reboot required expr : node_reboot_required > 0 labels : severity : warning - alert : NodeFilesystemSpaceFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up. summary : Filesystem is predicted to run out of space within the next 24 hours. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 40 and predict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 24*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemSpaceFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up fast. summary : Filesystem is predicted to run out of space within the next 4 hours. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 20 and predict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 4*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemAlmostOutOfSpace annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left. summary : Filesystem has less than 5% space left. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 5 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemAlmostOutOfSpace annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left. summary : Filesystem has less than 3% space left. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 3 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemFilesFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up. summary : Filesystem is predicted to run out of inodes within the next 24 hours. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 40 and predict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 24*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemFilesFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast. summary : Filesystem is predicted to run out of inodes within the next 4 hours. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 20 and predict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 4*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemAlmostOutOfFiles annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left. summary : Filesystem has less than 5% inodes left. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 5 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemAlmostOutOfFiles annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left. summary : Filesystem has less than 3% inodes left. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 3 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeNetworkReceiveErrs annotations : description : '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.' summary : Network interface is reporting many receive errors. expr : |- increase(node_network_receive_errs_total[2m]) > 10 for : 1h labels : severity : warning - alert : NodeNetworkTransmitErrs annotations : description : '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.' summary : Network interface is reporting many transmit errors. expr : |- increase(node_network_transmit_errs_total[2m]) > 10 for : 1h labels : severity : warning - alert : NodeHighNumberConntrackEntriesUsed annotations : description : '{{ $value | humanizePercentage }} of conntrack entries are used' summary : Number of conntrack are getting close to the limit expr : |- (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75 labels : severity : warning - alert : NodeClockSkewDetected annotations : message : Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host. summary : Clock skew detected. expr : |- ( node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0 ) or ( node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0 ) for : 10m labels : severity : warning - alert : NodeClockNotSynchronising annotations : message : Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host. summary : Clock not synchronising. expr : |- min_over_time(node_timex_sync_status[5m]) == 0 for : 10m labels : severity : warning Next steps \u2691 Configure the alertmanager alerts . Configure the Blackbox Exporter . Configure the grafana dashboards. Issues \u2691 Error: apiVersion 'v2' is not valid. The value must be \"v1\" : Update the warning above and update the clusters.", "title": "Prometheus Install"}, {"location": "devops/prometheus/prometheus_installation/#kubernetes", "text": "Helm 2 is not supported anymore. Later versions of the chart return an Error: apiVersion 'v2' is not valid. The value must be \"v1\" when using helm 2. Diving deeper , it seems that from 11.1.7 support for helm 2 was dropped. To install the operator we'll use helmfile to install the stable/prometheus-operator chart . Add the following lines to your helmfile.yaml . - name : prometheus-operator namespace : monitoring chart : stable/prometheus-operator values : - prometheus-operator/values.yaml Edit the chart values. mkdir prometheus-operator helm inspect values stable/prometheus-operator > prometheus-operator/values.yaml vi prometheus-operator/values.yaml I've implemented the following changes: If you are using a managed solution like EKS, the provider will hide kube-scheduler and kube-controller-manager so those metrics will fail. Therefore you need to disable: defaultRules.rules.kubeScheduler: false . kubeScheduler.enabled: false . kubeControllerManager.enabled: false . Enabled the ingress of alertmanager , grafana and prometheus . Set up the storage of alertmanager and prometheus with storageClassName: gp2 (for AWS). Change additionalPrometheusRules to additionalPrometheusRulesMap as the former is going to be deprecated in future releases. For private clusters, disable the admission webhook . prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false And install. helmfile diff helmfile apply Once it's installed you can check everything is working by accessing the grafana dashboard. First of all get the pod name (we'll asume you've used the monitoring namespace). kubectl get pods -n monitoring | grep grafana Then set up the proxies kubectl port-forward {{ grafana_pod }} -n monitoring 3000 :3000 kubectl port-forward -n monitoring \\ prometheus-prometheus-operator-prometheus-0 9090 :9090 To access grafana, go to http://localhost:3000 through your browser and at the top left, click on Home and select any dashboard. To access prometheus, go to http://localhost:9090 . If you're using the EKS helm chart, you'll need to manually edit the kube-proxy-config configmap until this bug has been solved. Edit the 127.0.0.1 value to 0.0.0.0 for the key metricsBindAddress in kubectl -n kube-system edit cm kube-proxy-config And restart the DaemonSet: kubectl rollout restart -n kube-system daemonset.apps/kube-proxy", "title": "Kubernetes"}, {"location": "devops/prometheus/prometheus_installation/#upgrading-notes", "text": "", "title": "Upgrading notes"}, {"location": "devops/prometheus/prometheus_installation/#10x-1117", "text": "If you have a private cluster in EKS, you are not able to use the admission webhooks as the nodes are not able to reach the master. Between those versions, something changed and you need to disable tls too with: prometheusOperator : tls : enabled : false admissionWebhooks : enabled : false If you run helmfile apply without that flag, the deployment gets tainted, and you may need to edit the deployment to remove the tls-secret volume.", "title": "10.x -&gt; 11.1.7"}, {"location": "devops/prometheus/prometheus_installation/#docker", "text": "To install it outside Kubernetes, use the cloudalchemy ansible role for host installations or the prom/prometheus docker with the following command: /usr/bin/docker run --rm \\ --name prometheus \\ -v /data/prometheus:/etc/prometheus \\ prom/prometheus:latest \\ --storage.tsdb.retention.time = 30d \\ --config.file = /etc/prometheus/prometheus.yml \\ With a basic prometheus configuration: File: /data/prometheus/prometheus.yml", "title": "Docker"}, {"location": "devops/prometheus/prometheus_installation/#yaml", "text": "", "title": "```yaml"}, {"location": "devops/prometheus/prometheus_installation/#httpprometheusiodocsoperatingconfiguration", "text": "global: evaluation_interval: 1m scrape_interval: 1m scrape_timeout: 10s external_labels: environment: helm rule_files: - /etc/prometheus/rules/*.rules scrape_configs: - job_name: prometheus metrics_path: /metrics static_configs: - targets: - prometheus:9090 ``` And some basic rules: File: /data/prometheus/rules/ groups : - name : ansible managed alert rules rules : - alert : Watchdog annotations : description : |- This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the \"DeadMansSnitch\" integration in PagerDuty. summary : Ensure entire alerting pipeline is functional expr : vector(1) for : 10m labels : severity : warning - alert : InstanceDown annotations : description : '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.' summary : Instance {{ $labels.instance }} down expr : up == 0 for : 5m labels : severity : critical - alert : RebootRequired annotations : description : '{{ $labels.instance }} requires a reboot.' summary : Instance {{ $labels.instance }} - reboot required expr : node_reboot_required > 0 labels : severity : warning - alert : NodeFilesystemSpaceFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up. summary : Filesystem is predicted to run out of space within the next 24 hours. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 40 and predict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 24*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemSpaceFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up fast. summary : Filesystem is predicted to run out of space within the next 4 hours. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 20 and predict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 4*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemAlmostOutOfSpace annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left. summary : Filesystem has less than 5% space left. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 5 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemAlmostOutOfSpace annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left. summary : Filesystem has less than 3% space left. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 3 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemFilesFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up. summary : Filesystem is predicted to run out of inodes within the next 24 hours. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 40 and predict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 24*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemFilesFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast. summary : Filesystem is predicted to run out of inodes within the next 4 hours. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 20 and predict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 4*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemAlmostOutOfFiles annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left. summary : Filesystem has less than 5% inodes left. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 5 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemAlmostOutOfFiles annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left. summary : Filesystem has less than 3% inodes left. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 3 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeNetworkReceiveErrs annotations : description : '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.' summary : Network interface is reporting many receive errors. expr : |- increase(node_network_receive_errs_total[2m]) > 10 for : 1h labels : severity : warning - alert : NodeNetworkTransmitErrs annotations : description : '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.' summary : Network interface is reporting many transmit errors. expr : |- increase(node_network_transmit_errs_total[2m]) > 10 for : 1h labels : severity : warning - alert : NodeHighNumberConntrackEntriesUsed annotations : description : '{{ $value | humanizePercentage }} of conntrack entries are used' summary : Number of conntrack are getting close to the limit expr : |- (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75 labels : severity : warning - alert : NodeClockSkewDetected annotations : message : Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host. summary : Clock skew detected. expr : |- ( node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0 ) or ( node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0 ) for : 10m labels : severity : warning - alert : NodeClockNotSynchronising annotations : message : Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host. summary : Clock not synchronising. expr : |- min_over_time(node_timex_sync_status[5m]) == 0 for : 10m labels : severity : warning", "title": "http://prometheus.io/docs/operating/configuration/"}, {"location": "devops/prometheus/prometheus_installation/#next-steps", "text": "Configure the alertmanager alerts . Configure the Blackbox Exporter . Configure the grafana dashboards.", "title": "Next steps"}, {"location": "devops/prometheus/prometheus_installation/#issues", "text": "Error: apiVersion 'v2' is not valid. The value must be \"v1\" : Update the warning above and update the clusters.", "title": "Issues"}, {"location": "devops/prometheus/prometheus_operator/", "text": "Prometheus has it's own kubernetes operator , which makes it simple to install with helm, and enables users to configure and manage instances of Prometheus using simple declarative configuration that will, in response, create, configure, and manage Prometheus monitoring instances. Once installed the Prometheus Operator provides the following features: Create/Destroy : Easily launch a Prometheus instance for your Kubernetes namespace, a specific application or team easily using the Operator. Simple Configuration : Configure the fundamentals of Prometheus like versions, persistence, retention policies, and replicas from a native Kubernetes resource. Target Services via Labels : Automatically generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language. How it works \u2691 The core idea of the Operator is to decouple deployment of Prometheus instances from the configuration of which entities they are monitoring. The Operator acts on the following custom resource definitions (CRDs): Prometheus : Defines the desired Prometheus deployment. The Operator ensures at all times that a deployment matching the resource definition is running. This entails aspects like the data retention time, persistent volume claims, number of replicas, the Prometheus version, and Alertmanager instances to send alerts to. ServiceMonitor : Specifies how metrics can be retrieved from a set of services exposing them in a common way. The Operator configures the Prometheus instance to monitor all services covered by included ServiceMonitors and keeps this configuration synchronized with any changes happening in the cluster. PrometheusRule : Defines a desired Prometheus rule file, which can be loaded by a Prometheus instance containing Prometheus alerting and recording rules. Alertmanager : Defines a desired Alertmanager deployment. The Operator ensures at all times that a deployment matching the resource definition is running. Links \u2691 Homepage CoreOS Prometheus operator presentation Sysdig Prometheus operator guide part 3", "title": "Prometheus Operator"}, {"location": "devops/prometheus/prometheus_operator/#how-it-works", "text": "The core idea of the Operator is to decouple deployment of Prometheus instances from the configuration of which entities they are monitoring. The Operator acts on the following custom resource definitions (CRDs): Prometheus : Defines the desired Prometheus deployment. The Operator ensures at all times that a deployment matching the resource definition is running. This entails aspects like the data retention time, persistent volume claims, number of replicas, the Prometheus version, and Alertmanager instances to send alerts to. ServiceMonitor : Specifies how metrics can be retrieved from a set of services exposing them in a common way. The Operator configures the Prometheus instance to monitor all services covered by included ServiceMonitors and keeps this configuration synchronized with any changes happening in the cluster. PrometheusRule : Defines a desired Prometheus rule file, which can be loaded by a Prometheus instance containing Prometheus alerting and recording rules. Alertmanager : Defines a desired Alertmanager deployment. The Operator ensures at all times that a deployment matching the resource definition is running.", "title": "How it works"}, {"location": "devops/prometheus/prometheus_operator/#links", "text": "Homepage CoreOS Prometheus operator presentation Sysdig Prometheus operator guide part 3", "title": "Links"}, {"location": "devops/prometheus/prometheus_troubleshooting/", "text": "Solutions for problems with Prometheus. Service monitor not being recognized \u2691 Probably the service monitor labels aren't properly configured. Each prometheus monitors it's own targets, to see how you need to label your resources, describe the prometheus instance and search for Service Monitor Selector . kubectl get prometheus -n monitoring kubectl describe prometheus prometheus-operator-prometheus -n monitoring The last one will return something like: Service Monitor Selector : Match Labels : Release : prometheus-operator Which means you need to label your service monitors with release: prometheus-operator , be careful if you use Release: prometheus-operator it won't work. Failed calling webhook prometheusrulemutate.monitoring.coreos.com \u2691 Error: UPGRADE FAILED: failed to create resource: Internal error occurred: failed calling webhook \"prometheusrulemutate.monitoring.coreos.com\": Post https://prometheus-operator-operator.monitoring.svc:443/admission-prometheusrules/mutate?timeout=30s: no endpoints available for service \"prometheus-operator-operator\" Since version 0.30 of the operator, there is an admission webhook to prevent malformed rules from being added to the cluster. Without this validation, creating an invalid resource will cause Prometheus not to load it. If the container then restarts, it will go into a crashloop. For the webhook to work, the control plane needs to be able to access the webhook service. That means the addition of a firewall rule in EKS and GKE deployments. People have succeeded with GKE , but people struggling with EKS have decided to disable the webhook. To disable it, the following options have to be set: prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false If you have deployed your release with the webhook enabled, you also need to remove all the resources that match the following: kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io kubectl get MutatingWebhookConfiguration Before executing helmfile apply again.", "title": "Prometheus Troubleshooting"}, {"location": "devops/prometheus/prometheus_troubleshooting/#service-monitor-not-being-recognized", "text": "Probably the service monitor labels aren't properly configured. Each prometheus monitors it's own targets, to see how you need to label your resources, describe the prometheus instance and search for Service Monitor Selector . kubectl get prometheus -n monitoring kubectl describe prometheus prometheus-operator-prometheus -n monitoring The last one will return something like: Service Monitor Selector : Match Labels : Release : prometheus-operator Which means you need to label your service monitors with release: prometheus-operator , be careful if you use Release: prometheus-operator it won't work.", "title": "Service monitor not being recognized"}, {"location": "devops/prometheus/prometheus_troubleshooting/#failed-calling-webhook-prometheusrulemutatemonitoringcoreoscom", "text": "Error: UPGRADE FAILED: failed to create resource: Internal error occurred: failed calling webhook \"prometheusrulemutate.monitoring.coreos.com\": Post https://prometheus-operator-operator.monitoring.svc:443/admission-prometheusrules/mutate?timeout=30s: no endpoints available for service \"prometheus-operator-operator\" Since version 0.30 of the operator, there is an admission webhook to prevent malformed rules from being added to the cluster. Without this validation, creating an invalid resource will cause Prometheus not to load it. If the container then restarts, it will go into a crashloop. For the webhook to work, the control plane needs to be able to access the webhook service. That means the addition of a firewall rule in EKS and GKE deployments. People have succeeded with GKE , but people struggling with EKS have decided to disable the webhook. To disable it, the following options have to be set: prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false If you have deployed your release with the webhook enabled, you also need to remove all the resources that match the following: kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io kubectl get MutatingWebhookConfiguration Before executing helmfile apply again.", "title": "Failed calling webhook prometheusrulemutate.monitoring.coreos.com"}, {"location": "linux/brew/", "text": "Complementary package manager to manage the programs that aren't in the Debian repositories. Usage \u2691 TBC References \u2691 Homebrew formula for a Go app", "title": "brew"}, {"location": "linux/brew/#usage", "text": "TBC", "title": "Usage"}, {"location": "linux/brew/#references", "text": "Homebrew formula for a Go app", "title": "References"}, {"location": "linux/cookiecutter/", "text": "Cookiecutter is a command-line utility that creates projects from cookiecutters (project templates). Install \u2691 pip install cookiecutter Use \u2691 DEPRECATION: use cruft instead You may want to use cruft to generate your templates instead, as it will help you maintain the project with the template updates. Something that it's not easy with cookiecutter alone cookiecutter {{ path_or_url_to_cookiecutter_template }} User config \u2691 If you use Cookiecutter a lot, you\u2019ll find it useful to have a user config file. By default Cookiecutter tries to retrieve settings from a .cookiecutterrc file in your home directory. Example user config: default_context : full_name : \"Audrey Roy\" email : \"audreyr@example.com\" github_username : \"audreyr\" cookiecutters_dir : \"/home/audreyr/my-custom-cookiecutters-dir/\" replay_dir : \"/home/audreyr/my-custom-replay-dir/\" abbreviations : python : https://github.com/audreyr/cookiecutter-pypackage.git gh : https://github.com/{0}.git bb : https://bitbucket.org/{0} Possible settings are: default_context A list of key/value pairs that you want injected as context whenever you generate a project with Cookiecutter. These values are treated like the defaults in cookiecutter.json , upon generation of any project. cookiecutters_dir Directory where your cookiecutters are cloned to when you use Cookiecutter with a repo argument. replay_dir Directory where Cookiecutter dumps context data to, which you can fetch later on when using the replay feature. abbreviations A list of abbreviations for cookiecutters. Abbreviations can be simple aliases for a repo name, or can be used as a prefix, in the form abbr:suffix . Any suffix will be inserted into the expansion in place of the text {0} , using standard Python string formatting. With the above aliases, you could use the cookiecutter-pypackage template simply by saying cookiecutter python . Write your own cookietemplates \u2691 Create files or directories with conditions \u2691 For files use a filename like '{{ \".vault_pass.sh\" if cookiecutter.vault_pass_entry != \"None\" else \"\" }}' . For directories I haven't yet found a nice way to do it (as the above will fail), check the issue or the hooks documentation for more information. File: post_gen_project.py import os import sys REMOVE_PATHS = [ '{ % i f cookiecutter.packaging != \"pip\" %} requirements.txt { % e ndif %}' , '{ % i f cookiecutter.packaging != \"poetry\" %} poetry.lock { % e ndif %}' , ] for path in REMOVE_PATHS : path = path . strip () if path and os . path . exists ( path ): if os . path . isdir ( path ): os . rmdir ( path ) else : os . unlink ( path ) Add some text to a file if a condition is met \u2691 Use jinja2 conditionals. Note the - at the end of the conditional opening, play with {%- ... -%} and {% ... %} for different results on line appending. { % if cookiecutter.install_docker == 'yes' -% } - src : git+ssh://mywebpage.org/ansible-roles/docker.git version : 1.0.3 { % - else -% } - src : git+ssh://mywebpage.org/ansible-roles/other-role.git version : 1.0.2 { % - endif % } Initialize git repository on the created cookiecutter \u2691 Added the following to the post generation hooks. File: hooks/post_gen_project.py import subprocess subprocess . call ([ 'git' , 'init' ]) subprocess . call ([ 'git' , 'add' , '*' ]) subprocess . call ([ 'git' , 'commit' , '-m' , 'Initial commit' ]) Prevent cookiecutter from processing some files \u2691 By default cookiecutter will try to process every file as a Jinja template. This behaviour produces wrong results if you have Jinja templates that are meant to be taken as literal. Starting with cookiecutter 1.1, you can tell cookiecutter to only copy some files without interpreting them as Jinja templates . Add a _copy_without_render key in the cookiecutter config file ( cookiecutter.json ). It takes a list of regular expressions. If a filename matches the regular expressions it will be copied and not processed as a Jinja template. { \"project_slug\" : \"sample\" , \"_copy_without_render\" : [ \"*.js\" , \"not_rendered_dir/*\" , \"rendered_dir/not_rendered_file.ini\" ] } Prevent additional whitespaces when jinja condition is not met. \u2691 Jinja2 has a whitespace control that can be used to manage the whitelines existent between the Jinja blocks. The problem comes when a condition is not met in an if block, in that case, Jinja adds a whitespace which will break most linters. This is the solution I've found out that works as expected. ### Multienvironment This playbook has support for the following environments: {% if cookiecutter.production_environment == \"True\" -%} * Production {% endif %} {%- if cookiecutter.staging_environment == \"True\" -%} * Staging {% endif %} {%- if cookiecutter.development_environment == \"True\" -%} * Development {% endif %} ### Tags Testing your own cookiecutter templates \u2691 The pytest-cookies plugin comes with a cookies fixture which is a wrapper for the cookiecutter API for generating projects. It helps you verify that your template is working as expected and takes care of cleaning up after running the tests. Install \u2691 pip install pytest-cookies Usage \u2691 @pytest.fixture def context(): return { \"playbook_name\": \"My Test Playbook\", } The cookies.bake() method generates a new project from your template based on the default values specified in cookiecutter.json: def test_bake_project ( cookies ): result = cookies . bake ( extra_context = { 'repo_name' : 'hello world' }) assert result . exit_code == 0 assert result . exception is None assert result . project . basename == 'hello world' assert result . project . isdir () It accepts the extra_context keyword argument that is passed to cookiecutter. The given dictionary will override the default values of the template context, allowing you to test arbitrary user input data. The cookiecutter-django has a nice test file using this fixture. Mocking the contents of the cookiecutter hooks \u2691 Sometimes it's interesting to add interactions with external services in the cookiecutter hooks, for example to activate a CI pipeline. If you want to test the cookiecutter template you need to mock those external interactions. But it's difficult to mock the contents of the hooks because their contents aren't run by the cookies.bake() code. Instead it delegates in cookiecutter to run them, which opens a subprocess to run them , so the mocks don't work. The alternative is setting an environmental variable in your tests to skip those steps: File: tests/conftest.py import os os . environ [ \"COOKIECUTTER_TESTING\" ] = \"true\" File: hooks/pre_gen_project.py def main (): # ... pre_hook content ... if __name__ == \"__main__\" : if os . environ . get ( \"COOKIECUTTER_TESTING\" ) != \"true\" : main () If you want to test the content of main , you can now mock each of the external interactions. But you'll face the problem that these files are jinja2 templates of python files, so it's tricky to test them, due to syntax errors. Debug failing template generation \u2691 Sometimes the generation of the templates will fail in the tests, I've found that the easier way to debug why is to inspect the result object of the result = cookies.bake() statement with pdb. It has an exception method with lineno argument and source . With that information I've been able to locate the failing line. It also has a filename attribute but it doesn't seem to work for me. References \u2691 Git Docs", "title": "cookiecutter"}, {"location": "linux/cookiecutter/#install", "text": "pip install cookiecutter", "title": "Install"}, {"location": "linux/cookiecutter/#use", "text": "DEPRECATION: use cruft instead You may want to use cruft to generate your templates instead, as it will help you maintain the project with the template updates. Something that it's not easy with cookiecutter alone cookiecutter {{ path_or_url_to_cookiecutter_template }}", "title": "Use"}, {"location": "linux/cookiecutter/#user-config", "text": "If you use Cookiecutter a lot, you\u2019ll find it useful to have a user config file. By default Cookiecutter tries to retrieve settings from a .cookiecutterrc file in your home directory. Example user config: default_context : full_name : \"Audrey Roy\" email : \"audreyr@example.com\" github_username : \"audreyr\" cookiecutters_dir : \"/home/audreyr/my-custom-cookiecutters-dir/\" replay_dir : \"/home/audreyr/my-custom-replay-dir/\" abbreviations : python : https://github.com/audreyr/cookiecutter-pypackage.git gh : https://github.com/{0}.git bb : https://bitbucket.org/{0} Possible settings are: default_context A list of key/value pairs that you want injected as context whenever you generate a project with Cookiecutter. These values are treated like the defaults in cookiecutter.json , upon generation of any project. cookiecutters_dir Directory where your cookiecutters are cloned to when you use Cookiecutter with a repo argument. replay_dir Directory where Cookiecutter dumps context data to, which you can fetch later on when using the replay feature. abbreviations A list of abbreviations for cookiecutters. Abbreviations can be simple aliases for a repo name, or can be used as a prefix, in the form abbr:suffix . Any suffix will be inserted into the expansion in place of the text {0} , using standard Python string formatting. With the above aliases, you could use the cookiecutter-pypackage template simply by saying cookiecutter python .", "title": "User config"}, {"location": "linux/cookiecutter/#write-your-own-cookietemplates", "text": "", "title": "Write your own cookietemplates"}, {"location": "linux/cookiecutter/#create-files-or-directories-with-conditions", "text": "For files use a filename like '{{ \".vault_pass.sh\" if cookiecutter.vault_pass_entry != \"None\" else \"\" }}' . For directories I haven't yet found a nice way to do it (as the above will fail), check the issue or the hooks documentation for more information. File: post_gen_project.py import os import sys REMOVE_PATHS = [ '{ % i f cookiecutter.packaging != \"pip\" %} requirements.txt { % e ndif %}' , '{ % i f cookiecutter.packaging != \"poetry\" %} poetry.lock { % e ndif %}' , ] for path in REMOVE_PATHS : path = path . strip () if path and os . path . exists ( path ): if os . path . isdir ( path ): os . rmdir ( path ) else : os . unlink ( path )", "title": "Create files or directories with conditions"}, {"location": "linux/cookiecutter/#add-some-text-to-a-file-if-a-condition-is-met", "text": "Use jinja2 conditionals. Note the - at the end of the conditional opening, play with {%- ... -%} and {% ... %} for different results on line appending. { % if cookiecutter.install_docker == 'yes' -% } - src : git+ssh://mywebpage.org/ansible-roles/docker.git version : 1.0.3 { % - else -% } - src : git+ssh://mywebpage.org/ansible-roles/other-role.git version : 1.0.2 { % - endif % }", "title": "Add some text to a file if a condition is met"}, {"location": "linux/cookiecutter/#initialize-git-repository-on-the-created-cookiecutter", "text": "Added the following to the post generation hooks. File: hooks/post_gen_project.py import subprocess subprocess . call ([ 'git' , 'init' ]) subprocess . call ([ 'git' , 'add' , '*' ]) subprocess . call ([ 'git' , 'commit' , '-m' , 'Initial commit' ])", "title": "Initialize git repository on the created cookiecutter"}, {"location": "linux/cookiecutter/#prevent-cookiecutter-from-processing-some-files", "text": "By default cookiecutter will try to process every file as a Jinja template. This behaviour produces wrong results if you have Jinja templates that are meant to be taken as literal. Starting with cookiecutter 1.1, you can tell cookiecutter to only copy some files without interpreting them as Jinja templates . Add a _copy_without_render key in the cookiecutter config file ( cookiecutter.json ). It takes a list of regular expressions. If a filename matches the regular expressions it will be copied and not processed as a Jinja template. { \"project_slug\" : \"sample\" , \"_copy_without_render\" : [ \"*.js\" , \"not_rendered_dir/*\" , \"rendered_dir/not_rendered_file.ini\" ] }", "title": "Prevent cookiecutter from processing some files"}, {"location": "linux/cookiecutter/#prevent-additional-whitespaces-when-jinja-condition-is-not-met", "text": "Jinja2 has a whitespace control that can be used to manage the whitelines existent between the Jinja blocks. The problem comes when a condition is not met in an if block, in that case, Jinja adds a whitespace which will break most linters. This is the solution I've found out that works as expected. ### Multienvironment This playbook has support for the following environments: {% if cookiecutter.production_environment == \"True\" -%} * Production {% endif %} {%- if cookiecutter.staging_environment == \"True\" -%} * Staging {% endif %} {%- if cookiecutter.development_environment == \"True\" -%} * Development {% endif %} ### Tags", "title": "Prevent additional whitespaces when jinja condition is not met."}, {"location": "linux/cookiecutter/#testing-your-own-cookiecutter-templates", "text": "The pytest-cookies plugin comes with a cookies fixture which is a wrapper for the cookiecutter API for generating projects. It helps you verify that your template is working as expected and takes care of cleaning up after running the tests.", "title": "Testing your own cookiecutter templates"}, {"location": "linux/cookiecutter/#install_1", "text": "pip install pytest-cookies", "title": "Install"}, {"location": "linux/cookiecutter/#usage", "text": "@pytest.fixture def context(): return { \"playbook_name\": \"My Test Playbook\", } The cookies.bake() method generates a new project from your template based on the default values specified in cookiecutter.json: def test_bake_project ( cookies ): result = cookies . bake ( extra_context = { 'repo_name' : 'hello world' }) assert result . exit_code == 0 assert result . exception is None assert result . project . basename == 'hello world' assert result . project . isdir () It accepts the extra_context keyword argument that is passed to cookiecutter. The given dictionary will override the default values of the template context, allowing you to test arbitrary user input data. The cookiecutter-django has a nice test file using this fixture.", "title": "Usage"}, {"location": "linux/cookiecutter/#mocking-the-contents-of-the-cookiecutter-hooks", "text": "Sometimes it's interesting to add interactions with external services in the cookiecutter hooks, for example to activate a CI pipeline. If you want to test the cookiecutter template you need to mock those external interactions. But it's difficult to mock the contents of the hooks because their contents aren't run by the cookies.bake() code. Instead it delegates in cookiecutter to run them, which opens a subprocess to run them , so the mocks don't work. The alternative is setting an environmental variable in your tests to skip those steps: File: tests/conftest.py import os os . environ [ \"COOKIECUTTER_TESTING\" ] = \"true\" File: hooks/pre_gen_project.py def main (): # ... pre_hook content ... if __name__ == \"__main__\" : if os . environ . get ( \"COOKIECUTTER_TESTING\" ) != \"true\" : main () If you want to test the content of main , you can now mock each of the external interactions. But you'll face the problem that these files are jinja2 templates of python files, so it's tricky to test them, due to syntax errors.", "title": "Mocking the contents of the cookiecutter hooks"}, {"location": "linux/cookiecutter/#debug-failing-template-generation", "text": "Sometimes the generation of the templates will fail in the tests, I've found that the easier way to debug why is to inspect the result object of the result = cookies.bake() statement with pdb. It has an exception method with lineno argument and source . With that information I've been able to locate the failing line. It also has a filename attribute but it doesn't seem to work for me.", "title": "Debug failing template generation"}, {"location": "linux/cookiecutter/#references", "text": "Git Docs", "title": "References"}, {"location": "linux/cruft/", "text": "cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates. Many project template utilities exist that automate the copying and pasting of code to create new projects. This seems great! However, once created, most leave you with that copy-and-pasted code to manage through the life of your project. Key Features \u2691 Cookiecutter Compatible cruft utilizes Cookiecutter as its template expansion engine. Meaning it retains full compatibility with all existing Cookiecutter templates. Template Validation cruft can quickly validate whether or not a project is using the latest version of a template using cruft check . This check can easily be added to CI pipelines to ensure your projects stay in-sync. Automatic Template Updates cruft automates the process of updating code to match the latest version of a template, making it easy to utilize template improvements across many projects. Installation \u2691 pip install cruft Usage \u2691 Creating a New Project \u2691 To create a new project using cruft run cruft create PROJECT_URL from the command line. cruft will then ask you any necessary questions to create your new project. It will use your answers to expand the provided template, and then return the directory it placed the expanded project. Behind the scenes, cruft uses Cookiecutter to do the project expansion. The only difference in the resulting output is a .cruft.json file that contains the git hash of the template used as well as the parameters specified. Updating a Project \u2691 To update an existing project, that was created using cruft, run cruft update in the root of the project. If there are any updates, cruft will have you review them before applying. If you accept the changes cruft will apply them to your project and update the .cruft.json file for you. Sometimes certain files just aren't good fits for updating. Such as test cases or __init__ files. You can tell cruft to always skip updating these files on a project by project basis by added them to a skip section within your .cruft.json file: { \"template\" : \"https://github.com/timothycrosley/cookiecutter-python\" , \"commit\" : \"8a65a360d51250221193ed0ec5ed292e72b32b0b\" , \"skip\" : [ \"cruft/__init__.py\" , \"tests\" ], ... } Or, if you have toml installed, you can add skip files directly to a tool.cruft section of your pyproject.toml file: [tool.cruft] skip = [\"cruft/__init__.py\", \"tests\"] Checking a Project \u2691 Checking to see if a project is missing a template update is as easy as running cruft check . If the project is out-of-date an error and exit code 1 will be returned. cruft check can be added to CI pipelines to ensure projects don't unintentionally drift. Linking an Existing Project \u2691 Have an existing project that you created from a template in the past using Cookiecutter directly? You can link it to the template that was used to create it using: cruft link TEMPLATE_REPOSITORY . You can then specify the last commit of the template the project has been updated to be consistent with, or accept the default of using the latest commit from the template. Compute the diff \u2691 With time, your boilerplate may end up being very different from the actual cookiecutter template. Cruft allows you to quickly see what changed in your local project compared to the template. It is as easy as running cruft diff . If any local file differs from the template, the diff will appear in your terminal in a similar fashion to git diff . The cruft diff command optionally accepts an --exit-code flag that will make cruft exit with a non-0 code should any diff is found. You can combine this flag with the skip section of your .cruft.json to make stricter CI checks that ensures any improvement to the template is always submitted upstream. Issues \u2691 Save config in the pyproject.toml : Update the template once it's supported. Error: Unable to interpret changes between current project and cookiecutter template as unicode. \u2691 Typically a result of hidden binary files in project folder. Maybe you have a hook that initializes the .git directory. Since 2.10.0 you can add a skip category inside the .cruft.json , so that it doesn't check that directory: { \"template\" : \"xxx\" , \"commit\" : \"xxx\" , \"checkout\" : null , \"context\" : { \"cookiecutter\" : { ... } }, \"directory\" : null , \"skip\" : [ \".git\" ] } References \u2691 Docs Git Issues", "title": "cruft"}, {"location": "linux/cruft/#key-features", "text": "Cookiecutter Compatible cruft utilizes Cookiecutter as its template expansion engine. Meaning it retains full compatibility with all existing Cookiecutter templates. Template Validation cruft can quickly validate whether or not a project is using the latest version of a template using cruft check . This check can easily be added to CI pipelines to ensure your projects stay in-sync. Automatic Template Updates cruft automates the process of updating code to match the latest version of a template, making it easy to utilize template improvements across many projects.", "title": "Key Features"}, {"location": "linux/cruft/#installation", "text": "pip install cruft", "title": "Installation"}, {"location": "linux/cruft/#usage", "text": "", "title": "Usage"}, {"location": "linux/cruft/#creating-a-new-project", "text": "To create a new project using cruft run cruft create PROJECT_URL from the command line. cruft will then ask you any necessary questions to create your new project. It will use your answers to expand the provided template, and then return the directory it placed the expanded project. Behind the scenes, cruft uses Cookiecutter to do the project expansion. The only difference in the resulting output is a .cruft.json file that contains the git hash of the template used as well as the parameters specified.", "title": "Creating a New Project"}, {"location": "linux/cruft/#updating-a-project", "text": "To update an existing project, that was created using cruft, run cruft update in the root of the project. If there are any updates, cruft will have you review them before applying. If you accept the changes cruft will apply them to your project and update the .cruft.json file for you. Sometimes certain files just aren't good fits for updating. Such as test cases or __init__ files. You can tell cruft to always skip updating these files on a project by project basis by added them to a skip section within your .cruft.json file: { \"template\" : \"https://github.com/timothycrosley/cookiecutter-python\" , \"commit\" : \"8a65a360d51250221193ed0ec5ed292e72b32b0b\" , \"skip\" : [ \"cruft/__init__.py\" , \"tests\" ], ... } Or, if you have toml installed, you can add skip files directly to a tool.cruft section of your pyproject.toml file: [tool.cruft] skip = [\"cruft/__init__.py\", \"tests\"]", "title": "Updating a Project"}, {"location": "linux/cruft/#checking-a-project", "text": "Checking to see if a project is missing a template update is as easy as running cruft check . If the project is out-of-date an error and exit code 1 will be returned. cruft check can be added to CI pipelines to ensure projects don't unintentionally drift.", "title": "Checking a Project"}, {"location": "linux/cruft/#linking-an-existing-project", "text": "Have an existing project that you created from a template in the past using Cookiecutter directly? You can link it to the template that was used to create it using: cruft link TEMPLATE_REPOSITORY . You can then specify the last commit of the template the project has been updated to be consistent with, or accept the default of using the latest commit from the template.", "title": "Linking an Existing Project"}, {"location": "linux/cruft/#compute-the-diff", "text": "With time, your boilerplate may end up being very different from the actual cookiecutter template. Cruft allows you to quickly see what changed in your local project compared to the template. It is as easy as running cruft diff . If any local file differs from the template, the diff will appear in your terminal in a similar fashion to git diff . The cruft diff command optionally accepts an --exit-code flag that will make cruft exit with a non-0 code should any diff is found. You can combine this flag with the skip section of your .cruft.json to make stricter CI checks that ensures any improvement to the template is always submitted upstream.", "title": "Compute the diff"}, {"location": "linux/cruft/#issues", "text": "Save config in the pyproject.toml : Update the template once it's supported.", "title": "Issues"}, {"location": "linux/cruft/#error-unable-to-interpret-changes-between-current-project-and-cookiecutter-template-as-unicode", "text": "Typically a result of hidden binary files in project folder. Maybe you have a hook that initializes the .git directory. Since 2.10.0 you can add a skip category inside the .cruft.json , so that it doesn't check that directory: { \"template\" : \"xxx\" , \"commit\" : \"xxx\" , \"checkout\" : null , \"context\" : { \"cookiecutter\" : { ... } }, \"directory\" : null , \"skip\" : [ \".git\" ] }", "title": "Error: Unable to interpret changes between current project and cookiecutter template as unicode."}, {"location": "linux/cruft/#references", "text": "Docs Git Issues", "title": "References"}, {"location": "linux/elasticsearch/", "text": "Searching documents \u2691 We use HTTP requests to talk to ElasticSearch. A HTTP request is made up of several components such as the URL to make the request to, HTTP verbs (GET, POST etc) and headers. In order to succinctly and consistently describe HTTP requests the ElasticSearch documentation uses cURL command line syntax. This is also the standard practice to describe requests made to ElasticSearch within the user community. Get all documents \u2691 An example HTTP request using CURL syntax looks like this: curl \\ -H 'Content-Type: application/json' \\ -XPOST \"https://localhost:9200/_search\" \\ -d ' { \"query\": { \"match_all\": {} }}' Get documents that match a string \u2691 curl \\ -H 'Content-Type: application/json' \\ -XPOST \"https://localhost:9200/_search\" \\ -d ' { \"query\": { \"query_string\": {\"query\": \"test company\"} }}' Backup \u2691 It's better to use the curator tool Create snapshot \u2691 curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} ?wait_for_completion = true Create snapshot of selected indices \u2691 curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} ?wait_for_completion = true curl -XPUT 'localhost:9200/_snapshot/my_backup/snapshot_1?pretty' -H 'Content-Type: application/json' -d ' { \"indices\": \"index_1,index_2\", \"ignore_unavailable\": true, \"include_global_state\": false } ' List all backups \u2691 Check for my-snapshot-repo curl {{ url }} /_snapshot/ {{ backup_path }} /*?pretty Restore backup \u2691 First you need to close the selected indices curl -X POST {{ url }} / {{ indice_name }} /_close Then restore curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} /_restore?wait_for_completion = true If you want to restore only one index, use: curl -X POST \"{{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}/_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"{{ index_to_restore }}\", }' Delete snapshot \u2691 curl -XDELETE {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} Delete snapshot repository \u2691 curl -XDELETE {{ url }} /_snapshot/ {{ backup_path }} Delete snapshots older than X \u2691 !!! note \"File: curator.yml\" ```yaml client: hosts: - 'a data node' port: 9200 url_prefix: use_ssl: False certificate: client_cert: client_key: ssl_no_validate: False http_auth: timeout: 30 master_only: False logging: loglevel: INFO logfile: D:\\CuratorLogs\\logs.txt logformat: default blacklist: ['elasticsearch', 'urllib3'] ``` File: delete_old_snapshots.yml yaml actions: 1: action: delete_snapshots description: >- Delete snapshots from the selected repository older than 100 days (based on creation_date), for everything but 'citydirectory-' prefixed snapshots. options: repository: 'dcs-elastic-snapshot' disable_action: False filters: - filtertype: pattern kind: prefix value: citydirectory- exclude: True - filtertype: age source: creation_date direction: older unit: days unit_count: 100 Information gathering \u2691 Get status of cluster \u2691 curl {{ url }} /_cluster/health?pretty curl {{ url }} /_cat/nodes?v curl {{ url }} /_cat/indices?v curl {{ url }} /_cat/shards If you've got red status, use the following command to choose the first unassigned shard that it finds and explains why it cannot be allocated to a node. curl {{ url }} /_cluster/allocation/explain?v Get settings \u2691 curl {{ url }} /_settings Get space left \u2691 curl {{ url }} /_nodes/stats/fs?pretty List plugins \u2691 curl {{ url }} /_nodes/plugins?pretty Upload \u2691 Single data upload \u2691 curl -XPOST '{{ url }}/{{ path_to_table }}' -d '{{ json_input }}' where json_input can be { \"field\" : \"value\" } Bulk upload of data \u2691 curl -H 'Content-Type: application/x-ndjson' -XPOST \\ '{{ url }}/{{ path_to_table }}/_bulk?pretty' --data-binary @ {{ json_file }} Delete \u2691 Delete data \u2691 curl -XDELETE {{ url }} / {{ path_to_ddbb }} Reindex an index \u2691 If you encountered errors while reindexing source_index to destination_index it can be because the cluster hit a timeout on the scroll locks. As a work around, you can increase the timeout period to a reasonable value and then reindex. The default AWS values are search context of 5 minutes, socket timeout of 30 seconds, and batch size of 1,000. First clear the cache of the index with: curl -X POST https://elastic.url/destination_index/_cache/clear If the index is big, they suggest to disable replicas in your destination index by setting number_of_replicas to 0 and re-enable them once the reindex process is complete. To get the current state use: curl https://elastic.url/destination_index/_settings Then disable the replicas with: curl -X PUT \\ https://elastic.url/destination_index \\ -H 'Content-Type: application/json' \\ -d ' { \"settings\" : { \"refresh_interval\" : -1, \"number_of_replicas\" : 0 }} Now you can reindex the index with: curl -X POST \\ https://elastic.url/_reindex?wait_for_completion = false \\& timeout = 10m \\& scroll = 10h \\& pretty = true \\ -H 'Content-Type: application/json' \\ -d '{\"source\": { \"remote\": { \"host\": \"https://elastic.url:443\", \"socket_timeout\": \"60m\" }, \"index\": \"source_index\" }, \"dest\": {\"index\": \"destination_index\"}}' And check the evolution of the task with: curl 'https://elastic.url/_tasks?detailed=true&actions=*reindex&group_by=parents&pretty=true' The output is quite verbose, so I use vimdiff to see the differences between instant states. If you see there are no tasks running, check the indices status to see if the reindex ended well. curl https://elastic.url/_cat/indices After the reindex process is complete, you can reset your desired replica count and remove the refresh interval setting. KNN \u2691 KNN sizing \u2691 Typically, in an Elasticsearch cluster, a certain portion of RAM is set aside for the JVM heap. The k-NN plugin allocates graphs to a portion of the remaining RAM. This portion\u2019s size is determined by the circuit_breaker_limit cluster setting. By default, the circuit breaker limit is set at 50%. The memory required for graphs is estimated to be `1.1 * (4 * dimension 8 * M)` bytes/vector. To get the dimension and m use the /index elasticsearch endpoint. To get the number of vectors, use /index/_count . The number of vectors is the same as the number of documents. As an example, assume that we have 1 Million vectors with a dimension of 256 and M of 16, and the memory required can be estimated as: 1.1 * (4 *256 + 8 * 16) * 1,000,000 ~= 1.26 GB !!! note \"Remember that having a replica will double the total number of vectors.\" I've seen some queries work with indices that required 120% of the available memory for the KNN. A good way to see if it fits, is warming up the knn vectors . If the process returns a timeout, you probably don't have enough memory. KNN warmup \u2691 The Hierarchical Navigable Small World (HNSW) graphs that are used to perform an approximate k-Nearest Neighbor (k-NN) search are stored as .hnsw files with other Apache Lucene segment files. In order for you to perform a search on these graphs using the k-NN plugin, these files need to be loaded into native memory. If the plugin has not loaded the graphs into native memory, it loads them when it receives a search request. This loading time can cause high latency during initial queries. To avoid this situation, users often run random queries during a warmup period. After this warmup period, the graphs are loaded into native memory and their production workloads can begin. This loading process is indirect and requires extra effort. As an alternative, you can avoid this latency issue by running the k-NN plugin warmup API operation on whatever indices you\u2019re interested in searching. This operation loads all the graphs for all of the shards (primaries and replicas) of all the indices specified in the request into native memory. After the process finishes, you can start searching against the indices with no initial latency penalties. The warmup API operation is idempotent, so if a segment\u2019s graphs are already loaded into memory, this operation has no impact on those graphs. It only loads graphs that aren\u2019t currently in memory. This request performs a warmup on three indices: GET /_opendistro/_knn/warmup/index1,index2,index3?pretty { \"_shards\" : { \"total\" : 6, \"successful\" : 6, \"failed\" : 0 } } total indicates how many shards the k-NN plugin attempted to warm up. The response also includes the number of shards the plugin succeeded and failed to warm up. The call does not return until the warmup operation is complete or the request times out. If the request times out, the operation still continues on the cluster. To monitor the warmup operation, use the Elasticsearch _tasks API: GET /_tasks Troubleshooting \u2691 Deal with the AWS service timeout \u2691 AWS' Elasticsearch service is exposed behind a load balancer that returns a timeout after 300 seconds. If the query you're sending takes longer you won't be able to retrieve the information. You can consider using Asynchronous search which requires Elasticsearch 7.10 or later. Asynchronous search lets you run search requests that run in the background. You can monitor the progress of these searches and get back partial results as they become available. After the search finishes, you can save the results to examine at a later time. If the query you're running is a KNN one, you can try: Using the knn warmup api before running initial queries. Scaling up the instances: Amazon ES uses half of an instance's RAM for the Java heap (up to a heap size of 32 GiB). By default, KNN uses up to 50% of the remaining half, so an instance type with 64 GiB of RAM can accommodate 16 GiB of graphs (64 * 0.5 * 0.5). Performance can suffer if graph memory usage exceeds this value. In a less recommended approach, you can make more percentage of memory available for KNN operations. Open Distro for Elasticsearch lets you modify all KNN settings using the _cluster/settings API. On Amazon ES, you can change all settings except knn.memory.circuit_breaker.enabled and knn.circuit_breaker.triggered . You can change the circuit breaker settings as: PUT /_cluster/settings { \"persistent\" : { \"knn.memory.circuit_breaker.limit\" : \"<value%>\" } } You could also do performance tuning your KNN request . Fix Circuit breakers triggers \u2691 The elasticsearch_exporter has a elasticsearch_breakers_tripped metric, which counts then number of Circuit Breakers triggered of the different kinds. The Grafana dashboard paints a count of all the triggers with a big red number, which may scare you at first. Lets first understand what are Circuit Breakers. Elasticsearch is built with Java and as such depends on the JVM heap for many operations and caching purposes. By default in AWS, each data node is assigned half the RAM to be used for heap for ES. In Elasticsearch the default Garbage Collector is Concurrent-Mark and Sweep (CMS). When the JVM Memory Pressure reaches 75%, this collector pauses some threads and attempts to reclaim some heap space. High heap usage occurs when the garbage collection process cannot keep up. An indicator of high heap usage is when the garbage collection is incapable of reducing the heap usage to around 30%. When a request reaches the ES nodes, circuit breakers estimate the amount of memory needed to load the required data. The cluster then compares the estimated size with the configured heap size limit. If the estimated size of your data is greater than the available heap size, the query is terminated. As a result, a CircuitBreakerException is thrown to prevent overloading the node. In essence, these breakers are present to prevent a request overloading a data node and consuming more heap space than that node can provide at that time. If these breakers weren't present, then the request will use up all the heap that the node can provide and this node will then restart due to OOM. Lets assume a data node has 16GB heap configured, When the parent circuit breaker is tripped, then a similar error is thrown: \"error\" : { \"root_cause\" : [ { \"type\" : \"circuit_breaking_exception\" , \"reason\" : \"[parent] Data too large, data for [<HTTP_request>] would be [16355096754/15.2gb], which is larger than the limit of [16213167308/15gb], real usage: [15283269136/14.2gb], new bytes reserved: [1071827618/1022.1mb]\" , } ] } The parent circuit breaker (a circuit breaker type) is responsible for the overall memory usage of your Elasticsearch cluster. When a parent circuit breaker exception occurs, the total memory used across all circuit breakers has exceeded the set limit. A parent breaker throws an exception when the cluster exceeds 95% of 16 GB, which is 15.2 GB of heap (in above example). A circuit breaking exception is generally caused by high JVM. When the JVM Memory Pressure is high, it indicates that a large portion of one or more data nodes configured heap is currently being used heavily, and as such, the frequency of the circuit breakers being tripped increases as there is not enough heap available at the time to process concurrent smaller or larger requests. It is worth noting that that the error can also be thrown by a certain request that would just consume all the available heap on a certain data node at the time such as an intensive search query. If you see numerous spikes to the high 90%, with occasionally spikes to 100%, it's not uncommon for the parent circuit breaker to be tripped in response to requests. To troubleshoot circuit breakers, you'll then have to address the High JVM issues, which can be caused by: Increase in the number of requests to the cluster. Check the IndexRate and SearchRate metrics in to determine your current load. Aggregation, wildcards, and using wide time ranges in your queries. Unbalanced shard allocation across nodes or too many shards in a cluster. Index mapping explosions. Using the fielddata data structure to query data. Fielddata can consume a large amount of heap space, and remains in the heap for the lifetime of a segment. As a result, JVM memory pressure remains high on the cluster when fielddata is used. Here's what happens as JVM memory pressure increases in AWS: At 75%: Amazon ES triggers the Concurrent Mark Sweep (CMS) garbage collector. The CMS collector runs alongside other processes to keep pauses and disruptions to a minimum. The garbage collection is a CPU-intensive process. If JVM memory pressure stays at this percentage for a few minutes, then you could encounter ClusterBlockException, JVM OutOfMemoryError, or other cluster performance issues. Above 75%: If the CMS collector fails to reclaim enough memory and usage remains above 75%, Amazon ES triggers a different garbage collection algorithm. This algorithm tries to free up memory and prevent a JVM OutOfMemoryError (OOM) exception by slowing or stopping processes. Above 92% for 30 minutes: Amazon ES blocks all write operations. Around 95%: Amazon ES kills processes that try to allocate memory. If a critical process is killed, one or more cluster nodes might fail. At 100%: Amazon ES JVM is configured to exit and eventually restarts on OutOfMemory (OOM). To resolve high JVM memory pressure, try the following tips: Reduce incoming traffic to your cluster, especially if you have a heavy workload. Consider scaling the cluster to obtain more JVM memory to support your workload. As mentioned above each data node gets half the RAM allocated to be used as Heap. Consider scaling to a data node type with more RAM and hence more Available Heap. Thereby increasing the parent circuit breaker limit. If cluster scaling isn't possible, try reducing the number of shards by deleting old or unused indices. Because shard metadata is stored in memory, reducing the number of shards can reduce overall memory usage. Enable slow logs to identify faulty requests. Note: Before enabling configuration changes, verify that JVM memory pressure is below 85%. This way, you can avoid additional overhead to existing resources. Optimize search and indexing requests, and choose the correct number of shards. Disable and avoid using fielddata. By default, fielddata is set to \"false\" on a text field unless it's explicitly defined as otherwise in index mappings. Field data is a potentially a huge consumer of JVM Heap space. This build up of field data occurs when aggregations are run on fields that are of type text . More on how you can periodically clear field data below. Change your index mapping type to a keyword , using reindex API. You can use the keyword type as an alternative for performing aggregations and sorting on text fields. As mentioned in above point, by aggregating on keyword type instead of text , no field data has to be built on demand and hence won't consume precious heap space. Look into the commonly aggregated fields in index mappings and ensure they are not of type text . If they are, you can consider changing them to keyword . You will have to create a new index with the desired mapping and then use the Reindex API to transfer over the documents from the source index to the new index. Once Re-index has completed then you can delete the old index. Avoid aggregating on text fields to prevent increases in field data. When you use more field data, more heap space is consumed. Use the cluster stats API operation to check your field data. Clear the fielddata cache with the following API call: POST /index_name/_cache/clear?fielddata=true (index-level cache) POST */_cache/clear?fielddata=true (cluster-level cache) Generally speaking, if you notice your workload (search rate and index rate) remaining consistent during these high spikes and non of the above optimizations can be applied or if they have already been applied and the JVM is still high during these workload times, then it is an indication that the cluster needs to be scaled in terms of JVM resources to cope with this workload. You can't reset the 'tripped' count. This is a Node level metric and thus will be reset to 0 when the Elasticsearch Service is restarted on that Node. Since in AWS it's a managed service, unfortunately you will not have access to the underlaying EC2 instance to restart the ES Process. However the ES Process can be restarted on your end (on all nodes) in the following ways: Initiate a Configuration Change that causes a blue/green deployment : When you initiate a configuration change, a subsequent blue/green deployment process is launched in which we launch a new fleet that matches the desired configuration. The old fleet continues to run and serve requests. Simultaneously, data in the form of shards are then migrated from the old fleet to the new fleet. Once all this data has been migrated the old fleet is terminated and the new one takes over. During this process ES is restarted on the Nodes. Ensure that CPU Utilization and JVM Memory Pressure are below the recommended 80% thresholds to prevent any issues with this process as it uses clusters resources to initiate and complete. You can scale the EBS Volumes attached to the data nodes by an arbitrary amount such as 1GB, wait for the blue/green to complete and then scale it back. Wait for a new service software release and update the service software of the Cluster. This will also cause a blue/green and hence ES process will be restarted on the nodes. Recover from yellow state \u2691 A yellow cluster represents that some of the replica shards in the cluster are unassigned. I can see that around 14 replica shards are unassigned. You can confirm the state of the cluster with the following commands curl <domain-endpoint>_cluster/health?pretty curl -X GET <domain-endpoint>/_cat/shards | grep UNASSIGNED curl -X GET <domain-endpoint>/_cat/indices | grep yellow If you have metrics of the JVMMemoryPressure of the nodes, check if the memory of a node reached 100% around the time the cluster reached yellow state. One can generally confirm the reason for a cluster going yellow by looking at the output of the following API call: curl -X GET <domain-endpoint>/_cluster/allocation/explain | jq If it shows a CircuitBreakerException , it confirms that a spike in the JVM metric caused the node to go down. Check the Fix Circuit breaker triggers section above to see how to solve that case. Reallocate unassigned shards \u2691 Elasticsearch makes 5 attempts to assign the shard but if it fails to be assigned after 5 attempts, the shards will remain unassigned. There is a solution to this issue in order to bring the cluster to green state. You can disable the replicas on the failing index and then enable replicas back. Disable Replica curl -X PUT \"<ES_endpoint>/<index_name>/_settings\" -H 'Content-Type: application/json' -d ' { \"index\" : { \"number_of_replicas\" : 0 } }' Enable the Replica back: curl -X PUT \"<ES_endpoint>/<index_name>/_settings\" -H 'Content-Type: application/json' -d ' { \"index\" : { \"number_of_replicas\" : 1 } }' Please note that it will take some time for the shards to be completely assigned and hence you might see intermittent cluster status as YELLOW.", "title": "elasticsearch"}, {"location": "linux/elasticsearch/#searching-documents", "text": "We use HTTP requests to talk to ElasticSearch. A HTTP request is made up of several components such as the URL to make the request to, HTTP verbs (GET, POST etc) and headers. In order to succinctly and consistently describe HTTP requests the ElasticSearch documentation uses cURL command line syntax. This is also the standard practice to describe requests made to ElasticSearch within the user community.", "title": "Searching documents"}, {"location": "linux/elasticsearch/#get-all-documents", "text": "An example HTTP request using CURL syntax looks like this: curl \\ -H 'Content-Type: application/json' \\ -XPOST \"https://localhost:9200/_search\" \\ -d ' { \"query\": { \"match_all\": {} }}'", "title": "Get all documents"}, {"location": "linux/elasticsearch/#get-documents-that-match-a-string", "text": "curl \\ -H 'Content-Type: application/json' \\ -XPOST \"https://localhost:9200/_search\" \\ -d ' { \"query\": { \"query_string\": {\"query\": \"test company\"} }}'", "title": "Get documents that match a string"}, {"location": "linux/elasticsearch/#backup", "text": "It's better to use the curator tool", "title": "Backup"}, {"location": "linux/elasticsearch/#create-snapshot", "text": "curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} ?wait_for_completion = true", "title": "Create snapshot"}, {"location": "linux/elasticsearch/#create-snapshot-of-selected-indices", "text": "curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} ?wait_for_completion = true curl -XPUT 'localhost:9200/_snapshot/my_backup/snapshot_1?pretty' -H 'Content-Type: application/json' -d ' { \"indices\": \"index_1,index_2\", \"ignore_unavailable\": true, \"include_global_state\": false } '", "title": "Create snapshot of selected indices"}, {"location": "linux/elasticsearch/#list-all-backups", "text": "Check for my-snapshot-repo curl {{ url }} /_snapshot/ {{ backup_path }} /*?pretty", "title": "List all backups"}, {"location": "linux/elasticsearch/#restore-backup", "text": "First you need to close the selected indices curl -X POST {{ url }} / {{ indice_name }} /_close Then restore curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} /_restore?wait_for_completion = true If you want to restore only one index, use: curl -X POST \"{{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}/_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"{{ index_to_restore }}\", }'", "title": "Restore backup"}, {"location": "linux/elasticsearch/#delete-snapshot", "text": "curl -XDELETE {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }}", "title": "Delete snapshot"}, {"location": "linux/elasticsearch/#delete-snapshot-repository", "text": "curl -XDELETE {{ url }} /_snapshot/ {{ backup_path }}", "title": "Delete snapshot repository"}, {"location": "linux/elasticsearch/#delete-snapshots-older-than-x", "text": "!!! note \"File: curator.yml\" ```yaml client: hosts: - 'a data node' port: 9200 url_prefix: use_ssl: False certificate: client_cert: client_key: ssl_no_validate: False http_auth: timeout: 30 master_only: False logging: loglevel: INFO logfile: D:\\CuratorLogs\\logs.txt logformat: default blacklist: ['elasticsearch', 'urllib3'] ``` File: delete_old_snapshots.yml yaml actions: 1: action: delete_snapshots description: >- Delete snapshots from the selected repository older than 100 days (based on creation_date), for everything but 'citydirectory-' prefixed snapshots. options: repository: 'dcs-elastic-snapshot' disable_action: False filters: - filtertype: pattern kind: prefix value: citydirectory- exclude: True - filtertype: age source: creation_date direction: older unit: days unit_count: 100", "title": "Delete snapshots older than X"}, {"location": "linux/elasticsearch/#information-gathering", "text": "", "title": "Information gathering"}, {"location": "linux/elasticsearch/#get-status-of-cluster", "text": "curl {{ url }} /_cluster/health?pretty curl {{ url }} /_cat/nodes?v curl {{ url }} /_cat/indices?v curl {{ url }} /_cat/shards If you've got red status, use the following command to choose the first unassigned shard that it finds and explains why it cannot be allocated to a node. curl {{ url }} /_cluster/allocation/explain?v", "title": "Get status of cluster"}, {"location": "linux/elasticsearch/#get-settings", "text": "curl {{ url }} /_settings", "title": "Get settings"}, {"location": "linux/elasticsearch/#get-space-left", "text": "curl {{ url }} /_nodes/stats/fs?pretty", "title": "Get space left"}, {"location": "linux/elasticsearch/#list-plugins", "text": "curl {{ url }} /_nodes/plugins?pretty", "title": "List plugins"}, {"location": "linux/elasticsearch/#upload", "text": "", "title": "Upload"}, {"location": "linux/elasticsearch/#single-data-upload", "text": "curl -XPOST '{{ url }}/{{ path_to_table }}' -d '{{ json_input }}' where json_input can be { \"field\" : \"value\" }", "title": "Single data upload"}, {"location": "linux/elasticsearch/#bulk-upload-of-data", "text": "curl -H 'Content-Type: application/x-ndjson' -XPOST \\ '{{ url }}/{{ path_to_table }}/_bulk?pretty' --data-binary @ {{ json_file }}", "title": "Bulk upload of data"}, {"location": "linux/elasticsearch/#delete", "text": "", "title": "Delete"}, {"location": "linux/elasticsearch/#delete-data", "text": "curl -XDELETE {{ url }} / {{ path_to_ddbb }}", "title": "Delete data"}, {"location": "linux/elasticsearch/#reindex-an-index", "text": "If you encountered errors while reindexing source_index to destination_index it can be because the cluster hit a timeout on the scroll locks. As a work around, you can increase the timeout period to a reasonable value and then reindex. The default AWS values are search context of 5 minutes, socket timeout of 30 seconds, and batch size of 1,000. First clear the cache of the index with: curl -X POST https://elastic.url/destination_index/_cache/clear If the index is big, they suggest to disable replicas in your destination index by setting number_of_replicas to 0 and re-enable them once the reindex process is complete. To get the current state use: curl https://elastic.url/destination_index/_settings Then disable the replicas with: curl -X PUT \\ https://elastic.url/destination_index \\ -H 'Content-Type: application/json' \\ -d ' { \"settings\" : { \"refresh_interval\" : -1, \"number_of_replicas\" : 0 }} Now you can reindex the index with: curl -X POST \\ https://elastic.url/_reindex?wait_for_completion = false \\& timeout = 10m \\& scroll = 10h \\& pretty = true \\ -H 'Content-Type: application/json' \\ -d '{\"source\": { \"remote\": { \"host\": \"https://elastic.url:443\", \"socket_timeout\": \"60m\" }, \"index\": \"source_index\" }, \"dest\": {\"index\": \"destination_index\"}}' And check the evolution of the task with: curl 'https://elastic.url/_tasks?detailed=true&actions=*reindex&group_by=parents&pretty=true' The output is quite verbose, so I use vimdiff to see the differences between instant states. If you see there are no tasks running, check the indices status to see if the reindex ended well. curl https://elastic.url/_cat/indices After the reindex process is complete, you can reset your desired replica count and remove the refresh interval setting.", "title": "Reindex an index"}, {"location": "linux/elasticsearch/#knn", "text": "", "title": "KNN"}, {"location": "linux/elasticsearch/#knn-sizing", "text": "Typically, in an Elasticsearch cluster, a certain portion of RAM is set aside for the JVM heap. The k-NN plugin allocates graphs to a portion of the remaining RAM. This portion\u2019s size is determined by the circuit_breaker_limit cluster setting. By default, the circuit breaker limit is set at 50%. The memory required for graphs is estimated to be `1.1 * (4 * dimension 8 * M)` bytes/vector. To get the dimension and m use the /index elasticsearch endpoint. To get the number of vectors, use /index/_count . The number of vectors is the same as the number of documents. As an example, assume that we have 1 Million vectors with a dimension of 256 and M of 16, and the memory required can be estimated as: 1.1 * (4 *256 + 8 * 16) * 1,000,000 ~= 1.26 GB !!! note \"Remember that having a replica will double the total number of vectors.\" I've seen some queries work with indices that required 120% of the available memory for the KNN. A good way to see if it fits, is warming up the knn vectors . If the process returns a timeout, you probably don't have enough memory.", "title": "KNN sizing"}, {"location": "linux/elasticsearch/#knn-warmup", "text": "The Hierarchical Navigable Small World (HNSW) graphs that are used to perform an approximate k-Nearest Neighbor (k-NN) search are stored as .hnsw files with other Apache Lucene segment files. In order for you to perform a search on these graphs using the k-NN plugin, these files need to be loaded into native memory. If the plugin has not loaded the graphs into native memory, it loads them when it receives a search request. This loading time can cause high latency during initial queries. To avoid this situation, users often run random queries during a warmup period. After this warmup period, the graphs are loaded into native memory and their production workloads can begin. This loading process is indirect and requires extra effort. As an alternative, you can avoid this latency issue by running the k-NN plugin warmup API operation on whatever indices you\u2019re interested in searching. This operation loads all the graphs for all of the shards (primaries and replicas) of all the indices specified in the request into native memory. After the process finishes, you can start searching against the indices with no initial latency penalties. The warmup API operation is idempotent, so if a segment\u2019s graphs are already loaded into memory, this operation has no impact on those graphs. It only loads graphs that aren\u2019t currently in memory. This request performs a warmup on three indices: GET /_opendistro/_knn/warmup/index1,index2,index3?pretty { \"_shards\" : { \"total\" : 6, \"successful\" : 6, \"failed\" : 0 } } total indicates how many shards the k-NN plugin attempted to warm up. The response also includes the number of shards the plugin succeeded and failed to warm up. The call does not return until the warmup operation is complete or the request times out. If the request times out, the operation still continues on the cluster. To monitor the warmup operation, use the Elasticsearch _tasks API: GET /_tasks", "title": "KNN warmup"}, {"location": "linux/elasticsearch/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "linux/elasticsearch/#deal-with-the-aws-service-timeout", "text": "AWS' Elasticsearch service is exposed behind a load balancer that returns a timeout after 300 seconds. If the query you're sending takes longer you won't be able to retrieve the information. You can consider using Asynchronous search which requires Elasticsearch 7.10 or later. Asynchronous search lets you run search requests that run in the background. You can monitor the progress of these searches and get back partial results as they become available. After the search finishes, you can save the results to examine at a later time. If the query you're running is a KNN one, you can try: Using the knn warmup api before running initial queries. Scaling up the instances: Amazon ES uses half of an instance's RAM for the Java heap (up to a heap size of 32 GiB). By default, KNN uses up to 50% of the remaining half, so an instance type with 64 GiB of RAM can accommodate 16 GiB of graphs (64 * 0.5 * 0.5). Performance can suffer if graph memory usage exceeds this value. In a less recommended approach, you can make more percentage of memory available for KNN operations. Open Distro for Elasticsearch lets you modify all KNN settings using the _cluster/settings API. On Amazon ES, you can change all settings except knn.memory.circuit_breaker.enabled and knn.circuit_breaker.triggered . You can change the circuit breaker settings as: PUT /_cluster/settings { \"persistent\" : { \"knn.memory.circuit_breaker.limit\" : \"<value%>\" } } You could also do performance tuning your KNN request .", "title": "Deal with the AWS service timeout"}, {"location": "linux/elasticsearch/#fix-circuit-breakers-triggers", "text": "The elasticsearch_exporter has a elasticsearch_breakers_tripped metric, which counts then number of Circuit Breakers triggered of the different kinds. The Grafana dashboard paints a count of all the triggers with a big red number, which may scare you at first. Lets first understand what are Circuit Breakers. Elasticsearch is built with Java and as such depends on the JVM heap for many operations and caching purposes. By default in AWS, each data node is assigned half the RAM to be used for heap for ES. In Elasticsearch the default Garbage Collector is Concurrent-Mark and Sweep (CMS). When the JVM Memory Pressure reaches 75%, this collector pauses some threads and attempts to reclaim some heap space. High heap usage occurs when the garbage collection process cannot keep up. An indicator of high heap usage is when the garbage collection is incapable of reducing the heap usage to around 30%. When a request reaches the ES nodes, circuit breakers estimate the amount of memory needed to load the required data. The cluster then compares the estimated size with the configured heap size limit. If the estimated size of your data is greater than the available heap size, the query is terminated. As a result, a CircuitBreakerException is thrown to prevent overloading the node. In essence, these breakers are present to prevent a request overloading a data node and consuming more heap space than that node can provide at that time. If these breakers weren't present, then the request will use up all the heap that the node can provide and this node will then restart due to OOM. Lets assume a data node has 16GB heap configured, When the parent circuit breaker is tripped, then a similar error is thrown: \"error\" : { \"root_cause\" : [ { \"type\" : \"circuit_breaking_exception\" , \"reason\" : \"[parent] Data too large, data for [<HTTP_request>] would be [16355096754/15.2gb], which is larger than the limit of [16213167308/15gb], real usage: [15283269136/14.2gb], new bytes reserved: [1071827618/1022.1mb]\" , } ] } The parent circuit breaker (a circuit breaker type) is responsible for the overall memory usage of your Elasticsearch cluster. When a parent circuit breaker exception occurs, the total memory used across all circuit breakers has exceeded the set limit. A parent breaker throws an exception when the cluster exceeds 95% of 16 GB, which is 15.2 GB of heap (in above example). A circuit breaking exception is generally caused by high JVM. When the JVM Memory Pressure is high, it indicates that a large portion of one or more data nodes configured heap is currently being used heavily, and as such, the frequency of the circuit breakers being tripped increases as there is not enough heap available at the time to process concurrent smaller or larger requests. It is worth noting that that the error can also be thrown by a certain request that would just consume all the available heap on a certain data node at the time such as an intensive search query. If you see numerous spikes to the high 90%, with occasionally spikes to 100%, it's not uncommon for the parent circuit breaker to be tripped in response to requests. To troubleshoot circuit breakers, you'll then have to address the High JVM issues, which can be caused by: Increase in the number of requests to the cluster. Check the IndexRate and SearchRate metrics in to determine your current load. Aggregation, wildcards, and using wide time ranges in your queries. Unbalanced shard allocation across nodes or too many shards in a cluster. Index mapping explosions. Using the fielddata data structure to query data. Fielddata can consume a large amount of heap space, and remains in the heap for the lifetime of a segment. As a result, JVM memory pressure remains high on the cluster when fielddata is used. Here's what happens as JVM memory pressure increases in AWS: At 75%: Amazon ES triggers the Concurrent Mark Sweep (CMS) garbage collector. The CMS collector runs alongside other processes to keep pauses and disruptions to a minimum. The garbage collection is a CPU-intensive process. If JVM memory pressure stays at this percentage for a few minutes, then you could encounter ClusterBlockException, JVM OutOfMemoryError, or other cluster performance issues. Above 75%: If the CMS collector fails to reclaim enough memory and usage remains above 75%, Amazon ES triggers a different garbage collection algorithm. This algorithm tries to free up memory and prevent a JVM OutOfMemoryError (OOM) exception by slowing or stopping processes. Above 92% for 30 minutes: Amazon ES blocks all write operations. Around 95%: Amazon ES kills processes that try to allocate memory. If a critical process is killed, one or more cluster nodes might fail. At 100%: Amazon ES JVM is configured to exit and eventually restarts on OutOfMemory (OOM). To resolve high JVM memory pressure, try the following tips: Reduce incoming traffic to your cluster, especially if you have a heavy workload. Consider scaling the cluster to obtain more JVM memory to support your workload. As mentioned above each data node gets half the RAM allocated to be used as Heap. Consider scaling to a data node type with more RAM and hence more Available Heap. Thereby increasing the parent circuit breaker limit. If cluster scaling isn't possible, try reducing the number of shards by deleting old or unused indices. Because shard metadata is stored in memory, reducing the number of shards can reduce overall memory usage. Enable slow logs to identify faulty requests. Note: Before enabling configuration changes, verify that JVM memory pressure is below 85%. This way, you can avoid additional overhead to existing resources. Optimize search and indexing requests, and choose the correct number of shards. Disable and avoid using fielddata. By default, fielddata is set to \"false\" on a text field unless it's explicitly defined as otherwise in index mappings. Field data is a potentially a huge consumer of JVM Heap space. This build up of field data occurs when aggregations are run on fields that are of type text . More on how you can periodically clear field data below. Change your index mapping type to a keyword , using reindex API. You can use the keyword type as an alternative for performing aggregations and sorting on text fields. As mentioned in above point, by aggregating on keyword type instead of text , no field data has to be built on demand and hence won't consume precious heap space. Look into the commonly aggregated fields in index mappings and ensure they are not of type text . If they are, you can consider changing them to keyword . You will have to create a new index with the desired mapping and then use the Reindex API to transfer over the documents from the source index to the new index. Once Re-index has completed then you can delete the old index. Avoid aggregating on text fields to prevent increases in field data. When you use more field data, more heap space is consumed. Use the cluster stats API operation to check your field data. Clear the fielddata cache with the following API call: POST /index_name/_cache/clear?fielddata=true (index-level cache) POST */_cache/clear?fielddata=true (cluster-level cache) Generally speaking, if you notice your workload (search rate and index rate) remaining consistent during these high spikes and non of the above optimizations can be applied or if they have already been applied and the JVM is still high during these workload times, then it is an indication that the cluster needs to be scaled in terms of JVM resources to cope with this workload. You can't reset the 'tripped' count. This is a Node level metric and thus will be reset to 0 when the Elasticsearch Service is restarted on that Node. Since in AWS it's a managed service, unfortunately you will not have access to the underlaying EC2 instance to restart the ES Process. However the ES Process can be restarted on your end (on all nodes) in the following ways: Initiate a Configuration Change that causes a blue/green deployment : When you initiate a configuration change, a subsequent blue/green deployment process is launched in which we launch a new fleet that matches the desired configuration. The old fleet continues to run and serve requests. Simultaneously, data in the form of shards are then migrated from the old fleet to the new fleet. Once all this data has been migrated the old fleet is terminated and the new one takes over. During this process ES is restarted on the Nodes. Ensure that CPU Utilization and JVM Memory Pressure are below the recommended 80% thresholds to prevent any issues with this process as it uses clusters resources to initiate and complete. You can scale the EBS Volumes attached to the data nodes by an arbitrary amount such as 1GB, wait for the blue/green to complete and then scale it back. Wait for a new service software release and update the service software of the Cluster. This will also cause a blue/green and hence ES process will be restarted on the nodes.", "title": "Fix Circuit breakers triggers"}, {"location": "linux/elasticsearch/#recover-from-yellow-state", "text": "A yellow cluster represents that some of the replica shards in the cluster are unassigned. I can see that around 14 replica shards are unassigned. You can confirm the state of the cluster with the following commands curl <domain-endpoint>_cluster/health?pretty curl -X GET <domain-endpoint>/_cat/shards | grep UNASSIGNED curl -X GET <domain-endpoint>/_cat/indices | grep yellow If you have metrics of the JVMMemoryPressure of the nodes, check if the memory of a node reached 100% around the time the cluster reached yellow state. One can generally confirm the reason for a cluster going yellow by looking at the output of the following API call: curl -X GET <domain-endpoint>/_cluster/allocation/explain | jq If it shows a CircuitBreakerException , it confirms that a spike in the JVM metric caused the node to go down. Check the Fix Circuit breaker triggers section above to see how to solve that case.", "title": "Recover from yellow state"}, {"location": "linux/elasticsearch/#reallocate-unassigned-shards", "text": "Elasticsearch makes 5 attempts to assign the shard but if it fails to be assigned after 5 attempts, the shards will remain unassigned. There is a solution to this issue in order to bring the cluster to green state. You can disable the replicas on the failing index and then enable replicas back. Disable Replica curl -X PUT \"<ES_endpoint>/<index_name>/_settings\" -H 'Content-Type: application/json' -d ' { \"index\" : { \"number_of_replicas\" : 0 } }' Enable the Replica back: curl -X PUT \"<ES_endpoint>/<index_name>/_settings\" -H 'Content-Type: application/json' -d ' { \"index\" : { \"number_of_replicas\" : 1 } }' Please note that it will take some time for the shards to be completely assigned and hence you might see intermittent cluster status as YELLOW.", "title": "Reallocate unassigned shards"}, {"location": "linux/fail2ban/", "text": "Usage \u2691 Unban IP \u2691 fail2ban-client set {{ jail }} unbanip {{ ip }} Where jail can be ssh .", "title": "fail2ban"}, {"location": "linux/fail2ban/#usage", "text": "", "title": "Usage"}, {"location": "linux/fail2ban/#unban-ip", "text": "fail2ban-client set {{ jail }} unbanip {{ ip }} Where jail can be ssh .", "title": "Unban IP"}, {"location": "linux/google_chrome/", "text": "Although I hate it, there are web pages that don't work on Firefox or Chromium. In those cases I install google-chrome and uninstall as soon as I don't need to use that service. Installation \u2691 Debian \u2691 wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb sudo apt install ./google-chrome-stable_current_amd64.deb", "title": "google chrome"}, {"location": "linux/google_chrome/#installation", "text": "", "title": "Installation"}, {"location": "linux/google_chrome/#debian", "text": "wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb sudo apt install ./google-chrome-stable_current_amd64.deb", "title": "Debian"}, {"location": "linux/haproxy/", "text": "HAProxy is free, open source software that provides a high availability load balancer and proxy server for TCP and HTTP-based applications that spreads requests across multiple servers. It is written in C and has a reputation for being fast and efficient (in terms of processor and memory usage). Use HAProxy as a reverse proxy \u2691 reverse proxy is a type of proxy server that retrieves resources on behalf of a client from one or more servers. These resources are then returned to the client, appearing as if they originated from the server itself. Unlike a forward proxy, which is an intermediary for its associated clients to contact any server, a reverse proxy is an intermediary for its associated servers to be contacted by any client. In other words, a proxy is associated with the client(s), while a reverse proxy is associated with the server(s); a reverse proxy is usually an internal-facing proxy used as a 'front-end' to control and protect access to a server on a private network. It can be done at Web server level (Nginx, Apache, ...) or at load balancer level. This HAProxy post shows how to translate Apache's proxy pass directives to the HAProxy configuration. frontend ft_global acl host_dom.com req.hdr(Host) dom.com acl path_mirror_foo path -m beg /mirror/foo/ use_backend bk_myapp if host_dom.com path_mirror_foo backend bk_myapp [...] # external URL => internal URL # http://dom.com/mirror/foo/bar => http://bk.dom.com/bar # ProxyPass /mirror/foo/ http://bk.dom.com/bar http-request set-header Host bk.dom.com reqirep ^([^ :]*)\\ /mirror/foo/(.*) \\1\\ /\\2 # ProxyPassReverse /mirror/foo/ http://bk.dom.com/bar # Note: we turn the urls into absolute in the mean time acl hdr_location res.hdr(Location) -m found rspirep ^Location:\\ (https?://bk.dom.com(:[0-9]+)?)?(/.*) Location:\\ /mirror/foo3 if hdr_location # ProxyPassReverseCookieDomain bk.dom.com dom.com acl hdr_set_cookie_dom res.hdr(Set-cookie) -m sub Domain= bk.dom.com rspirep ^(Set-Cookie:.*)\\ Domain=bk.dom.com(.*) \\1\\ Domain=dom.com\\2 if hdr_set_cookie_dom # ProxyPassReverseCookieDomain / /mirror/foo/ acl hdr_set_cookie_path res.hdr(Set-cookie) -m sub Path= rspirep ^(Set-Cookie:.*)\\ Path=(.*) \\1\\ Path=/mirror/foo2 if hdr_set_cookie_path Other useful examples can be retrieved from drmalex07 or ferdinandosimonetti gists. References \u2691 Guidelines for HAProxy termination in AWS", "title": "HAProxy"}, {"location": "linux/haproxy/#use-haproxy-as-a-reverse-proxy", "text": "reverse proxy is a type of proxy server that retrieves resources on behalf of a client from one or more servers. These resources are then returned to the client, appearing as if they originated from the server itself. Unlike a forward proxy, which is an intermediary for its associated clients to contact any server, a reverse proxy is an intermediary for its associated servers to be contacted by any client. In other words, a proxy is associated with the client(s), while a reverse proxy is associated with the server(s); a reverse proxy is usually an internal-facing proxy used as a 'front-end' to control and protect access to a server on a private network. It can be done at Web server level (Nginx, Apache, ...) or at load balancer level. This HAProxy post shows how to translate Apache's proxy pass directives to the HAProxy configuration. frontend ft_global acl host_dom.com req.hdr(Host) dom.com acl path_mirror_foo path -m beg /mirror/foo/ use_backend bk_myapp if host_dom.com path_mirror_foo backend bk_myapp [...] # external URL => internal URL # http://dom.com/mirror/foo/bar => http://bk.dom.com/bar # ProxyPass /mirror/foo/ http://bk.dom.com/bar http-request set-header Host bk.dom.com reqirep ^([^ :]*)\\ /mirror/foo/(.*) \\1\\ /\\2 # ProxyPassReverse /mirror/foo/ http://bk.dom.com/bar # Note: we turn the urls into absolute in the mean time acl hdr_location res.hdr(Location) -m found rspirep ^Location:\\ (https?://bk.dom.com(:[0-9]+)?)?(/.*) Location:\\ /mirror/foo3 if hdr_location # ProxyPassReverseCookieDomain bk.dom.com dom.com acl hdr_set_cookie_dom res.hdr(Set-cookie) -m sub Domain= bk.dom.com rspirep ^(Set-Cookie:.*)\\ Domain=bk.dom.com(.*) \\1\\ Domain=dom.com\\2 if hdr_set_cookie_dom # ProxyPassReverseCookieDomain / /mirror/foo/ acl hdr_set_cookie_path res.hdr(Set-cookie) -m sub Path= rspirep ^(Set-Cookie:.*)\\ Path=(.*) \\1\\ Path=/mirror/foo2 if hdr_set_cookie_path Other useful examples can be retrieved from drmalex07 or ferdinandosimonetti gists.", "title": "Use HAProxy as a reverse proxy"}, {"location": "linux/haproxy/#references", "text": "Guidelines for HAProxy termination in AWS", "title": "References"}, {"location": "linux/hypothesis/", "text": "Hypothesis is an open-source software project that aims to collect comments about statements made in any web-accessible content, and filter and rank those comments to assess each statement's credibility. It offers an online web application where registered users share highlights and annotations over any webpage. As of 2020-06-11, although the service can be self-hosted, it's not yet easy to do so. Install \u2691 Client \u2691 If you're using Chrome or any derivative there is an official extension. Unfortunately if you use Firefox the extension is still being developed #310 although an unofficial release works just fine . Alternatively you can use the Hypothesis bookmarklet . The only problem is that both the extensions and the bookmarklet only works for the official service. In theory you can tweak the extension build process to use your custom settings . Though there is yet no documentation on this topic. I've thought of opening them a bug regarding this issue, but their github issues are only for bug reports, they use a google group to track the feature requests, I don't have an easy way to post there, so if you follow this path, please contact me . Server \u2691 The infrastructure can be deployed with Docker-compose. version : '3' services : postgres : image : postgres:11.5-alpine ports : - 5432 # - '5432:5432' elasticsearch : image : hypothesis/elasticsearch:latest ports : - 9200 #- '9200:9200' environment : - discovery.type=single-node rabbit : image : rabbitmq:3.6-management-alpine ports : - 5672 - 15672 #- '5672:5672' #- '15672:15672' web : image : hypothesis/hypothesis:latest environment : - APP_URL=http://localhost:5000 - AUTHORITY=localhost - BROKER_URL=amqp://guest:guest@rabbit:5672// - CLIENT_OAUTH_ID - CLIENT_URL=http://localhost:3001/hypothesis - DATABASE_URL=postgresql://postgres@postgres/postgres - ELASTICSEARCH_URL=http://elasticsearch:9200 - NEW_RELIC_APP_NAME=h (dev) - NEW_RELIC_LICENSE_KEY - SECRET_KEY=notasecret ports : - '5000:5000' depends_on : - postgres - elasticsearch - rabbit docker-compose up Initialize the database and create the admin user. docker-compose exec web /bin/sh hypothesis init hypothesis user add hypothesis user admin <username> The service is available at http://localhost:5000 . To check the latest developments of the Docker compose deployment follow the issue #4899 . They also provide the tools they use to deploy the production service into AWS. References \u2691 Homepage FAQ Bug tracker Feature request tracker Server deployment open issues \u2691 Self-hosting Docker compose Create admin user when using Docker compose Steps required to run both h and serve the client from internal server How to deploy h on VM", "title": "hypothesis"}, {"location": "linux/hypothesis/#install", "text": "", "title": "Install"}, {"location": "linux/hypothesis/#client", "text": "If you're using Chrome or any derivative there is an official extension. Unfortunately if you use Firefox the extension is still being developed #310 although an unofficial release works just fine . Alternatively you can use the Hypothesis bookmarklet . The only problem is that both the extensions and the bookmarklet only works for the official service. In theory you can tweak the extension build process to use your custom settings . Though there is yet no documentation on this topic. I've thought of opening them a bug regarding this issue, but their github issues are only for bug reports, they use a google group to track the feature requests, I don't have an easy way to post there, so if you follow this path, please contact me .", "title": "Client"}, {"location": "linux/hypothesis/#server", "text": "The infrastructure can be deployed with Docker-compose. version : '3' services : postgres : image : postgres:11.5-alpine ports : - 5432 # - '5432:5432' elasticsearch : image : hypothesis/elasticsearch:latest ports : - 9200 #- '9200:9200' environment : - discovery.type=single-node rabbit : image : rabbitmq:3.6-management-alpine ports : - 5672 - 15672 #- '5672:5672' #- '15672:15672' web : image : hypothesis/hypothesis:latest environment : - APP_URL=http://localhost:5000 - AUTHORITY=localhost - BROKER_URL=amqp://guest:guest@rabbit:5672// - CLIENT_OAUTH_ID - CLIENT_URL=http://localhost:3001/hypothesis - DATABASE_URL=postgresql://postgres@postgres/postgres - ELASTICSEARCH_URL=http://elasticsearch:9200 - NEW_RELIC_APP_NAME=h (dev) - NEW_RELIC_LICENSE_KEY - SECRET_KEY=notasecret ports : - '5000:5000' depends_on : - postgres - elasticsearch - rabbit docker-compose up Initialize the database and create the admin user. docker-compose exec web /bin/sh hypothesis init hypothesis user add hypothesis user admin <username> The service is available at http://localhost:5000 . To check the latest developments of the Docker compose deployment follow the issue #4899 . They also provide the tools they use to deploy the production service into AWS.", "title": "Server"}, {"location": "linux/hypothesis/#references", "text": "Homepage FAQ Bug tracker Feature request tracker", "title": "References"}, {"location": "linux/hypothesis/#server-deployment-open-issues", "text": "Self-hosting Docker compose Create admin user when using Docker compose Steps required to run both h and serve the client from internal server How to deploy h on VM", "title": "Server deployment open issues"}, {"location": "linux/mkdocs/", "text": "MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Note: I've automated the creation of the mkdocs site in this cookiecutter template . Installation \u2691 Install the basic packages. pip install \\ mkdocs \\ mkdocs-material \\ mkdocs-autolink-plugin \\ mkdocs-minify-plugin \\ pymdown-extensions \\ mkdocs-git-revision-date-localized-plugin Create the docs repository. mkdocs new docs Although there are several themes , I usually use the material one. I won't dive into the different options, just show a working template of the mkdocs.yaml file. site_name : {{ site_name : null }: null } site_author : {{ your_name : null }: null } site_url : {{ site_url : null }: null } nav : - Introduction : index.md - Basic Usage : basic_usage.md - Configuration : configuration.md - Update : update.md - Advanced Usage : - Projects : projects.md - Tags : tags.md plugins : - search - autolinks - git-revision-date-localized : type : timeago - minify : minify_html : true markdown_extensions : - admonition - meta - toc : permalink : true baselevel : 2 - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !%21python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde theme : name : material custom_dir : theme logo : images/logo.png palette : primary : blue grey accent : light blue extra_css : - stylesheets/extra.css - stylesheets/links.css repo_name : {{ repository_name : null }: null } # for example: 'lyz-code/pydo' repo_url : {{ repository_url : null }: null } # for example: 'https://github.com/lyz-code/pydo' Configure your logo by saving it into docs/images/logo.png . I like to show a small image above each link so you know where is it pointing to. To do so add the content of this directory to theme . and these files under docs/stylesheets . Initialize the git repository and create the first commit. Start the server to see everything is alright. mkdocs serve Material theme customizations \u2691 Color palette toggle \u2691 Since 7.1.0, you can have a light-dark mode on the site using a toggle in the upper bar. To enable it add to your mkdocs.yml : theme : palette : # Light mode - media : '(prefers-color-scheme: light)' scheme : default primary : blue grey accent : light blue toggle : icon : material/toggle-switch-off-outline name : Switch to dark mode # Dark mode - media : '(prefers-color-scheme: dark)' scheme : slate primary : blue grey accent : light blue toggle : icon : material/toggle-switch name : Switch to light mode Changing your desired colors for each mode Back to top button \u2691 Since 7.1.0, a back-to-top button can be shown when the user, after scrolling down, starts to scroll up again. It's rendered in the lower right corner of the viewport. Add the following lines to mkdocs.yml: theme : features : - navigation.top Add a github pages hook. \u2691 Save your requirements.txt . pip freeze > requirements.txt Create the .github/workflows/gh-pages.yml file with the following contents. name : Github pages on : push : branches : - master jobs : deploy : runs-on : ubuntu-18.04 steps : - uses : actions/checkout@v2 with : # Number of commits to fetch. 0 indicates all history. # Default: 1 fetch-depth : 0 - name : Setup Python uses : actions/setup-python@v1 with : python-version : '3.7' architecture : x64 - name : Cache dependencies uses : actions/cache@v1 with : path : ~/.cache/pip key : ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }} restore-keys : | ${{ runner.os }}-pip- - name : Install dependencies run : | python3 -m pip install --upgrade pip python3 -m pip install -r ./requirements.txt - run : | cd docs mkdocs build - name : Deploy uses : peaceiris/actions-gh-pages@v3 with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir : ./docs/site Create an SSH deploy key Activate GitHub Pages repository configuration with gh-pages branch . Make a new commit and push to check it's working. Create MermaidJS diagrams \u2691 Even though the Material theme supports mermaid diagrams it's only giving it for the paid users. The funding needs to reach 5000$ so it's released to the general public. The alternative is to use the mkdocs-mermaid2-plugin plugin, which can't be used with mkdocs-minify-plugin and doesn't adapt to dark mode. To install it : Download the package: pip install mkdocs-mermaid2-plugin . Enable the plugin in mkdocs.yml . plugins : # Not compatible with mermaid2 # - minify: # minify_html: true - mermaid2 : arguments : securityLevel : loose markdown_extensions : - pymdownx.superfences : # make exceptions to highlighting of code: custom_fences : - name : mermaid class : mermaid format : !%21python/name:mermaid2.fence_mermaid Check the MermaidJS article to see how to create the diagrams. Plugin development \u2691 Like MkDocs, plugins must be written in Python. It is expected that each plugin would be distributed as a separate Python module. At a minimum, a MkDocs Plugin must consist of a BasePlugin subclass and an entry point which points to it. The BasePlugin class is meant to have on_<event_name> methods that run actions on the MkDocs defined events . The same object is called at the different events, so you can save objects from one event to the other in the object attributes. Keep in mind that the order of execution of the plugins follows the ordering of the list of the mkdocs.yml file where they are defined. Interesting objects \u2691 Files \u2691 mkdocs.structure.files.Files contains a list of File objects under the ._files attribute and allows you to append files to the collection. As well as extracting the different file types: documentation_pages : Iterable of markdown page file objects. static_pages : Iterable of static page file objects. media_files : Iterable of all files that are not documentation or static pages. javascript_files : Iterable of javascript files. css_files : Iterable of css files. It is initialized with a list of File objects. File \u2691 mkdocs.structure.files.File objects points to the source and destination locations of a file. It has the following interesting attributes: name : Name of the file without the extension. src_path or abs_src_path : Relative or absolute path to the original path, for example the markdown file. dest_path or abs_dest_path : Relative or absolute path to the destination path, for example the html file generated from the markdown one. url : Url where the file is going to be exposed. It is initialized with the arguments: path : Must be a path that exists relative to src_dir . src_dir : Absolute path on the local file system to the directory where the docs are. dest_dir : Absolute path on the local file system to the directory where the site is going to be built. use_directory_urls : If False , a Markdown file is mapped to an HTML file of the same name (the file extension is changed to .html ). If True, a Markdown file is mapped to an HTML index file ( index.html ) nested in a directory using the \"name\" of the file in path . The use_directory_urls argument has no effect on non-Markdown files. By default MkDocs uses True . Navigation \u2691 mkdocs.structure.nav.Navigation objects hold the information to build the navigation of the site. It has the following interesting attributes: items : Nested List with full navigation of Sections, SectionPages, Pages, and Links. pages : Flat List of subset of Pages in nav, in order. The Navigation object has no __eq__ method, so when testing, instead of trying to build a similar Navigation object and compare them, you need to assert that the contents of the object are what you expect. Page \u2691 mkdocs.structure.pages.Page models each page of the site. To initialize it you need the title , the File object of the page, and the MkDocs config object. Section \u2691 mkdocs.structure.nav.Section object models a section of the navigation of a MkDocs site. To initialize it you need the title of the section and the children which are the elements that belong to the section. If you don't yet know the children, pass an empty list [] . SectionPage \u2691 mkdocs_section_index.SectionPage , part of the mkdocs-section-index plugin, models Section objects that have an associated Page , allowing you to have nav sections that when clicked, load the Page and not only opens the menu for the children elements. To initialize it you need the title of the section, the File object of the page, , the MkDocs config object, and the children which are the elements that belong to the section. If you don't yet know the children, pass an empty list [] . Events \u2691 on_config \u2691 The config event is the first event called on build and is run immediately after the user configuration is loaded and validated. Any alterations to the config should be made here. Parameters: config : global configuration object Returns: global configuration object on_files \u2691 The files event is called after the files collection is populated from the docs_dir . Use this event to add, remove, or alter files in the collection. Note that Page objects have not yet been associated with the file objects in the collection. Use Page Events to manipulate page specific data. Parameters: files : global files collection config : global configuration object Returns: global files collection on_nav \u2691 The nav event is called after the site navigation is created and can be used to alter the site navigation. Warning: Read the following section if you want to add new files . Parameters: nav : global navigation object . config : global configuration object. files : global files collection . Returns: global navigation object Adding new files \u2691 Note: \"TL;DR: Add them in the on_config event.\" To add new files to the repository you will need two phases: Create the markdown article pages. Add them to the navigation. My first idea as a MkDocs user, and newborn plugin developer was to add the navigation items to the nav key in the config object, as it's more easy to add items to a dictionary I'm used to work with than to dive into the code and understand how MkDocs creates the navigation. As I understood from the docs, the files should be created in the on_files event. the problem with this approach is that the only event that allows you to change the config is the on_config event, which is before the on_files one, so you can't build the navigation this way after you've created the files. Next idea was to add the items in the on_nav event, that means creating yourself the Section , Pages , SectionPages or Link objects and append them to the nav.items . The problem is that MkDocs initializes and processes the Navigation object in the get_navigation function. If you want to add items with a plugin in the on_nav event, you need to manually run all the post processing functions such as building the pages attribute, by running the _get_by_type , _add_previous_and_next_links or _add_parent_links yourself. Additionally, when building the site you'll get the The following pages exist in the docs directory, but are not included in the \"nav\" configuration error, because that check is done before all plugins change the navigation in the on_nav object. The last approach is to build the files and tweak the navigation in the on_config event. This approach has the next advantages: You need less knowledge of how MkDocs works. You don't need to create the File or Files objects. You don't need to create the Page , Section , SectionPage objects. More robust as you rely on existent MkDocs functionality. Testing \u2691 I haven't found any official documentation on how to test MkDocs plugins, in the issues they suggest you look at how they test it in the search plugin . I've looked at other plugins such as mkdocs_blog and used the next way to test mkdocs-newsletter . I see the plugin definition as an entrypoint to the functionality of our program, that's why I feel the definition should be in src/mkdocs_newsletter/entrypoints/mkdocs_plugin.py . As any entrypoint, the best way to test them are in end-to-end tests. You need to have a working test site in tests/assets/test_data , with it's mkdocs.yml file that loads your plugin and some fake articles. To prepare the test we can define the next fixture that prepares the building of the site: File: tests/conftest.py : import os import shutil from mkdocs import config from mkdocs.config.base import Config @pytest . fixture ( name = \"config\" ) def config_ ( tmp_path : Path ) -> Config : \"\"\"Load the mkdocs configuration.\"\"\" repo_path = tmp_path / \"test_data\" shutil . copytree ( \"tests/assets/test_data\" , repo_path ) mkdocs_config = config . load_config ( os . path . join ( repo_path , \"mkdocs.yml\" )) mkdocs_config [ \"site_dir\" ] = os . path . join ( repo_path , \"site\" ) return mkdocs_config It does the next steps: Copy the fake MkDocs site to a temporal directory Prepare the MkDocs Config object to build the site. Now we can use it in the e2e tests: File: tests/e2e/test_plugin.py : def test_plugin_builds_newsletters ( full_repo : Repo , config : Config ) -> None : build . build ( config ) # act newsletter_path = f \" { full_repo . working_dir } /site/newsletter/2021_02/index.html\" with open ( newsletter_path , \"r\" ) as newsletter_file : newsletter = newsletter_file . read () assert \"<title>February of 2021 - The Blue Book</title>\" in newsletter That test is meant to ensure that our plugin works with the MkDocs ecosystem, so the assertions should be done against the created html files. If your functionality can't be covered by the happy path of the end-to-end test, it's better to create unit tests to make sure that they work as you want. You can see a full example here . Issues \u2691 Once they are closed: Mkdocs Deprecation warning , once it's solved remove the warning filter on mkdocs-newsletter pyproject.toml . Mkdocs-Material Deprecation warning , once it's solved remove the warning filter on mkdocs-newsletter pyproject.toml . References \u2691 Git Homepage . Material theme configuration guide Plugin development \u2691 User guide List of events Plugin testing example", "title": "mkdocs"}, {"location": "linux/mkdocs/#installation", "text": "Install the basic packages. pip install \\ mkdocs \\ mkdocs-material \\ mkdocs-autolink-plugin \\ mkdocs-minify-plugin \\ pymdown-extensions \\ mkdocs-git-revision-date-localized-plugin Create the docs repository. mkdocs new docs Although there are several themes , I usually use the material one. I won't dive into the different options, just show a working template of the mkdocs.yaml file. site_name : {{ site_name : null }: null } site_author : {{ your_name : null }: null } site_url : {{ site_url : null }: null } nav : - Introduction : index.md - Basic Usage : basic_usage.md - Configuration : configuration.md - Update : update.md - Advanced Usage : - Projects : projects.md - Tags : tags.md plugins : - search - autolinks - git-revision-date-localized : type : timeago - minify : minify_html : true markdown_extensions : - admonition - meta - toc : permalink : true baselevel : 2 - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !%21python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde theme : name : material custom_dir : theme logo : images/logo.png palette : primary : blue grey accent : light blue extra_css : - stylesheets/extra.css - stylesheets/links.css repo_name : {{ repository_name : null }: null } # for example: 'lyz-code/pydo' repo_url : {{ repository_url : null }: null } # for example: 'https://github.com/lyz-code/pydo' Configure your logo by saving it into docs/images/logo.png . I like to show a small image above each link so you know where is it pointing to. To do so add the content of this directory to theme . and these files under docs/stylesheets . Initialize the git repository and create the first commit. Start the server to see everything is alright. mkdocs serve", "title": "Installation"}, {"location": "linux/mkdocs/#material-theme-customizations", "text": "", "title": "Material theme customizations"}, {"location": "linux/mkdocs/#color-palette-toggle", "text": "Since 7.1.0, you can have a light-dark mode on the site using a toggle in the upper bar. To enable it add to your mkdocs.yml : theme : palette : # Light mode - media : '(prefers-color-scheme: light)' scheme : default primary : blue grey accent : light blue toggle : icon : material/toggle-switch-off-outline name : Switch to dark mode # Dark mode - media : '(prefers-color-scheme: dark)' scheme : slate primary : blue grey accent : light blue toggle : icon : material/toggle-switch name : Switch to light mode Changing your desired colors for each mode", "title": "Color palette toggle"}, {"location": "linux/mkdocs/#back-to-top-button", "text": "Since 7.1.0, a back-to-top button can be shown when the user, after scrolling down, starts to scroll up again. It's rendered in the lower right corner of the viewport. Add the following lines to mkdocs.yml: theme : features : - navigation.top", "title": "Back to top button"}, {"location": "linux/mkdocs/#add-a-github-pages-hook", "text": "Save your requirements.txt . pip freeze > requirements.txt Create the .github/workflows/gh-pages.yml file with the following contents. name : Github pages on : push : branches : - master jobs : deploy : runs-on : ubuntu-18.04 steps : - uses : actions/checkout@v2 with : # Number of commits to fetch. 0 indicates all history. # Default: 1 fetch-depth : 0 - name : Setup Python uses : actions/setup-python@v1 with : python-version : '3.7' architecture : x64 - name : Cache dependencies uses : actions/cache@v1 with : path : ~/.cache/pip key : ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }} restore-keys : | ${{ runner.os }}-pip- - name : Install dependencies run : | python3 -m pip install --upgrade pip python3 -m pip install -r ./requirements.txt - run : | cd docs mkdocs build - name : Deploy uses : peaceiris/actions-gh-pages@v3 with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir : ./docs/site Create an SSH deploy key Activate GitHub Pages repository configuration with gh-pages branch . Make a new commit and push to check it's working.", "title": "Add a github pages hook."}, {"location": "linux/mkdocs/#create-mermaidjs-diagrams", "text": "Even though the Material theme supports mermaid diagrams it's only giving it for the paid users. The funding needs to reach 5000$ so it's released to the general public. The alternative is to use the mkdocs-mermaid2-plugin plugin, which can't be used with mkdocs-minify-plugin and doesn't adapt to dark mode. To install it : Download the package: pip install mkdocs-mermaid2-plugin . Enable the plugin in mkdocs.yml . plugins : # Not compatible with mermaid2 # - minify: # minify_html: true - mermaid2 : arguments : securityLevel : loose markdown_extensions : - pymdownx.superfences : # make exceptions to highlighting of code: custom_fences : - name : mermaid class : mermaid format : !%21python/name:mermaid2.fence_mermaid Check the MermaidJS article to see how to create the diagrams.", "title": "Create MermaidJS diagrams"}, {"location": "linux/mkdocs/#plugin-development", "text": "Like MkDocs, plugins must be written in Python. It is expected that each plugin would be distributed as a separate Python module. At a minimum, a MkDocs Plugin must consist of a BasePlugin subclass and an entry point which points to it. The BasePlugin class is meant to have on_<event_name> methods that run actions on the MkDocs defined events . The same object is called at the different events, so you can save objects from one event to the other in the object attributes. Keep in mind that the order of execution of the plugins follows the ordering of the list of the mkdocs.yml file where they are defined.", "title": "Plugin development"}, {"location": "linux/mkdocs/#interesting-objects", "text": "", "title": "Interesting objects"}, {"location": "linux/mkdocs/#files", "text": "mkdocs.structure.files.Files contains a list of File objects under the ._files attribute and allows you to append files to the collection. As well as extracting the different file types: documentation_pages : Iterable of markdown page file objects. static_pages : Iterable of static page file objects. media_files : Iterable of all files that are not documentation or static pages. javascript_files : Iterable of javascript files. css_files : Iterable of css files. It is initialized with a list of File objects.", "title": "Files"}, {"location": "linux/mkdocs/#file", "text": "mkdocs.structure.files.File objects points to the source and destination locations of a file. It has the following interesting attributes: name : Name of the file without the extension. src_path or abs_src_path : Relative or absolute path to the original path, for example the markdown file. dest_path or abs_dest_path : Relative or absolute path to the destination path, for example the html file generated from the markdown one. url : Url where the file is going to be exposed. It is initialized with the arguments: path : Must be a path that exists relative to src_dir . src_dir : Absolute path on the local file system to the directory where the docs are. dest_dir : Absolute path on the local file system to the directory where the site is going to be built. use_directory_urls : If False , a Markdown file is mapped to an HTML file of the same name (the file extension is changed to .html ). If True, a Markdown file is mapped to an HTML index file ( index.html ) nested in a directory using the \"name\" of the file in path . The use_directory_urls argument has no effect on non-Markdown files. By default MkDocs uses True .", "title": "File"}, {"location": "linux/mkdocs/#navigation", "text": "mkdocs.structure.nav.Navigation objects hold the information to build the navigation of the site. It has the following interesting attributes: items : Nested List with full navigation of Sections, SectionPages, Pages, and Links. pages : Flat List of subset of Pages in nav, in order. The Navigation object has no __eq__ method, so when testing, instead of trying to build a similar Navigation object and compare them, you need to assert that the contents of the object are what you expect.", "title": "Navigation"}, {"location": "linux/mkdocs/#page", "text": "mkdocs.structure.pages.Page models each page of the site. To initialize it you need the title , the File object of the page, and the MkDocs config object.", "title": "Page"}, {"location": "linux/mkdocs/#section", "text": "mkdocs.structure.nav.Section object models a section of the navigation of a MkDocs site. To initialize it you need the title of the section and the children which are the elements that belong to the section. If you don't yet know the children, pass an empty list [] .", "title": "Section"}, {"location": "linux/mkdocs/#sectionpage", "text": "mkdocs_section_index.SectionPage , part of the mkdocs-section-index plugin, models Section objects that have an associated Page , allowing you to have nav sections that when clicked, load the Page and not only opens the menu for the children elements. To initialize it you need the title of the section, the File object of the page, , the MkDocs config object, and the children which are the elements that belong to the section. If you don't yet know the children, pass an empty list [] .", "title": "SectionPage"}, {"location": "linux/mkdocs/#events", "text": "", "title": "Events"}, {"location": "linux/mkdocs/#on_config", "text": "The config event is the first event called on build and is run immediately after the user configuration is loaded and validated. Any alterations to the config should be made here. Parameters: config : global configuration object Returns: global configuration object", "title": "on_config"}, {"location": "linux/mkdocs/#on_files", "text": "The files event is called after the files collection is populated from the docs_dir . Use this event to add, remove, or alter files in the collection. Note that Page objects have not yet been associated with the file objects in the collection. Use Page Events to manipulate page specific data. Parameters: files : global files collection config : global configuration object Returns: global files collection", "title": "on_files"}, {"location": "linux/mkdocs/#on_nav", "text": "The nav event is called after the site navigation is created and can be used to alter the site navigation. Warning: Read the following section if you want to add new files . Parameters: nav : global navigation object . config : global configuration object. files : global files collection . Returns: global navigation object", "title": "on_nav"}, {"location": "linux/mkdocs/#adding-new-files", "text": "Note: \"TL;DR: Add them in the on_config event.\" To add new files to the repository you will need two phases: Create the markdown article pages. Add them to the navigation. My first idea as a MkDocs user, and newborn plugin developer was to add the navigation items to the nav key in the config object, as it's more easy to add items to a dictionary I'm used to work with than to dive into the code and understand how MkDocs creates the navigation. As I understood from the docs, the files should be created in the on_files event. the problem with this approach is that the only event that allows you to change the config is the on_config event, which is before the on_files one, so you can't build the navigation this way after you've created the files. Next idea was to add the items in the on_nav event, that means creating yourself the Section , Pages , SectionPages or Link objects and append them to the nav.items . The problem is that MkDocs initializes and processes the Navigation object in the get_navigation function. If you want to add items with a plugin in the on_nav event, you need to manually run all the post processing functions such as building the pages attribute, by running the _get_by_type , _add_previous_and_next_links or _add_parent_links yourself. Additionally, when building the site you'll get the The following pages exist in the docs directory, but are not included in the \"nav\" configuration error, because that check is done before all plugins change the navigation in the on_nav object. The last approach is to build the files and tweak the navigation in the on_config event. This approach has the next advantages: You need less knowledge of how MkDocs works. You don't need to create the File or Files objects. You don't need to create the Page , Section , SectionPage objects. More robust as you rely on existent MkDocs functionality.", "title": "Adding new files"}, {"location": "linux/mkdocs/#testing", "text": "I haven't found any official documentation on how to test MkDocs plugins, in the issues they suggest you look at how they test it in the search plugin . I've looked at other plugins such as mkdocs_blog and used the next way to test mkdocs-newsletter . I see the plugin definition as an entrypoint to the functionality of our program, that's why I feel the definition should be in src/mkdocs_newsletter/entrypoints/mkdocs_plugin.py . As any entrypoint, the best way to test them are in end-to-end tests. You need to have a working test site in tests/assets/test_data , with it's mkdocs.yml file that loads your plugin and some fake articles. To prepare the test we can define the next fixture that prepares the building of the site: File: tests/conftest.py : import os import shutil from mkdocs import config from mkdocs.config.base import Config @pytest . fixture ( name = \"config\" ) def config_ ( tmp_path : Path ) -> Config : \"\"\"Load the mkdocs configuration.\"\"\" repo_path = tmp_path / \"test_data\" shutil . copytree ( \"tests/assets/test_data\" , repo_path ) mkdocs_config = config . load_config ( os . path . join ( repo_path , \"mkdocs.yml\" )) mkdocs_config [ \"site_dir\" ] = os . path . join ( repo_path , \"site\" ) return mkdocs_config It does the next steps: Copy the fake MkDocs site to a temporal directory Prepare the MkDocs Config object to build the site. Now we can use it in the e2e tests: File: tests/e2e/test_plugin.py : def test_plugin_builds_newsletters ( full_repo : Repo , config : Config ) -> None : build . build ( config ) # act newsletter_path = f \" { full_repo . working_dir } /site/newsletter/2021_02/index.html\" with open ( newsletter_path , \"r\" ) as newsletter_file : newsletter = newsletter_file . read () assert \"<title>February of 2021 - The Blue Book</title>\" in newsletter That test is meant to ensure that our plugin works with the MkDocs ecosystem, so the assertions should be done against the created html files. If your functionality can't be covered by the happy path of the end-to-end test, it's better to create unit tests to make sure that they work as you want. You can see a full example here .", "title": "Testing"}, {"location": "linux/mkdocs/#issues", "text": "Once they are closed: Mkdocs Deprecation warning , once it's solved remove the warning filter on mkdocs-newsletter pyproject.toml . Mkdocs-Material Deprecation warning , once it's solved remove the warning filter on mkdocs-newsletter pyproject.toml .", "title": "Issues"}, {"location": "linux/mkdocs/#references", "text": "Git Homepage . Material theme configuration guide", "title": "References"}, {"location": "linux/mkdocs/#plugin-development_1", "text": "User guide List of events Plugin testing example", "title": "Plugin development"}, {"location": "linux/monica/", "text": "Monica is an open-source web application to organize the interactions with your loved ones. They call it a PRM, or Personal Relationship Management. Think of it as a CRM (a popular tool used by sales teams in the corporate world) for your friends or family. Monica allows people to keep track of everything that's important about their friends and family. Like the activities done with them. When you last called someone. What you talked about. It will help you remember the name and the age of the kids. It can also remind you to call someone you haven't talked to in a while. They have pricing plans for their hosted service, but the self-hosted solution has all the features. It also has a nice API to interact with. Install \u2691 They provide a very throughout documented Docker installation . If you just want to test it, use this docker compose File: docker-compose.yml version: \"3.4\" services: app: image: monicahq/monicahq depends_on: - db ports: - 8080:80 environment: # generate with `pwgen -s 32 1` for instance: - APP_KEY=DoKMvhGu795QcMBP1I5sw8uk85MMAPS9 - DB_HOST=db volumes: - data:/var/www/monica/storage restart: always db: image: mysql:5.7 environment: - MYSQL_RANDOM_ROOT_PASSWORD=true - MYSQL_DATABASE=monica - MYSQL_USER=homestead - MYSQL_PASSWORD=secret volumes: - mysql:/var/lib/mysql restart: always volumes: data: name: data mysql: name: mysql Once you install your own, you may want to: Change the APP_KEY Change the database credentials. In the application docker are loaded as DB_USERNAME , DB_HOST and DB_PASSWORD . Set up the environment and the application url with APP_ENV=production and APP_URL . Set up the email configuration MAIL_MAILER : smtp MAIL_HOST : smtp.service.com # ex: smtp.sendgrid.net MAIL_PORT : 587 # is using tls, as you should MAIL_USERNAME : my_service_username # ex: apikey MAIL_PASSWORD : my_service_password # ex: SG.Psuoc6NZTrGHAF9fdsgsdgsbvjQ.JuxNWVYmJ8LE0 MAIL_ENCRYPTION : tls MAIL_FROM_ADDRESS : no-reply@xxx.com # ex: email you want the email to be FROM MAIL_FROM_NAME : Monica # ex: name of the sender Here is an example of all the possible configurations. They also share other configuration examples where you can take ideas of alternate setups. If you don't want to use docker, check the other installation documentation . References \u2691 Homepage Git Docs Blog", "title": "monica"}, {"location": "linux/monica/#install", "text": "They provide a very throughout documented Docker installation . If you just want to test it, use this docker compose File: docker-compose.yml version: \"3.4\" services: app: image: monicahq/monicahq depends_on: - db ports: - 8080:80 environment: # generate with `pwgen -s 32 1` for instance: - APP_KEY=DoKMvhGu795QcMBP1I5sw8uk85MMAPS9 - DB_HOST=db volumes: - data:/var/www/monica/storage restart: always db: image: mysql:5.7 environment: - MYSQL_RANDOM_ROOT_PASSWORD=true - MYSQL_DATABASE=monica - MYSQL_USER=homestead - MYSQL_PASSWORD=secret volumes: - mysql:/var/lib/mysql restart: always volumes: data: name: data mysql: name: mysql Once you install your own, you may want to: Change the APP_KEY Change the database credentials. In the application docker are loaded as DB_USERNAME , DB_HOST and DB_PASSWORD . Set up the environment and the application url with APP_ENV=production and APP_URL . Set up the email configuration MAIL_MAILER : smtp MAIL_HOST : smtp.service.com # ex: smtp.sendgrid.net MAIL_PORT : 587 # is using tls, as you should MAIL_USERNAME : my_service_username # ex: apikey MAIL_PASSWORD : my_service_password # ex: SG.Psuoc6NZTrGHAF9fdsgsdgsbvjQ.JuxNWVYmJ8LE0 MAIL_ENCRYPTION : tls MAIL_FROM_ADDRESS : no-reply@xxx.com # ex: email you want the email to be FROM MAIL_FROM_NAME : Monica # ex: name of the sender Here is an example of all the possible configurations. They also share other configuration examples where you can take ideas of alternate setups. If you don't want to use docker, check the other installation documentation .", "title": "Install"}, {"location": "linux/monica/#references", "text": "Homepage Git Docs Blog", "title": "References"}, {"location": "linux/nodejs/", "text": "Node.js is a JavaScript runtime built on Chrome's V8 JavaScript engine. Install \u2691 The debian base repositories are really outdated, so add the NodeSource repository curl -fsSL https://deb.nodesource.com/setup_16.x | bash - apt-get install -y nodejs npm nodejs --version Links \u2691 Home", "title": "nodejs"}, {"location": "linux/nodejs/#install", "text": "The debian base repositories are really outdated, so add the NodeSource repository curl -fsSL https://deb.nodesource.com/setup_16.x | bash - apt-get install -y nodejs npm nodejs --version", "title": "Install"}, {"location": "linux/nodejs/#links", "text": "Home", "title": "Links"}, {"location": "linux/rm/", "text": "rm definition In computing, rm (short for remove) is a basic command on Unix and Unix-like operating systems used to remove objects such as computer files, directories and symbolic links from file systems and also special files such as device nodes, pipes and sockets Debugging \u2691 Cannot remove file: \u201cStructure needs cleaning\u201d \u2691 From Victoria Stuart and Depressed Daniel answer You first need to: Umount the partition. Do a sector level backup of your disk. If your filesystem is ext4 run: fsck.ext4 {{ device }} Accept all suggested fixes. Mount again the partition.", "title": "rm"}, {"location": "linux/rm/#debugging", "text": "", "title": "Debugging"}, {"location": "linux/rm/#cannot-remove-file-structure-needs-cleaning", "text": "From Victoria Stuart and Depressed Daniel answer You first need to: Umount the partition. Do a sector level backup of your disk. If your filesystem is ext4 run: fsck.ext4 {{ device }} Accept all suggested fixes. Mount again the partition.", "title": "Cannot remove file: \u201cStructure needs cleaning\u201d"}, {"location": "linux/syncthing/", "text": "Syncthing is a continuous file synchronization program. It synchronizes files between two or more computers in real time, safely protected from prying eyes. Your data is your data alone and you deserve to choose where it is stored, whether it is shared with some third party, and how it's transmitted over the internet. Installation \u2691 Debian or Ubuntu \u2691 # Add the release PGP keys: curl -s https://syncthing.net/release-key.txt | sudo apt-key add - # Add the \"stable\" channel to your APT sources: echo \"deb https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list # Update and install syncthing: sudo apt-get update sudo apt-get install syncthing Docker \u2691 Use Linuxserver Docker Configuration \u2691 If you're only going to use syncthing in an internal network, or you're going to fix the IPs of the devices you can disable the Global Discovery and Relaying connections so that you don't leak the existence of your services to the syncthing servers. Troubleshooting \u2691 Syncthing over Tor \u2691 There are many posts on this topic ( 1 , 2 ) but I wasn't able to connect two clients through Tor. Here are the steps I took in case anyone is interested. If you make it work, please contact me. Suggest to use a relay , go to relays.syncthing.net to see the public ones. You need to add the required servers to the Sync Protocol Listen Address field, under Actions and Settings . The syntax is: relay://<host name|IP>[:port]/?id=<relay device ID> The only way I've found to get the relay device ID is setting a fake one, and getting the correct one from the logs of syncthing. It will say that the fingerprint ( what you put ) doesn't match ( actual fingerprint ) . Steps \u2691 Configure the client: export all_proxy = socks5://127.0.0.1:9058 export ALL_PROXY_NO_FALLBACK = 1 syncthing --home /tmp/syncthing_1 Allow the connection to the local server: sudo iptables -I OUTPUT -o lo -p tcp --dport 8384 -j ACCEPT If you're using Tails and Tor Browser, you'll need to set the about:config setting network.proxy.allow_hijacking_localhost to false . Otherwise you won't be able to access the user interface. Issues \u2691 Wifi run condition needs location to be turned on : update and check that you no longer need it. Links \u2691 Home Getting Started", "title": "Syncthing"}, {"location": "linux/syncthing/#installation", "text": "", "title": "Installation"}, {"location": "linux/syncthing/#debian-or-ubuntu", "text": "# Add the release PGP keys: curl -s https://syncthing.net/release-key.txt | sudo apt-key add - # Add the \"stable\" channel to your APT sources: echo \"deb https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list # Update and install syncthing: sudo apt-get update sudo apt-get install syncthing", "title": "Debian or Ubuntu"}, {"location": "linux/syncthing/#docker", "text": "Use Linuxserver Docker", "title": "Docker"}, {"location": "linux/syncthing/#configuration", "text": "If you're only going to use syncthing in an internal network, or you're going to fix the IPs of the devices you can disable the Global Discovery and Relaying connections so that you don't leak the existence of your services to the syncthing servers.", "title": "Configuration"}, {"location": "linux/syncthing/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "linux/syncthing/#syncthing-over-tor", "text": "There are many posts on this topic ( 1 , 2 ) but I wasn't able to connect two clients through Tor. Here are the steps I took in case anyone is interested. If you make it work, please contact me. Suggest to use a relay , go to relays.syncthing.net to see the public ones. You need to add the required servers to the Sync Protocol Listen Address field, under Actions and Settings . The syntax is: relay://<host name|IP>[:port]/?id=<relay device ID> The only way I've found to get the relay device ID is setting a fake one, and getting the correct one from the logs of syncthing. It will say that the fingerprint ( what you put ) doesn't match ( actual fingerprint ) .", "title": "Syncthing over Tor"}, {"location": "linux/syncthing/#steps", "text": "Configure the client: export all_proxy = socks5://127.0.0.1:9058 export ALL_PROXY_NO_FALLBACK = 1 syncthing --home /tmp/syncthing_1 Allow the connection to the local server: sudo iptables -I OUTPUT -o lo -p tcp --dport 8384 -j ACCEPT If you're using Tails and Tor Browser, you'll need to set the about:config setting network.proxy.allow_hijacking_localhost to false . Otherwise you won't be able to access the user interface.", "title": "Steps"}, {"location": "linux/syncthing/#issues", "text": "Wifi run condition needs location to be turned on : update and check that you no longer need it.", "title": "Issues"}, {"location": "linux/syncthing/#links", "text": "Home Getting Started", "title": "Links"}, {"location": "linux/wireguard/", "text": "Wireguard is an simple yet fast and modern VPN that utilizes state-of-the-art cryptography. It aims to be faster, simpler, leaner, and more useful than IPsec, while avoiding the massive headache. It intends to be considerably more performant than OpenVPN. WireGuard is a general purpose VPN for running on embedded interfaces and super computers alike. Initially released for the Linux kernel, it's now cross-platform (Windows, macOS, BSD, iOS, Android) and widely deployable. Although it's under heavy development, it already might be the most secure, easiest to use, and simplest VPN solution in the industry. Features: Simple and easy to use: WireGuard aims to be as easy to configure and deploy as SSH. A VPN connection is made by exchanging public keys \u2013 exactly like exchanging SSH keys \u2013 and all the rest is transparently handled by WireGuard. It's even capable of roaming between IP addresses, like Mosh. There is no need to manage connections, worry about state, manage daemons, or worry about what's under the hood. WireGuard presents a basic yet powerful interface. Cryptographically Sound: WireGuard uses state-of-the-art cryptography, such as the Noise protocol framework, Curve25519, ChaCha20, Poly1305, BLAKE2, SipHash24, HKDF, and secure trusted constructions. It makes conservative and reasonable choices and has been reviewed by cryptographers. Minimal Attack Surface: WireGuard is designed with ease-of-implementation and simplicity in mind. It's meant to be implemented in very few lines of code, and auditable for security vulnerabilities. Compared to behemoths like *Swan/IPsec or OpenVPN/OpenSSL, in which auditing the gigantic codebases is an overwhelming task even for large teams of security experts, WireGuard is meant to be comprehensively reviewable by single individuals. High Performance: A combination of extremely high-speed cryptographic primitives and the fact that WireGuard lives inside the Linux kernel means that secure networking can be very high-speed. It is suitable for both small embedded devices like smartphones and fully loaded backbone routers. Well Defined & Thoroughly Considered: WireGuard is the result of a lengthy and thoroughly considered academic process, resulting in the technical whitepaper, an academic research paper which clearly defines the protocol and the intense considerations that went into each decision. Plus it's created by the same guy as pass , which uses Gentoo, I like this guy. Conceptual Overview \u2691 WireGuard securely encapsulates IP packets over UDP. You add a WireGuard interface, configure it with your private key and your peers' public keys, and then you send packets across it. All issues of key distribution and pushed configurations are out of scope of WireGuard; these are issues much better left for other layers. It mimics the model of SSH and Mosh; both parties have each other's public keys, and then they're simply able to begin exchanging packets through the interface. Simple Network Interface \u2691 WireGuard works by adding network interfaces, called wg0 (or wg1, wg2, wg3, etc). This network interface can then be configured normally using the ordinary networking utilities. The specific WireGuard aspects of the interface are configured using the wg tool. This interface acts as a tunnel interface. WireGuard associates tunnel IP addresses with public keys and remote endpoints. When the interface sends a packet to a peer, it does the following: This packet is meant for 192.168.30.8. Which peer is that? Let me look... Okay, it's for peer ABCDEFGH. (Or if it's not for any configured peer, drop the packet.) Encrypt entire IP packet using peer ABCDEFGH's public key. What is the remote endpoint of peer ABCDEFGH? Let me look... Okay, the endpoint is UDP port 53133 on host 216.58.211.110. Send encrypted bytes from step 2 over the Internet to 216.58.211.110:53133 using UDP. When the interface receives a packet, this happens: I just got a packet from UDP port 7361 on host 98.139.183.24. Let's decrypt it! It decrypted and authenticated properly for peer LMNOPQRS. Okay, let's remember that peer LMNOPQRS's most recent Internet endpoint is 98.139.183.24:7361 using UDP. Once decrypted, the plain-text packet is from 192.168.43.89. Is peer LMNOPQRS allowed to be sending us packets as 192.168.43.89? If so, accept the packet on the interface. If not, drop it. Behind the scenes there is much happening to provide proper privacy, authenticity, and perfect forward secrecy, using state-of-the-art cryptography. Cryptokey Routing \u2691 At the heart of WireGuard is a concept called Cryptokey Routing, which works by associating public keys with a list of tunnel IP addresses that are allowed inside the tunnel. Each network interface has a private key and a list of peers. Each peer has a public key. For example, a server computer might have this configuration: [Interface] PrivateKey = yAnz5TF+lXXJte14tji3zlMNq+hd2rYUIgJBgB3fBmk= ListenPort = 51820 [Peer] PublicKey = xTIBA5rboUvnH4htodjb6e697QjLERt1NAB4mZqp8Dg= AllowedIPs = 10.192.122.3/32, 10.192.124.1/24 [Peer] PublicKey = TrMvSoP4jYQlY6RIzBgbssQqY3vxI2Pi+y71lOWWXX0= AllowedIPs = 10.192.122.4/32, 192.168.0.0/16 [Peer] PublicKey = gN65BkIKy1eCE9pP1wdc8ROUtkHLF2PfAqYdyYBz6EA= AllowedIPs = 10.10.10.230/32 And a client computer might have this simpler configuration: [Interface] PrivateKey = gI6EdUSYvn8ugXOt8QQD6Yc+JyiZxIhp3GInSWRfWGE= ListenPort = 21841 [Peer] PublicKey = HIgo9xNzJMWLKASShiTqIybxZ0U3wGLiUeJ1PKf8ykw= Endpoint = 192.95.5.69:51820 AllowedIPs = 0.0.0.0/0 In the server configuration, each peer (a client) will be able to send packets to the network interface with a source IP matching his corresponding list of allowed IPs. For example, when a packet is received by the server from peer gN65BkIK..., after being decrypted and authenticated, if its source IP is 10.10.10.230, then it's allowed onto the interface; otherwise it's dropped. In the server configuration, when the network interface wants to send a packet to a peer (a client), it looks at that packet's destination IP and compares it to each peer's list of allowed IPs to see which peer to send it to. For example, if the network interface is asked to send a packet with a destination IP of 10.10.10.230, it will encrypt it using the public key of peer gN65BkIK..., and then send it to that peer's most recent Internet endpoint. In the client configuration, its single peer (the server) will be able to send packets to the network interface with any source IP (since 0.0.0.0/0 is a wildcard). For example, when a packet is received from peer HIgo9xNz..., if it decrypts and authenticates correctly, with any source IP, then it's allowed onto the interface; otherwise it's dropped. In the client configuration, when the network interface wants to send a packet to its single peer (the server), it will encrypt packets for the single peer with any destination IP address (since 0.0.0.0/0 is a wildcard). For example, if the network interface is asked to send a packet with any destination IP, it will encrypt it using the public key of the single peer HIgo9xNz..., and then send it to the single peer's most recent Internet endpoint. In other words, when sending packets, the list of allowed IPs behaves as a sort of routing table, and when receiving packets, the list of allowed IPs behaves as a sort of access control list. This is what we call a Cryptokey Routing Table: the simple association of public keys and allowed IPs. Because all packets sent on the WireGuard interface are encrypted and authenticated, and because there is such a tight coupling between the identity of a peer and the allowed IP address of a peer, system administrators do not need complicated firewall extensions, such as in the case of IPsec, but rather they can simply match on \"is it from this IP? on this interface?\", and be assured that it is a secure and authentic packet. This greatly simplifies network management and access control, and provides a great deal more assurance that your iptables rules are actually doing what you intended for them to do. Built-in Roaming \u2691 The client configuration contains an initial endpoint of its single peer (the server), so that it knows where to send encrypted data before it has received encrypted data. The server configuration doesn't have any initial endpoints of its peers (the clients). This is because the server discovers the endpoint of its peers by examining from where correctly authenticated data originates. If the server itself changes its own endpoint, and sends data to the clients, the clients will discover the new server endpoint and update the configuration just the same. Both client and server send encrypted data to the most recent IP endpoint for which they authentically decrypted data. Thus, there is full IP roaming on both ends.", "title": "Wireguard"}, {"location": "linux/wireguard/#conceptual-overview", "text": "WireGuard securely encapsulates IP packets over UDP. You add a WireGuard interface, configure it with your private key and your peers' public keys, and then you send packets across it. All issues of key distribution and pushed configurations are out of scope of WireGuard; these are issues much better left for other layers. It mimics the model of SSH and Mosh; both parties have each other's public keys, and then they're simply able to begin exchanging packets through the interface.", "title": "Conceptual Overview"}, {"location": "linux/wireguard/#simple-network-interface", "text": "WireGuard works by adding network interfaces, called wg0 (or wg1, wg2, wg3, etc). This network interface can then be configured normally using the ordinary networking utilities. The specific WireGuard aspects of the interface are configured using the wg tool. This interface acts as a tunnel interface. WireGuard associates tunnel IP addresses with public keys and remote endpoints. When the interface sends a packet to a peer, it does the following: This packet is meant for 192.168.30.8. Which peer is that? Let me look... Okay, it's for peer ABCDEFGH. (Or if it's not for any configured peer, drop the packet.) Encrypt entire IP packet using peer ABCDEFGH's public key. What is the remote endpoint of peer ABCDEFGH? Let me look... Okay, the endpoint is UDP port 53133 on host 216.58.211.110. Send encrypted bytes from step 2 over the Internet to 216.58.211.110:53133 using UDP. When the interface receives a packet, this happens: I just got a packet from UDP port 7361 on host 98.139.183.24. Let's decrypt it! It decrypted and authenticated properly for peer LMNOPQRS. Okay, let's remember that peer LMNOPQRS's most recent Internet endpoint is 98.139.183.24:7361 using UDP. Once decrypted, the plain-text packet is from 192.168.43.89. Is peer LMNOPQRS allowed to be sending us packets as 192.168.43.89? If so, accept the packet on the interface. If not, drop it. Behind the scenes there is much happening to provide proper privacy, authenticity, and perfect forward secrecy, using state-of-the-art cryptography.", "title": "Simple Network Interface"}, {"location": "linux/wireguard/#cryptokey-routing", "text": "At the heart of WireGuard is a concept called Cryptokey Routing, which works by associating public keys with a list of tunnel IP addresses that are allowed inside the tunnel. Each network interface has a private key and a list of peers. Each peer has a public key. For example, a server computer might have this configuration: [Interface] PrivateKey = yAnz5TF+lXXJte14tji3zlMNq+hd2rYUIgJBgB3fBmk= ListenPort = 51820 [Peer] PublicKey = xTIBA5rboUvnH4htodjb6e697QjLERt1NAB4mZqp8Dg= AllowedIPs = 10.192.122.3/32, 10.192.124.1/24 [Peer] PublicKey = TrMvSoP4jYQlY6RIzBgbssQqY3vxI2Pi+y71lOWWXX0= AllowedIPs = 10.192.122.4/32, 192.168.0.0/16 [Peer] PublicKey = gN65BkIKy1eCE9pP1wdc8ROUtkHLF2PfAqYdyYBz6EA= AllowedIPs = 10.10.10.230/32 And a client computer might have this simpler configuration: [Interface] PrivateKey = gI6EdUSYvn8ugXOt8QQD6Yc+JyiZxIhp3GInSWRfWGE= ListenPort = 21841 [Peer] PublicKey = HIgo9xNzJMWLKASShiTqIybxZ0U3wGLiUeJ1PKf8ykw= Endpoint = 192.95.5.69:51820 AllowedIPs = 0.0.0.0/0 In the server configuration, each peer (a client) will be able to send packets to the network interface with a source IP matching his corresponding list of allowed IPs. For example, when a packet is received by the server from peer gN65BkIK..., after being decrypted and authenticated, if its source IP is 10.10.10.230, then it's allowed onto the interface; otherwise it's dropped. In the server configuration, when the network interface wants to send a packet to a peer (a client), it looks at that packet's destination IP and compares it to each peer's list of allowed IPs to see which peer to send it to. For example, if the network interface is asked to send a packet with a destination IP of 10.10.10.230, it will encrypt it using the public key of peer gN65BkIK..., and then send it to that peer's most recent Internet endpoint. In the client configuration, its single peer (the server) will be able to send packets to the network interface with any source IP (since 0.0.0.0/0 is a wildcard). For example, when a packet is received from peer HIgo9xNz..., if it decrypts and authenticates correctly, with any source IP, then it's allowed onto the interface; otherwise it's dropped. In the client configuration, when the network interface wants to send a packet to its single peer (the server), it will encrypt packets for the single peer with any destination IP address (since 0.0.0.0/0 is a wildcard). For example, if the network interface is asked to send a packet with any destination IP, it will encrypt it using the public key of the single peer HIgo9xNz..., and then send it to the single peer's most recent Internet endpoint. In other words, when sending packets, the list of allowed IPs behaves as a sort of routing table, and when receiving packets, the list of allowed IPs behaves as a sort of access control list. This is what we call a Cryptokey Routing Table: the simple association of public keys and allowed IPs. Because all packets sent on the WireGuard interface are encrypted and authenticated, and because there is such a tight coupling between the identity of a peer and the allowed IP address of a peer, system administrators do not need complicated firewall extensions, such as in the case of IPsec, but rather they can simply match on \"is it from this IP? on this interface?\", and be assured that it is a secure and authentic packet. This greatly simplifies network management and access control, and provides a great deal more assurance that your iptables rules are actually doing what you intended for them to do.", "title": "Cryptokey Routing"}, {"location": "linux/wireguard/#built-in-roaming", "text": "The client configuration contains an initial endpoint of its single peer (the server), so that it knows where to send encrypted data before it has received encrypted data. The server configuration doesn't have any initial endpoints of its peers (the clients). This is because the server discovers the endpoint of its peers by examining from where correctly authenticated data originates. If the server itself changes its own endpoint, and sends data to the clients, the clients will discover the new server endpoint and update the configuration just the same. Both client and server send encrypted data to the most recent IP endpoint for which they authentically decrypted data. Thus, there is full IP roaming on both ends.", "title": "Built-in Roaming"}, {"location": "linux/zfs/", "text": "OpenZFS is a file system with volume management capabilities designed specifically for storage servers. Some neat features of ZFS include: Aggregating multiple physical disks into a single filesystem. Automatically repairing data corruption. Creating point-in-time snapshots of data on disk. Optionally encrypting or compressing data on disk. Learning \u2691 I've found that learning about ZFS was an interesting, intense and time consuming task. If you want a quick overview check this video . If you prefer to read, head to the awesome Aaron Toponce articles and read all of them sequentially, each is a jewel. The docs on the other hand are not that pleasant to read. For further information check JRS articles . Storage planning \u2691 There are many variables that affect the number and type of disks, you first need to have an idea of what kind of data you want to store and what use are you going to give to that data. Robustness \u2691 ZFS is designed to survive disk failures, so it stores each block of data redundantly. This feature complicates capacity planning because your total usable storage is not just the sum of each disk\u2019s capacity. ZFS creates filesystems out of \u201cpools\u201d of disks. The more disks in the pool, the more efficiently ZFS can use their storage capacity. For example, if you give ZFS two 10 TB drives, you can only use half of your total disk capacity. If you instead use five 4 TB drives, ZFS gives you 14 TB of usable storage. Even though your total disk space is the same in either scenario, the five smaller drives give you 40% more usable space. When you\u2019re building a NAS server, you need to decide whether to use a smaller quantity of large disks or a larger quantity of small disks. Smaller drives are usually cheaper in terms of $/TB, but they\u2019re more expensive to operate. Two 4 TB drives require twice the electricity of a single 8 TB drive. Also keep in mind that so far ZFS doesn't let you add a new drive to an existing vdev, but that feature is under active development . If you want to be safe, plan your vdev definition so that they don't need to change the disk numbers. Preventing concurrent disk failures \u2691 Naively, the probability of two disks failing at once seems vanishingly small. Based on Backblaze\u2019s stats , high-quality disk drives fail at 0.5-4% per year. A 4% risk per year is a 0.08% chance in any given week. Two simultaneous failures would happen once every 30,000 years, so you should be fine, right? The problem is that disks aren\u2019t statistically independent. If one disk fails, its neighbor has a substantially higher risk of dying. This is especially true if the disks are the same model, from the same manufacturing batch, and processed the same workloads. Further, rebuilding a ZFS pool puts an unusual amount of strain on all of the surviving disks. A disk that would have lasted a few more months under normal usage might die under the additional load of a pool rebuild. Given these risks, you can reduce the risk of concurrent disk failures by choosing two different models of disk from two different manufacturers. To reduce the chances of getting disks from the same manufacturing batch, you can buy them from different vendors. Choosing the disks to hold data \u2691 Check diskprices.com to get an idea of the cost of disks in the market. If you can, try to avoid buying to Amazon as it's the devil. Try to buy them from a local store instead, that way you interact with a human and promote a local commerce. Note: If you want a TL;DR you can jump to the conclusion . To choose your disks take into account: Disk speed Disk load Disk type Disk homogeneity Disk Warranty Disk Brands Data disk speed \u2691 When comes to disk speed there are three kinds, the slow (5400 RPM), normal (7200 RPM) and fast (10k RPM). The higher the RPM, the louder the disk is, the more heat it creates and the more power it will consume. In exchange they will have higher writing and reading speeds. Slower disks expand the lifecycle of the device, but in the case of a failed disk in a RAID scenario, the rebuild time will be higher than on faster ones therefore increasing the risk on concurrent failing disks. Before choosing a high number of RPM make sure that it's your bottleneck, which usually is the network if you're using a 1Gbps network. In this case a 10k RPM disk won't offer better performance than a 7200 RPM, even a 7200 one won't be better than a 5400. The need of higher speeds can be fixed by using an SSD as a cache for reading and writing. Data disk load \u2691 Disk specifications tell you the amount of TB/year they support, it gives you an idea of the fault tolerance. Some examples Disk Fault tolerance (TB/year) WD RED 8TB 180 Data disk type \u2691 It\u2019s easy to think that all hard drives are equal, save for the form factor and connection type. However, there\u2019s a difference between the work your hard drive does in your computer versus the workload of a NAS hard drive. A drive in your computer may only read and write data for a couple hours at a time, while a NAS drive may read and write data for weeks on end, or even longer. The environment inside of a NAS box is much different than a typical desktop or laptop computer. When you pack in a handful of hard drives close together, several things happen: there\u2019s more vibration, more heat, and a lot more action going on in general. To cope with this, NAS hard drives usually have better vibration tolerance and produce less heat than regular hard drives, thanks to slightly-slower spindle speeds and reduced seek noise. Most popular brands are Western Digital Red and Seagate IronWolf which use 5400 RPM, if you want to go on the 7200 RPM speeds you can buy the Pro version of each. I initially tried checking Backblaze\u2019s hard drive stats to avoid failure-prone disks, but they use drives on the pricier side. The last pitfall to avoid is shingled magnetic recording (SMR) technology. ZFS performs poorly on SMR drives , so if you\u2019re building a NAS, avoid known SMR drives . If the drive is labeled as CMR, that\u2019s conventional magnetic recording, which is fine for ZFS. SMR is well suited for high-capacity, low-cost use where writes are few and reads are many. It has worse sustained write performance than CMR, which can cause severe issues during resilvering or other write-intensive operations. There are three types of SMR: Drive Managed, DM-SMR: It's opaque to the OS. This means ZFS cannot \"target\" writes, and is the worst type for ZFS use. As a rule of thumb, avoid DM-SMR drives, unless you have a specific use case where the increased resilver time (a week or longer) is acceptable, and you know the drive will function for ZFS during resilver. Host Aware, HA-SMR: It's designed to give ZFS insight into the SMR process. Note that ZFS code to use HA-SMR does not appear to exist. Without that code, a HA-SMR drive behaves like a DM-SMR drive where ZFS is concerned. Host Managed, HM-SMR: It's not backwards compatible and requires ZFS to manage the SMR process. Data disk homogeneity \u2691 It's recommended that all the disks in your pool (or is it by vdev?) have the same RPM and size. Data disk warranty \u2691 Disks are going to fail, so it's good to have a good warranty to return them. Data disk brands \u2691 Western Digital \u2691 The Western Digital Red series of NAS drives are very similar to Seagate\u2019s offering and you should consider these if you can find them at more affordable prices. WD splits its NAS drives into three sub-categories, normal, Plus, and Pro. Specs WD Red WD Red Plus WD Red Pro Technology SMR CMR CMR Bays 1-8 1-8 1-24 Capacity 2-6TB 1-14TB 2-18TB Speed 5,400 RPM 5,400 RPM (1-4TB) 7200 RPM Speed 5,400 RPM 5,640 RPM (6-8TB) 7200 RPM Speed 5,400 RPM 7,200 RPM (8-14TB) 7200 RPM Speed ? 210MB/s 235MB/s Cache 256MB 16MB (1TB) Cache 256MB 64MB (1TB) 64MB (2TB) Cache 256MB 128MB (2-8TB) 256MB (4-12TB) Cache 256MB 256MB (8-12TB) 512MB (14-18TB) Cache 256MB 512MB (14TB) Workload 180TB/yr 180TB/yr 300TB/yr MTBF 1 million 1 million 1 million Warranty 3 years 3 years 5 years Power Consumption ? ? 8.8 W Power Consumption Rest ? ? 4.6 W Price From $50 From $45 From $78 Seagate \u2691 Seagate's \"cheap\" NAS disks are the IronWolf gama, there are two variations IronWolf and IronWolf Pro . Seagate Exos is a premium series of drives from the company. They\u2019re even more advanced than IronWolf Pro and are best suited for server environments. They sport incredible levels of performance and reliability, including a workload rate of 550TB per year. Specs IronWolf IronWolf Pro Exos 7E8 8TB Exos 7E10 8TB Technology CMR CMR CMR SMR Bays 1-8 1-24 ? ? Capacity 1-12TB 2-20TB 8TB 8TB RPM 5,400 RPM (3-6TB) 7200 RPM 7200 RPM 7200 RPM RPM 5,900 RPM (1-3TB) 7200 RPM 7200 RPM 7200 RPM RPM 7,200 RPM (8-12TB) 7200 RPM 7200 RPM 7200 RPM Speed 180MB/s (1-12TB) 214-260MB/s (4-18TB) 249 MB/s 255 MB/s Cache 64MB (1-4TB) 256 MB 256 MB 256 MB Cache 256MB (3-12TB) 256 MB 256 MB 256 MB Power Consumption (8TB) 10.1 W 10.1 W 12.81 W 11.03 W Power Consumption Rest (8TB) 7.8 W 7.8 W 7.64 W 7.06 W Workload 180TB/yr 300TB/yr 550TB/yr 550TB/yr MTBF 1 million 1 million 2 millions 2 millions Warranty 3 years 5 years 5 years 5 years Price From $60 From $83 249$ 249$ Exos 7E10 is SMR so it's ruled out. Where MTBF stands for Medium Time Between Failures in hours Data disk conclusion \u2691 I'm more interested on the 5400 RPM drives, but of all the NAS disks available to purchase only the WD RED of 8TB use it, and they use the SMR technology, so they aren't a choice. The disk prices offered by my cheapest provider are: Disk Size Price Seagate IronWolf 8TB 225$ Seagate IronWolf Pro 8TB 254$ WD Red Plus 8TB 265$ Seagate Exos 7E8 8TB 277$ WD Red Pro 8TB 278$ WD Red Plus has 5,640 RPM which is different than the rest, so it's ruled out. Between the IronWolf and IronWolf Pro, they offer 180MB/s and 214MB/s respectively. The Seagate Exos 7E8 provides much better performance than the WD Red Pro so I'm afraid that WD is out of the question. There are three possibilities in order to have two different brands. Imagining we want 4 disks: Combination Total Price IronWolf + IronWolf Pro 958$ IronWolf + Exos 7E8 1004$ (+46$ +4.5%) IronWolf Pro + Exos 7E8 1062$ (+54$ +5.4%) In terms of: Consumption: both IronWolfs are equal, the Exos uses 2.7W more on normal use and uses 0.2W less on rest. Warranty: IronWolf has only 3 years, the others 5. Speed: Ironwolf has 210MB/s, much less than the Pro (255MB/s) and Exos (249MB/s), which are more similar. Sostenibility: The Exos disks are much more robust (more workload, MTBF and Warranty). I'd say that for 104$ it makes sense to go with the IronWolf Pro + Exos 7E8 combination. Choosing the disks for the cache \u2691 Using a ZLOG greatly improves the writing speed , equally using an SSD disk for the L2ARC cache improves the read speeds and improves the health of the rotational disks. The best M.2 NVMe SSD for NAS caching are the ones that have enough capacity to actually make a difference to overall system performance. It also requires a good endurance rating for better reliability and longer lifespan, and you should look for a drive with a specific NAND technology if possible. Note: If you want a TL;DR you can jump to the conclusion . To choose your disks take into account: Cache disk NAND technology DWPD Cache disk NAND technology \u2691 Not all flash-based storage drives are the same. NAND flash cells are usually categorised based on the number of bits that can be stored per cell. Watch out for the following terms when shopping around for an SSD: Single-Level Cell (SLC): one bit per cell. Multi-Level Cell (MLC): two bits per cell. Triple-Level Cell (TLC): three bits per cell. Quad-Level Cell (QLC): four bits per cell. When looking for the best M.2 NVMe SSD for NAS data caching, it\u2019s important to bear the NAND technology in mind. SLC is the best technology for SSDs that will be used for NAS caching. This does mean you\u2019re paying out more per GB and won\u2019t be able to select high-capacity drives, but reliability and the protection of stored data is the most important factor here. Another benefit of SLC is the lower impact of write amplification, which can quickly creep up and chomp through a drive\u2019s DWPD endurance rating. It\u2019s important to configure an SSD for caching correctly too regardless of which technology you pick. Doing so will lessen the likelihood of losing data through a drive hanging and causing the system to crash. Anything stored on the cache drive that has yet to be written to the main drive array would be lost. This is mostly a reported issue for NVMe drives, as opposed to SATA. DWPD \u2691 DWPD stands for drive writes per day. This is often used as a measurement of a drive\u2019s endurance. The higher this number, the more writes the drive can perform on a daily basis, as is rated by the manufacturer. For caching, especially which involves writing data, you\u2019ll want to aim for as high a DWPD rating as possible. Cache disk conclusion \u2691 Overall, I\u2019d recommend the Western Digital Red SN700, which has a good 1 DWPD endurance rating, is available in sizes up to 4TB, and is using SLC NAND technology, which is great for enhancing reliability through heavy caching workloads. A close second place goes to the Seagate IronWolf 525, which has similar specifications to the SN700 but utilizes TLC. Disk Size Speed Endurance Warranty Tech Price WD Red SN700 500 GB 3430MB/s 1 DWPD 5 years SLC 73$ SG IronWolf 525 500 GB 5000MB/s 0.8 DWPD 5 years TLC ? WD Red SN700 1 TB 3430MB/s 1 DWPD 5 years SLC 127$ SG IronWolf 525 1 TB 5000MB/s 0.8 DWPD 5 years TLC ? Choosing the cold spare disks \u2691 It's good to think how much time you want to have your raids to be inconsistent once a drive has failed. In my case, for the data I want to restore the raid as soon as I can, therefore I'll buy another rotational disk. For the SSDs I have more confidence that they won't break so I don't feel like having a spare one. Usage \u2691 Mount a pool as readonly \u2691 zpool import -o readonly = on {{ pool_name }} Mount a ZFS snapshot in a directory as readonly \u2691 mount -t zfs {{ pool_name }} / {{ snapshot_name }} {{ mount_path }} -o ro List volumes \u2691 zpool list List snapshots \u2691 zfs list -t snapshot Get read and write stats from pool \u2691 zpool iostat {{ pool_name }} {{ refresh_time_in_seconds }} Resources \u2691 Docs Aaron Toponce articles JRS articles ZFS basic introduction video", "title": "OpenZFS"}, {"location": "linux/zfs/#learning", "text": "I've found that learning about ZFS was an interesting, intense and time consuming task. If you want a quick overview check this video . If you prefer to read, head to the awesome Aaron Toponce articles and read all of them sequentially, each is a jewel. The docs on the other hand are not that pleasant to read. For further information check JRS articles .", "title": "Learning"}, {"location": "linux/zfs/#storage-planning", "text": "There are many variables that affect the number and type of disks, you first need to have an idea of what kind of data you want to store and what use are you going to give to that data.", "title": "Storage planning"}, {"location": "linux/zfs/#robustness", "text": "ZFS is designed to survive disk failures, so it stores each block of data redundantly. This feature complicates capacity planning because your total usable storage is not just the sum of each disk\u2019s capacity. ZFS creates filesystems out of \u201cpools\u201d of disks. The more disks in the pool, the more efficiently ZFS can use their storage capacity. For example, if you give ZFS two 10 TB drives, you can only use half of your total disk capacity. If you instead use five 4 TB drives, ZFS gives you 14 TB of usable storage. Even though your total disk space is the same in either scenario, the five smaller drives give you 40% more usable space. When you\u2019re building a NAS server, you need to decide whether to use a smaller quantity of large disks or a larger quantity of small disks. Smaller drives are usually cheaper in terms of $/TB, but they\u2019re more expensive to operate. Two 4 TB drives require twice the electricity of a single 8 TB drive. Also keep in mind that so far ZFS doesn't let you add a new drive to an existing vdev, but that feature is under active development . If you want to be safe, plan your vdev definition so that they don't need to change the disk numbers.", "title": "Robustness"}, {"location": "linux/zfs/#preventing-concurrent-disk-failures", "text": "Naively, the probability of two disks failing at once seems vanishingly small. Based on Backblaze\u2019s stats , high-quality disk drives fail at 0.5-4% per year. A 4% risk per year is a 0.08% chance in any given week. Two simultaneous failures would happen once every 30,000 years, so you should be fine, right? The problem is that disks aren\u2019t statistically independent. If one disk fails, its neighbor has a substantially higher risk of dying. This is especially true if the disks are the same model, from the same manufacturing batch, and processed the same workloads. Further, rebuilding a ZFS pool puts an unusual amount of strain on all of the surviving disks. A disk that would have lasted a few more months under normal usage might die under the additional load of a pool rebuild. Given these risks, you can reduce the risk of concurrent disk failures by choosing two different models of disk from two different manufacturers. To reduce the chances of getting disks from the same manufacturing batch, you can buy them from different vendors.", "title": "Preventing concurrent disk failures"}, {"location": "linux/zfs/#choosing-the-disks-to-hold-data", "text": "Check diskprices.com to get an idea of the cost of disks in the market. If you can, try to avoid buying to Amazon as it's the devil. Try to buy them from a local store instead, that way you interact with a human and promote a local commerce. Note: If you want a TL;DR you can jump to the conclusion . To choose your disks take into account: Disk speed Disk load Disk type Disk homogeneity Disk Warranty Disk Brands", "title": "Choosing the disks to hold data"}, {"location": "linux/zfs/#data-disk-speed", "text": "When comes to disk speed there are three kinds, the slow (5400 RPM), normal (7200 RPM) and fast (10k RPM). The higher the RPM, the louder the disk is, the more heat it creates and the more power it will consume. In exchange they will have higher writing and reading speeds. Slower disks expand the lifecycle of the device, but in the case of a failed disk in a RAID scenario, the rebuild time will be higher than on faster ones therefore increasing the risk on concurrent failing disks. Before choosing a high number of RPM make sure that it's your bottleneck, which usually is the network if you're using a 1Gbps network. In this case a 10k RPM disk won't offer better performance than a 7200 RPM, even a 7200 one won't be better than a 5400. The need of higher speeds can be fixed by using an SSD as a cache for reading and writing.", "title": "Data disk speed"}, {"location": "linux/zfs/#data-disk-load", "text": "Disk specifications tell you the amount of TB/year they support, it gives you an idea of the fault tolerance. Some examples Disk Fault tolerance (TB/year) WD RED 8TB 180", "title": "Data disk load"}, {"location": "linux/zfs/#data-disk-type", "text": "It\u2019s easy to think that all hard drives are equal, save for the form factor and connection type. However, there\u2019s a difference between the work your hard drive does in your computer versus the workload of a NAS hard drive. A drive in your computer may only read and write data for a couple hours at a time, while a NAS drive may read and write data for weeks on end, or even longer. The environment inside of a NAS box is much different than a typical desktop or laptop computer. When you pack in a handful of hard drives close together, several things happen: there\u2019s more vibration, more heat, and a lot more action going on in general. To cope with this, NAS hard drives usually have better vibration tolerance and produce less heat than regular hard drives, thanks to slightly-slower spindle speeds and reduced seek noise. Most popular brands are Western Digital Red and Seagate IronWolf which use 5400 RPM, if you want to go on the 7200 RPM speeds you can buy the Pro version of each. I initially tried checking Backblaze\u2019s hard drive stats to avoid failure-prone disks, but they use drives on the pricier side. The last pitfall to avoid is shingled magnetic recording (SMR) technology. ZFS performs poorly on SMR drives , so if you\u2019re building a NAS, avoid known SMR drives . If the drive is labeled as CMR, that\u2019s conventional magnetic recording, which is fine for ZFS. SMR is well suited for high-capacity, low-cost use where writes are few and reads are many. It has worse sustained write performance than CMR, which can cause severe issues during resilvering or other write-intensive operations. There are three types of SMR: Drive Managed, DM-SMR: It's opaque to the OS. This means ZFS cannot \"target\" writes, and is the worst type for ZFS use. As a rule of thumb, avoid DM-SMR drives, unless you have a specific use case where the increased resilver time (a week or longer) is acceptable, and you know the drive will function for ZFS during resilver. Host Aware, HA-SMR: It's designed to give ZFS insight into the SMR process. Note that ZFS code to use HA-SMR does not appear to exist. Without that code, a HA-SMR drive behaves like a DM-SMR drive where ZFS is concerned. Host Managed, HM-SMR: It's not backwards compatible and requires ZFS to manage the SMR process.", "title": "Data disk type"}, {"location": "linux/zfs/#data-disk-homogeneity", "text": "It's recommended that all the disks in your pool (or is it by vdev?) have the same RPM and size.", "title": "Data disk homogeneity"}, {"location": "linux/zfs/#data-disk-warranty", "text": "Disks are going to fail, so it's good to have a good warranty to return them.", "title": "Data disk warranty"}, {"location": "linux/zfs/#data-disk-brands", "text": "", "title": "Data disk brands"}, {"location": "linux/zfs/#western-digital", "text": "The Western Digital Red series of NAS drives are very similar to Seagate\u2019s offering and you should consider these if you can find them at more affordable prices. WD splits its NAS drives into three sub-categories, normal, Plus, and Pro. Specs WD Red WD Red Plus WD Red Pro Technology SMR CMR CMR Bays 1-8 1-8 1-24 Capacity 2-6TB 1-14TB 2-18TB Speed 5,400 RPM 5,400 RPM (1-4TB) 7200 RPM Speed 5,400 RPM 5,640 RPM (6-8TB) 7200 RPM Speed 5,400 RPM 7,200 RPM (8-14TB) 7200 RPM Speed ? 210MB/s 235MB/s Cache 256MB 16MB (1TB) Cache 256MB 64MB (1TB) 64MB (2TB) Cache 256MB 128MB (2-8TB) 256MB (4-12TB) Cache 256MB 256MB (8-12TB) 512MB (14-18TB) Cache 256MB 512MB (14TB) Workload 180TB/yr 180TB/yr 300TB/yr MTBF 1 million 1 million 1 million Warranty 3 years 3 years 5 years Power Consumption ? ? 8.8 W Power Consumption Rest ? ? 4.6 W Price From $50 From $45 From $78", "title": "Western Digital"}, {"location": "linux/zfs/#seagate", "text": "Seagate's \"cheap\" NAS disks are the IronWolf gama, there are two variations IronWolf and IronWolf Pro . Seagate Exos is a premium series of drives from the company. They\u2019re even more advanced than IronWolf Pro and are best suited for server environments. They sport incredible levels of performance and reliability, including a workload rate of 550TB per year. Specs IronWolf IronWolf Pro Exos 7E8 8TB Exos 7E10 8TB Technology CMR CMR CMR SMR Bays 1-8 1-24 ? ? Capacity 1-12TB 2-20TB 8TB 8TB RPM 5,400 RPM (3-6TB) 7200 RPM 7200 RPM 7200 RPM RPM 5,900 RPM (1-3TB) 7200 RPM 7200 RPM 7200 RPM RPM 7,200 RPM (8-12TB) 7200 RPM 7200 RPM 7200 RPM Speed 180MB/s (1-12TB) 214-260MB/s (4-18TB) 249 MB/s 255 MB/s Cache 64MB (1-4TB) 256 MB 256 MB 256 MB Cache 256MB (3-12TB) 256 MB 256 MB 256 MB Power Consumption (8TB) 10.1 W 10.1 W 12.81 W 11.03 W Power Consumption Rest (8TB) 7.8 W 7.8 W 7.64 W 7.06 W Workload 180TB/yr 300TB/yr 550TB/yr 550TB/yr MTBF 1 million 1 million 2 millions 2 millions Warranty 3 years 5 years 5 years 5 years Price From $60 From $83 249$ 249$ Exos 7E10 is SMR so it's ruled out. Where MTBF stands for Medium Time Between Failures in hours", "title": "Seagate"}, {"location": "linux/zfs/#data-disk-conclusion", "text": "I'm more interested on the 5400 RPM drives, but of all the NAS disks available to purchase only the WD RED of 8TB use it, and they use the SMR technology, so they aren't a choice. The disk prices offered by my cheapest provider are: Disk Size Price Seagate IronWolf 8TB 225$ Seagate IronWolf Pro 8TB 254$ WD Red Plus 8TB 265$ Seagate Exos 7E8 8TB 277$ WD Red Pro 8TB 278$ WD Red Plus has 5,640 RPM which is different than the rest, so it's ruled out. Between the IronWolf and IronWolf Pro, they offer 180MB/s and 214MB/s respectively. The Seagate Exos 7E8 provides much better performance than the WD Red Pro so I'm afraid that WD is out of the question. There are three possibilities in order to have two different brands. Imagining we want 4 disks: Combination Total Price IronWolf + IronWolf Pro 958$ IronWolf + Exos 7E8 1004$ (+46$ +4.5%) IronWolf Pro + Exos 7E8 1062$ (+54$ +5.4%) In terms of: Consumption: both IronWolfs are equal, the Exos uses 2.7W more on normal use and uses 0.2W less on rest. Warranty: IronWolf has only 3 years, the others 5. Speed: Ironwolf has 210MB/s, much less than the Pro (255MB/s) and Exos (249MB/s), which are more similar. Sostenibility: The Exos disks are much more robust (more workload, MTBF and Warranty). I'd say that for 104$ it makes sense to go with the IronWolf Pro + Exos 7E8 combination.", "title": "Data disk conclusion"}, {"location": "linux/zfs/#choosing-the-disks-for-the-cache", "text": "Using a ZLOG greatly improves the writing speed , equally using an SSD disk for the L2ARC cache improves the read speeds and improves the health of the rotational disks. The best M.2 NVMe SSD for NAS caching are the ones that have enough capacity to actually make a difference to overall system performance. It also requires a good endurance rating for better reliability and longer lifespan, and you should look for a drive with a specific NAND technology if possible. Note: If you want a TL;DR you can jump to the conclusion . To choose your disks take into account: Cache disk NAND technology DWPD", "title": "Choosing the disks for the cache"}, {"location": "linux/zfs/#cache-disk-nand-technology", "text": "Not all flash-based storage drives are the same. NAND flash cells are usually categorised based on the number of bits that can be stored per cell. Watch out for the following terms when shopping around for an SSD: Single-Level Cell (SLC): one bit per cell. Multi-Level Cell (MLC): two bits per cell. Triple-Level Cell (TLC): three bits per cell. Quad-Level Cell (QLC): four bits per cell. When looking for the best M.2 NVMe SSD for NAS data caching, it\u2019s important to bear the NAND technology in mind. SLC is the best technology for SSDs that will be used for NAS caching. This does mean you\u2019re paying out more per GB and won\u2019t be able to select high-capacity drives, but reliability and the protection of stored data is the most important factor here. Another benefit of SLC is the lower impact of write amplification, which can quickly creep up and chomp through a drive\u2019s DWPD endurance rating. It\u2019s important to configure an SSD for caching correctly too regardless of which technology you pick. Doing so will lessen the likelihood of losing data through a drive hanging and causing the system to crash. Anything stored on the cache drive that has yet to be written to the main drive array would be lost. This is mostly a reported issue for NVMe drives, as opposed to SATA.", "title": "Cache disk NAND technology"}, {"location": "linux/zfs/#dwpd", "text": "DWPD stands for drive writes per day. This is often used as a measurement of a drive\u2019s endurance. The higher this number, the more writes the drive can perform on a daily basis, as is rated by the manufacturer. For caching, especially which involves writing data, you\u2019ll want to aim for as high a DWPD rating as possible.", "title": "DWPD"}, {"location": "linux/zfs/#cache-disk-conclusion", "text": "Overall, I\u2019d recommend the Western Digital Red SN700, which has a good 1 DWPD endurance rating, is available in sizes up to 4TB, and is using SLC NAND technology, which is great for enhancing reliability through heavy caching workloads. A close second place goes to the Seagate IronWolf 525, which has similar specifications to the SN700 but utilizes TLC. Disk Size Speed Endurance Warranty Tech Price WD Red SN700 500 GB 3430MB/s 1 DWPD 5 years SLC 73$ SG IronWolf 525 500 GB 5000MB/s 0.8 DWPD 5 years TLC ? WD Red SN700 1 TB 3430MB/s 1 DWPD 5 years SLC 127$ SG IronWolf 525 1 TB 5000MB/s 0.8 DWPD 5 years TLC ?", "title": "Cache disk conclusion"}, {"location": "linux/zfs/#choosing-the-cold-spare-disks", "text": "It's good to think how much time you want to have your raids to be inconsistent once a drive has failed. In my case, for the data I want to restore the raid as soon as I can, therefore I'll buy another rotational disk. For the SSDs I have more confidence that they won't break so I don't feel like having a spare one.", "title": "Choosing the cold spare disks"}, {"location": "linux/zfs/#usage", "text": "", "title": "Usage"}, {"location": "linux/zfs/#mount-a-pool-as-readonly", "text": "zpool import -o readonly = on {{ pool_name }}", "title": "Mount a pool as readonly"}, {"location": "linux/zfs/#mount-a-zfs-snapshot-in-a-directory-as-readonly", "text": "mount -t zfs {{ pool_name }} / {{ snapshot_name }} {{ mount_path }} -o ro", "title": "Mount a ZFS snapshot in a directory as readonly"}, {"location": "linux/zfs/#list-volumes", "text": "zpool list", "title": "List volumes"}, {"location": "linux/zfs/#list-snapshots", "text": "zfs list -t snapshot", "title": "List snapshots"}, {"location": "linux/zfs/#get-read-and-write-stats-from-pool", "text": "zpool iostat {{ pool_name }} {{ refresh_time_in_seconds }}", "title": "Get read and write stats from pool"}, {"location": "linux/zfs/#resources", "text": "Docs Aaron Toponce articles JRS articles ZFS basic introduction video", "title": "Resources"}, {"location": "linux/zip/", "text": "zip is an UNIX command line tool to package and compress files. Usage \u2691 Create a zip file \u2691 zip -r {{ zip_file }} {{ files_to_save }} Split files to a specific size \u2691 zip -s {{ size }} -r {{ destination_zip }} {{ files }} Where {{ size }} can be 950m Compress with password \u2691 zip -er {{ zip_file }} {{ files_to_save }} Read files to compress from a file \u2691 cat {{ files_to_compress.txt }} | zip -er {{ destination_zip }} -@ Uncompress a zip file \u2691 unzip {{ zip_file }}", "title": "zip"}, {"location": "linux/zip/#usage", "text": "", "title": "Usage"}, {"location": "linux/zip/#create-a-zip-file", "text": "zip -r {{ zip_file }} {{ files_to_save }}", "title": "Create a zip file"}, {"location": "linux/zip/#split-files-to-a-specific-size", "text": "zip -s {{ size }} -r {{ destination_zip }} {{ files }} Where {{ size }} can be 950m", "title": "Split files to a specific size"}, {"location": "linux/zip/#compress-with-password", "text": "zip -er {{ zip_file }} {{ files_to_save }}", "title": "Compress with password"}, {"location": "linux/zip/#read-files-to-compress-from-a-file", "text": "cat {{ files_to_compress.txt }} | zip -er {{ destination_zip }} -@", "title": "Read files to compress from a file"}, {"location": "linux/zip/#uncompress-a-zip-file", "text": "unzip {{ zip_file }}", "title": "Uncompress a zip file"}, {"location": "linux/luks/luks/", "text": "LUKS definition The Linux Unified Key Setup (LUKS) is a disk encryption specification created by Clemens Fruhwirth in 2004 and was originally intended for Linux. While most disk encryption software implements different, incompatible, and undocumented formats, LUKS implements a platform-independent standard on-disk format for use in various tools. This not only facilitates compatibility and interoperability among different programs, but also assures that they all implement password management in a secure and documented manner. The reference implementation for LUKS operates on Linux and is based on an enhanced version of cryptsetup, using dm-crypt as the disk encryption backend. LUKS is designed to conform to the TKS1 secure key setup scheme. LUKS Commands \u2691 We use the cryptsetup command to interact with LUKS partitions. Header management \u2691 Get the disk header \u2691 cryptsetup luksDump /dev/sda3 Backup header \u2691 cryptsetup luksHeaderBackup --header-backup-file {{ file }} {{ device }} Key management \u2691 Add a key \u2691 cryptsetup luksAddKey --key-slot 1 {{ luks_device }} Change a key \u2691 cryptsetup luksChangeKey {{ luks_device }} -s 0 Test if you remember the key \u2691 Try to add a new key and cancel the process cryptsetup luksAddKey --key-slot 3 {{ luks_device }} Delete some keys \u2691 cryptsetup luksDump {{ device }} cryptsetup luksKillSlot {{ device }} {{ slot_number }} Delete all keys \u2691 cryptsetup luksErase {{ device }} Encrypt hard drive \u2691 Configure LUKS partition cryptsetup -y -v luksFormat /dev/sdg Open the container cryptsetup luksOpen /dev/sdg crypt Fill it with zeros pv -tpreb /dev/zero | dd of = /dev/mapper/crypt bs = 128M Make filesystem mkfs.ext4 /dev/mapper/crypt Break a luks password \u2691 You can use bruteforce-luks LUKS debugging \u2691 Resource busy \u2691 Umount the lv first lvscan lvchange -a n {{ partition_name }} Then close the luks device cryptsetup luksClose {{ device_name }}", "title": "LUKS"}, {"location": "linux/luks/luks/#luks-commands", "text": "We use the cryptsetup command to interact with LUKS partitions.", "title": "LUKS Commands"}, {"location": "linux/luks/luks/#header-management", "text": "", "title": "Header management"}, {"location": "linux/luks/luks/#get-the-disk-header", "text": "cryptsetup luksDump /dev/sda3", "title": "Get the disk header"}, {"location": "linux/luks/luks/#backup-header", "text": "cryptsetup luksHeaderBackup --header-backup-file {{ file }} {{ device }}", "title": "Backup header"}, {"location": "linux/luks/luks/#key-management", "text": "", "title": "Key management"}, {"location": "linux/luks/luks/#add-a-key", "text": "cryptsetup luksAddKey --key-slot 1 {{ luks_device }}", "title": "Add a key"}, {"location": "linux/luks/luks/#change-a-key", "text": "cryptsetup luksChangeKey {{ luks_device }} -s 0", "title": "Change a key"}, {"location": "linux/luks/luks/#test-if-you-remember-the-key", "text": "Try to add a new key and cancel the process cryptsetup luksAddKey --key-slot 3 {{ luks_device }}", "title": "Test if you remember the key"}, {"location": "linux/luks/luks/#delete-some-keys", "text": "cryptsetup luksDump {{ device }} cryptsetup luksKillSlot {{ device }} {{ slot_number }}", "title": "Delete some keys"}, {"location": "linux/luks/luks/#delete-all-keys", "text": "cryptsetup luksErase {{ device }}", "title": "Delete all keys"}, {"location": "linux/luks/luks/#encrypt-hard-drive", "text": "Configure LUKS partition cryptsetup -y -v luksFormat /dev/sdg Open the container cryptsetup luksOpen /dev/sdg crypt Fill it with zeros pv -tpreb /dev/zero | dd of = /dev/mapper/crypt bs = 128M Make filesystem mkfs.ext4 /dev/mapper/crypt", "title": "Encrypt hard drive"}, {"location": "linux/luks/luks/#break-a-luks-password", "text": "You can use bruteforce-luks", "title": "Break a luks password"}, {"location": "linux/luks/luks/#luks-debugging", "text": "", "title": "LUKS debugging"}, {"location": "linux/luks/luks/#resource-busy", "text": "Umount the lv first lvscan lvchange -a n {{ partition_name }} Then close the luks device cryptsetup luksClose {{ device_name }}", "title": "Resource busy"}, {"location": "linux/vim/vim_plugins/", "text": "Black \u2691 To install Black you first need python3-venv . sudo apt-get install python3-venv Add the plugin and configure it so vim runs it each time you save. File ~/.vimrc Plugin 'psf/black' \" Black autocmd BufWritePre *.py execute ':Black' A configuration issue exists for neovim. If you encounter the error AttributeError: module 'black' has no attribute 'find_pyproject_toml' , do the following: cd ~/.vim/bundle/black git checkout 19 .10b0 As the default line length is 88 (ugly number by the way), we need to change the indent, python-mode configuration as well \"\" python indent autocmd BufNewFile,BufRead *.py setlocal foldmethod=indent tabstop=4 softtabstop=4 shiftwidth=4 textwidth=88 smarttab expandtab \" python-mode let g:pymode_options_max_line_length = 88 let g:pymode_lint_options_pep8 = {'max_line_length': g:pymode_options_max_line_length} ALE \u2691 ALE (Asynchronous Lint Engine) is a plugin providing linting (syntax checking and semantic errors) in NeoVim 0.2.0+ and Vim 8 while you edit your text files, and acts as a Vim Language Server Protocol client. ALE makes use of NeoVim and Vim 8 job control functions and timers to run linters on the contents of text buffers and return errors as text changes in Vim. This allows for displaying warnings and errors in files before they are saved back to a filesystem. In other words, this plugin allows you to lint while you type. ALE offers support for fixing code with command line tools in a non-blocking manner with the :ALEFix feature, supporting tools in many languages , like prettier, eslint, autopep8, and more. Installation \u2691 Install with Vundle: Plugin 'dense-analysis/ale' Configuration \u2691 let g :ale_sign_error = '\u2718' let g :ale_sign_warning = '\u26a0' highlight ALEErrorSign ctermbg = NONE ctermfg = red highlight ALEWarningSign ctermbg = NONE ctermfg = yellow let g :ale_linters_explicit = 1 let g :ale_lint_on_text_changed = 'normal' \" let g:ale_lint_on_text_changed = 'never' let g :ale_lint_on_enter = 0 let g :ale_lint_on_save = 1 let g :ale_fix_on_save = 1 let g :ale_linters = { \\ 'markdown' : [ 'markdownlint' , 'writegood' , 'alex' , 'proselint' ] , \\ 'json' : [ 'jsonlint' ] , \\ 'python' : [ 'flake8' , 'mypy' , 'pylint' , 'alex' ] , \\ 'yaml' : [ 'yamllint' , 'alex' ] , \\ '*' : [ 'alex' , 'writegood' ] , \\} let g :ale_fixers = { \\ '*' : [ 'remove_trailing_lines' , 'trim_whitespace' ] , \\ 'json' : [ 'jq' ] , \\ 'python' : [ 'isort' ] , \\ 'terraform' : [ 'terraform' ] , \\} inoremap < leader > e < esc > :ALENext < cr > nnoremap < leader > e :ALENext < cr > inoremap < leader > p < esc > :ALEPrevious < cr > nnoremap < leader > p :ALEPrevious < cr > Where: let g:ale_linters_explicit : Prevent ALE load only the selected linters. use <leader>e and <leader>p to navigate through the warnings. If you feel that it's too heavy, use ale_lint_on_enter or increase the ale_lint_delay . Use :ALEInfo to see the ALE configuration and any errors when running :ALEFix for the specific buffer. Flakehell \u2691 Flakehell is not supported yet . Until that issue is closed we need the following configuration: let g:ale_python_flake8_executable = flake8helled let g:ale_python_flake8_use_global = 1 Toggle fixers on save \u2691 There are cases when you don't want to run the fixers in your code. Ale doesn't have an option to do it , but zArubaru showed how to do it. If you add to your configuration command! ALEToggleFixer execute \"let g:ale_fix_on_save = get(g:, 'ale_fix_on_save', 0) ? 0 : 1\" You can then use :ALEToggleFixer to activate an deactivate them. vim-easymotion \u2691 EasyMotion provides a much simpler way to use some motions in vim. It takes the <number> out of <number>w or <number>f{char} by highlighting all possible choices and allowing you to press one key to jump directly to the target. When one of the available motions is triggered, all visible text preceding or following the cursor is faded, and motion targets are highlighted. Installation \u2691 Add to Vundle Plugin 'easymotion/vim-easymotion' The configuration can be quite complex, but I'm starting with the basics: \" Easymotion let g :EasyMotion_do_mapping = 0 \" Disable default mappings let g :EasyMotion_keys = 'asdfghjkl' \" Jump to anywhere you want with minimal keystrokes, with just one key binding. \" `s{char}{label}` nmap s < Plug >( easymotion - overwin - f ) \" JK motions: Line motions map < Leader > j < Plug >( easymotion - j ) map < Leader > k < Plug >( easymotion - k ) It's awesome to move between windows with s . Vim Fugitive \u2691 Add portions of file to the index \u2691 To stage only part of the file to a commit, open it and launch :Gdiff . With diffput and diffobtain Vim's functionality you move to the index file (the one in the left) the changes you want to stage. Prepare environment to write the commit message \u2691 When I write the commit message I like to review what changes I'm commiting. To do it I find useful to close all windows and do a vertical split with the changes so I can write the commit message in one of the window while I scroll down in the other. As the changes are usually no the at the top of the file, I scroll the window of the right to the first change and then switch back to the left one in insert mode to start writing. I've also made some movement remappings: jj , kk , <C-d> and <C-u> in insert mode will insert normal mode and go to the window in the right to continue seeing the changes. i , a , o , O : if you are in the changes window it will go to the commit message window in insert mode. Once I've made the commit I want to only retain one buffer. Add the following snippet to do just that: \" Open commit message buffer in fullscreen with a vertical split, and close it with \" leader q au BufNewFile,BufRead *COMMIT_EDITMSG call CommitMessage() function! RestoreBindings() inoremap jj <esc>j inoremap kk <esc>k inoremap <C-d> <C-d> inoremap <C-u> <C-u> nnoremap i i nnoremap a a nnoremap o o nnoremap O O endfunction function! CommitMessage() \" Remap the saving mappings \" Close buffer when saving inoremap <silent> <leader>q <esc>:w<cr> \\| :only<cr> \\| :call RestoreBindings()<cr> \\|:Sayonara<CR> nnoremap <silent> <leader>q <esc>:w<cr> \\| :only<cr> \\| :call RestoreBindings()<cr> \\|:Sayonara<CR> inoremap jj <esc>:wincmd l<cr>j inoremap kk <esc>:wincmd l<cr>k inoremap <C-d> <esc>:wincmd l<cr><C-d> inoremap <C-u> <esc>:wincmd l<cr><C-u> nnoremap i :wincmd h<cr>i nnoremap a :wincmd h<cr>a nnoremap o :wincmd h<cr>o nnoremap O :wincmd h<cr>O \" Remove bad habits inoremap jk <nop> inoremap ZZ <nop> nnoremap ZZ <nop> \" Close all other windows only \" Create a vertical split vsplit \" Go to the right split wincmd l \" Go to the first change execute \"normal! /^diff\\<cr>8j\" \" Clear the search highlights nohl \" Go back to the left split wincmd h \" Enter insert mode execute \"startinsert\" endfunction I'm assuming that you save with <leader>w and that you're using Sayonara to close your buffers. Git push sets the upstream by default \u2691 Add to your config: nnoremap < leader > gp :Git - c push.default = current push < CR > If you want to see the output of the push command , use :copen after the successful push. Vim-test \u2691 A Vim wrapper for running tests on different granularities. Currently the following testing frameworks are supported: Language Frameworks Identifiers C# .NET dotnettest Clojure Fireplace.vim fireplacetest Crystal Crystal crystalspec Elixir ESpec, ExUnit espec , exunit Erlang CommonTest commontest Go Ginkgo, Go ginkgo , gotest Java Maven maventest JavaScript Intern, Jasmine, Jest, Karma, Lab, Mocha, TAP, intern , jasmine , jest , karma , lab , mocha , tap Lua Busted busted PHP Behat, Codeception, Kahlan, Peridot, PHPUnit, PHPSpec behat , codeception , kahlan , peridot , phpunit , phpspec Perl Prove prove Python Django, Nose, Nose2, PyTest, PyUnit djangotest , djangonose nose , nose2 , pytest , pyunit Racket RackUnit rackunit Ruby Cucumber, [M], [Minitest][minitest], Rails, RSpec cucumber , m , minitest , rails , rspec Rust Cargo cargotest Shell Bats bats VimScript Vader.vim, VSpec vader , vspec Features \u2691 Zero dependencies Zero configuration required (it Does the Right Thing\u2122, see Philosophy ) Wide range of test runners which are automagically detected Polyfills for nearest tests (by constructing regexes ) Wide range of execution environments (\" strategies \") Fully customized CLI options configuration Extendable with new runners and strategies Test.vim consists of a core which provides an abstraction over running any kind of tests from the command-line. Concrete test runners are then simply plugged in, so they all work in the same unified way. Issues \u2691 Vim-Abolish \u2691 Error adding elipsis instead of three dots : Pope said that it's not possible :(. References \u2691 ALE supported tools", "title": "Vim Plugins"}, {"location": "linux/vim/vim_plugins/#black", "text": "To install Black you first need python3-venv . sudo apt-get install python3-venv Add the plugin and configure it so vim runs it each time you save. File ~/.vimrc Plugin 'psf/black' \" Black autocmd BufWritePre *.py execute ':Black' A configuration issue exists for neovim. If you encounter the error AttributeError: module 'black' has no attribute 'find_pyproject_toml' , do the following: cd ~/.vim/bundle/black git checkout 19 .10b0 As the default line length is 88 (ugly number by the way), we need to change the indent, python-mode configuration as well \"\" python indent autocmd BufNewFile,BufRead *.py setlocal foldmethod=indent tabstop=4 softtabstop=4 shiftwidth=4 textwidth=88 smarttab expandtab \" python-mode let g:pymode_options_max_line_length = 88 let g:pymode_lint_options_pep8 = {'max_line_length': g:pymode_options_max_line_length}", "title": "Black"}, {"location": "linux/vim/vim_plugins/#ale", "text": "ALE (Asynchronous Lint Engine) is a plugin providing linting (syntax checking and semantic errors) in NeoVim 0.2.0+ and Vim 8 while you edit your text files, and acts as a Vim Language Server Protocol client. ALE makes use of NeoVim and Vim 8 job control functions and timers to run linters on the contents of text buffers and return errors as text changes in Vim. This allows for displaying warnings and errors in files before they are saved back to a filesystem. In other words, this plugin allows you to lint while you type. ALE offers support for fixing code with command line tools in a non-blocking manner with the :ALEFix feature, supporting tools in many languages , like prettier, eslint, autopep8, and more.", "title": "ALE"}, {"location": "linux/vim/vim_plugins/#installation", "text": "Install with Vundle: Plugin 'dense-analysis/ale'", "title": "Installation"}, {"location": "linux/vim/vim_plugins/#configuration", "text": "let g :ale_sign_error = '\u2718' let g :ale_sign_warning = '\u26a0' highlight ALEErrorSign ctermbg = NONE ctermfg = red highlight ALEWarningSign ctermbg = NONE ctermfg = yellow let g :ale_linters_explicit = 1 let g :ale_lint_on_text_changed = 'normal' \" let g:ale_lint_on_text_changed = 'never' let g :ale_lint_on_enter = 0 let g :ale_lint_on_save = 1 let g :ale_fix_on_save = 1 let g :ale_linters = { \\ 'markdown' : [ 'markdownlint' , 'writegood' , 'alex' , 'proselint' ] , \\ 'json' : [ 'jsonlint' ] , \\ 'python' : [ 'flake8' , 'mypy' , 'pylint' , 'alex' ] , \\ 'yaml' : [ 'yamllint' , 'alex' ] , \\ '*' : [ 'alex' , 'writegood' ] , \\} let g :ale_fixers = { \\ '*' : [ 'remove_trailing_lines' , 'trim_whitespace' ] , \\ 'json' : [ 'jq' ] , \\ 'python' : [ 'isort' ] , \\ 'terraform' : [ 'terraform' ] , \\} inoremap < leader > e < esc > :ALENext < cr > nnoremap < leader > e :ALENext < cr > inoremap < leader > p < esc > :ALEPrevious < cr > nnoremap < leader > p :ALEPrevious < cr > Where: let g:ale_linters_explicit : Prevent ALE load only the selected linters. use <leader>e and <leader>p to navigate through the warnings. If you feel that it's too heavy, use ale_lint_on_enter or increase the ale_lint_delay . Use :ALEInfo to see the ALE configuration and any errors when running :ALEFix for the specific buffer.", "title": "Configuration"}, {"location": "linux/vim/vim_plugins/#flakehell", "text": "Flakehell is not supported yet . Until that issue is closed we need the following configuration: let g:ale_python_flake8_executable = flake8helled let g:ale_python_flake8_use_global = 1", "title": "Flakehell"}, {"location": "linux/vim/vim_plugins/#toggle-fixers-on-save", "text": "There are cases when you don't want to run the fixers in your code. Ale doesn't have an option to do it , but zArubaru showed how to do it. If you add to your configuration command! ALEToggleFixer execute \"let g:ale_fix_on_save = get(g:, 'ale_fix_on_save', 0) ? 0 : 1\" You can then use :ALEToggleFixer to activate an deactivate them.", "title": "Toggle fixers on save"}, {"location": "linux/vim/vim_plugins/#vim-easymotion", "text": "EasyMotion provides a much simpler way to use some motions in vim. It takes the <number> out of <number>w or <number>f{char} by highlighting all possible choices and allowing you to press one key to jump directly to the target. When one of the available motions is triggered, all visible text preceding or following the cursor is faded, and motion targets are highlighted.", "title": "vim-easymotion"}, {"location": "linux/vim/vim_plugins/#installation_1", "text": "Add to Vundle Plugin 'easymotion/vim-easymotion' The configuration can be quite complex, but I'm starting with the basics: \" Easymotion let g :EasyMotion_do_mapping = 0 \" Disable default mappings let g :EasyMotion_keys = 'asdfghjkl' \" Jump to anywhere you want with minimal keystrokes, with just one key binding. \" `s{char}{label}` nmap s < Plug >( easymotion - overwin - f ) \" JK motions: Line motions map < Leader > j < Plug >( easymotion - j ) map < Leader > k < Plug >( easymotion - k ) It's awesome to move between windows with s .", "title": "Installation"}, {"location": "linux/vim/vim_plugins/#vim-fugitive", "text": "", "title": "Vim Fugitive"}, {"location": "linux/vim/vim_plugins/#add-portions-of-file-to-the-index", "text": "To stage only part of the file to a commit, open it and launch :Gdiff . With diffput and diffobtain Vim's functionality you move to the index file (the one in the left) the changes you want to stage.", "title": "Add portions of file to the index"}, {"location": "linux/vim/vim_plugins/#prepare-environment-to-write-the-commit-message", "text": "When I write the commit message I like to review what changes I'm commiting. To do it I find useful to close all windows and do a vertical split with the changes so I can write the commit message in one of the window while I scroll down in the other. As the changes are usually no the at the top of the file, I scroll the window of the right to the first change and then switch back to the left one in insert mode to start writing. I've also made some movement remappings: jj , kk , <C-d> and <C-u> in insert mode will insert normal mode and go to the window in the right to continue seeing the changes. i , a , o , O : if you are in the changes window it will go to the commit message window in insert mode. Once I've made the commit I want to only retain one buffer. Add the following snippet to do just that: \" Open commit message buffer in fullscreen with a vertical split, and close it with \" leader q au BufNewFile,BufRead *COMMIT_EDITMSG call CommitMessage() function! RestoreBindings() inoremap jj <esc>j inoremap kk <esc>k inoremap <C-d> <C-d> inoremap <C-u> <C-u> nnoremap i i nnoremap a a nnoremap o o nnoremap O O endfunction function! CommitMessage() \" Remap the saving mappings \" Close buffer when saving inoremap <silent> <leader>q <esc>:w<cr> \\| :only<cr> \\| :call RestoreBindings()<cr> \\|:Sayonara<CR> nnoremap <silent> <leader>q <esc>:w<cr> \\| :only<cr> \\| :call RestoreBindings()<cr> \\|:Sayonara<CR> inoremap jj <esc>:wincmd l<cr>j inoremap kk <esc>:wincmd l<cr>k inoremap <C-d> <esc>:wincmd l<cr><C-d> inoremap <C-u> <esc>:wincmd l<cr><C-u> nnoremap i :wincmd h<cr>i nnoremap a :wincmd h<cr>a nnoremap o :wincmd h<cr>o nnoremap O :wincmd h<cr>O \" Remove bad habits inoremap jk <nop> inoremap ZZ <nop> nnoremap ZZ <nop> \" Close all other windows only \" Create a vertical split vsplit \" Go to the right split wincmd l \" Go to the first change execute \"normal! /^diff\\<cr>8j\" \" Clear the search highlights nohl \" Go back to the left split wincmd h \" Enter insert mode execute \"startinsert\" endfunction I'm assuming that you save with <leader>w and that you're using Sayonara to close your buffers.", "title": "Prepare environment to write the commit message"}, {"location": "linux/vim/vim_plugins/#git-push-sets-the-upstream-by-default", "text": "Add to your config: nnoremap < leader > gp :Git - c push.default = current push < CR > If you want to see the output of the push command , use :copen after the successful push.", "title": "Git push sets the upstream by default"}, {"location": "linux/vim/vim_plugins/#vim-test", "text": "A Vim wrapper for running tests on different granularities. Currently the following testing frameworks are supported: Language Frameworks Identifiers C# .NET dotnettest Clojure Fireplace.vim fireplacetest Crystal Crystal crystalspec Elixir ESpec, ExUnit espec , exunit Erlang CommonTest commontest Go Ginkgo, Go ginkgo , gotest Java Maven maventest JavaScript Intern, Jasmine, Jest, Karma, Lab, Mocha, TAP, intern , jasmine , jest , karma , lab , mocha , tap Lua Busted busted PHP Behat, Codeception, Kahlan, Peridot, PHPUnit, PHPSpec behat , codeception , kahlan , peridot , phpunit , phpspec Perl Prove prove Python Django, Nose, Nose2, PyTest, PyUnit djangotest , djangonose nose , nose2 , pytest , pyunit Racket RackUnit rackunit Ruby Cucumber, [M], [Minitest][minitest], Rails, RSpec cucumber , m , minitest , rails , rspec Rust Cargo cargotest Shell Bats bats VimScript Vader.vim, VSpec vader , vspec", "title": "Vim-test"}, {"location": "linux/vim/vim_plugins/#features", "text": "Zero dependencies Zero configuration required (it Does the Right Thing\u2122, see Philosophy ) Wide range of test runners which are automagically detected Polyfills for nearest tests (by constructing regexes ) Wide range of execution environments (\" strategies \") Fully customized CLI options configuration Extendable with new runners and strategies Test.vim consists of a core which provides an abstraction over running any kind of tests from the command-line. Concrete test runners are then simply plugged in, so they all work in the same unified way.", "title": "Features"}, {"location": "linux/vim/vim_plugins/#issues", "text": "", "title": "Issues"}, {"location": "linux/vim/vim_plugins/#vim-abolish", "text": "Error adding elipsis instead of three dots : Pope said that it's not possible :(.", "title": "Vim-Abolish"}, {"location": "linux/vim/vim_plugins/#references", "text": "ALE supported tools", "title": "References"}, {"location": "linuxbook/basic_commands/", "text": "Basic Commands \u2691 HISTORY \u2691 Die History Datei liegt unter echo $HISTFILE angucken mit cat $HISTFILE oder history ECHO \u2691 Mit echo kann man Variablen ausgeben ochse=1 echo $x echo \"alles ausgeben \\n bricht nicht um\" echo -e \"test \\n test mit -e wird Zeilenumbruch ausgef\u00fchrt\" man echo Umgebugnsvariablen \u2691 Alle Umgebungsvariablen anzeigen env die Path-Varialbe anziegen: echo $PATH CAT \u2691 Datei(en) \u00f6ffnen cat datei cat datei1 datei2 > datei3 LESS \u2691 seitenweise gro\u00dfe Dateien anzeigen less datei1 mit q verlassen HEAD \u2691 ersten 10 zeilen anzeigen head /etc/passwd head /etc/passwd -n 10 TAIL \u2691 wie head, nur von hinten Live \u00fcberwachen tail -f /var/log/dpkg.log NL \u2691 gibt die Anzahl aller Zeilen aus nl /etc/passwd Leerzeilen haben keine Zeilennummer! WC \u2691 Wordcount wc /etc/passwd die erte Zahl Zeilenanzhl die zweite zahl: anzal wr\u00f6rter die dirtte zhtl: bytes OD \u2691 od - dump files in octal and other formats od -c /etc/passwd od /etc/passwd SORT \u2691 table.csv 1,Hans 23,Axenmensch 24,peter 2,kanter sort -n table.csv Sort sagen, dass wir komma als seperator haben und das wir nach der spalte 2 sortiern wollen (name) sort -t \",\" -k2 table.csv -t Trenner -k Key numerisch Dateiname UNIQ \u2691 uniq l\u00f6scht untereinanderstehende gleiche Zeilen. mit -c kann man sich anzeigen lassen, wie oft die Zeile vor kam. uniq uniq.txt uniq -c uniq.txt uniq --group uniq.txt TR \u2691 translate: ersetzt zeichen durch anderes zeichen ersetze komma durch semikolon: echo \"one,two\" | tr ',' ';' one;two cat table.csv | tr ',' ';' > table2.csv echo \"one,two\" | tr 'a-z' 'A-Z' ONE,TWO echo \"one,two\" | tr 'a-z' 'A-Z' | tr ',' ';' ONE;TWO CUT \u2691 wie cat, nur mit ausschneiden. -d Delimiter -f Field cut -d ',' -f 2 table2.csv Hans peter kanter Axenmensch \u276f cat table2.csv 1,Hans,34 24,peter,22 2,kanter,11 23,Axenmensch,21 man kann auch zwei Fields rausschneiden cut -d ',' -f 2,3 table2.csv Hans,34 peter,22 kanter,11 Axenmensch,21 PASTE \u2691 Dateien zusammenf\u00fchren cat a.txt 1,alfa 2,beute cat b.txt apple bea paste -d ',' a.txt b.txt 1,alfa,apple 2,beute,bea SED \u2691 Inhalte erstezen s = substitute finden ersetzen g global (alles ersetzten) \u00e4ndere alle vorkommen von er durch ra in table2.csv sed 's/er/ra/g' table2.csv wenn man die Datei ver\u00e4ndern will, muss man kleines i nutzen sed 's/er/ra/g' -i table2.csv SPLIT \u2691 eine DAtei in kleinere Dateien zerlegen (20 bytes gro\u00dfe dateien) split -b 20 table2.csv SCP \u2691 Auf entfernten Server kopieren (mit ssh-key) scp -i ~/.ssh/id_rsa.pub FILENAME USER@SERVER:/home/USER/FILENAME", "title": "basic commands"}, {"location": "linuxbook/basic_commands/#basic-commands", "text": "", "title": "Basic Commands"}, {"location": "linuxbook/basic_commands/#history", "text": "Die History Datei liegt unter echo $HISTFILE angucken mit cat $HISTFILE oder history", "title": "HISTORY"}, {"location": "linuxbook/basic_commands/#echo", "text": "Mit echo kann man Variablen ausgeben ochse=1 echo $x echo \"alles ausgeben \\n bricht nicht um\" echo -e \"test \\n test mit -e wird Zeilenumbruch ausgef\u00fchrt\" man echo", "title": "ECHO"}, {"location": "linuxbook/basic_commands/#umgebugnsvariablen", "text": "Alle Umgebungsvariablen anzeigen env die Path-Varialbe anziegen: echo $PATH", "title": "Umgebugnsvariablen"}, {"location": "linuxbook/basic_commands/#cat", "text": "Datei(en) \u00f6ffnen cat datei cat datei1 datei2 > datei3", "title": "CAT"}, {"location": "linuxbook/basic_commands/#less", "text": "seitenweise gro\u00dfe Dateien anzeigen less datei1 mit q verlassen", "title": "LESS"}, {"location": "linuxbook/basic_commands/#head", "text": "ersten 10 zeilen anzeigen head /etc/passwd head /etc/passwd -n 10", "title": "HEAD"}, {"location": "linuxbook/basic_commands/#tail", "text": "wie head, nur von hinten Live \u00fcberwachen tail -f /var/log/dpkg.log", "title": "TAIL"}, {"location": "linuxbook/basic_commands/#nl", "text": "gibt die Anzahl aller Zeilen aus nl /etc/passwd Leerzeilen haben keine Zeilennummer!", "title": "NL"}, {"location": "linuxbook/basic_commands/#wc", "text": "Wordcount wc /etc/passwd die erte Zahl Zeilenanzhl die zweite zahl: anzal wr\u00f6rter die dirtte zhtl: bytes", "title": "WC"}, {"location": "linuxbook/basic_commands/#od", "text": "od - dump files in octal and other formats od -c /etc/passwd od /etc/passwd", "title": "OD"}, {"location": "linuxbook/basic_commands/#sort", "text": "table.csv 1,Hans 23,Axenmensch 24,peter 2,kanter sort -n table.csv Sort sagen, dass wir komma als seperator haben und das wir nach der spalte 2 sortiern wollen (name) sort -t \",\" -k2 table.csv -t Trenner -k Key numerisch Dateiname", "title": "SORT"}, {"location": "linuxbook/basic_commands/#uniq", "text": "uniq l\u00f6scht untereinanderstehende gleiche Zeilen. mit -c kann man sich anzeigen lassen, wie oft die Zeile vor kam. uniq uniq.txt uniq -c uniq.txt uniq --group uniq.txt", "title": "UNIQ"}, {"location": "linuxbook/basic_commands/#tr", "text": "translate: ersetzt zeichen durch anderes zeichen ersetze komma durch semikolon: echo \"one,two\" | tr ',' ';' one;two cat table.csv | tr ',' ';' > table2.csv echo \"one,two\" | tr 'a-z' 'A-Z' ONE,TWO echo \"one,two\" | tr 'a-z' 'A-Z' | tr ',' ';' ONE;TWO", "title": "TR"}, {"location": "linuxbook/basic_commands/#cut", "text": "wie cat, nur mit ausschneiden. -d Delimiter -f Field cut -d ',' -f 2 table2.csv Hans peter kanter Axenmensch \u276f cat table2.csv 1,Hans,34 24,peter,22 2,kanter,11 23,Axenmensch,21 man kann auch zwei Fields rausschneiden cut -d ',' -f 2,3 table2.csv Hans,34 peter,22 kanter,11 Axenmensch,21", "title": "CUT"}, {"location": "linuxbook/basic_commands/#paste", "text": "Dateien zusammenf\u00fchren cat a.txt 1,alfa 2,beute cat b.txt apple bea paste -d ',' a.txt b.txt 1,alfa,apple 2,beute,bea", "title": "PASTE"}, {"location": "linuxbook/basic_commands/#sed", "text": "Inhalte erstezen s = substitute finden ersetzen g global (alles ersetzten) \u00e4ndere alle vorkommen von er durch ra in table2.csv sed 's/er/ra/g' table2.csv wenn man die Datei ver\u00e4ndern will, muss man kleines i nutzen sed 's/er/ra/g' -i table2.csv", "title": "SED"}, {"location": "linuxbook/basic_commands/#split", "text": "eine DAtei in kleinere Dateien zerlegen (20 bytes gro\u00dfe dateien) split -b 20 table2.csv", "title": "SPLIT"}, {"location": "linuxbook/basic_commands/#scp", "text": "Auf entfernten Server kopieren (mit ssh-key) scp -i ~/.ssh/id_rsa.pub FILENAME USER@SERVER:/home/USER/FILENAME", "title": "SCP"}, {"location": "linuxbook/booting/", "text": "Systeminformationen beim Booten \u2691 dmesg: Kernelinformationen und Fehler: \u2691 dmesg nach Fehler filtern: dmesg | grep -i error jourcnalct: systemd Journal anfragen \u2691 Kernelinformationen mit -k journalctl -k", "title": "dmesg"}, {"location": "linuxbook/booting/#systeminformationen-beim-booten", "text": "", "title": "Systeminformationen beim Booten"}, {"location": "linuxbook/booting/#dmesg-kernelinformationen-und-fehler", "text": "dmesg nach Fehler filtern: dmesg | grep -i error", "title": "dmesg: Kernelinformationen und Fehler:"}, {"location": "linuxbook/booting/#jourcnalct-systemd-journal-anfragen", "text": "Kernelinformationen mit -k journalctl -k", "title": "jourcnalct: systemd Journal anfragen"}, {"location": "linuxbook/devices/", "text": "Ger\u00e4teverzeichnis \u2691 Unsere Ger\u00e4te liegen als Dateien in /dev . cd /dev ls Hot Plug \u2691 Hardware Ger\u00e4t, welches w\u00e4hrend der Laufzeit verwendet werden kann, zb. USB-Stick . Cold Plug \u2691 nur in ausgeschalteten Zustand einh\u00e4ngen kann, z.B. eine IDE Festplatte", "title": "devices"}, {"location": "linuxbook/devices/#gerateverzeichnis", "text": "Unsere Ger\u00e4te liegen als Dateien in /dev . cd /dev ls", "title": "Ger\u00e4teverzeichnis"}, {"location": "linuxbook/devices/#hot-plug", "text": "Hardware Ger\u00e4t, welches w\u00e4hrend der Laufzeit verwendet werden kann, zb. USB-Stick .", "title": "Hot Plug"}, {"location": "linuxbook/devices/#cold-plug", "text": "nur in ausgeschalteten Zustand einh\u00e4ngen kann, z.B. eine IDE Festplatte", "title": "Cold Plug"}, {"location": "linuxbook/hardware/", "text": "Ger\u00e4te auflisten \u2691 PCI Ger\u00e4te auflisten \u2691 mit dem Argument -vv erhalten wir eine Verbose Ansicht. lspci -vv mit dem Argument -k kann man sehen, welche Module das Ger\u00e4t verwenden: lspci -k USB Ger\u00e4te \u2691 lsusb -vv lsusb", "title": "hardware"}, {"location": "linuxbook/hardware/#gerate-auflisten", "text": "", "title": "Ger\u00e4te auflisten"}, {"location": "linuxbook/hardware/#pci-gerate-auflisten", "text": "mit dem Argument -vv erhalten wir eine Verbose Ansicht. lspci -vv mit dem Argument -k kann man sehen, welche Module das Ger\u00e4t verwenden: lspci -k", "title": "PCI Ger\u00e4te auflisten"}, {"location": "linuxbook/hardware/#usb-gerate", "text": "lsusb -vv lsusb", "title": "USB Ger\u00e4te"}, {"location": "linuxbook/modules/", "text": "Kernelmodule \u2691 Welche Module hat der Kernel aktuell geladen? lsmod 4-spaltige \u00dcbersicht aller Module (Modulname, Modulgr\u00f6\u00dfe, benutzt von wievielen, benutzt von wem). Beispiel: \u2691 lsmod [...] lpc_ich 28672 0 sdhci 81920 1 sdhci_pci libahci 45056 1 ahci crc_itu_t 16384 1 firewire_core wmi 32768 2 wmi_bmof,think_lmi video 57344 2 thinkpad_acpi,i915 Kernelmodule Informationen \u2691 Um mehr Informationen \u00fcber ein Modul zu erhalten, gibt es modinfo modinfo libahci \u276f modinfo libahci filename: /lib/modules/5.15.0-58-generic/kernel/drivers/ata/libahci.ko license: GPL description: Common AHCI SATA low-level routines author: Jeff Garzik srcversion: D5902F34F56CD0E9B29D99F depends: retpoline: Y intree: Y name: libahci vermagic: 5.15.0-58-generic SMP mod_unload modversions sig_id: PKCS#7 .. Kernelmodule laden und entladen mit modprobe \u2691 Um zur Laufzeit Module entladen: sudo modprobe -r ip_tables Pr\u00fcfen, ob Module noch l\u00e4uft: lsmod | grep ip_tables Modul wieder laden: sudo modprobe ip_tables", "title": "modules"}, {"location": "linuxbook/modules/#kernelmodule", "text": "Welche Module hat der Kernel aktuell geladen? lsmod 4-spaltige \u00dcbersicht aller Module (Modulname, Modulgr\u00f6\u00dfe, benutzt von wievielen, benutzt von wem).", "title": "Kernelmodule"}, {"location": "linuxbook/modules/#beispiel", "text": "lsmod [...] lpc_ich 28672 0 sdhci 81920 1 sdhci_pci libahci 45056 1 ahci crc_itu_t 16384 1 firewire_core wmi 32768 2 wmi_bmof,think_lmi video 57344 2 thinkpad_acpi,i915", "title": "Beispiel:"}, {"location": "linuxbook/modules/#kernelmodule-informationen", "text": "Um mehr Informationen \u00fcber ein Modul zu erhalten, gibt es modinfo modinfo libahci \u276f modinfo libahci filename: /lib/modules/5.15.0-58-generic/kernel/drivers/ata/libahci.ko license: GPL description: Common AHCI SATA low-level routines author: Jeff Garzik srcversion: D5902F34F56CD0E9B29D99F depends: retpoline: Y intree: Y name: libahci vermagic: 5.15.0-58-generic SMP mod_unload modversions sig_id: PKCS#7 ..", "title": "Kernelmodule Informationen"}, {"location": "linuxbook/modules/#kernelmodule-laden-und-entladen-mit-modprobe", "text": "Um zur Laufzeit Module entladen: sudo modprobe -r ip_tables Pr\u00fcfen, ob Module noch l\u00e4uft: lsmod | grep ip_tables Modul wieder laden: sudo modprobe ip_tables", "title": "Kernelmodule laden und entladen mit modprobe"}, {"location": "linuxbook/systemd/", "text": "systemd \u2691 Ist ein System- und Sitzungs-Manager (Init-System), der f\u00fcr die Verwaltung aller auf dem System laufenden Dienste \u00fcber die gesamte Betriebszeit des Rechners, vom Startvorgang bis zum Herunterfahren, zust\u00e4ndig ist. Prozesse werden dabei immer (soweit m\u00f6glich) parallel gestartet, um den Bootvorgang m\u00f6glichst kurz zu halten. Units \u2691 systemd holt alle seine Vorgaben und Einstellungen zur Verwaltung aus Dateien, in der Terminologie von systemd sind dies \"Units\". Dabei wird zwischen systemweit geltenden Units und solchen, die nur f\u00fcr den jeweiligen Benutzer-Bereich gelten User Units unterschieden. Es gibt diverse Arten von Units wie z.B. Service Units zum Starten von Diensten oder Timer Units zum (wiederholten) Ausf\u00fchren einer Aktion zu einem bestimmten Zeitpunkt. Dateien finden sich in zwei Dateien Unter /etc/systemd/system finden sich die Units, dh. Dateien die gestartet werden. Weitere Dateien: /lib/systemd/system systemctl \u2691 Manager f\u00fcr das system.d Alle Units auflisten: systemctl list-units Status eines Diestes erfragen systemctl status docker", "title": "systemd"}, {"location": "linuxbook/systemd/#systemd", "text": "Ist ein System- und Sitzungs-Manager (Init-System), der f\u00fcr die Verwaltung aller auf dem System laufenden Dienste \u00fcber die gesamte Betriebszeit des Rechners, vom Startvorgang bis zum Herunterfahren, zust\u00e4ndig ist. Prozesse werden dabei immer (soweit m\u00f6glich) parallel gestartet, um den Bootvorgang m\u00f6glichst kurz zu halten.", "title": "systemd"}, {"location": "linuxbook/systemd/#units", "text": "systemd holt alle seine Vorgaben und Einstellungen zur Verwaltung aus Dateien, in der Terminologie von systemd sind dies \"Units\". Dabei wird zwischen systemweit geltenden Units und solchen, die nur f\u00fcr den jeweiligen Benutzer-Bereich gelten User Units unterschieden. Es gibt diverse Arten von Units wie z.B. Service Units zum Starten von Diensten oder Timer Units zum (wiederholten) Ausf\u00fchren einer Aktion zu einem bestimmten Zeitpunkt. Dateien finden sich in zwei Dateien Unter /etc/systemd/system finden sich die Units, dh. Dateien die gestartet werden. Weitere Dateien: /lib/systemd/system", "title": "Units"}, {"location": "linuxbook/systemd/#systemctl", "text": "Manager f\u00fcr das system.d Alle Units auflisten: systemctl list-units Status eines Diestes erfragen systemctl status docker", "title": "systemctl"}, {"location": "recipes/stirfry/", "text": "Stirfry ( Notiz an mich: Quellen f\u00fcr ein Stirfry mit Austernso\u00dfe, Soya und Kochwein: Stirfry mit Rind und Brocoli Black Pepper Stirfry Sichuan Gai Lan", "title": "stirfry"}, {"location": "writing/build_your_own_wiki/", "text": "This page is an early version of a WIP If you don't want to start from scratch, you can fork the blue book and start writing straight away. Enable clickable navigation sections \u2691 By default, mkdocs doesn't let you to have clickable sections that lead to an index page. It has been long discussed in #2060 , #1042 , #1139 and #1975 . Thankfully, oprypin has solved it with the mkdocs-section-index plugin. Install the plugin with pip install mkdocs-section-index and configure your mkdocs.yml as: nav : - Frob : index.md - Baz : baz.md - Borgs : - borgs/index.md - Bar : borgs/bar.md - Foo : borgs/foo.md plugins : - section-index Unconnected thoughts \u2691 Set up ci with pre-commit and ALE . Create a custom commitizen changelog configuration to generate the rss entries periodically and publish them as github releases. If some code needs to be in a file use: !!! note \"File ~/script.py\" ```python print ( 'hi' ) ``` Define how to add links to newly created documents in the whole wiki. Deploy fast, deploy early Grab the ideas of todo Define a meaningful tag policy. https://www.gwern.net/About#confidence-tags https://www.gwern.net/About#importance-tags Decide a meaningful nav policy. How to decide when to create a new section, Add to the index nav once it's finished, not before Use the tbd tag to mark the articles that need attention. Avoid file hardcoding, use mkdocs autolinks plugin , or mkdocs-altlink-plugin if #15 is not solved and you don't need to link images. Use underscores for the file names, so the autocompletion feature of your editor works. Add a link to the github pages site both in the git repository description and in the README. Use Github Actions to build your blog. Make redirections of refactored articles. The newsletter could be split in years with one summary once the year has ended I want an rss support for my newsletters, mkdocs is not going to support it , so the solution is to use a static template rss.xml that is manually generated each time you create a new newsletter article. I could develop a plugin so it creates it at build time. Use of custom domains Material guide to enable code blocks highlight : From what I've seen in some github issue, you should not use it with codehilite, although you still have their syntax. User Guide: Writing your docs Example of markdown writing with Material theme and it's source Add revision date Add test for rotten links , it seems that htmltest was meant to be faster. Adds tooltips to preview the content of page links using tooltipster . I only managed to make it work with internal links and in an ugly way... So I'm not using it. Plugin to generate Python Docstrings Add Mermaid graphs Add Plantuml graphs Analyze the text readability About page \u2691 Nikita Gwern Create a local server to visualize the documentation \u2691 mkdocs serve Links \u2691 Nikita's wiki workflow", "title": "Build your own wiki"}, {"location": "writing/build_your_own_wiki/#enable-clickable-navigation-sections", "text": "By default, mkdocs doesn't let you to have clickable sections that lead to an index page. It has been long discussed in #2060 , #1042 , #1139 and #1975 . Thankfully, oprypin has solved it with the mkdocs-section-index plugin. Install the plugin with pip install mkdocs-section-index and configure your mkdocs.yml as: nav : - Frob : index.md - Baz : baz.md - Borgs : - borgs/index.md - Bar : borgs/bar.md - Foo : borgs/foo.md plugins : - section-index", "title": "Enable clickable navigation sections"}, {"location": "writing/build_your_own_wiki/#unconnected-thoughts", "text": "Set up ci with pre-commit and ALE . Create a custom commitizen changelog configuration to generate the rss entries periodically and publish them as github releases. If some code needs to be in a file use: !!! note \"File ~/script.py\" ```python print ( 'hi' ) ``` Define how to add links to newly created documents in the whole wiki. Deploy fast, deploy early Grab the ideas of todo Define a meaningful tag policy. https://www.gwern.net/About#confidence-tags https://www.gwern.net/About#importance-tags Decide a meaningful nav policy. How to decide when to create a new section, Add to the index nav once it's finished, not before Use the tbd tag to mark the articles that need attention. Avoid file hardcoding, use mkdocs autolinks plugin , or mkdocs-altlink-plugin if #15 is not solved and you don't need to link images. Use underscores for the file names, so the autocompletion feature of your editor works. Add a link to the github pages site both in the git repository description and in the README. Use Github Actions to build your blog. Make redirections of refactored articles. The newsletter could be split in years with one summary once the year has ended I want an rss support for my newsletters, mkdocs is not going to support it , so the solution is to use a static template rss.xml that is manually generated each time you create a new newsletter article. I could develop a plugin so it creates it at build time. Use of custom domains Material guide to enable code blocks highlight : From what I've seen in some github issue, you should not use it with codehilite, although you still have their syntax. User Guide: Writing your docs Example of markdown writing with Material theme and it's source Add revision date Add test for rotten links , it seems that htmltest was meant to be faster. Adds tooltips to preview the content of page links using tooltipster . I only managed to make it work with internal links and in an ugly way... So I'm not using it. Plugin to generate Python Docstrings Add Mermaid graphs Add Plantuml graphs Analyze the text readability", "title": "Unconnected thoughts"}, {"location": "writing/build_your_own_wiki/#about-page", "text": "Nikita Gwern", "title": "About page"}, {"location": "writing/build_your_own_wiki/#create-a-local-server-to-visualize-the-documentation", "text": "mkdocs serve", "title": "Create a local server to visualize the documentation"}, {"location": "writing/build_your_own_wiki/#links", "text": "Nikita's wiki workflow", "title": "Links"}, {"location": "writing/orthography/", "text": "Bad grammar can thwart communication. It's especially important in today's world where we don't have the chance to have in person interactions or when your words might reach a wide audience. The quality of your text will impact how your message reaches people. This article is an effort to gather all my common pitfalls. Use of z or s in some words \u2691 It looks like American english uses z while British uses s , some examples: Organizations vs organisation . Authorization vs authorisation . Customized vs customised . Both forms are correct, so choose the one that suits your liking. Stop saying \"I know\" \u2691 Using \"I know\" may not be the best way to show the other person that you've received the information. You can take the chance to use other words that additionally gives more context on how you stand with the information you've received, thus improving the communication and creating a bond. Each of the next words carries a different nuance: Recognize : You acknowledge the truth, existence or validity of the information. From my perspective : You're showing that given the information, you see the person's point and state that you have a different one. Appreciate : You recognize the implications and true value of the subject and are being thankful for the information. Understand : You show that you've perceived the underlying meaning of the information. It implies a deeper level of information processing. I see : You show that you understand and that you're paying all your attention. Besides showing where you stand or how you feel, you can use other phrases that make connection at the visual, audio and kinesthetic levels to improve the communication. I hear what you're saying : Shows auditory connection. I get the picture or I see what you mean : Shows visual connection. I catch your drift : Shows kinesthetic connection. You can also add information when saying that you don't know. For example you can use: Misread : You give the idea that you perceived the information wrong. Misunderstood : You perceived the information well, but formed the wrong idea in your head. Mixed up : You had the correct information and idea, but you ended up saying or doing the wrong answer. Confused : You have the correct information but you can't form a clear idea. Use collocations \u2691 Collocation refers to a natural combination of words that are closely affiliated with each other. They make it easier to avoid overused or ambiguous words like \"very\", \"nice\", or \"beautiful\", by using a pair of words that fit the context better and that have a more precise meaning. Skilled users of the language can produce effects such as humor by varying the normal patterns of collocation. Stop saying \"very\" \u2691 wrong collocation Very full stuffed Very risky perilous Very thin slender Very long extensive Very interesting intriguing Very happy jubilant, delighted, joyful, overjoyed Very worried anxious Very Thirsty parched Very dirty squalid Very clean spotless Very rude vulgar Very short brief Very boring dull Very good superb, marvelous, excellent, extraordinary, splendid, spectacular Very hot scalding / scorching Very cold freezing Very hungry ravenous Very slow sluggish Very fast rapid Very tired exhausted Very poor destitute Very rich wealthy Very hard challenging Very smart bright Very beautiful mesmerizing, stunning, astonishing, charming, magnificent Very sad depressing Very funny hilarious, absurd Very scared petrified / frightened / fearful Very sleepy drowsy Very full crowded Very ugly hideous Very wicked villainous Very quiet silent Very accurate exact Very large huge Very powerful compelling Very lazy indolent Very fat obese Very often frequently Very smooth sleek Very long term enduring Very strong unyelding Very tasty delicious Very valuable precious Very creative innovative Very light luminous Very wet soaked Very bright blinding Very strange abnormal. bizarre, outlandish Very small tiny Very big giant, immense, massive Very bad horrendous, atrocious, horrible Very important essential Very exciting engaging Very calm peaceful Very painful agonizing Very expensive priceless Very drunk intoxicated Very humble polite Very smart intelligent I'm good or I'm well \u2691 TL;DR Use I'm well when referring to being ill, use I'm good for the rest. Good is an adjective. Well is usually an adverb, but it can also act as an adjective. Adjectives modify nouns. When you say you're having a good day, the adjective good modifies the noun day . When people say I'm good , they're using good to modify I . Because I is a noun, this use of good is correct. The confusion comes when using the verb am , which makes people think we need an adverb. For example, you might say, I play the piano poorly . The adverb poorly is modifying the verb play , so that sentence is correct. But the sentence I'm good ', good is modifying I , it's not modifying am , so good is correct, and well is not. Well is an adverb, they are here to modify verbs, adjectives or other adverbs. You might say I play the piano well . Here well changes the verb play . However, well can also be an adjective, usually to describe someone who is in good health. So when some one says I'm well , they're using well as an adjective modifying I . Won't vs Wont \u2691 Won't is the correct way to contract will not. Wont is a synonym of \"a habit\". For example, \"He went for a morning jog, as was his wont\". How to use the singular they \u2691 The singular \u201cthey\u201d is a generic third-person pronoun used in English. When readers see a gendered pronoun, such as he or she, they make assumptions about the gender of the person being described. It's better to use the singular \u201cthey\u201d because it is inclusive of all people and helps writers avoid making assumptions about gender. You should use it in these cases: When referring to a generic person whose gender is unknown or irrelevant to the context and When referring to a specific, known person who uses \u201cthey\u201d as their pronoun. When \u201cthey\u201d is the subject of a sentence, \u201cthey\u201d takes a plural verb regardless of whether \u201cthey\u201d is meant to be singular or plural. For example, write \u201cthey are\u201d not \u201cthey is\u201d. The singular \u201cthey\u201d works similarly to the singular \u201cyou\u201d, even though \u201cyou\u201d may refer to one person or multiple people. However, if the noun in one sentence is a word like \u201cindividual\u201d or a person\u2019s name, use a singular verb. Where to add your pronouns \u2691 The correct place to add your pronouns is after you present yourself, such as: Hi, I\u2019m Lyz (he/him), I'm writing to tell you\u2026 When to capitalize after a question mark \u2691 TL;DR If the sentence ends after the question mark you should capitalize, if it doesn't end, you shouldn't have used the question mark, since it ends a sentence. The capitalization rule that we care about here is that the first word of a sentence starts with a capital letter, so the question is really about what ends a sentence. The answer to that is easy: terminal punctuation, i.e. a full stop, question mark or exclamation mark. There's a visual clue in that ? and ! are decorated full stops; you just have to remember that a colon ( : ) isn't really a decorated full stop, not that you'd ever know by looking at it. Colons, semicolons and commas aren't terminal punctuation, so they don't end a sentence and so don't force the next letter to be a capital. It may be a capital letter for some other reason such as being the start of a proper name, but not because it is starting a sentence. There are exceptions to this rule, occasions when ? and ! become non terminal punctuation. The most obvious is in quoted speech: if the speaker asks a question or makes an exclamation, the ? or ! doesn't have to be terminal if the sentence carries on after the quote. \"Should I write it like this?\" he asked. \"Or perhaps like this?\" The other class of exception is for what are probably really parenthetical comments. If you have a short phrase that you could have put aside in parentheses or dashes, then a question mark or exclamation mark can be used at the end of that phrase without ending the sentence. Be sparing with this. It looks wrong at a first read. Should I write it like this, or abracadabra! like this? When joining many questions you might may have the doubt of which of the following is correct: Should I write it like this? Or perhaps like this? Should I write it like this? or perhaps like this? Should I write it like this, or perhaps like that? \"Should I write it like this?\" he asked, \"or perhaps like that?\" The second with the lowercase or is just plain wrong. Crusty old grammarians who disapprove of starting sentences with conjunctions may frown at example 1 all they like, but it's a perfectly acceptable fragmentary sentence. Whether it's the right answer or not is another question entirely. Example 1 makes the point that the questions are distinct, though they are strongly linked otherwise the whole structure wouldn't work. Example 3 on the other hand emphasizes that the two questions are options in a common situation, as well as reflecting a different way of saying them. That is clear in this case because the two questions are tightly coupled alternatives. However, consider the following: Are the lights green? Or is the switch up? Are the lights green, or is the switch up? Both of these examples imply that the state of the lights and the state of the switch are related somehow. Version 2 couples them more tightly; I would usually assume (without more context) that either this is the same question being asked in two different ways (i.e. that the switch being up should cause the lights to be green), or that they are an exhaustive list of possibilities (either the switch is up or the lights are green, but not both or neither). This isn't an absolute rule, but it's quite strongly implied. Example 4 is also wrong, though it has a better disguise. If you unwrap the quotes, what you get is: Should I write it like this? or perhaps like this? Which is example 2 back again. What you actually want is one of: \"Should I write it like this?\" he asked. \"Or perhaps like this?\" (i.e. example 1) \"Should I write it like this,\" he asked, \"or perhaps like that? (i.e. example 3) Exclamation marks work like question marks for this purpose. Semicolons don't; they end a clause, not a sentence. When to write Apostrophes before an s \u2691 For most singular nouns, add apostrophe + s : The writer's desk . For most plural nouns, add apostrophe : The writers' desk (multiple writers). For plural nouns that do not end in s, add apostrophe + s : The geese's migration route . For singular proper nouns both apostrophe and apostrophe + s is accepted, but as the plural proper nouns ending in s, the correct form is apostrophe I'd use that for both, so: Charles Dickens' novels and The Smiths' vacation . The personal pronouns, do not have apostrophes to form possessives, such as your, yours, hers, its, ours, their, whose, and theirs. In fact, for some of these pronouns, adding an apostrophe forms a contraction instead of a possessive. Who vs Whom \u2691 If you can replace the word with she or he , use who . If you can replace it with her or him , use whom . Who : Should be used to refer to the subject of a sentence. Whom : Should be used to refer to the object of a verb or preposition. A vs An \u2691 We were all taught that a precedes a word starting with a consonant and that an precedes a word starting with a vowel (a, e, i, o, u, and sometimes y). But what matters is the sound of the letter beginning the word, not just the letter itself. The way we say the word will determine whether or not we use a or an . If the word begins with a vowel sound, you must use an . If it begins with a consonant sound, you must use a . Comma before and \u2691 There are two cases: It's required to put a comma before and when it\u2019s connecting two independent clauses. It\u2019s almost always optional the use of comma before and in lists. This case is also known as serial commas or Oxford commas . Since in some cases is useful, I'm going to use them to reduce the mental load. Bear with me or Bare with me \u2691 \"Bear with me\" is the correct form. References \u2691 Julia Olech article on grammar , even though is a bit sensational and I don't like the overall tone, it has good insights on common grammar mistakes. Thanks for the link Dave :) JamesESL videos", "title": "Orthography Rules"}, {"location": "writing/orthography/#use-of-z-or-s-in-some-words", "text": "It looks like American english uses z while British uses s , some examples: Organizations vs organisation . Authorization vs authorisation . Customized vs customised . Both forms are correct, so choose the one that suits your liking.", "title": "Use of z or s in some words"}, {"location": "writing/orthography/#stop-saying-i-know", "text": "Using \"I know\" may not be the best way to show the other person that you've received the information. You can take the chance to use other words that additionally gives more context on how you stand with the information you've received, thus improving the communication and creating a bond. Each of the next words carries a different nuance: Recognize : You acknowledge the truth, existence or validity of the information. From my perspective : You're showing that given the information, you see the person's point and state that you have a different one. Appreciate : You recognize the implications and true value of the subject and are being thankful for the information. Understand : You show that you've perceived the underlying meaning of the information. It implies a deeper level of information processing. I see : You show that you understand and that you're paying all your attention. Besides showing where you stand or how you feel, you can use other phrases that make connection at the visual, audio and kinesthetic levels to improve the communication. I hear what you're saying : Shows auditory connection. I get the picture or I see what you mean : Shows visual connection. I catch your drift : Shows kinesthetic connection. You can also add information when saying that you don't know. For example you can use: Misread : You give the idea that you perceived the information wrong. Misunderstood : You perceived the information well, but formed the wrong idea in your head. Mixed up : You had the correct information and idea, but you ended up saying or doing the wrong answer. Confused : You have the correct information but you can't form a clear idea.", "title": "Stop saying \"I know\""}, {"location": "writing/orthography/#use-collocations", "text": "Collocation refers to a natural combination of words that are closely affiliated with each other. They make it easier to avoid overused or ambiguous words like \"very\", \"nice\", or \"beautiful\", by using a pair of words that fit the context better and that have a more precise meaning. Skilled users of the language can produce effects such as humor by varying the normal patterns of collocation.", "title": "Use collocations"}, {"location": "writing/orthography/#stop-saying-very", "text": "wrong collocation Very full stuffed Very risky perilous Very thin slender Very long extensive Very interesting intriguing Very happy jubilant, delighted, joyful, overjoyed Very worried anxious Very Thirsty parched Very dirty squalid Very clean spotless Very rude vulgar Very short brief Very boring dull Very good superb, marvelous, excellent, extraordinary, splendid, spectacular Very hot scalding / scorching Very cold freezing Very hungry ravenous Very slow sluggish Very fast rapid Very tired exhausted Very poor destitute Very rich wealthy Very hard challenging Very smart bright Very beautiful mesmerizing, stunning, astonishing, charming, magnificent Very sad depressing Very funny hilarious, absurd Very scared petrified / frightened / fearful Very sleepy drowsy Very full crowded Very ugly hideous Very wicked villainous Very quiet silent Very accurate exact Very large huge Very powerful compelling Very lazy indolent Very fat obese Very often frequently Very smooth sleek Very long term enduring Very strong unyelding Very tasty delicious Very valuable precious Very creative innovative Very light luminous Very wet soaked Very bright blinding Very strange abnormal. bizarre, outlandish Very small tiny Very big giant, immense, massive Very bad horrendous, atrocious, horrible Very important essential Very exciting engaging Very calm peaceful Very painful agonizing Very expensive priceless Very drunk intoxicated Very humble polite Very smart intelligent", "title": "Stop saying \"very\""}, {"location": "writing/orthography/#im-good-or-im-well", "text": "TL;DR Use I'm well when referring to being ill, use I'm good for the rest. Good is an adjective. Well is usually an adverb, but it can also act as an adjective. Adjectives modify nouns. When you say you're having a good day, the adjective good modifies the noun day . When people say I'm good , they're using good to modify I . Because I is a noun, this use of good is correct. The confusion comes when using the verb am , which makes people think we need an adverb. For example, you might say, I play the piano poorly . The adverb poorly is modifying the verb play , so that sentence is correct. But the sentence I'm good ', good is modifying I , it's not modifying am , so good is correct, and well is not. Well is an adverb, they are here to modify verbs, adjectives or other adverbs. You might say I play the piano well . Here well changes the verb play . However, well can also be an adjective, usually to describe someone who is in good health. So when some one says I'm well , they're using well as an adjective modifying I .", "title": "I'm good or I'm well"}, {"location": "writing/orthography/#wont-vs-wont", "text": "Won't is the correct way to contract will not. Wont is a synonym of \"a habit\". For example, \"He went for a morning jog, as was his wont\".", "title": "Won't vs Wont"}, {"location": "writing/orthography/#how-to-use-the-singular-they", "text": "The singular \u201cthey\u201d is a generic third-person pronoun used in English. When readers see a gendered pronoun, such as he or she, they make assumptions about the gender of the person being described. It's better to use the singular \u201cthey\u201d because it is inclusive of all people and helps writers avoid making assumptions about gender. You should use it in these cases: When referring to a generic person whose gender is unknown or irrelevant to the context and When referring to a specific, known person who uses \u201cthey\u201d as their pronoun. When \u201cthey\u201d is the subject of a sentence, \u201cthey\u201d takes a plural verb regardless of whether \u201cthey\u201d is meant to be singular or plural. For example, write \u201cthey are\u201d not \u201cthey is\u201d. The singular \u201cthey\u201d works similarly to the singular \u201cyou\u201d, even though \u201cyou\u201d may refer to one person or multiple people. However, if the noun in one sentence is a word like \u201cindividual\u201d or a person\u2019s name, use a singular verb.", "title": "How to use the singular they"}, {"location": "writing/orthography/#where-to-add-your-pronouns", "text": "The correct place to add your pronouns is after you present yourself, such as: Hi, I\u2019m Lyz (he/him), I'm writing to tell you\u2026", "title": "Where to add your pronouns"}, {"location": "writing/orthography/#when-to-capitalize-after-a-question-mark", "text": "TL;DR If the sentence ends after the question mark you should capitalize, if it doesn't end, you shouldn't have used the question mark, since it ends a sentence. The capitalization rule that we care about here is that the first word of a sentence starts with a capital letter, so the question is really about what ends a sentence. The answer to that is easy: terminal punctuation, i.e. a full stop, question mark or exclamation mark. There's a visual clue in that ? and ! are decorated full stops; you just have to remember that a colon ( : ) isn't really a decorated full stop, not that you'd ever know by looking at it. Colons, semicolons and commas aren't terminal punctuation, so they don't end a sentence and so don't force the next letter to be a capital. It may be a capital letter for some other reason such as being the start of a proper name, but not because it is starting a sentence. There are exceptions to this rule, occasions when ? and ! become non terminal punctuation. The most obvious is in quoted speech: if the speaker asks a question or makes an exclamation, the ? or ! doesn't have to be terminal if the sentence carries on after the quote. \"Should I write it like this?\" he asked. \"Or perhaps like this?\" The other class of exception is for what are probably really parenthetical comments. If you have a short phrase that you could have put aside in parentheses or dashes, then a question mark or exclamation mark can be used at the end of that phrase without ending the sentence. Be sparing with this. It looks wrong at a first read. Should I write it like this, or abracadabra! like this? When joining many questions you might may have the doubt of which of the following is correct: Should I write it like this? Or perhaps like this? Should I write it like this? or perhaps like this? Should I write it like this, or perhaps like that? \"Should I write it like this?\" he asked, \"or perhaps like that?\" The second with the lowercase or is just plain wrong. Crusty old grammarians who disapprove of starting sentences with conjunctions may frown at example 1 all they like, but it's a perfectly acceptable fragmentary sentence. Whether it's the right answer or not is another question entirely. Example 1 makes the point that the questions are distinct, though they are strongly linked otherwise the whole structure wouldn't work. Example 3 on the other hand emphasizes that the two questions are options in a common situation, as well as reflecting a different way of saying them. That is clear in this case because the two questions are tightly coupled alternatives. However, consider the following: Are the lights green? Or is the switch up? Are the lights green, or is the switch up? Both of these examples imply that the state of the lights and the state of the switch are related somehow. Version 2 couples them more tightly; I would usually assume (without more context) that either this is the same question being asked in two different ways (i.e. that the switch being up should cause the lights to be green), or that they are an exhaustive list of possibilities (either the switch is up or the lights are green, but not both or neither). This isn't an absolute rule, but it's quite strongly implied. Example 4 is also wrong, though it has a better disguise. If you unwrap the quotes, what you get is: Should I write it like this? or perhaps like this? Which is example 2 back again. What you actually want is one of: \"Should I write it like this?\" he asked. \"Or perhaps like this?\" (i.e. example 1) \"Should I write it like this,\" he asked, \"or perhaps like that? (i.e. example 3) Exclamation marks work like question marks for this purpose. Semicolons don't; they end a clause, not a sentence.", "title": "When to capitalize after a question mark"}, {"location": "writing/orthography/#when-to-write-apostrophes-before-an-s", "text": "For most singular nouns, add apostrophe + s : The writer's desk . For most plural nouns, add apostrophe : The writers' desk (multiple writers). For plural nouns that do not end in s, add apostrophe + s : The geese's migration route . For singular proper nouns both apostrophe and apostrophe + s is accepted, but as the plural proper nouns ending in s, the correct form is apostrophe I'd use that for both, so: Charles Dickens' novels and The Smiths' vacation . The personal pronouns, do not have apostrophes to form possessives, such as your, yours, hers, its, ours, their, whose, and theirs. In fact, for some of these pronouns, adding an apostrophe forms a contraction instead of a possessive.", "title": "When to write Apostrophes before an s"}, {"location": "writing/orthography/#who-vs-whom", "text": "If you can replace the word with she or he , use who . If you can replace it with her or him , use whom . Who : Should be used to refer to the subject of a sentence. Whom : Should be used to refer to the object of a verb or preposition.", "title": "Who vs Whom"}, {"location": "writing/orthography/#a-vs-an", "text": "We were all taught that a precedes a word starting with a consonant and that an precedes a word starting with a vowel (a, e, i, o, u, and sometimes y). But what matters is the sound of the letter beginning the word, not just the letter itself. The way we say the word will determine whether or not we use a or an . If the word begins with a vowel sound, you must use an . If it begins with a consonant sound, you must use a .", "title": "A vs An"}, {"location": "writing/orthography/#comma-before-and", "text": "There are two cases: It's required to put a comma before and when it\u2019s connecting two independent clauses. It\u2019s almost always optional the use of comma before and in lists. This case is also known as serial commas or Oxford commas . Since in some cases is useful, I'm going to use them to reduce the mental load.", "title": "Comma before and"}, {"location": "writing/orthography/#bear-with-me-or-bare-with-me", "text": "\"Bear with me\" is the correct form.", "title": "Bear with me or Bare with me"}, {"location": "writing/orthography/#references", "text": "Julia Olech article on grammar , even though is a bit sensational and I don't like the overall tone, it has good insights on common grammar mistakes. Thanks for the link Dave :) JamesESL videos", "title": "References"}, {"location": "writing/writing/", "text": "Writing is difficult, at least for me. Even more if you aren't using your native tongue. I'm focusing my efforts in improving my grammar and orthography and writing style . Tests \u2691 Using automatic tools that highlight the violations of the previous principles may help you to internalize all the measures, even more with the new ones. Configure your editor to: Run a spell checker that you can check as you write. Alert you on new orthography rules you want to adopt. Use linters to raise your awareness on the rest of issues. alex to find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. markdownlint : style checker and lint tool for Markdown/CommonMark files. proselint : Is another linter for prose. write-good is a naive linter for English prose. Use formatters to make your writing experience more pleasant. mdformat : I haven't tested it yet, but looks promising. There are some checks that I wasn't able to adopt: Try to use less than 30 words per sentence. Check that every sentence is ended with a dot. Be consistent across document structures, use References instead of Links , or Installation instead of Install . gwern markdown-lint.sh script file . Avoid the use of here , use descriptive link text. Rotten links: use linkchecker (I think there was a mkdocs plugin to do this). Also read how to archive urls . check for use of the word \"significant\"/\"significance\" and insert \"[statistically]\" as appropriate (to disambiguate between effect sizes and statistical significance; this common confusion is one reason for \"statistical-significance considered harmful\" ) Vim enhancements \u2691 vim-pencil looks promising but it's still not ready mdnav opens links to urls or files when pressing enter in normal mode over a markdown link, similar to gx but more powerful. I specially like the ability of following [self referencing link][] links, that allows storing the links at the bottom. Writing workflow \u2691 Start with a template. Use synoptical reading to gather ideas in an unconnected thoughts section. Once you've got several refactor them in sections with markdown headers. Ideally you'll want to wrap your whole blog post into a story or timeline. Add an abstract so the reader may decide if she wants to read it. Publication \u2691 Think how to publicize: Hacker News Reddit LessWrong (and further sites as appropriate) References \u2691 Awesome: Nikita's writing notes Gwern's writing checklist Good: Long Naomi pen post with some key ideas Doing \u2691 https://github.com/mnielsen/notes-on-writing/blob/master/notes_on_writing.md#readme Todo \u2691 https://www.scottadamssays.com/2015/08/22/the-day-you-became-a-better-writer-2nd-look/ https://blog.stephsmith.io/learning-to-write-with-confidence/ https://styleguide.mailchimp.com/tldr/ https://content-guide.18f.gov/inclusive-language/ https://performancejs.com/post/31b361c/13-Tips-for-Writing-a-Technical-Book https://github.com/RacheltheEditor/ProductionGuide#readme https://mkaz.blog/misc/notes-on-technical-writing/ https://www.swyx.io/writing/cfp-advice/ https://sivers.org/d22 https://homes.cs.washington.edu/~mernst/advice/write-technical-paper.html Investigate on readability tests: Definition Introduction on Readability List of readability tests and formulas An example of a formula Vim plugins \u2691 Vim-lexical Vim-textobj-quote Vim-textobj-sentence Vim-ditto Vim-exchange Books \u2691 https://www.amazon.de/dp/0060891548/ref=as_li_tl?ie=UTF8&linkCode=gs2&linkId=39f2ab8ab47769b2a106e9667149df30&creativeASIN=0060891548&tag=gregdoesit03-21&creative=9325&camp=1789 https://www.amazon.de/dp/0143127799/ref=as_li_tl?ie=UTF8&linkCode=gs2&linkId=ff9322c17ca288b1d9d6b5fb8d6df619&creativeASIN=0143127799&tag=gregdoesit03-21&creative=9325&camp=1789", "title": "Writing"}, {"location": "writing/writing/#tests", "text": "Using automatic tools that highlight the violations of the previous principles may help you to internalize all the measures, even more with the new ones. Configure your editor to: Run a spell checker that you can check as you write. Alert you on new orthography rules you want to adopt. Use linters to raise your awareness on the rest of issues. alex to find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. markdownlint : style checker and lint tool for Markdown/CommonMark files. proselint : Is another linter for prose. write-good is a naive linter for English prose. Use formatters to make your writing experience more pleasant. mdformat : I haven't tested it yet, but looks promising. There are some checks that I wasn't able to adopt: Try to use less than 30 words per sentence. Check that every sentence is ended with a dot. Be consistent across document structures, use References instead of Links , or Installation instead of Install . gwern markdown-lint.sh script file . Avoid the use of here , use descriptive link text. Rotten links: use linkchecker (I think there was a mkdocs plugin to do this). Also read how to archive urls . check for use of the word \"significant\"/\"significance\" and insert \"[statistically]\" as appropriate (to disambiguate between effect sizes and statistical significance; this common confusion is one reason for \"statistical-significance considered harmful\" )", "title": "Tests"}, {"location": "writing/writing/#vim-enhancements", "text": "vim-pencil looks promising but it's still not ready mdnav opens links to urls or files when pressing enter in normal mode over a markdown link, similar to gx but more powerful. I specially like the ability of following [self referencing link][] links, that allows storing the links at the bottom.", "title": "Vim enhancements"}, {"location": "writing/writing/#writing-workflow", "text": "Start with a template. Use synoptical reading to gather ideas in an unconnected thoughts section. Once you've got several refactor them in sections with markdown headers. Ideally you'll want to wrap your whole blog post into a story or timeline. Add an abstract so the reader may decide if she wants to read it.", "title": "Writing workflow"}, {"location": "writing/writing/#publication", "text": "Think how to publicize: Hacker News Reddit LessWrong (and further sites as appropriate)", "title": "Publication"}, {"location": "writing/writing/#references", "text": "Awesome: Nikita's writing notes Gwern's writing checklist Good: Long Naomi pen post with some key ideas", "title": "References"}, {"location": "writing/writing/#doing", "text": "https://github.com/mnielsen/notes-on-writing/blob/master/notes_on_writing.md#readme", "title": "Doing"}, {"location": "writing/writing/#todo", "text": "https://www.scottadamssays.com/2015/08/22/the-day-you-became-a-better-writer-2nd-look/ https://blog.stephsmith.io/learning-to-write-with-confidence/ https://styleguide.mailchimp.com/tldr/ https://content-guide.18f.gov/inclusive-language/ https://performancejs.com/post/31b361c/13-Tips-for-Writing-a-Technical-Book https://github.com/RacheltheEditor/ProductionGuide#readme https://mkaz.blog/misc/notes-on-technical-writing/ https://www.swyx.io/writing/cfp-advice/ https://sivers.org/d22 https://homes.cs.washington.edu/~mernst/advice/write-technical-paper.html Investigate on readability tests: Definition Introduction on Readability List of readability tests and formulas An example of a formula", "title": "Todo"}, {"location": "writing/writing/#vim-plugins", "text": "Vim-lexical Vim-textobj-quote Vim-textobj-sentence Vim-ditto Vim-exchange", "title": "Vim plugins"}, {"location": "writing/writing/#books", "text": "https://www.amazon.de/dp/0060891548/ref=as_li_tl?ie=UTF8&linkCode=gs2&linkId=39f2ab8ab47769b2a106e9667149df30&creativeASIN=0060891548&tag=gregdoesit03-21&creative=9325&camp=1789 https://www.amazon.de/dp/0143127799/ref=as_li_tl?ie=UTF8&linkCode=gs2&linkId=ff9322c17ca288b1d9d6b5fb8d6df619&creativeASIN=0143127799&tag=gregdoesit03-21&creative=9325&camp=1789", "title": "Books"}]}